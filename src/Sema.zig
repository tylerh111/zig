//! Semantic analysis of ZIR instructions.
//! Shared to every Block. Stored on the stack.
//! State used for compiling a ZIR into AIR.
//! Transforms untyped ZIR instructions into semantically-analyzed AIR instructions.
//! Does type checking, comptime control flow, and safety-check generation.
//! This is the the heart of the Zig compiler.

mod: *Module,
/// Alias to `mod.gpa`.
gpa: Allocator,
/// Points to the temporary arena allocator of the Sema.
/// This arena will be cleared when the sema is destroyed.
arena: Allocator,
code: Zir,
air_instructions: std.MultiArrayList(Air.Inst) = .{},
air_extra: std.ArrayListUnmanaged(u32) = .{},
/// Maps ZIR to AIR.
inst_map: InstMap = .{},
/// When analyzing an inline function call, owner_decl is the Decl of the caller
/// and `src_decl` of `Block` is the `Decl` of the callee.
/// This `Decl` owns the arena memory of this `Sema`.
owner_decl: *Decl,
owner_decl_index: InternPool.DeclIndex,
/// For an inline or comptime function call, this will be the root parent function
/// which contains the callsite. Corresponds to `owner_decl`.
/// This could be `none`, a `func_decl`, or a `func_instance`.
owner_func_index: InternPool.Index,
/// The function this ZIR code is the body of, according to the source code.
/// This starts out the same as `owner_func_index` and then diverges in the case of
/// an inline or comptime function call.
/// This could be `none`, a `func_decl`, or a `func_instance`.
func_index: InternPool.Index,
/// Whether the type of func_index has a calling convention of `.Naked`.
func_is_naked: bool,
/// Used to restore the error return trace when returning a non-error from a function.
error_return_trace_index_on_fn_entry: Air.Inst.Ref = .none,
comptime_err_ret_trace: *std.ArrayList(Module.SrcLoc),
/// When semantic analysis needs to know the return type of the function whose body
/// is being analyzed, this `Type` should be used instead of going through `func`.
/// This will correctly handle the case of a comptime/inline function call of a
/// generic function which uses a type expression for the return type.
/// The type will be `void` in the case that `func` is `null`.
fn_ret_ty: Type,
/// In case of the return type being an error union with an inferred error
/// set, this is the inferred error set. `null` otherwise. Allocated with
/// `Sema.arena`.
fn_ret_ty_ies: ?*InferredErrorSet,
branch_quota: u32 = default_branch_quota,
branch_count: u32 = 0,
/// Populated when returning `error.ComptimeBreak`. Used to communicate the
/// break instruction up the stack to find the corresponding Block.
comptime_break_inst: Zir.Inst.Index = undefined,
decl_val_table: std.AutoHashMapUnmanaged(InternPool.DeclIndex, Air.Inst.Ref) = .{},
/// When doing a generic function instantiation, this array collects a value
/// for each parameter of the generic owner. `none` for non-comptime parameters.
/// This is a separate array from `block.params` so that it can be passed
/// directly to `comptime_args` when calling `InternPool.get_func_instance`.
/// This memory is allocated by a parent `Sema` in the temporary arena, and is
/// used only to add a `func_instance` into the `InternPool`.
comptime_args: []InternPool.Index = &.{},
/// Used to communicate from a generic function instantiation to the logic that
/// creates a generic function instantiation value in `func_common`.
generic_owner: InternPool.Index = .none,
/// When `generic_owner` is not none, this contains the generic function
/// instantiation callsite so that compile errors on the parameter types of the
/// instantiation can point back to the instantiation site in addition to the
/// declaration site.
generic_call_src: LazySrcLoc = .unneeded,
/// Corresponds to `generic_call_src`.
generic_call_decl: InternPool.OptionalDeclIndex = .none,
/// The key is types that must be fully resolved prior to machine code
/// generation pass. Types are added to this set when resolving them
/// immediately could cause a dependency loop, but they do need to be resolved
/// before machine code generation passes process the AIR.
/// It would work fine if this were an array list instead of an array hash map.
/// I chose array hash map with the intention to save time by omitting
/// duplicates.
types_to_resolve: std.AutoArrayHashMapUnmanaged(InternPool.Index, void) = .{},
/// These are lazily created runtime blocks from block_inline instructions.
/// They are created when an break_inline passes through a runtime condition, because
/// Sema must convert comptime control flow to runtime control flow, which means
/// breaking from a block.
post_hoc_blocks: std.AutoHashMapUnmanaged(Air.Inst.Index, *LabeledBlock) = .{},
/// Populated with the last compile error created.
err: ?*Module.ErrorMsg = null,
/// Set to true when analyzing a func type instruction so that nested generic
/// function types will emit generic poison instead of a partial type.
no_partial_func_ty: bool = false,

/// The temporary arena is used for the memory of the `InferredAlloc` values
/// here so the values can be dropped without any cleanup.
unresolved_inferred_allocs: std.AutoArrayHashMapUnmanaged(Air.Inst.Index, InferredAlloc) = .{},

/// This is populated when `@setAlignStack` occurs so that if there is a duplicate
/// one encountered, the conflicting source location can be shown.
prev_stack_alignment_src: ?LazySrcLoc = null,

/// While analyzing a type which has a special InternPool index, this is set to the index at which
/// the struct/enum/union type created should be placed. Otherwise, it is `.none`.
builtin_type_target_index: InternPool.Index = .none,

/// Links every pointer derived from a base `alloc` back to that `alloc`. Used
/// to detect comptime-known `const`s.
/// TODO: ZIR liveness analysis would allow us to remove elements from this map.
base_allocs: std.AutoHashMapUnmanaged(Air.Inst.Index, Air.Inst.Index) = .{},

/// Runtime `alloc`s are placed in this map to track all comptime-known writes
/// before the corresponding `make_ptr_const` instruction.
/// If any store to the alloc depends on a runtime condition or stores a runtime
/// value, the corresponding element in this map is erased, to indicate that the
/// alloc is not comptime-known.
/// If the alloc remains in this map when `make_ptr_const` is reached, its value
/// is comptime-known, and all stores to the pointer must be applied at comptime
/// to determine the comptime value.
/// Backed by gpa.
maybe_comptime_allocs: std.AutoHashMapUnmanaged(Air.Inst.Index, MaybeComptimeAlloc) = .{},

/// Comptime-mutable allocs, and any comptime allocs which reference it, are
/// stored as elements of this array.
/// Pointers to such memory are represented via an index into this array.
/// Backed by gpa.
comptime_allocs: std.ArrayListUnmanaged(ComptimeAlloc) = .{},

const MaybeComptimeAlloc = struct {
    /// The runtime index of the `alloc` instruction.
    runtime_index: Value.RuntimeIndex,
    /// Backed by sema.arena. Tracks all comptime-known stores to this `alloc`. Due to
    /// RLS, a single comptime-known allocation may have arbitrarily many stores.
    /// This list also contains `set_union_tag`, `optional_payload_ptr_set`, and
    /// `errunion_payload_ptr_set` instructions.
    /// If the instruction is one of these three tags, `src` may be `.unneeded`.
    stores: std.MultiArrayList(struct {
        inst: Air.Inst.Index,
        src_decl: InternPool.DeclIndex,
        src: LazySrcLoc,
    }) = .{},
};

const ComptimeAlloc = struct {
    val: MutableValue,
    is_const: bool,
    /// `.none` indicates that the alignment is the natural alignment of `val`.
    alignment: Alignment,
    /// This is the `runtime_index` at the point of this allocation. If an store
    /// to this alloc ever occurs with a runtime index greater than this one, it
    /// is behind a runtime condition, so a compile error will be emitted.
    runtime_index: Value.RuntimeIndex,
};

fn new_comptime_alloc(sema: *Sema, block: *Block, ty: Type, alignment: Alignment) !ComptimeAllocIndex {
    const idx = sema.comptime_allocs.items.len;
    try sema.comptime_allocs.append(sema.gpa, .{
        .val = .{ .interned = try sema.mod.intern(.{ .undef = ty.to_intern() }) },
        .is_const = false,
        .alignment = alignment,
        .runtime_index = block.runtime_index,
    });
    return @enumFromInt(idx);
}

pub fn get_comptime_alloc(sema: *Sema, idx: ComptimeAllocIndex) *ComptimeAlloc {
    return &sema.comptime_allocs.items[@int_from_enum(idx)];
}

const std = @import("std");
const math = std.math;
const mem = std.mem;
const Allocator = mem.Allocator;
const assert = std.debug.assert;
const log = std.log.scoped(.sema);

const Sema = @This();
const Value = @import("Value.zig");
const MutableValue = @import("mutable_value.zig").MutableValue;
const Type = @import("type.zig").Type;
const Air = @import("Air.zig");
const Zir = std.zig.Zir;
const Zcu = @import("Module.zig");
const Module = Zcu;
const trace = @import("tracy.zig").trace;
const Namespace = Module.Namespace;
const CompileError = Module.CompileError;
const SemaError = Module.SemaError;
const Decl = Module.Decl;
const LazySrcLoc = std.zig.LazySrcLoc;
const RangeSet = @import("RangeSet.zig");
const target_util = @import("target.zig");
const Package = @import("Package.zig");
const crash_report = @import("crash_report.zig");
const build_options = @import("build_options");
const Compilation = @import("Compilation.zig");
const InternPool = @import("InternPool.zig");
const Alignment = InternPool.Alignment;
const ComptimeAllocIndex = InternPool.ComptimeAllocIndex;

pub const default_branch_quota = 1000;
pub const default_reference_trace_len = 2;

pub const InferredErrorSet = struct {
    /// The function body from which this error set originates.
    /// This is `none` in the case of a comptime/inline function call, corresponding to
    /// `InternPool.Index.adhoc_inferred_error_set_type`.
    /// The function's resolved error set is not set until analysis of the
    /// function body completes.
    func: InternPool.Index,
    /// All currently known errors that this error set contains. This includes
    /// direct additions via `return error.Foo;`, and possibly also errors that
    /// are returned from any dependent functions.
    errors: NameMap = .{},
    /// Other inferred error sets which this inferred error set should include.
    inferred_error_sets: std.AutoArrayHashMapUnmanaged(InternPool.Index, void) = .{},
    /// The regular error set created by resolving this inferred error set.
    resolved: InternPool.Index = .none,

    pub const NameMap = std.AutoArrayHashMapUnmanaged(InternPool.NullTerminatedString, void);

    pub fn add_error_set(
        self: *InferredErrorSet,
        err_set_ty: Type,
        ip: *InternPool,
        arena: Allocator,
    ) !void {
        switch (err_set_ty.to_intern()) {
            .anyerror_type => self.resolved = .anyerror_type,
            .adhoc_inferred_error_set_type => {}, // Adding an inferred error set to itself.

            else => switch (ip.index_to_key(err_set_ty.to_intern())) {
                .error_set_type => |error_set_type| {
                    for (error_set_type.names.get(ip)) |name| {
                        try self.errors.put(arena, name, {});
                    }
                },
                .inferred_error_set_type => {
                    try self.inferred_error_sets.put(arena, err_set_ty.to_intern(), {});
                },
                else => unreachable,
            },
        }
    }
};

/// Stores the mapping from `Zir.Inst.Index -> Air.Inst.Ref`, which is used by sema to resolve
/// instructions during analysis.
/// Instead of a hash table approach, InstMap is simply a slice that is indexed into using the
/// zir instruction index and a start offset. An index is not pressent in the map if the value
/// at the index is `Air.Inst.Ref.none`.
/// `ensure_space_for_instructions` can be called to force InstMap to have a mapped range that
/// includes all instructions in a slice. After calling this function, `put_assume_capacity*` can
/// be called safely for any of the instructions passed in.
pub const InstMap = struct {
    items: []Air.Inst.Ref = &[_]Air.Inst.Ref{},
    start: Zir.Inst.Index = @enumFromInt(0),

    pub fn deinit(map: InstMap, allocator: mem.Allocator) void {
        allocator.free(map.items);
    }

    pub fn get(map: InstMap, key: Zir.Inst.Index) ?Air.Inst.Ref {
        if (!map.contains(key)) return null;
        return map.items[@int_from_enum(key) - @int_from_enum(map.start)];
    }

    pub fn put_assume_capacity(
        map: *InstMap,
        key: Zir.Inst.Index,
        ref: Air.Inst.Ref,
    ) void {
        map.items[@int_from_enum(key) - @int_from_enum(map.start)] = ref;
    }

    pub fn put_assume_capacity_no_clobber(
        map: *InstMap,
        key: Zir.Inst.Index,
        ref: Air.Inst.Ref,
    ) void {
        assert(!map.contains(key));
        map.put_assume_capacity(key, ref);
    }

    pub const GetOrPutResult = struct {
        value_ptr: *Air.Inst.Ref,
        found_existing: bool,
    };

    pub fn get_or_put_assume_capacity(
        map: *InstMap,
        key: Zir.Inst.Index,
    ) GetOrPutResult {
        const index = @int_from_enum(key) - @int_from_enum(map.start);
        return GetOrPutResult{
            .value_ptr = &map.items[index],
            .found_existing = map.items[index] != .none,
        };
    }

    pub fn remove(map: InstMap, key: Zir.Inst.Index) bool {
        if (!map.contains(key)) return false;
        map.items[@int_from_enum(key) - @int_from_enum(map.start)] = .none;
        return true;
    }

    pub fn contains(map: InstMap, key: Zir.Inst.Index) bool {
        return map.items[@int_from_enum(key) - @int_from_enum(map.start)] != .none;
    }

    pub fn ensure_space_for_instructions(
        map: *InstMap,
        allocator: mem.Allocator,
        insts: []const Zir.Inst.Index,
    ) !void {
        const start, const end = mem.min_max(u32, @ptr_cast(insts));
        const map_start = @int_from_enum(map.start);
        if (map_start <= start and end < map.items.len + map_start)
            return;

        const old_start = if (map.items.len == 0) start else map_start;
        var better_capacity = map.items.len;
        var better_start = old_start;
        while (true) {
            const extra_capacity = better_capacity / 2 + 16;
            better_capacity += extra_capacity;
            better_start -|= @int_cast(extra_capacity / 2);
            if (better_start <= start and end < better_capacity + better_start)
                break;
        }

        const start_diff = old_start - better_start;
        const new_items = try allocator.alloc(Air.Inst.Ref, better_capacity);
        @memset(new_items[0..start_diff], .none);
        @memcpy(new_items[start_diff..][0..map.items.len], map.items);
        @memset(new_items[start_diff + map.items.len ..], .none);

        allocator.free(map.items);
        map.items = new_items;
        map.start = @enumFromInt(better_start);
    }
};

/// This is the context needed to semantically analyze ZIR instructions and
/// produce AIR instructions.
/// This is a temporary structure stored on the stack; references to it are valid only
/// during semantic analysis of the block.
pub const Block = struct {
    parent: ?*Block,
    /// Shared among all child blocks.
    sema: *Sema,
    /// The namespace to use for lookups from this source block
    /// When analyzing fields, this is different from src_decl.src_namespace.
    namespace: InternPool.NamespaceIndex,
    /// The AIR instructions generated for this block.
    instructions: std.ArrayListUnmanaged(Air.Inst.Index),
    // `param` instructions are collected here to be used by the `func` instruction.
    /// When doing a generic function instantiation, this array collects a type
    /// for each *runtime-known* parameter. This array corresponds to the instance
    /// function type, while `Sema.comptime_args` corresponds to the generic owner
    /// function type.
    /// This memory is allocated by a parent `Sema` in the temporary arena, and is
    /// used to add a `func_instance` into the `InternPool`.
    params: std.MultiArrayList(Param) = .{},

    label: ?*Label = null,
    inlining: ?*Inlining,
    /// If runtime_index is not 0 then one of these is guaranteed to be non null.
    runtime_cond: ?Module.SrcLoc = null,
    runtime_loop: ?Module.SrcLoc = null,
    /// This Decl is the Decl according to the Zig source code corresponding to this Block.
    /// This can vary during inline or comptime function calls. See `Sema.owner_decl`
    /// for the one that will be the same for all Block instances.
    src_decl: InternPool.DeclIndex,
    /// Non zero if a non-inline loop or a runtime conditional have been encountered.
    /// Stores to comptime variables are only allowed when var.runtime_index <= runtime_index.
    runtime_index: Value.RuntimeIndex = .zero,
    inline_block: Zir.Inst.OptionalIndex = .none,

    comptime_reason: ?*const ComptimeReason = null,
    // TODO is_comptime and comptime_reason should probably be merged together.
    is_comptime: bool,
    is_typeof: bool = false,

    /// Keep track of the active error return trace index around blocks so that we can correctly
    /// pop the error trace upon block exit.
    error_return_trace_index: Air.Inst.Ref = .none,

    /// when null, it is determined by build mode, changed by @setRuntimeSafety
    want_safety: ?bool = null,

    /// What mode to generate float operations in, set by @setFloatMode
    float_mode: std.builtin.FloatMode = .strict,

    c_import_buf: ?*std.ArrayList(u8) = null,

    /// If not `null`, this boolean is set when a `dbg_var_ptr` or `dbg_var_val`
    /// instruction is emitted. It signals that the innermost lexically
    /// enclosing `block`/`block_inline` should be translated into a real AIR
    /// `block` in order for codegen to match lexical scoping for debug vars.
    need_debug_scope: ?*bool = null,

    const ComptimeReason = union(enum) {
        c_import: struct {
            block: *Block,
            src: LazySrcLoc,
        },
        comptime_ret_ty: struct {
            block: *Block,
            func: Air.Inst.Ref,
            func_src: LazySrcLoc,
            return_ty: Type,
        },

        fn explain(cr: ComptimeReason, sema: *Sema, msg: ?*Module.ErrorMsg) !void {
            const parent = msg orelse return;
            const mod = sema.mod;
            const prefix = "expression is evaluated at comptime because ";
            switch (cr) {
                .c_import => |ci| {
                    try sema.err_note(ci.block, ci.src, parent, prefix ++ "it is inside a @c_import", .{});
                },
                .comptime_ret_ty => |rt| {
                    const src_loc = if (try sema.func_decl_src(rt.func)) |fn_decl| blk: {
                        var src_loc = fn_decl.src_loc(mod);
                        src_loc.lazy = .{ .node_offset_fn_type_ret_ty = 0 };
                        break :blk src_loc;
                    } else blk: {
                        const src_decl = mod.decl_ptr(rt.block.src_decl);
                        break :blk src_decl.to_src_loc(rt.func_src, mod);
                    };
                    if (rt.return_ty.is_generic_poison()) {
                        return mod.err_note_non_lazy(src_loc, parent, prefix ++ "the generic function was instantiated with a comptime-only return type", .{});
                    }
                    try mod.err_note_non_lazy(
                        src_loc,
                        parent,
                        prefix ++ "the function returns a comptime-only type '{}'",
                        .{rt.return_ty.fmt(mod)},
                    );
                    try sema.explain_why_type_is_comptime(parent, src_loc, rt.return_ty);
                },
            }
        }
    };

    const Param = struct {
        /// `none` means `anytype`.
        ty: InternPool.Index,
        is_comptime: bool,
        name: Zir.NullTerminatedString,
    };

    /// This `Block` maps a block ZIR instruction to the corresponding
    /// AIR instruction for break instruction analysis.
    pub const Label = struct {
        zir_block: Zir.Inst.Index,
        merges: Merges,
    };

    /// This `Block` indicates that an inline function call is happening
    /// and return instructions should be analyzed as a break instruction
    /// to this AIR block instruction.
    /// It is shared among all the blocks in an inline or comptime called
    /// function.
    pub const Inlining = struct {
        call_block: *Block,
        call_src: LazySrcLoc,
        has_comptime_args: bool,
        func: InternPool.Index,
        comptime_result: Air.Inst.Ref,
        merges: Merges,
    };

    pub const Merges = struct {
        block_inst: Air.Inst.Index,
        /// Separate array list from break_inst_list so that it can be passed directly
        /// to resolve_peer_types.
        results: std.ArrayListUnmanaged(Air.Inst.Ref),
        /// Keeps track of the break instructions so that the operand can be replaced
        /// if we need to add type coercion at the end of block analysis.
        /// Same indexes, capacity, length as `results`.
        br_list: std.ArrayListUnmanaged(Air.Inst.Index),
        /// Keeps the source location of the rhs operand of the break instruction,
        /// to enable more precise compile errors.
        /// Same indexes, capacity, length as `results`.
        src_locs: std.ArrayListUnmanaged(?LazySrcLoc),

        pub fn deinit(merges: *@This(), allocator: mem.Allocator) void {
            merges.results.deinit(allocator);
            merges.br_list.deinit(allocator);
            merges.src_locs.deinit(allocator);
        }
    };

    /// For debugging purposes.
    pub fn dump(block: *Block, mod: Module) void {
        Zir.dumpBlock(mod, block);
    }

    pub fn make_sub_block(parent: *Block) Block {
        return .{
            .parent = parent,
            .sema = parent.sema,
            .src_decl = parent.src_decl,
            .namespace = parent.namespace,
            .instructions = .{},
            .label = null,
            .inlining = parent.inlining,
            .is_comptime = parent.is_comptime,
            .comptime_reason = parent.comptime_reason,
            .is_typeof = parent.is_typeof,
            .runtime_cond = parent.runtime_cond,
            .runtime_loop = parent.runtime_loop,
            .runtime_index = parent.runtime_index,
            .want_safety = parent.want_safety,
            .float_mode = parent.float_mode,
            .c_import_buf = parent.c_import_buf,
            .error_return_trace_index = parent.error_return_trace_index,
            .need_debug_scope = parent.need_debug_scope,
        };
    }

    pub fn want_safety(block: *const Block) bool {
        return block.want_safety orelse switch (block.sema.mod.optimize_mode()) {
            .Debug => true,
            .ReleaseSafe => true,
            .ReleaseFast => false,
            .ReleaseSmall => false,
        };
    }

    pub fn get_file_scope(block: *Block, mod: *Module) *Module.File {
        return mod.namespace_ptr(block.namespace).file_scope;
    }

    fn add_ty(
        block: *Block,
        tag: Air.Inst.Tag,
        ty: Type,
    ) error{OutOfMemory}!Air.Inst.Ref {
        return block.add_inst(.{
            .tag = tag,
            .data = .{ .ty = ty },
        });
    }

    fn add_ty_op(
        block: *Block,
        tag: Air.Inst.Tag,
        ty: Type,
        operand: Air.Inst.Ref,
    ) error{OutOfMemory}!Air.Inst.Ref {
        return block.add_inst(.{
            .tag = tag,
            .data = .{ .ty_op = .{
                .ty = Air.interned_to_ref(ty.to_intern()),
                .operand = operand,
            } },
        });
    }

    fn add_bit_cast(block: *Block, ty: Type, operand: Air.Inst.Ref) Allocator.Error!Air.Inst.Ref {
        return block.add_inst(.{
            .tag = .bitcast,
            .data = .{ .ty_op = .{
                .ty = Air.interned_to_ref(ty.to_intern()),
                .operand = operand,
            } },
        });
    }

    fn add_no_op(block: *Block, tag: Air.Inst.Tag) error{OutOfMemory}!Air.Inst.Ref {
        return block.add_inst(.{
            .tag = tag,
            .data = .{ .no_op = {} },
        });
    }

    fn add_un_op(
        block: *Block,
        tag: Air.Inst.Tag,
        operand: Air.Inst.Ref,
    ) error{OutOfMemory}!Air.Inst.Ref {
        return block.add_inst(.{
            .tag = tag,
            .data = .{ .un_op = operand },
        });
    }

    fn add_br(
        block: *Block,
        target_block: Air.Inst.Index,
        operand: Air.Inst.Ref,
    ) error{OutOfMemory}!Air.Inst.Ref {
        return block.add_inst(.{
            .tag = .br,
            .data = .{ .br = .{
                .block_inst = target_block,
                .operand = operand,
            } },
        });
    }

    fn add_bin_op(
        block: *Block,
        tag: Air.Inst.Tag,
        lhs: Air.Inst.Ref,
        rhs: Air.Inst.Ref,
    ) error{OutOfMemory}!Air.Inst.Ref {
        return block.add_inst(.{
            .tag = tag,
            .data = .{ .bin_op = .{
                .lhs = lhs,
                .rhs = rhs,
            } },
        });
    }

    fn add_struct_field_ptr(
        block: *Block,
        struct_ptr: Air.Inst.Ref,
        field_index: u32,
        ptr_field_ty: Type,
    ) !Air.Inst.Ref {
        const ty = Air.interned_to_ref(ptr_field_ty.to_intern());
        const tag: Air.Inst.Tag = switch (field_index) {
            0 => .struct_field_ptr_index_0,
            1 => .struct_field_ptr_index_1,
            2 => .struct_field_ptr_index_2,
            3 => .struct_field_ptr_index_3,
            else => {
                return block.add_inst(.{
                    .tag = .struct_field_ptr,
                    .data = .{ .ty_pl = .{
                        .ty = ty,
                        .payload = try block.sema.add_extra(Air.StructField{
                            .struct_operand = struct_ptr,
                            .field_index = field_index,
                        }),
                    } },
                });
            },
        };
        return block.add_inst(.{
            .tag = tag,
            .data = .{ .ty_op = .{
                .ty = ty,
                .operand = struct_ptr,
            } },
        });
    }

    fn add_struct_field_val(
        block: *Block,
        struct_val: Air.Inst.Ref,
        field_index: u32,
        field_ty: Type,
    ) !Air.Inst.Ref {
        return block.add_inst(.{
            .tag = .struct_field_val,
            .data = .{ .ty_pl = .{
                .ty = Air.interned_to_ref(field_ty.to_intern()),
                .payload = try block.sema.add_extra(Air.StructField{
                    .struct_operand = struct_val,
                    .field_index = field_index,
                }),
            } },
        });
    }

    fn add_slice_elem_ptr(
        block: *Block,
        slice: Air.Inst.Ref,
        elem_index: Air.Inst.Ref,
        elem_ptr_ty: Type,
    ) !Air.Inst.Ref {
        return block.add_inst(.{
            .tag = .slice_elem_ptr,
            .data = .{ .ty_pl = .{
                .ty = Air.interned_to_ref(elem_ptr_ty.to_intern()),
                .payload = try block.sema.add_extra(Air.Bin{
                    .lhs = slice,
                    .rhs = elem_index,
                }),
            } },
        });
    }

    fn add_ptr_elem_ptr(
        block: *Block,
        array_ptr: Air.Inst.Ref,
        elem_index: Air.Inst.Ref,
        elem_ptr_ty: Type,
    ) !Air.Inst.Ref {
        const ty_ref = Air.interned_to_ref(elem_ptr_ty.to_intern());
        return block.add_ptr_elem_ptr_type_ref(array_ptr, elem_index, ty_ref);
    }

    fn add_ptr_elem_ptr_type_ref(
        block: *Block,
        array_ptr: Air.Inst.Ref,
        elem_index: Air.Inst.Ref,
        elem_ptr_ty: Air.Inst.Ref,
    ) !Air.Inst.Ref {
        return block.add_inst(.{
            .tag = .ptr_elem_ptr,
            .data = .{ .ty_pl = .{
                .ty = elem_ptr_ty,
                .payload = try block.sema.add_extra(Air.Bin{
                    .lhs = array_ptr,
                    .rhs = elem_index,
                }),
            } },
        });
    }

    fn add_cmp_vector(block: *Block, lhs: Air.Inst.Ref, rhs: Air.Inst.Ref, cmp_op: std.math.CompareOperator) !Air.Inst.Ref {
        const sema = block.sema;
        const mod = sema.mod;
        return block.add_inst(.{
            .tag = if (block.float_mode == .optimized) .cmp_vector_optimized else .cmp_vector,
            .data = .{ .ty_pl = .{
                .ty = Air.interned_to_ref((try mod.vector_type(.{
                    .len = sema.type_of(lhs).vector_len(mod),
                    .child = .bool_type,
                })).to_intern()),
                .payload = try sema.add_extra(Air.VectorCmp{
                    .lhs = lhs,
                    .rhs = rhs,
                    .op = Air.VectorCmp.encode_op(cmp_op),
                }),
            } },
        });
    }

    fn add_aggregate_init(
        block: *Block,
        aggregate_ty: Type,
        elements: []const Air.Inst.Ref,
    ) !Air.Inst.Ref {
        const sema = block.sema;
        const ty_ref = Air.interned_to_ref(aggregate_ty.to_intern());
        try sema.air_extra.ensure_unused_capacity(sema.gpa, elements.len);
        const extra_index: u32 = @int_cast(sema.air_extra.items.len);
        sema.append_refs_assume_capacity(elements);

        return block.add_inst(.{
            .tag = .aggregate_init,
            .data = .{ .ty_pl = .{
                .ty = ty_ref,
                .payload = extra_index,
            } },
        });
    }

    fn add_union_init(
        block: *Block,
        union_ty: Type,
        field_index: u32,
        init: Air.Inst.Ref,
    ) !Air.Inst.Ref {
        return block.add_inst(.{
            .tag = .union_init,
            .data = .{ .ty_pl = .{
                .ty = Air.interned_to_ref(union_ty.to_intern()),
                .payload = try block.sema.add_extra(Air.UnionInit{
                    .field_index = field_index,
                    .init = init,
                }),
            } },
        });
    }

    pub fn add_inst(block: *Block, inst: Air.Inst) error{OutOfMemory}!Air.Inst.Ref {
        return (try block.add_inst_as_index(inst)).to_ref();
    }

    pub fn add_inst_as_index(block: *Block, inst: Air.Inst) error{OutOfMemory}!Air.Inst.Index {
        const sema = block.sema;
        const gpa = sema.gpa;

        try sema.air_instructions.ensure_unused_capacity(gpa, 1);
        try block.instructions.ensure_unused_capacity(gpa, 1);

        const result_index: Air.Inst.Index = @enumFromInt(sema.air_instructions.len);
        sema.air_instructions.append_assume_capacity(inst);
        block.instructions.append_assume_capacity(result_index);
        return result_index;
    }

    /// Insert an instruction into the block at `index`. Moves all following
    /// instructions forward in the block to make room. Operation is O(N).
    pub fn insert_inst(block: *Block, index: Air.Inst.Index, inst: Air.Inst) error{OutOfMemory}!Air.Inst.Ref {
        return (try block.insert_inst_as_index(index, inst)).to_ref();
    }

    pub fn insert_inst_as_index(block: *Block, index: Air.Inst.Index, inst: Air.Inst) error{OutOfMemory}!Air.Inst.Index {
        const sema = block.sema;
        const gpa = sema.gpa;

        try sema.air_instructions.ensure_unused_capacity(gpa, 1);

        const result_index: Air.Inst.Index = @enumFromInt(sema.air_instructions.len);
        sema.air_instructions.append_assume_capacity(inst);

        try block.instructions.insert(gpa, @int_from_enum(index), result_index);
        return result_index;
    }

    fn add_unreachable(block: *Block, src: LazySrcLoc, safety_check: bool) !void {
        if (safety_check and block.want_safety()) {
            try block.sema.safety_panic(block, src, .unreach);
        } else {
            _ = try block.add_no_op(.unreach);
        }
    }

    pub fn owner_module(block: Block) *Package.Module {
        const zcu = block.sema.mod;
        return zcu.namespace_ptr(block.namespace).file_scope.mod;
    }
};

const LabeledBlock = struct {
    block: Block,
    label: Block.Label,

    fn destroy(lb: *LabeledBlock, gpa: Allocator) void {
        lb.block.instructions.deinit(gpa);
        lb.label.merges.deinit(gpa);
        gpa.destroy(lb);
    }
};

/// The value stored in the inferred allocation. This will go into
/// peer type resolution. This is stored in a separate list so that
/// the items are contiguous in memory and thus can be passed to
/// `Module.resolve_peer_types`.
const InferredAlloc = struct {
    /// The placeholder `store` instructions used before the result pointer type
    /// is known. These should be rewritten to perform any required coercions
    /// when the type is resolved.
    /// Allocated from `sema.arena`.
    prongs: std.ArrayListUnmanaged(Air.Inst.Index) = .{},
};

const NeededComptimeReason = struct {
    needed_comptime_reason: []const u8,
    block_comptime_reason: ?*const Block.ComptimeReason = null,
};

pub fn deinit(sema: *Sema) void {
    const gpa = sema.gpa;
    sema.air_instructions.deinit(gpa);
    sema.air_extra.deinit(gpa);
    sema.inst_map.deinit(gpa);
    sema.decl_val_table.deinit(gpa);
    sema.types_to_resolve.deinit(gpa);
    {
        var it = sema.post_hoc_blocks.iterator();
        while (it.next()) |entry| {
            const labeled_block = entry.value_ptr.*;
            labeled_block.destroy(gpa);
        }
        sema.post_hoc_blocks.deinit(gpa);
    }
    sema.unresolved_inferred_allocs.deinit(gpa);
    sema.base_allocs.deinit(gpa);
    sema.maybe_comptime_allocs.deinit(gpa);
    sema.comptime_allocs.deinit(gpa);
    sema.* = undefined;
}

/// Performs semantic analysis of a ZIR body which is behind a runtime condition. If comptime
/// control flow happens here, Sema will convert it to runtime control flow by introducing post-hoc
/// blocks where necessary.
fn analyze_body_runtime_break(sema: *Sema, block: *Block, body: []const Zir.Inst.Index) !void {
    sema.analyze_body_inner(block, body) catch |err| switch (err) {
        error.ComptimeBreak => {
            const zir_datas = sema.code.instructions.items(.data);
            const break_data = zir_datas[@int_from_enum(sema.comptime_break_inst)].@"break";
            const extra = sema.code.extra_data(Zir.Inst.Break, break_data.payload_index).data;
            try sema.add_runtime_break(block, extra.block_inst, break_data.operand);
        },
        else => |e| return e,
    };
}

/// Semantically analyze a ZIR function body. It is guranteed by AstGen that such a body cannot
/// trigger comptime control flow to move above the function body.
pub fn analyze_fn_body(
    sema: *Sema,
    block: *Block,
    body: []const Zir.Inst.Index,
) !void {
    sema.analyze_body_inner(block, body) catch |err| switch (err) {
        error.ComptimeBreak => unreachable, // unexpected comptime control flow
        else => |e| return e,
    };
}

/// Given a ZIR body which can be exited via a `break_inline` instruction, or a non-inline body which
/// we are evaluating at comptime, semantically analyze the body and return the result from it.
/// Returns `null` if control flow did not break from this block, but instead terminated with some
/// other runtime noreturn instruction. Compile-time breaks to blocks further up the stack still
/// return `error.ComptimeBreak`. If `block.is_comptime`, this function will never return `null`.
fn analyze_inline_body(
    sema: *Sema,
    block: *Block,
    body: []const Zir.Inst.Index,
    /// The index which a break instruction can target to break from this body.
    break_target: Zir.Inst.Index,
) CompileError!?Air.Inst.Ref {
    if (sema.analyze_body_inner(block, body)) |_| {
        return null;
    } else |err| switch (err) {
        error.ComptimeBreak => {},
        else => |e| return e,
    }
    const break_inst = sema.comptime_break_inst;
    const break_data = sema.code.instructions.items(.data)[@int_from_enum(break_inst)].@"break";
    const extra = sema.code.extra_data(Zir.Inst.Break, break_data.payload_index).data;
    if (extra.block_inst != break_target) {
        // This control flow goes further up the stack.
        return error.ComptimeBreak;
    }
    return try sema.resolve_inst(break_data.operand);
}

/// Like `analyze_inline_body`, but if the body does not break with a value, returns
/// `.unreachable_value` instead of `null`. Notably, use this to evaluate an arbitrary
/// body at comptime to a single result value.
pub fn resolve_inline_body(
    sema: *Sema,
    block: *Block,
    body: []const Zir.Inst.Index,
    /// The index which a break instruction can target to break from this body.
    break_target: Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    return (try sema.analyze_inline_body(block, body, break_target)) orelse .unreachable_value;
}

/// This function is the main loop of `Sema`. It analyzes a single body of ZIR instructions.
///
/// If this function returns normally, the merges of `block` were populated with all possible
/// (runtime) results of this block. Peer type resolution should be performed on the result,
/// and relevant runtime instructions written to perform necessary coercions and breaks. See
/// `resolve_analyzed_block`. This form of return is impossible if `block.is_comptime == true`.
///
/// Alternatively, this function may return `error.ComptimeBreak`. This indicates that comptime
/// control flow is happening, and we are breaking at comptime from a block indicated by the
/// break instruction in `sema.comptime_break_inst`. This occurs for any `break_inline`, or for a
/// standard `break` at comptime. This error is pushed up the stack until the target block is
/// reached, at which point the break operand will be fetched.
///
/// It is rare to call this function directly. Usually, you want one of the following wrappers:
/// * If the body is exited via a `break_inline`, or is being evaluated at comptime,
///   use `Sema.analyze_inline_body` or `Sema.resolve_inline_body`.
/// * If the body is behind a fresh runtime condition, use `Sema.analyze_body_runtime_break`.
/// * If the body is an entire function body, use `Sema.analyze_fn_body`.
/// * If the body is to be generated into an AIR `block`, use `Sema.resolve_block_body`.
/// * Otherwise, direct usage of `Sema.analyze_body_inner` may be necessary.
fn analyze_body_inner(
    sema: *Sema,
    block: *Block,
    body: []const Zir.Inst.Index,
) CompileError!void {
    // No tracy calls here, to avoid interfering with the tail call mechanism.

    try sema.inst_map.ensure_space_for_instructions(sema.gpa, body);

    const mod = sema.mod;
    const map = &sema.inst_map;
    const tags = sema.code.instructions.items(.tag);
    const datas = sema.code.instructions.items(.data);

    var crash_info = crash_report.prep_analyze_body(sema, block, body);
    crash_info.push();
    defer crash_info.pop();

    // We use a while (true) loop here to avoid a redundant way of breaking out of
    // the loop. The only way to break out of the loop is with a `noreturn`
    // instruction.
    var i: u32 = 0;
    while (true) {
        crash_info.set_body_index(i);
        const inst = body[i];
        std.log.scoped(.sema_zir).debug("sema ZIR {s} %{d}", .{
            mod.namespace_ptr(mod.decl_ptr(block.src_decl).src_namespace).file_scope.sub_file_path, inst,
        });
        const air_inst: Air.Inst.Ref = switch (tags[@int_from_enum(inst)]) {
            // zig fmt: off
            .alloc                        => try sema.zir_alloc(block, inst),
            .alloc_inferred               => try sema.zir_alloc_inferred(block, true),
            .alloc_inferred_mut           => try sema.zir_alloc_inferred(block, false),
            .alloc_inferred_comptime      => try sema.zir_alloc_inferred_comptime(true),
            .alloc_inferred_comptime_mut  => try sema.zir_alloc_inferred_comptime(false),
            .alloc_mut                    => try sema.zir_alloc_mut(block, inst),
            .alloc_comptime_mut           => try sema.zir_alloc_comptime(block, inst),
            .make_ptr_const               => try sema.zir_make_ptr_const(block, inst),
            .anyframe_type                => try sema.zir_anyframe_type(block, inst),
            .array_cat                    => try sema.zir_array_cat(block, inst),
            .array_mul                    => try sema.zir_array_mul(block, inst),
            .array_type                   => try sema.zir_array_type(block, inst),
            .array_type_sentinel          => try sema.zir_array_type_sentinel(block, inst),
            .vector_type                  => try sema.zir_vector_type(block, inst),
            .as_node                      => try sema.zir_as_node(block, inst),
            .as_shift_operand             => try sema.zir_as_shift_operand(block, inst),
            .bit_and                      => try sema.zir_bitwise(block, inst, .bit_and),
            .bit_not                      => try sema.zir_bit_not(block, inst),
            .bit_or                       => try sema.zir_bitwise(block, inst, .bit_or),
            .bitcast                      => try sema.zir_bitcast(block, inst),
            .suspend_block                => try sema.zir_suspend_block(block, inst),
            .bool_not                     => try sema.zir_bool_not(block, inst),
            .bool_br_and                  => try sema.zir_bool_br(block, inst, false),
            .bool_br_or                   => try sema.zir_bool_br(block, inst, true),
            .c_import                     => try sema.zir_cimport(block, inst),
            .call                         => try sema.zir_call(block, inst, .direct),
            .field_call                   => try sema.zir_call(block, inst, .field),
            .cmp_lt                       => try sema.zir_cmp(block, inst, .lt),
            .cmp_lte                      => try sema.zir_cmp(block, inst, .lte),
            .cmp_eq                       => try sema.zir_cmp_eq(block, inst, .eq, Air.Inst.Tag.from_cmp_op(.eq, block.float_mode == .optimized)),
            .cmp_gte                      => try sema.zir_cmp(block, inst, .gte),
            .cmp_gt                       => try sema.zir_cmp(block, inst, .gt),
            .cmp_neq                      => try sema.zir_cmp_eq(block, inst, .neq, Air.Inst.Tag.from_cmp_op(.neq, block.float_mode == .optimized)),
            .decl_ref                     => try sema.zir_decl_ref(block, inst),
            .decl_val                     => try sema.zir_decl_val(block, inst),
            .load                         => try sema.zir_load(block, inst),
            .elem_ptr                     => try sema.zir_elem_ptr(block, inst),
            .elem_ptr_node                => try sema.zir_elem_ptr_node(block, inst),
            .elem_val                     => try sema.zir_elem_val(block, inst),
            .elem_val_node                => try sema.zir_elem_val_node(block, inst),
            .elem_val_imm                 => try sema.zir_elem_val_imm(block, inst),
            .elem_type                    => try sema.zir_elem_type(block, inst),
            .indexable_ptr_elem_type      => try sema.zir_indexable_ptr_elem_type(block, inst),
            .vector_elem_type             => try sema.zir_vector_elem_type(block, inst),
            .enum_literal                 => try sema.zir_enum_literal(block, inst),
            .int_from_enum                => try sema.zir_int_from_enum(block, inst),
            .enum_from_int                => try sema.zir_enum_from_int(block, inst),
            .err_union_code               => try sema.zir_err_union_code(block, inst),
            .err_union_code_ptr           => try sema.zir_err_union_code_ptr(block, inst),
            .err_union_payload_unsafe     => try sema.zir_err_union_payload(block, inst),
            .err_union_payload_unsafe_ptr => try sema.zir_err_union_payload_ptr(block, inst),
            .error_union_type             => try sema.zir_error_union_type(block, inst),
            .error_value                  => try sema.zir_error_value(block, inst),
            .field_ptr                    => try sema.zir_field_ptr(block, inst),
            .field_ptr_named              => try sema.zir_field_ptr_named(block, inst),
            .field_val                    => try sema.zir_field_val(block, inst),
            .field_val_named              => try sema.zir_field_val_named(block, inst),
            .func                         => try sema.zir_func(block, inst, false),
            .func_inferred                => try sema.zir_func(block, inst, true),
            .func_fancy                   => try sema.zir_func_fancy(block, inst),
            .import                       => try sema.zir_import(block, inst),
            .indexable_ptr_len            => try sema.zir_indexable_ptr_len(block, inst),
            .int                          => try sema.zir_int(block, inst),
            .int_big                      => try sema.zir_int_big(block, inst),
            .float                        => try sema.zir_float(block, inst),
            .float128                     => try sema.zir_float128(block, inst),
            .int_type                     => try sema.zir_int_type(inst),
            .is_non_err                   => try sema.zir_is_non_err(block, inst),
            .is_non_err_ptr               => try sema.zir_is_non_err_ptr(block, inst),
            .ret_is_non_err               => try sema.zir_ret_is_non_err(block, inst),
            .is_non_null                  => try sema.zir_is_non_null(block, inst),
            .is_non_null_ptr              => try sema.zir_is_non_null_ptr(block, inst),
            .merge_error_sets             => try sema.zir_merge_error_sets(block, inst),
            .negate                       => try sema.zir_negate(block, inst),
            .negate_wrap                  => try sema.zir_negate_wrap(block, inst),
            .optional_payload_safe        => try sema.zir_optional_payload(block, inst, true),
            .optional_payload_safe_ptr    => try sema.zir_optional_payload_ptr(block, inst, true),
            .optional_payload_unsafe      => try sema.zir_optional_payload(block, inst, false),
            .optional_payload_unsafe_ptr  => try sema.zir_optional_payload_ptr(block, inst, false),
            .optional_type                => try sema.zir_optional_type(block, inst),
            .ptr_type                     => try sema.zir_ptr_type(block, inst),
            .ref                          => try sema.zir_ref(block, inst),
            .ret_err_value_code           => try sema.zir_ret_err_value_code(inst),
            .shr                          => try sema.zir_shr(block, inst, .shr),
            .shr_exact                    => try sema.zir_shr(block, inst, .shr_exact),
            .slice_end                    => try sema.zir_slice_end(block, inst),
            .slice_sentinel               => try sema.zir_slice_sentinel(block, inst),
            .slice_start                  => try sema.zir_slice_start(block, inst),
            .slice_length                 => try sema.zir_slice_length(block, inst),
            .str                          => try sema.zir_str(inst),
            .switch_block                 => try sema.zir_switch_block(block, inst, false),
            .switch_block_ref             => try sema.zir_switch_block(block, inst, true),
            .switch_block_err_union       => try sema.zir_switch_block_err_union(block, inst),
            .type_info                    => try sema.zir_type_info(block, inst),
            .size_of                      => try sema.zir_size_of(block, inst),
            .bit_size_of                  => try sema.zir_bit_size_of(block, inst),
            .typeof                       => try sema.zir_typeof(block, inst),
            .typeof_builtin               => try sema.zir_typeof_builtin(block, inst),
            .typeof_log2_int_type         => try sema.zir_typeof_log2_int_type(block, inst),
            .xor                          => try sema.zir_bitwise(block, inst, .xor),
            .struct_init_empty            => try sema.zir_struct_init_empty(block, inst),
            .struct_init_empty_result     => try sema.zir_struct_init_empty_result(block, inst, false),
            .struct_init_empty_ref_result => try sema.zir_struct_init_empty_result(block, inst, true),
            .struct_init_anon             => try sema.zir_struct_init_anon(block, inst),
            .struct_init                  => try sema.zir_struct_init(block, inst, false),
            .struct_init_ref              => try sema.zir_struct_init(block, inst, true),
            .struct_init_field_type       => try sema.zir_struct_init_field_type(block, inst),
            .struct_init_field_ptr        => try sema.zir_struct_init_field_ptr(block, inst),
            .array_init_anon              => try sema.zir_array_init_anon(block, inst),
            .array_init                   => try sema.zir_array_init(block, inst, false),
            .array_init_ref               => try sema.zir_array_init(block, inst, true),
            .array_init_elem_type         => try sema.zir_array_init_elem_type(block, inst),
            .array_init_elem_ptr          => try sema.zir_array_init_elem_ptr(block, inst),
            .union_init                   => try sema.zir_union_init(block, inst),
            .field_type_ref               => try sema.zir_field_type_ref(block, inst),
            .int_from_ptr                 => try sema.zir_int_from_ptr(block, inst),
            .align_of                     => try sema.zir_align_of(block, inst),
            .int_from_bool                => try sema.zir_int_from_bool(block, inst),
            .embed_file                   => try sema.zir_embed_file(block, inst),
            .error_name                   => try sema.zir_error_name(block, inst),
            .tag_name                     => try sema.zir_tag_name(block, inst),
            .type_name                    => try sema.zir_type_name(block, inst),
            .frame_type                   => try sema.zir_frame_type(block, inst),
            .frame_size                   => try sema.zir_frame_size(block, inst),
            .int_from_float               => try sema.zir_int_from_float(block, inst),
            .float_from_int               => try sema.zir_float_from_int(block, inst),
            .ptr_from_int                 => try sema.zir_ptr_from_int(block, inst),
            .float_cast                   => try sema.zir_float_cast(block, inst),
            .int_cast                     => try sema.zir_int_cast(block, inst),
            .ptr_cast                     => try sema.zir_ptr_cast(block, inst),
            .truncate                     => try sema.zir_truncate(block, inst),
            .has_decl                     => try sema.zir_has_decl(block, inst),
            .has_field                    => try sema.zir_has_field(block, inst),
            .byte_swap                    => try sema.zir_byte_swap(block, inst),
            .bit_reverse                  => try sema.zir_bit_reverse(block, inst),
            .bit_offset_of                => try sema.zir_bit_offset_of(block, inst),
            .offset_of                    => try sema.zir_offset_of(block, inst),
            .splat                        => try sema.zir_splat(block, inst),
            .reduce                       => try sema.zir_reduce(block, inst),
            .shuffle                      => try sema.zir_shuffle(block, inst),
            .atomic_load                  => try sema.zir_atomic_load(block, inst),
            .atomic_rmw                   => try sema.zir_atomic_rmw(block, inst),
            .mul_add                      => try sema.zir_mul_add(block, inst),
            .builtin_call                 => try sema.zir_builtin_call(block, inst),
            .@"resume"                    => try sema.zir_resume(block, inst),
            .@"await"                     => try sema.zir_await(block, inst),
            .for_len                      => try sema.zir_for_len(block, inst),
            .validate_array_init_ref_ty   => try sema.zir_validate_array_init_ref_ty(block, inst),
            .opt_eu_base_ptr_init         => try sema.zir_opt_eu_base_ptr_init(block, inst),
            .coerce_ptr_elem_ty           => try sema.zir_coerce_ptr_elem_ty(block, inst),

            .clz       => try sema.zir_bit_count(block, inst, .clz,      Value.clz),
            .ctz       => try sema.zir_bit_count(block, inst, .ctz,      Value.ctz),
            .pop_count => try sema.zir_bit_count(block, inst, .popcount, Value.pop_count),
            .abs       => try sema.zir_abs(block, inst),

            .sqrt  => try sema.zir_unary_math(block, inst, .sqrt, Value.sqrt),
            .sin   => try sema.zir_unary_math(block, inst, .sin, Value.sin),
            .cos   => try sema.zir_unary_math(block, inst, .cos, Value.cos),
            .tan   => try sema.zir_unary_math(block, inst, .tan, Value.tan),
            .exp   => try sema.zir_unary_math(block, inst, .exp, Value.exp),
            .exp2  => try sema.zir_unary_math(block, inst, .exp2, Value.exp2),
            .log   => try sema.zir_unary_math(block, inst, .log, Value.log),
            .log2  => try sema.zir_unary_math(block, inst, .log2, Value.log2),
            .log10 => try sema.zir_unary_math(block, inst, .log10, Value.log10),
            .floor => try sema.zir_unary_math(block, inst, .floor, Value.floor),
            .ceil  => try sema.zir_unary_math(block, inst, .ceil, Value.ceil),
            .round => try sema.zir_unary_math(block, inst, .round, Value.round),
            .trunc => try sema.zir_unary_math(block, inst, .trunc_float, Value.trunc),

            .error_set_decl      => try sema.zir_error_set_decl(block, inst, .parent),
            .error_set_decl_anon => try sema.zir_error_set_decl(block, inst, .anon),
            .error_set_decl_func => try sema.zir_error_set_decl(block, inst, .func),

            .add        => try sema.zir_arithmetic(block, inst, .add,        true),
            .addwrap    => try sema.zir_arithmetic(block, inst, .addwrap,    true),
            .add_sat    => try sema.zir_arithmetic(block, inst, .add_sat,    true),
            .add_unsafe => try sema.zir_arithmetic(block, inst, .add_unsafe, false),
            .mul        => try sema.zir_arithmetic(block, inst, .mul,        true),
            .mulwrap    => try sema.zir_arithmetic(block, inst, .mulwrap,    true),
            .mul_sat    => try sema.zir_arithmetic(block, inst, .mul_sat,    true),
            .sub        => try sema.zir_arithmetic(block, inst, .sub,        true),
            .subwrap    => try sema.zir_arithmetic(block, inst, .subwrap,    true),
            .sub_sat    => try sema.zir_arithmetic(block, inst, .sub_sat,    true),

            .div       => try sema.zir_div(block, inst),
            .div_exact => try sema.zir_div_exact(block, inst),
            .div_floor => try sema.zir_div_floor(block, inst),
            .div_trunc => try sema.zir_div_trunc(block, inst),

            .mod_rem => try sema.zir_mod_rem(block, inst),
            .mod     => try sema.zir_mod(block, inst),
            .rem     => try sema.zir_rem(block, inst),

            .max => try sema.zir_min_max(block, inst, .max),
            .min => try sema.zir_min_max(block, inst, .min),

            .shl       => try sema.zir_shl(block, inst, .shl),
            .shl_exact => try sema.zir_shl(block, inst, .shl_exact),
            .shl_sat   => try sema.zir_shl(block, inst, .shl_sat),

            .ret_ptr  => try sema.zir_ret_ptr(block),
            .ret_type => Air.interned_to_ref(sema.fn_ret_ty.to_intern()),

            // Instructions that we know to *always* be noreturn based solely on their tag.
            // These functions match the return type of analyze_body so that we can
            // tail call them here.
            .compile_error  => break try sema.zir_compile_error(block, inst),
            .ret_implicit   => break try sema.zir_ret_implicit(block, inst),
            .ret_node       => break try sema.zir_ret_node(block, inst),
            .ret_load       => break try sema.zir_ret_load(block, inst),
            .ret_err_value  => break try sema.zir_ret_err_value(block, inst),
            .@"unreachable" => break try sema.zir_unreachable(block, inst),
            .panic          => break try sema.zir_panic(block, inst),
            .trap           => break try sema.zir_trap(block, inst),
            // zig fmt: on

            // This instruction never exists in an analyzed body. It exists only in the declaration
            // list for a container type.
            .declaration => unreachable,

            .extended => ext: {
                const extended = datas[@int_from_enum(inst)].extended;
                break :ext switch (extended.opcode) {
                    // zig fmt: off
                    .variable           => try sema.zir_var_extended(       block, extended),
                    .struct_decl        => try sema.zir_struct_decl(        block, extended, inst),
                    .enum_decl          => try sema.zir_enum_decl(          block, extended, inst),
                    .union_decl         => try sema.zir_union_decl(         block, extended, inst),
                    .opaque_decl        => try sema.zir_opaque_decl(        block, extended, inst),
                    .this               => try sema.zir_this(              block, extended),
                    .ret_addr           => try sema.zir_ret_addr(           block, extended),
                    .builtin_src        => try sema.zir_builtin_src(        block, extended),
                    .error_return_trace => try sema.zir_error_return_trace(  block),
                    .frame              => try sema.zir_frame(             block, extended),
                    .frame_address      => try sema.zir_frame_address(      block, extended),
                    .alloc              => try sema.zir_alloc_extended(     block, extended),
                    .builtin_extern     => try sema.zir_builtin_extern(     block, extended),
                    .@"asm"             => try sema.zir_asm(               block, extended, false),
                    .asm_expr           => try sema.zir_asm(               block, extended, true),
                    .typeof_peer        => try sema.zir_typeof_peer(        block, extended, inst),
                    .compile_log        => try sema.zir_compile_log(               extended),
                    .min_multi          => try sema.zir_min_max_multi(       block, extended, .min),
                    .max_multi          => try sema.zir_min_max_multi(       block, extended, .max),
                    .add_with_overflow  => try sema.zir_overflow_arithmetic(block, extended, extended.opcode),
                    .sub_with_overflow  => try sema.zir_overflow_arithmetic(block, extended, extended.opcode),
                    .mul_with_overflow  => try sema.zir_overflow_arithmetic(block, extended, extended.opcode),
                    .shl_with_overflow  => try sema.zir_overflow_arithmetic(block, extended, extended.opcode),
                    .c_undef            => try sema.zir_cundef(            block, extended),
                    .c_include          => try sema.zir_cinclude(          block, extended),
                    .c_define           => try sema.zir_cdefine(           block, extended),
                    .wasm_memory_size   => try sema.zir_wasm_memory_size(    block, extended),
                    .wasm_memory_grow   => try sema.zir_wasm_memory_grow(    block, extended),
                    .prefetch           => try sema.zir_prefetch(          block, extended),
                    .error_cast         => try sema.zir_error_cast(         block, extended),
                    .await_nosuspend    => try sema.zir_await_nosuspend(    block, extended),
                    .select             => try sema.zir_select(            block, extended),
                    .int_from_error     => try sema.zir_int_from_error(      block, extended),
                    .error_from_int     => try sema.zir_error_from_int(      block, extended),
                    .reify              => try sema.zir_reify(             block, extended, inst),
                    .builtin_async_call => try sema.zir_builtin_async_call(  block, extended),
                    .cmpxchg            => try sema.zir_cmpxchg(           block, extended),
                    .c_va_arg           => try sema.zir_cva_arg(            block, extended),
                    .c_va_copy          => try sema.zir_cva_copy(           block, extended),
                    .c_va_end           => try sema.zir_cva_end(            block, extended),
                    .c_va_start         => try sema.zir_cva_start(          block, extended),
                    .ptr_cast_full      => try sema.zir_ptr_cast_full(       block, extended),
                    .ptr_cast_no_dest   => try sema.zir_ptr_cast_no_dest(     block, extended),
                    .work_item_id       => try sema.zir_work_item(          block, extended, extended.opcode),
                    .work_group_size    => try sema.zir_work_item(          block, extended, extended.opcode),
                    .work_group_id      => try sema.zir_work_item(          block, extended, extended.opcode),
                    .in_comptime        => try sema.zir_in_comptime(        block),
                    .closure_get        => try sema.zir_closure_get(        block, extended),
                    // zig fmt: on

                    .fence => {
                        try sema.zir_fence(block, extended);
                        i += 1;
                        continue;
                    },
                    .set_float_mode => {
                        try sema.zir_set_float_mode(block, extended);
                        i += 1;
                        continue;
                    },
                    .set_align_stack => {
                        try sema.zir_set_align_stack(block, extended);
                        i += 1;
                        continue;
                    },
                    .set_cold => {
                        try sema.zir_set_cold(block, extended);
                        i += 1;
                        continue;
                    },
                    .breakpoint => {
                        if (!block.is_comptime) {
                            _ = try block.add_no_op(.breakpoint);
                        }
                        i += 1;
                        continue;
                    },
                    .restore_err_ret_index => {
                        try sema.zir_restore_err_ret_index(block, extended);
                        i += 1;
                        continue;
                    },
                    .value_placeholder => unreachable, // never appears in a body
                    .field_parent_ptr => try sema.zir_field_parent_ptr(block, extended),
                };
            },

            // Instructions that we know can *never* be noreturn based solely on
            // their tag. We avoid needlessly checking if they are noreturn and
            // continue the loop.
            // We also know that they cannot be referenced later, so we avoid
            // putting them into the map.
            .dbg_stmt => {
                try sema.zir_dbg_stmt(block, inst);
                i += 1;
                continue;
            },
            .dbg_var_ptr => {
                try sema.zir_dbg_var(block, inst, .dbg_var_ptr);
                i += 1;
                continue;
            },
            .dbg_var_val => {
                try sema.zir_dbg_var(block, inst, .dbg_var_val);
                i += 1;
                continue;
            },
            .ensure_err_union_payload_void => {
                try sema.zir_ensure_err_union_payload_void(block, inst);
                i += 1;
                continue;
            },
            .ensure_result_non_error => {
                try sema.zir_ensure_result_non_error(block, inst);
                i += 1;
                continue;
            },
            .ensure_result_used => {
                try sema.zir_ensure_result_used(block, inst);
                i += 1;
                continue;
            },
            .set_eval_branch_quota => {
                try sema.zir_set_eval_branch_quota(block, inst);
                i += 1;
                continue;
            },
            .atomic_store => {
                try sema.zir_atomic_store(block, inst);
                i += 1;
                continue;
            },
            .store_node => {
                try sema.zir_store_node(block, inst);
                i += 1;
                continue;
            },
            .store_to_inferred_ptr => {
                try sema.zir_store_to_inferred_ptr(block, inst);
                i += 1;
                continue;
            },
            .resolve_inferred_alloc => {
                try sema.zir_resolve_inferred_alloc(block, inst);
                i += 1;
                continue;
            },
            .validate_struct_init_ty => {
                try sema.zir_validate_struct_init_ty(block, inst, false);
                i += 1;
                continue;
            },
            .validate_struct_init_result_ty => {
                try sema.zir_validate_struct_init_ty(block, inst, true);
                i += 1;
                continue;
            },
            .validate_array_init_ty => {
                try sema.zir_validate_array_init_ty(block, inst, false);
                i += 1;
                continue;
            },
            .validate_array_init_result_ty => {
                try sema.zir_validate_array_init_ty(block, inst, true);
                i += 1;
                continue;
            },
            .validate_ptr_struct_init => {
                try sema.zir_validate_ptr_struct_init(block, inst);
                i += 1;
                continue;
            },
            .validate_ptr_array_init => {
                try sema.zir_validate_ptr_array_init(block, inst);
                i += 1;
                continue;
            },
            .validate_deref => {
                try sema.zir_validate_deref(block, inst);
                i += 1;
                continue;
            },
            .validate_destructure => {
                try sema.zir_validate_destructure(block, inst);
                i += 1;
                continue;
            },
            .validate_ref_ty => {
                try sema.zir_validate_ref_ty(block, inst);
                i += 1;
                continue;
            },
            .@"export" => {
                try sema.zir_export(block, inst);
                i += 1;
                continue;
            },
            .export_value => {
                try sema.zir_export_value(block, inst);
                i += 1;
                continue;
            },
            .set_runtime_safety => {
                try sema.zir_set_runtime_safety(block, inst);
                i += 1;
                continue;
            },
            .param => {
                try sema.zir_param(block, inst, false);
                i += 1;
                continue;
            },
            .param_comptime => {
                try sema.zir_param(block, inst, true);
                i += 1;
                continue;
            },
            .param_anytype => {
                try sema.zir_param_anytype(block, inst, false);
                i += 1;
                continue;
            },
            .param_anytype_comptime => {
                try sema.zir_param_anytype(block, inst, true);
                i += 1;
                continue;
            },
            .memcpy => {
                try sema.zir_memcpy(block, inst);
                i += 1;
                continue;
            },
            .memset => {
                try sema.zir_memset(block, inst);
                i += 1;
                continue;
            },
            .check_comptime_control_flow => {
                if (!block.is_comptime) {
                    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
                    const src = inst_data.src();
                    const inline_block = inst_data.operand.to_index().?;

                    var check_block = block;
                    const target_runtime_index = while (true) {
                        if (check_block.inline_block == inline_block.to_optional()) {
                            break check_block.runtime_index;
                        }
                        check_block = check_block.parent.?;
                    };

                    if (@int_from_enum(target_runtime_index) < @int_from_enum(block.runtime_index)) {
                        const runtime_src = block.runtime_cond orelse block.runtime_loop.?;
                        const msg = msg: {
                            const msg = try sema.err_msg(block, src, "comptime control flow inside runtime block", .{});
                            errdefer msg.destroy(sema.gpa);

                            try mod.err_note_non_lazy(runtime_src, msg, "runtime control flow here", .{});
                            break :msg msg;
                        };
                        return sema.fail_with_owned_error_msg(block, msg);
                    }
                }
                i += 1;
                continue;
            },
            .save_err_ret_index => {
                try sema.zir_save_err_ret_index(block, inst);
                i += 1;
                continue;
            },
            .restore_err_ret_index_unconditional => {
                const un_node = datas[@int_from_enum(inst)].un_node;
                try sema.restore_err_ret_index(block, un_node.src(), un_node.operand, .none);
                i += 1;
                continue;
            },
            .restore_err_ret_index_fn_entry => {
                const un_node = datas[@int_from_enum(inst)].un_node;
                try sema.restore_err_ret_index(block, un_node.src(), .none, un_node.operand);
                i += 1;
                continue;
            },

            // Special case instructions to handle comptime control flow.
            .@"break" => {
                if (block.is_comptime) {
                    sema.comptime_break_inst = inst;
                    return error.ComptimeBreak;
                } else {
                    try sema.zir_break(block, inst);
                    break;
                }
            },
            .break_inline => {
                sema.comptime_break_inst = inst;
                return error.ComptimeBreak;
            },
            .repeat => {
                if (block.is_comptime) {
                    // Send comptime control flow back to the beginning of this block.
                    const src = LazySrcLoc.nodeOffset(datas[@int_from_enum(inst)].node);
                    try sema.emit_backward_branch(block, src);
                    i = 0;
                    continue;
                } else {
                    // We are definitely called by `zir_loop`, which will treat the
                    // fact that this body does not terminate `noreturn` as an
                    // implicit repeat.
                    break;
                }
            },
            .repeat_inline => {
                // Send comptime control flow back to the beginning of this block.
                const src = LazySrcLoc.nodeOffset(datas[@int_from_enum(inst)].node);
                try sema.emit_backward_branch(block, src);
                i = 0;
                continue;
            },
            .loop => blk: {
                if (!block.is_comptime) break :blk try sema.zir_loop(block, inst);
                // Same as `block_inline`. TODO https://github.com/ziglang/zig/issues/8220
                const inst_data = datas[@int_from_enum(inst)].pl_node;
                const extra = sema.code.extra_data(Zir.Inst.Block, inst_data.payload_index);
                const inline_body = sema.code.body_slice(extra.end, extra.data.body_len);

                // Create a temporary child block so that this loop is properly
                // labeled for any .restore_err_ret_index instructions
                var child_block = block.make_sub_block();

                var label: Block.Label = .{
                    .zir_block = inst,
                    .merges = undefined,
                };
                child_block.label = &label;

                // Write these instructions directly into the parent block
                child_block.instructions = block.instructions;
                defer block.instructions = child_block.instructions;

                const result = try sema.analyze_inline_body(&child_block, inline_body, inst) orelse break;
                break :blk result;
            },
            .block, .block_comptime => blk: {
                if (!block.is_comptime) {
                    break :blk try sema.zir_block(block, inst, tags[@int_from_enum(inst)] == .block_comptime);
                }
                // Same as `block_inline`. TODO https://github.com/ziglang/zig/issues/8220
                const inst_data = datas[@int_from_enum(inst)].pl_node;
                const extra = sema.code.extra_data(Zir.Inst.Block, inst_data.payload_index);
                const inline_body = sema.code.body_slice(extra.end, extra.data.body_len);

                // Create a temporary child block so that this block is properly
                // labeled for any .restore_err_ret_index instructions
                var child_block = block.make_sub_block();

                var label: Block.Label = .{
                    .zir_block = inst,
                    .merges = undefined,
                };
                child_block.label = &label;

                // Write these instructions directly into the parent block
                child_block.instructions = block.instructions;
                defer block.instructions = child_block.instructions;

                const result = try sema.analyze_inline_body(&child_block, inline_body, inst) orelse break;
                break :blk result;
            },
            .block_inline => blk: {
                // Directly analyze the block body without introducing a new block.
                // However, in the case of a corresponding break_inline which reaches
                // through a runtime conditional branch, we must retroactively emit
                // a block, so we remember the block index here just in case.
                const block_index = block.instructions.items.len;
                const inst_data = datas[@int_from_enum(inst)].pl_node;
                const extra = sema.code.extra_data(Zir.Inst.Block, inst_data.payload_index);
                const inline_body = sema.code.body_slice(extra.end, extra.data.body_len);
                const gpa = sema.gpa;

                const BreakResult = struct {
                    block_inst: Zir.Inst.Index,
                    operand: Zir.Inst.Ref,
                };

                const opt_break_data: ?BreakResult, const need_debug_scope = b: {
                    // Create a temporary child block so that this inline block is properly
                    // labeled for any .restore_err_ret_index instructions
                    var child_block = block.make_sub_block();
                    var need_debug_scope = false;
                    child_block.need_debug_scope = &need_debug_scope;

                    // If this block contains a function prototype, we need to reset the
                    // current list of parameters and restore it later.
                    // Note: this probably needs to be resolved in a more general manner.
                    const tag_index = @int_from_enum(inline_body[inline_body.len - 1]);
                    child_block.inline_block = (if (tags[tag_index] == .repeat_inline)
                        inline_body[0]
                    else
                        inst).to_optional();

                    var label: Block.Label = .{
                        .zir_block = inst,
                        .merges = undefined,
                    };
                    child_block.label = &label;

                    // Write these instructions directly into the parent block
                    child_block.instructions = block.instructions;
                    defer block.instructions = child_block.instructions;

                    const break_result: ?BreakResult = if (sema.analyze_body_inner(&child_block, inline_body)) |_| r: {
                        break :r null;
                    } else |err| switch (err) {
                        error.ComptimeBreak => brk_res: {
                            const break_inst = sema.comptime_break_inst;
                            const break_data = sema.code.instructions.items(.data)[@int_from_enum(break_inst)].@"break";
                            const break_extra = sema.code.extra_data(Zir.Inst.Break, break_data.payload_index).data;
                            break :brk_res .{
                                .block_inst = break_extra.block_inst,
                                .operand = break_data.operand,
                            };
                        },
                        else => |e| return e,
                    };

                    if (need_debug_scope) {
                        _ = try sema.ensure_post_hoc(block, inst);
                    }

                    break :b .{ break_result, need_debug_scope };
                };

                // A runtime conditional branch that needs a post-hoc block to be
                // emitted communicates this by mapping the block index into the inst map.
                if (map.get(inst)) |new_block_ref| ph: {
                    // Comptime control flow populates the map, so we don't actually know
                    // if this is a post-hoc runtime block until we check the
                    // post_hoc_block map.
                    const new_block_inst = new_block_ref.to_index() orelse break :ph;
                    const labeled_block = sema.post_hoc_blocks.get(new_block_inst) orelse
                        break :ph;

                    // In this case we need to move all the instructions starting at
                    // block_index from the current block into this new one.

                    if (opt_break_data) |break_data| {
                        // This is a comptime break which we now change to a runtime break
                        // since it crosses a runtime branch.
                        // It may pass through our currently being analyzed block_inline or it
                        // may point directly to it. In the latter case, this modifies the
                        // block that we looked up in the post_hoc_blocks map above.
                        try sema.add_runtime_break(block, break_data.block_inst, break_data.operand);
                    }

                    try labeled_block.block.instructions.append_slice(gpa, block.instructions.items[block_index..]);
                    block.instructions.items.len = block_index;

                    const block_result = try sema.resolve_analyzed_block(block, inst_data.src(), &labeled_block.block, &labeled_block.label.merges, need_debug_scope);
                    {
                        // Destroy the ad-hoc block entry so that it does not interfere with
                        // the next iteration of comptime control flow, if any.
                        labeled_block.destroy(gpa);
                        assert(sema.post_hoc_blocks.remove(new_block_inst));
                    }

                    break :blk block_result;
                }

                const break_data = opt_break_data orelse break;
                if (inst == break_data.block_inst) {
                    break :blk try sema.resolve_inst(break_data.operand);
                } else {
                    // `comptime_break_inst` preserved from `analyze_body_inner` above.
                    return error.ComptimeBreak;
                }
            },
            .condbr => blk: {
                if (!block.is_comptime) {
                    try sema.zir_condbr(block, inst);
                    break;
                }
                // Same as condbr_inline. TODO https://github.com/ziglang/zig/issues/8220
                const inst_data = datas[@int_from_enum(inst)].pl_node;
                const cond_src: LazySrcLoc = .{ .node_offset_if_cond = inst_data.src_node };
                const extra = sema.code.extra_data(Zir.Inst.CondBr, inst_data.payload_index);
                const then_body = sema.code.body_slice(extra.end, extra.data.then_body_len);
                const else_body = sema.code.body_slice(
                    extra.end + then_body.len,
                    extra.data.else_body_len,
                );
                const cond = try sema.resolve_inst_const(block, cond_src, extra.data.condition, .{
                    .needed_comptime_reason = "condition in comptime branch must be comptime-known",
                    .block_comptime_reason = block.comptime_reason,
                });
                const inline_body = if (cond.to_bool()) then_body else else_body;

                try sema.maybe_error_unwrap_condbr(block, inline_body, extra.data.condition, cond_src);

                const result = try sema.analyze_inline_body(block, inline_body, inst) orelse break;
                break :blk result;
            },
            .condbr_inline => blk: {
                const inst_data = datas[@int_from_enum(inst)].pl_node;
                const cond_src: LazySrcLoc = .{ .node_offset_if_cond = inst_data.src_node };
                const extra = sema.code.extra_data(Zir.Inst.CondBr, inst_data.payload_index);
                const then_body = sema.code.body_slice(extra.end, extra.data.then_body_len);
                const else_body = sema.code.body_slice(
                    extra.end + then_body.len,
                    extra.data.else_body_len,
                );
                const cond = try sema.resolve_inst_const(block, cond_src, extra.data.condition, .{
                    .needed_comptime_reason = "condition in comptime branch must be comptime-known",
                    .block_comptime_reason = block.comptime_reason,
                });
                const inline_body = if (cond.to_bool()) then_body else else_body;

                try sema.maybe_error_unwrap_condbr(block, inline_body, extra.data.condition, cond_src);
                const old_runtime_index = block.runtime_index;
                defer block.runtime_index = old_runtime_index;

                const result = try sema.analyze_inline_body(block, inline_body, inst) orelse break;
                break :blk result;
            },
            .@"try" => blk: {
                if (!block.is_comptime) break :blk try sema.zir_try(block, inst);
                const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
                const src = inst_data.src();
                const operand_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
                const extra = sema.code.extra_data(Zir.Inst.Try, inst_data.payload_index);
                const inline_body = sema.code.body_slice(extra.end, extra.data.body_len);
                const err_union = try sema.resolve_inst(extra.data.operand);
                const err_union_ty = sema.type_of(err_union);
                if (err_union_ty.zig_type_tag(mod) != .ErrorUnion) {
                    return sema.fail(block, operand_src, "expected error union type, found '{}'", .{
                        err_union_ty.fmt(mod),
                    });
                }
                const is_non_err = try sema.analyze_is_non_err_comptime_only(block, operand_src, err_union);
                assert(is_non_err != .none);
                const is_non_err_val = try sema.resolve_const_defined_value(block, operand_src, is_non_err, .{
                    .needed_comptime_reason = "try operand inside comptime block must be comptime-known",
                    .block_comptime_reason = block.comptime_reason,
                });
                if (is_non_err_val.to_bool()) {
                    break :blk try sema.analyze_err_union_payload(block, src, err_union_ty, err_union, operand_src, false);
                }
                const result = try sema.analyze_inline_body(block, inline_body, inst) orelse break;
                break :blk result;
            },
            .try_ptr => blk: {
                if (!block.is_comptime) break :blk try sema.zir_try_ptr(block, inst);
                const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
                const src = inst_data.src();
                const operand_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
                const extra = sema.code.extra_data(Zir.Inst.Try, inst_data.payload_index);
                const inline_body = sema.code.body_slice(extra.end, extra.data.body_len);
                const operand = try sema.resolve_inst(extra.data.operand);
                const err_union = try sema.analyze_load(block, src, operand, operand_src);
                const is_non_err = try sema.analyze_is_non_err_comptime_only(block, operand_src, err_union);
                assert(is_non_err != .none);
                const is_non_err_val = try sema.resolve_const_defined_value(block, operand_src, is_non_err, .{
                    .needed_comptime_reason = "try operand inside comptime block must be comptime-known",
                    .block_comptime_reason = block.comptime_reason,
                });
                if (is_non_err_val.to_bool()) {
                    break :blk try sema.analyze_err_union_payload_ptr(block, src, operand, false, false);
                }
                const result = try sema.analyze_inline_body(block, inline_body, inst) orelse break;
                break :blk result;
            },
            .@"defer" => blk: {
                const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].@"defer";
                const defer_body = sema.code.body_slice(inst_data.index, inst_data.len);
                if (sema.analyze_body_inner(block, defer_body)) |_| {
                    // The defer terminated noreturn - no more analysis needed.
                    break;
                } else |err| switch (err) {
                    error.ComptimeBreak => {},
                    else => |e| return e,
                }
                if (sema.comptime_break_inst != defer_body[defer_body.len - 1]) {
                    return error.ComptimeBreak;
                }
                break :blk .void_value;
            },
            .defer_err_code => blk: {
                const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].defer_err_code;
                const extra = sema.code.extra_data(Zir.Inst.DeferErrCode, inst_data.payload_index).data;
                const defer_body = sema.code.body_slice(extra.index, extra.len);
                const err_code = try sema.resolve_inst(inst_data.err_code);
                map.put_assume_capacity(extra.remapped_err_code, err_code);
                if (sema.analyze_body_inner(block, defer_body)) |_| {
                    // The defer terminated noreturn - no more analysis needed.
                    break;
                } else |err| switch (err) {
                    error.ComptimeBreak => {},
                    else => |e| return e,
                }
                if (sema.comptime_break_inst != defer_body[defer_body.len - 1]) {
                    return error.ComptimeBreak;
                }
                break :blk .void_value;
            },
        };
        if (sema.is_no_return(air_inst)) {
            // We're going to assume that the body itself is noreturn, so let's ensure that now
            assert(block.instructions.items.len > 0);
            assert(sema.is_no_return(block.instructions.items[block.instructions.items.len - 1].to_ref()));
            break;
        }
        map.put_assume_capacity(inst, air_inst);
        i += 1;
    }
}

pub fn resolve_inst_allow_none(sema: *Sema, zir_ref: Zir.Inst.Ref) !Air.Inst.Ref {
    if (zir_ref == .none) {
        return .none;
    } else {
        return resolve_inst(sema, zir_ref);
    }
}

pub fn resolve_inst(sema: *Sema, zir_ref: Zir.Inst.Ref) !Air.Inst.Ref {
    assert(zir_ref != .none);
    if (zir_ref.to_index()) |i| {
        const inst = sema.inst_map.get(i).?;
        if (inst == .generic_poison) return error.GenericPoison;
        return inst;
    }
    // First section of indexes correspond to a set number of constant values.
    // We intentionally map the same indexes to the same values between ZIR and AIR.
    return @enumFromInt(@int_from_enum(zir_ref));
}

fn resolve_const_bool(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    zir_ref: Zir.Inst.Ref,
    reason: NeededComptimeReason,
) !bool {
    const air_inst = try sema.resolve_inst(zir_ref);
    const wanted_type = Type.bool;
    const coerced_inst = try sema.coerce(block, wanted_type, air_inst, src);
    const val = try sema.resolve_const_defined_value(block, src, coerced_inst, reason);
    return val.to_bool();
}

fn resolve_const_string(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    zir_ref: Zir.Inst.Ref,
    reason: NeededComptimeReason,
) ![]u8 {
    const air_inst = try sema.resolve_inst(zir_ref);
    return sema.to_const_string(block, src, air_inst, reason);
}

pub fn to_const_string(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    air_inst: Air.Inst.Ref,
    reason: NeededComptimeReason,
) ![]u8 {
    const coerced_inst = try sema.coerce(block, Type.slice_const_u8, air_inst, src);
    const slice_val = try sema.resolve_const_defined_value(block, src, coerced_inst, reason);
    const arr_val = try sema.deref_slice_as_array(block, src, slice_val, reason);
    return arr_val.to_allocated_bytes(arr_val.type_of(sema.mod), sema.arena, sema.mod);
}

pub fn resolve_const_string_intern(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    zir_ref: Zir.Inst.Ref,
    reason: NeededComptimeReason,
) !InternPool.NullTerminatedString {
    const air_inst = try sema.resolve_inst(zir_ref);
    const wanted_type = Type.slice_const_u8;
    const coerced_inst = try sema.coerce(block, wanted_type, air_inst, src);
    const val = try sema.resolve_const_defined_value(block, src, coerced_inst, reason);
    return sema.slice_to_ip_string(block, src, val, reason);
}

pub fn resolve_type(sema: *Sema, block: *Block, src: LazySrcLoc, zir_ref: Zir.Inst.Ref) !Type {
    const air_inst = try sema.resolve_inst(zir_ref);
    const ty = try sema.analyze_as_type(block, src, air_inst);
    if (ty.is_generic_poison()) return error.GenericPoison;
    return ty;
}

fn resolve_dest_type(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    zir_ref: Zir.Inst.Ref,
    strat: enum { remove_eu_opt, remove_eu, remove_opt },
    builtin_name: []const u8,
) !Type {
    const mod = sema.mod;
    const remove_eu = switch (strat) {
        .remove_eu_opt, .remove_eu => true,
        .remove_opt => false,
    };
    const remove_opt = switch (strat) {
        .remove_eu_opt, .remove_opt => true,
        .remove_eu => false,
    };

    const raw_ty = sema.resolve_type(block, src, zir_ref) catch |err| switch (err) {
        error.GenericPoison => {
            // Cast builtins use their result type as the destination type, but
            // it could be an anytype argument, which we can't catch in AstGen.
            const msg = msg: {
                const msg = try sema.err_msg(block, src, "{s} must have a known result type", .{builtin_name});
                errdefer msg.destroy(sema.gpa);
                switch (sema.generic_poison_reason(zir_ref)) {
                    .anytype_param => |call_src| try sema.err_note(block, call_src, msg, "result type is unknown due to anytype parameter", .{}),
                    .anyopaque_ptr => |ptr_src| try sema.err_note(block, ptr_src, msg, "result type is unknown due to opaque pointer type", .{}),
                    .unknown => {},
                }
                try sema.err_note(block, src, msg, "use @as to provide explicit result type", .{});
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        },
        else => |e| return e,
    };

    if (remove_eu and raw_ty.zig_type_tag(mod) == .ErrorUnion) {
        const eu_child = raw_ty.error_union_payload(mod);
        if (remove_opt and eu_child.zig_type_tag(mod) == .Optional) {
            return eu_child.child_type(mod);
        }
        return eu_child;
    }
    if (remove_opt and raw_ty.zig_type_tag(mod) == .Optional) {
        return raw_ty.child_type(mod);
    }
    return raw_ty;
}

const GenericPoisonReason = union(enum) {
    anytype_param: LazySrcLoc,
    anyopaque_ptr: LazySrcLoc,
    unknown,
};

/// Backtracks through ZIR instructions to determine the reason a generic poison
/// type was created. Used for error reporting.
fn generic_poison_reason(sema: *Sema, ref: Zir.Inst.Ref) GenericPoisonReason {
    var cur = ref;
    while (true) {
        const inst = cur.to_index() orelse return .unknown;
        switch (sema.code.instructions.items(.tag)[@int_from_enum(inst)]) {
            .validate_array_init_ref_ty => {
                const pl_node = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
                const extra = sema.code.extra_data(Zir.Inst.ArrayInitRefTy, pl_node.payload_index).data;
                cur = extra.ptr_ty;
            },
            .array_init_elem_type => {
                const bin = sema.code.instructions.items(.data)[@int_from_enum(inst)].bin;
                cur = bin.lhs;
            },
            .indexable_ptr_elem_type, .vector_elem_type => {
                const un_node = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
                cur = un_node.operand;
            },
            .struct_init_field_type => {
                const pl_node = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
                const extra = sema.code.extra_data(Zir.Inst.FieldType, pl_node.payload_index).data;
                cur = extra.container_type;
            },
            .elem_type => {
                // There are two cases here: the pointer type may already have been
                // generic poison, or it may have been an anyopaque pointer.
                const un_node = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
                const operand_ref = sema.resolve_inst(un_node.operand) catch |err| switch (err) {
                    error.GenericPoison => unreachable, // this is a type, not a value
                };
                const operand_val = operand_ref.to_interned() orelse return .unknown;
                if (operand_val == .generic_poison_type) {
                    // The pointer was generic poison - keep looking.
                    cur = un_node.operand;
                } else {
                    // This must be an anyopaque pointer!
                    return .{ .anyopaque_ptr = un_node.src() };
                }
            },
            .call, .field_call => {
                // A function call can never return generic poison, so we must be
                // evaluating an `anytype` function parameter.
                // TODO: better source location - function decl rather than call
                const pl_node = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
                return .{ .anytype_param = pl_node.src() };
            },
            else => return .unknown,
        }
    }
}

fn analyze_as_type(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    air_inst: Air.Inst.Ref,
) !Type {
    const wanted_type = Type.type;
    const coerced_inst = try sema.coerce(block, wanted_type, air_inst, src);
    const val = try sema.resolve_const_defined_value(block, src, coerced_inst, .{
        .needed_comptime_reason = "types must be comptime-known",
    });
    return val.to_type();
}

pub fn setup_error_return_trace(sema: *Sema, block: *Block, last_arg_index: usize) !void {
    const mod = sema.mod;
    const comp = mod.comp;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;
    if (!comp.config.any_error_tracing) return;

    assert(!block.is_comptime);
    var err_trace_block = block.make_sub_block();
    defer err_trace_block.instructions.deinit(gpa);

    const src: LazySrcLoc = .unneeded;

    // var addrs: [err_return_trace_addr_count]usize = undefined;
    const err_return_trace_addr_count = 32;
    const addr_arr_ty = try mod.array_type(.{
        .len = err_return_trace_addr_count,
        .child = .usize_type,
    });
    const addrs_ptr = try err_trace_block.add_ty(.alloc, try mod.single_mut_ptr_type(addr_arr_ty));

    // var st: StackTrace = undefined;
    const stack_trace_ty = try sema.get_builtin_type("StackTrace");
    try sema.resolve_type_fields(stack_trace_ty);
    const st_ptr = try err_trace_block.add_ty(.alloc, try mod.single_mut_ptr_type(stack_trace_ty));

    // st.instruction_addresses = &addrs;
    const instruction_addresses_field_name = try ip.get_or_put_string(gpa, "instruction_addresses", .no_embedded_nulls);
    const addr_field_ptr = try sema.field_ptr(&err_trace_block, src, st_ptr, instruction_addresses_field_name, src, true);
    try sema.store_ptr2(&err_trace_block, src, addr_field_ptr, src, addrs_ptr, src, .store);

    // st.index = 0;
    const index_field_name = try ip.get_or_put_string(gpa, "index", .no_embedded_nulls);
    const index_field_ptr = try sema.field_ptr(&err_trace_block, src, st_ptr, index_field_name, src, true);
    try sema.store_ptr2(&err_trace_block, src, index_field_ptr, src, .zero_usize, src, .store);

    // @errorReturnTrace() = &st;
    _ = try err_trace_block.add_un_op(.set_err_return_trace, st_ptr);

    try block.instructions.insert_slice(gpa, last_arg_index, err_trace_block.instructions.items);
}

/// Return the Value corresponding to a given AIR ref, or `null` if it refers to a runtime value.
/// InternPool key `variable` is considered a runtime value.
/// Generic poison causes `error.GenericPoison` to be returned.
fn resolve_value(sema: *Sema, inst: Air.Inst.Ref) CompileError!?Value {
    const val = (try sema.resolve_value_allow_variables(inst)) orelse return null;
    if (val.is_generic_poison()) return error.GenericPoison;
    if (sema.mod.intern_pool.is_variable(val.to_intern())) return null;
    return val;
}

/// Like `resolve_value`, but emits an error if the value is not comptime-known.
fn resolve_const_value(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    inst: Air.Inst.Ref,
    reason: NeededComptimeReason,
) CompileError!Value {
    return try sema.resolve_value(inst) orelse {
        return sema.fail_with_needed_comptime(block, src, reason);
    };
}

/// Like `resolve_value`, but emits an error if the value is comptime-known to be undefined.
fn resolve_defined_value(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    air_ref: Air.Inst.Ref,
) CompileError!?Value {
    const mod = sema.mod;
    const val = try sema.resolve_value(air_ref) orelse return null;
    if (val.is_undef(mod)) {
        return sema.fail_with_use_of_undef(block, src);
    }
    return val;
}

/// Like `resolve_value`, but emits an error if the value is not comptime-known or is undefined.
fn resolve_const_defined_value(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    air_ref: Air.Inst.Ref,
    reason: NeededComptimeReason,
) CompileError!Value {
    const val = try sema.resolve_const_value(block, src, air_ref, reason);
    if (val.is_undef(sema.mod)) return sema.fail_with_use_of_undef(block, src);
    return val;
}

/// Like `resolve_value`, but recursively resolves lazy values before returning.
fn resolve_value_resolve_lazy(sema: *Sema, inst: Air.Inst.Ref) CompileError!?Value {
    return try sema.resolve_lazy_value((try sema.resolve_value(inst)) orelse return null);
}

/// Like `resolve_value`, but any pointer value which does not correspond
/// to a comptime-known integer (e.g. a decl pointer) returns `null`.
/// Lazy values are recursively resolved.
fn resolve_value_intable(sema: *Sema, inst: Air.Inst.Ref) CompileError!?Value {
    const val = (try sema.resolve_value(inst)) orelse return null;
    if (sema.mod.intern_pool.get_backing_addr_tag(val.to_intern())) |addr| switch (addr) {
        .decl, .anon_decl, .comptime_alloc, .comptime_field => return null,
        .int => {},
        .eu_payload, .opt_payload, .arr_elem, .field => unreachable,
    };
    return try sema.resolve_lazy_value(val);
}

/// Returns all InternPool keys representing values, including `variable`, `undef`, and `generic_poison`.
fn resolve_value_allow_variables(sema: *Sema, inst: Air.Inst.Ref) CompileError!?Value {
    assert(inst != .none);
    // First section of indexes correspond to a set number of constant values.
    if (@int_from_enum(inst) < InternPool.static_len) {
        return Value.from_interned(@as(InternPool.Index, @enumFromInt(@int_from_enum(inst))));
    }

    const air_tags = sema.air_instructions.items(.tag);
    if (try sema.type_has_one_possible_value(sema.type_of(inst))) |opv| {
        if (inst.to_interned()) |ip_index| {
            const val = Value.from_interned(ip_index);
            if (val.get_variable(sema.mod) != null) return val;
        }
        return opv;
    }
    const ip_index = inst.to_interned() orelse {
        switch (air_tags[@int_from_enum(inst.to_index().?)]) {
            .inferred_alloc => unreachable,
            .inferred_alloc_comptime => unreachable,
            else => return null,
        }
    };
    const val = Value.from_interned(ip_index);
    if (val.is_ptr_to_thread_local(sema.mod)) return null;
    return val;
}

/// Returns a compile error if the value has tag `variable`.
fn resolve_inst_const(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    zir_ref: Zir.Inst.Ref,
    reason: NeededComptimeReason,
) CompileError!Value {
    const air_ref = try sema.resolve_inst(zir_ref);
    return sema.resolve_const_defined_value(block, src, air_ref, reason);
}

/// Value Tag may be `undef` or `variable`.
pub fn resolve_final_decl_value(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    air_ref: Air.Inst.Ref,
) CompileError!Value {
    const val = try sema.resolve_value_allow_variables(air_ref) orelse {
        return sema.fail_with_needed_comptime(block, src, .{
            .needed_comptime_reason = "global variable initializer must be comptime-known",
        });
    };
    if (val.is_generic_poison()) return error.GenericPoison;
    if (val.can_mutate_comptime_var_state(sema.mod)) {
        return sema.fail(block, src, "global variable contains reference to comptime var", .{});
    }
    return val;
}

fn fail_with_needed_comptime(sema: *Sema, block: *Block, src: LazySrcLoc, reason: NeededComptimeReason) CompileError {
    const msg = msg: {
        const msg = try sema.err_msg(block, src, "unable to resolve comptime value", .{});
        errdefer msg.destroy(sema.gpa);
        try sema.err_note(block, src, msg, "{s}", .{reason.needed_comptime_reason});

        if (reason.block_comptime_reason) |block_comptime_reason| {
            try block_comptime_reason.explain(sema, msg);
        }
        break :msg msg;
    };
    return sema.fail_with_owned_error_msg(block, msg);
}

fn fail_with_use_of_undef(sema: *Sema, block: *Block, src: LazySrcLoc) CompileError {
    return sema.fail(block, src, "use of undefined value here causes undefined behavior", .{});
}

fn fail_with_divide_by_zero(sema: *Sema, block: *Block, src: LazySrcLoc) CompileError {
    return sema.fail(block, src, "division by zero here causes undefined behavior", .{});
}

fn fail_with_mod_rem_negative(sema: *Sema, block: *Block, src: LazySrcLoc, lhs_ty: Type, rhs_ty: Type) CompileError {
    return sema.fail(block, src, "remainder division with '{}' and '{}': signed integers and floats must use @rem or @mod", .{
        lhs_ty.fmt(sema.mod), rhs_ty.fmt(sema.mod),
    });
}

fn fail_with_expected_optional_type(sema: *Sema, block: *Block, src: LazySrcLoc, non_optional_ty: Type) CompileError {
    const mod = sema.mod;
    const msg = msg: {
        const msg = try sema.err_msg(block, src, "expected optional type, found '{}'", .{
            non_optional_ty.fmt(mod),
        });
        errdefer msg.destroy(sema.gpa);
        if (non_optional_ty.zig_type_tag(mod) == .ErrorUnion) {
            try sema.err_note(block, src, msg, "consider using 'try', 'catch', or 'if'", .{});
        }
        try add_declared_here_note(sema, msg, non_optional_ty);
        break :msg msg;
    };
    return sema.fail_with_owned_error_msg(block, msg);
}

fn fail_with_array_init_not_supported(sema: *Sema, block: *Block, src: LazySrcLoc, ty: Type) CompileError {
    const mod = sema.mod;
    const msg = msg: {
        const msg = try sema.err_msg(block, src, "type '{}' does not support array initialization syntax", .{
            ty.fmt(mod),
        });
        errdefer msg.destroy(sema.gpa);
        if (ty.is_slice(mod)) {
            try sema.err_note(block, src, msg, "inferred array length is specified with an underscore: '[_]{}'", .{ty.elem_type2(mod).fmt(mod)});
        }
        break :msg msg;
    };
    return sema.fail_with_owned_error_msg(block, msg);
}

fn fail_with_struct_init_not_supported(sema: *Sema, block: *Block, src: LazySrcLoc, ty: Type) CompileError {
    return sema.fail(block, src, "type '{}' does not support struct initialization syntax", .{
        ty.fmt(sema.mod),
    });
}

fn fail_with_error_set_code_missing(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    dest_err_set_ty: Type,
    src_err_set_ty: Type,
) CompileError {
    return sema.fail(block, src, "expected type '{}', found type '{}'", .{
        dest_err_set_ty.fmt(sema.mod), src_err_set_ty.fmt(sema.mod),
    });
}

fn fail_with_integer_overflow(sema: *Sema, block: *Block, src: LazySrcLoc, int_ty: Type, val: Value, vector_index: usize) CompileError {
    const zcu = sema.mod;
    if (int_ty.zig_type_tag(zcu) == .Vector) {
        const msg = msg: {
            const msg = try sema.err_msg(block, src, "overflow of vector type '{}' with value '{}'", .{
                int_ty.fmt(zcu), val.fmt_value(zcu, sema),
            });
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, src, msg, "when computing vector element at index '{d}'", .{vector_index});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }
    return sema.fail(block, src, "overflow of integer type '{}' with value '{}'", .{
        int_ty.fmt(zcu), val.fmt_value(zcu, sema),
    });
}

fn fail_with_invalid_comptime_field_store(sema: *Sema, block: *Block, init_src: LazySrcLoc, container_ty: Type, field_index: usize) CompileError {
    const mod = sema.mod;
    const msg = msg: {
        const msg = try sema.err_msg(block, init_src, "value stored in comptime field does not match the default value of the field", .{});
        errdefer msg.destroy(sema.gpa);

        const struct_type = mod.type_to_struct(container_ty) orelse break :msg msg;
        const default_value_src = mod.field_src_loc(struct_type.decl.unwrap().?, .{
            .index = field_index,
            .range = .value,
        });
        try mod.err_note_non_lazy(default_value_src, msg, "default value set here", .{});
        break :msg msg;
    };
    return sema.fail_with_owned_error_msg(block, msg);
}

fn fail_with_use_of_async(sema: *Sema, block: *Block, src: LazySrcLoc) CompileError {
    const msg = msg: {
        const msg = try sema.err_msg(block, src, "async has not been implemented in the self-hosted compiler yet", .{});
        errdefer msg.destroy(sema.gpa);
        break :msg msg;
    };
    return sema.fail_with_owned_error_msg(block, msg);
}

fn fail_with_invalid_field_access(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    object_ty: Type,
    field_name: InternPool.NullTerminatedString,
) CompileError {
    const mod = sema.mod;
    const inner_ty = if (object_ty.is_single_pointer(mod)) object_ty.child_type(mod) else object_ty;

    if (inner_ty.zig_type_tag(mod) == .Optional) opt: {
        const child_ty = inner_ty.optional_child(mod);
        if (!type_supports_field_access(mod, child_ty, field_name)) break :opt;
        const msg = msg: {
            const msg = try sema.err_msg(block, src, "optional type '{}' does not support field access", .{object_ty.fmt(sema.mod)});
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, src, msg, "consider using '.?', 'orelse', or 'if'", .{});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    } else if (inner_ty.zig_type_tag(mod) == .ErrorUnion) err: {
        const child_ty = inner_ty.error_union_payload(mod);
        if (!type_supports_field_access(mod, child_ty, field_name)) break :err;
        const msg = msg: {
            const msg = try sema.err_msg(block, src, "error union type '{}' does not support field access", .{object_ty.fmt(sema.mod)});
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, src, msg, "consider using 'try', 'catch', or 'if'", .{});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }
    return sema.fail(block, src, "type '{}' does not support field access", .{object_ty.fmt(sema.mod)});
}

fn type_supports_field_access(mod: *const Module, ty: Type, field_name: InternPool.NullTerminatedString) bool {
    const ip = &mod.intern_pool;
    switch (ty.zig_type_tag(mod)) {
        .Array => return field_name.eql_slice("len", ip),
        .Pointer => {
            const ptr_info = ty.ptr_info(mod);
            if (ptr_info.flags.size == .Slice) {
                return field_name.eql_slice("ptr", ip) or field_name.eql_slice("len", ip);
            } else if (Type.from_interned(ptr_info.child).zig_type_tag(mod) == .Array) {
                return field_name.eql_slice("len", ip);
            } else return false;
        },
        .Type, .Struct, .Union => return true,
        else => return false,
    }
}

fn fail_with_comptime_error_ret_trace(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    name: InternPool.NullTerminatedString,
) CompileError {
    const mod = sema.mod;
    const msg = msg: {
        const msg = try sema.err_msg(block, src, "caught unexpected error '{}'", .{name.fmt(&mod.intern_pool)});
        errdefer msg.destroy(sema.gpa);

        for (sema.comptime_err_ret_trace.items) |src_loc| {
            try mod.err_note_non_lazy(src_loc, msg, "error returned here", .{});
        }
        break :msg msg;
    };
    return sema.fail_with_owned_error_msg(block, msg);
}

/// We don't return a pointer to the new error note because the pointer
/// becomes invalid when you add another one.
fn err_note(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    parent: *Module.ErrorMsg,
    comptime format: []const u8,
    args: anytype,
) error{OutOfMemory}!void {
    const mod = sema.mod;
    const src_decl = mod.decl_ptr(block.src_decl);
    return mod.err_note_non_lazy(src_decl.to_src_loc(src, mod), parent, format, args);
}

fn add_field_err_note(
    sema: *Sema,
    container_ty: Type,
    field_index: usize,
    parent: *Module.ErrorMsg,
    comptime format: []const u8,
    args: anytype,
) !void {
    @setCold(true);
    const mod = sema.mod;
    const decl_index = container_ty.get_owner_decl(mod);
    const decl = mod.decl_ptr(decl_index);

    const field_src = blk: {
        const tree = decl.get_file_scope(mod).get_tree(sema.gpa) catch |err| {
            log.err("unable to load AST to report compile error: {s}", .{@errorName(err)});
            break :blk decl.src_loc(mod);
        };

        const container_node = decl.relative_to_node_index(0);
        const node_tags = tree.nodes.items(.tag);
        var buf: [2]std.zig.Ast.Node.Index = undefined;
        const container_decl = tree.full_container_decl(&buf, container_node) orelse break :blk decl.src_loc(mod);

        var it_index: usize = 0;
        for (container_decl.ast.members) |member_node| {
            switch (node_tags[member_node]) {
                .container_field_init,
                .container_field_align,
                .container_field,
                => {
                    if (it_index == field_index) {
                        break :blk decl.node_offset_src_loc(decl.node_index_to_relative(member_node), mod);
                    }
                    it_index += 1;
                },
                else => continue,
            }
        }
        unreachable;
    };
    try mod.err_note_non_lazy(field_src, parent, format, args);
}

pub fn err_msg(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    comptime format: []const u8,
    args: anytype,
) error{ NeededSourceLocation, OutOfMemory }!*Module.ErrorMsg {
    const mod = sema.mod;
    if (src == .unneeded) return error.NeededSourceLocation;
    const src_decl = mod.decl_ptr(block.src_decl);
    return Module.ErrorMsg.create(sema.gpa, src_decl.to_src_loc(src, mod), format, args);
}

pub fn fail(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    comptime format: []const u8,
    args: anytype,
) CompileError {
    const err_msg = try sema.err_msg(block, src, format, args);
    inline for (args) |arg| {
        if (@TypeOf(arg) == Type.Formatter) {
            try add_declared_here_note(sema, err_msg, arg.data.ty);
        }
    }
    return sema.fail_with_owned_error_msg(block, err_msg);
}

pub fn fail_with_owned_error_msg(sema: *Sema, block: ?*Block, err_msg: *Module.ErrorMsg) error{ AnalysisFail, OutOfMemory } {
    @setCold(true);
    const gpa = sema.gpa;
    const mod = sema.mod;

    ref: {
        errdefer err_msg.destroy(gpa);

        if (build_options.enable_debug_extensions and mod.comp.debug_compile_errors) {
            var wip_errors: std.zig.ErrorBundle.Wip = undefined;
            wip_errors.init(gpa) catch unreachable;
            Compilation.add_module_error_msg(mod, &wip_errors, err_msg.*) catch unreachable;
            std.debug.print("compile error during Sema:\n", .{});
            var error_bundle = wip_errors.to_owned_bundle("") catch unreachable;
            error_bundle.render_to_std_err(.{ .ttyconf = .no_color });
            crash_report.compiler_panic("unexpected compile error occurred", null, null);
        }

        try mod.failed_decls.ensure_unused_capacity(gpa, 1);
        try mod.failed_files.ensure_unused_capacity(gpa, 1);

        if (block) |start_block| {
            var block_it = start_block;
            while (block_it.inlining) |inlining| {
                try sema.err_note(
                    inlining.call_block,
                    inlining.call_src,
                    err_msg,
                    "called from here",
                    .{},
                );
                block_it = inlining.call_block;
            }

            const max_references = refs: {
                if (mod.comp.reference_trace) |num| break :refs num;
                // Do not add multiple traces without explicit request.
                if (mod.failed_decls.count() > 0) break :ref;
                break :refs default_reference_trace_len;
            };

            var referenced_by = if (sema.owner_func_index != .none)
                mod.func_owner_decl_index(sema.owner_func_index)
            else
                sema.owner_decl_index;
            var reference_stack = std.ArrayList(Module.ErrorMsg.Trace).init(gpa);
            defer reference_stack.deinit();

            // Avoid infinite loops.
            var seen = std.AutoHashMap(InternPool.DeclIndex, void).init(gpa);
            defer seen.deinit();

            while (mod.reference_table.get(referenced_by)) |ref| {
                const gop = try seen.get_or_put(ref.referencer);
                if (gop.found_existing) break;
                if (reference_stack.items.len < max_references) {
                    const decl = mod.decl_ptr(ref.referencer);
                    try reference_stack.append(.{
                        .decl = decl.name,
                        .src_loc = decl.to_src_loc(ref.src, mod),
                    });
                }
                referenced_by = ref.referencer;
            }
            err_msg.reference_trace = try reference_stack.to_owned_slice();
            err_msg.hidden_references = @int_cast(seen.count() -| max_references);
        }
    }
    const ip = &mod.intern_pool;
    if (sema.owner_func_index != .none) {
        ip.func_analysis(sema.owner_func_index).state = .sema_failure;
    } else {
        sema.owner_decl.analysis = .sema_failure;
    }
    if (sema.func_index != .none) {
        ip.func_analysis(sema.func_index).state = .sema_failure;
    }
    const gop = mod.failed_decls.get_or_put_assume_capacity(sema.owner_decl_index);
    if (gop.found_existing) {
        // If there are multiple errors for the same Decl, prefer the first one added.
        sema.err = null;
        err_msg.destroy(gpa);
    } else {
        sema.err = err_msg;
        gop.value_ptr.* = err_msg;
    }
    return error.AnalysisFail;
}

/// Given an ErrorMsg, modify its message and source location to the given values, turning the
/// original message into a note. Notes on the original message are preserved as further notes.
/// Reference trace is preserved.
fn reparent_owned_error_msg(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    msg: *Module.ErrorMsg,
    comptime format: []const u8,
    args: anytype,
) !void {
    const mod = sema.mod;
    const src_decl = mod.decl_ptr(block.src_decl);
    const resolved_src = src_decl.to_src_loc(src, mod);
    const msg_str = try std.fmt.alloc_print(mod.gpa, format, args);

    const orig_notes = msg.notes.len;
    msg.notes = try sema.gpa.realloc(msg.notes, orig_notes + 1);
    std.mem.copy_backwards(Module.ErrorMsg, msg.notes[1..], msg.notes[0..orig_notes]);
    msg.notes[0] = .{
        .src_loc = msg.src_loc,
        .msg = msg.msg,
    };

    msg.src_loc = resolved_src;
    msg.msg = msg_str;
}

const align_ty = Type.u29;

pub fn analyze_as_align(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    air_ref: Air.Inst.Ref,
) !Alignment {
    const alignment_big = try sema.analyze_as_int(block, src, air_ref, align_ty, .{
        .needed_comptime_reason = "alignment must be comptime-known",
    });
    return sema.validate_align(block, src, alignment_big);
}

fn validate_align(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    alignment: u64,
) !Alignment {
    const result = try validate_align_allow_zero(sema, block, src, alignment);
    if (result == .none) return sema.fail(block, src, "alignment must be >= 1", .{});
    return result;
}

fn validate_align_allow_zero(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    alignment: u64,
) !Alignment {
    if (alignment == 0) return .none;
    if (!std.math.is_power_of_two(alignment)) {
        return sema.fail(block, src, "alignment value '{d}' is not a power of two", .{
            alignment,
        });
    }
    return Alignment.from_nonzero_byte_units(alignment);
}

fn resolve_align(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    zir_ref: Zir.Inst.Ref,
) !Alignment {
    const air_ref = try sema.resolve_inst(zir_ref);
    return sema.analyze_as_align(block, src, air_ref);
}

fn resolve_int(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    zir_ref: Zir.Inst.Ref,
    dest_ty: Type,
    reason: NeededComptimeReason,
) !u64 {
    const air_ref = try sema.resolve_inst(zir_ref);
    return sema.analyze_as_int(block, src, air_ref, dest_ty, reason);
}

fn analyze_as_int(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    air_ref: Air.Inst.Ref,
    dest_ty: Type,
    reason: NeededComptimeReason,
) !u64 {
    const mod = sema.mod;
    const coerced = try sema.coerce(block, dest_ty, air_ref, src);
    const val = try sema.resolve_const_defined_value(block, src, coerced, reason);
    return (try val.get_unsigned_int_advanced(mod, sema)).?;
}

/// Given a ZIR extra index which points to a list of `Zir.Inst.Capture`,
/// resolves this into a list of `InternPool.CaptureValue` allocated by `arena`.
fn get_captures(sema: *Sema, block: *Block, type_src: LazySrcLoc, extra_index: usize, captures_len: u32) ![]InternPool.CaptureValue {
    const zcu = sema.mod;
    const ip = &zcu.intern_pool;
    const parent_captures: InternPool.CaptureValue.Slice = zcu.namespace_ptr(block.namespace).get_type(zcu).get_captures(zcu);

    const captures = try sema.arena.alloc(InternPool.CaptureValue, captures_len);

    for (sema.code.extra[extra_index..][0..captures_len], captures) |raw, *capture| {
        const zir_capture: Zir.Inst.Capture = @bit_cast(raw);
        capture.* = switch (zir_capture.unwrap()) {
            .nested => |parent_idx| parent_captures.get(ip)[parent_idx],
            .instruction_load => |ptr_inst| InternPool.CaptureValue.wrap(capture: {
                const ptr_ref = try sema.resolve_inst(ptr_inst.to_ref());
                const ptr_val = try sema.resolve_value(ptr_ref) orelse {
                    break :capture .{ .runtime = sema.type_of(ptr_ref).child_type(zcu).to_intern() };
                };
                // TODO: better source location
                const unresolved_loaded_val = try sema.pointer_deref(block, type_src, ptr_val, sema.type_of(ptr_ref)) orelse {
                    break :capture .{ .runtime = sema.type_of(ptr_ref).child_type(zcu).to_intern() };
                };
                const loaded_val = try sema.resolve_lazy_value(unresolved_loaded_val);
                if (loaded_val.can_mutate_comptime_var_state(zcu)) {
                    // TODO: source location of captured value
                    return sema.fail(block, type_src, "type capture contains reference to comptime var", .{});
                }
                break :capture .{ .@"comptime" = loaded_val.to_intern() };
            }),
            .instruction => |inst| InternPool.CaptureValue.wrap(capture: {
                const air_ref = try sema.resolve_inst(inst.to_ref());
                if (try sema.resolve_value_resolve_lazy(air_ref)) |val| {
                    if (val.can_mutate_comptime_var_state(zcu)) {
                        // TODO: source location of captured value
                        return sema.fail(block, type_src, "type capture contains reference to comptime var", .{});
                    }
                    break :capture .{ .@"comptime" = val.to_intern() };
                }
                break :capture .{ .runtime = sema.type_of(air_ref).to_intern() };
            }),
            .decl_val => |str| capture: {
                const decl_name = try ip.get_or_put_string(
                    sema.gpa,
                    sema.code.null_terminated_string(str),
                    .no_embedded_nulls,
                );
                const decl = try sema.lookup_identifier(block, .unneeded, decl_name); // TODO: could we need this src loc?
                break :capture InternPool.CaptureValue.wrap(.{ .decl_val = decl });
            },
            .decl_ref => |str| capture: {
                const decl_name = try ip.get_or_put_string(
                    sema.gpa,
                    sema.code.null_terminated_string(str),
                    .no_embedded_nulls,
                );
                const decl = try sema.lookup_identifier(block, .unneeded, decl_name); // TODO: could we need this src loc?
                break :capture InternPool.CaptureValue.wrap(.{ .decl_ref = decl });
            },
        };
    }

    return captures;
}

/// Given an `InternPool.WipNamespaceType` or `InternPool.WipEnumType`, apply
/// `sema.builtin_type_target_index` to it if necessary.
fn wrap_wip_ty(sema: *Sema, wip_ty: anytype) @TypeOf(wip_ty) {
    if (sema.builtin_type_target_index == .none) return wip_ty;
    var new = wip_ty;
    new.index = sema.builtin_type_target_index;
    sema.mod.intern_pool.resolve_builtin_type(new.index, wip_ty.index);
    return new;
}

/// Given a type just looked up in the `InternPool`, check whether it is
/// considered outdated on this update. If so, remove it from the pool
/// and return `true`.
fn maybe_remove_outdated_type(sema: *Sema, ty: InternPool.Index) !bool {
    const zcu = sema.mod;

    if (!zcu.comp.debug_incremental) return false;

    const decl_index = Type.from_interned(ty).get_owner_decl(zcu);
    const decl_as_depender = InternPool.Depender.wrap(.{ .decl = decl_index });
    const was_outdated = zcu.outdated.swap_remove(decl_as_depender) or
        zcu.potentially_outdated.swap_remove(decl_as_depender);
    if (!was_outdated) return false;
    _ = zcu.outdated_ready.swap_remove(decl_as_depender);
    zcu.intern_pool.remove_dependencies_for_depender(zcu.gpa, InternPool.Depender.wrap(.{ .decl = decl_index }));
    zcu.intern_pool.remove(ty);
    zcu.decl_ptr(decl_index).analysis = .dependency_failure;
    try zcu.mark_dependee_outdated(.{ .decl_val = decl_index });
    return true;
}

fn zir_struct_decl(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
    inst: Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;
    const small: Zir.Inst.StructDecl.Small = @bit_cast(extended.small);
    const extra = sema.code.extra_data(Zir.Inst.StructDecl, extended.operand);
    const src = extra.data.src();
    var extra_index = extra.end;

    const captures_len = if (small.has_captures_len) blk: {
        const captures_len = sema.code.extra[extra_index];
        extra_index += 1;
        break :blk captures_len;
    } else 0;
    const fields_len = if (small.has_fields_len) blk: {
        const fields_len = sema.code.extra[extra_index];
        extra_index += 1;
        break :blk fields_len;
    } else 0;
    const decls_len = if (small.has_decls_len) blk: {
        const decls_len = sema.code.extra[extra_index];
        extra_index += 1;
        break :blk decls_len;
    } else 0;

    const captures = try sema.get_captures(block, src, extra_index, captures_len);
    extra_index += captures_len;

    if (small.has_backing_int) {
        const backing_int_body_len = sema.code.extra[extra_index];
        extra_index += 1; // backing_int_body_len
        if (backing_int_body_len == 0) {
            extra_index += 1; // backing_int_ref
        } else {
            extra_index += backing_int_body_len; // backing_int_body_inst
        }
    }

    const struct_init: InternPool.StructTypeInit = .{
        .layout = small.layout,
        .fields_len = fields_len,
        .known_non_opv = small.known_non_opv,
        .requires_comptime = if (small.known_comptime_only) .yes else .unknown,
        .is_tuple = small.is_tuple,
        .any_comptime_fields = small.any_comptime_fields,
        .any_default_inits = small.any_default_inits,
        .inits_resolved = false,
        .any_aligned_fields = small.any_aligned_fields,
        .has_namespace = true or decls_len > 0, // TODO: see below
        .key = .{ .declared = .{
            .zir_index = try ip.track_zir(gpa, block.get_file_scope(mod), inst),
            .captures = captures,
        } },
    };
    const wip_ty = sema.wrap_wip_ty(switch (try ip.get_struct_type(gpa, struct_init)) {
        .existing => |ty| wip: {
            if (!try sema.maybe_remove_outdated_type(ty)) return Air.interned_to_ref(ty);
            break :wip (try ip.get_struct_type(gpa, struct_init)).wip;
        },
        .wip => |wip| wip,
    });
    errdefer wip_ty.cancel(ip);

    const new_decl_index = try sema.create_anonymous_decl_type_named(
        block,
        src,
        Value.from_interned(wip_ty.index),
        small.name_strategy,
        "struct",
        inst,
    );
    mod.decl_ptr(new_decl_index).owns_tv = true;
    errdefer mod.abort_anon_decl(new_decl_index);

    if (sema.mod.comp.debug_incremental) {
        try ip.add_dependency(
            sema.gpa,
            InternPool.Depender.wrap(.{ .decl = new_decl_index }),
            .{ .src_hash = try ip.track_zir(sema.gpa, block.get_file_scope(mod), inst) },
        );
    }

    // TODO: if AstGen tells us `@This` was not used in the fields, we can elide the namespace.
    const new_namespace_index: InternPool.OptionalNamespaceIndex = if (true or decls_len > 0) (try mod.create_namespace(.{
        .parent = block.namespace.to_optional(),
        .decl_index = new_decl_index,
        .file_scope = block.get_file_scope(mod),
    })).to_optional() else .none;
    errdefer if (new_namespace_index.unwrap()) |ns| mod.destroy_namespace(ns);

    if (new_namespace_index.unwrap()) |ns| {
        const decls = sema.code.body_slice(extra_index, decls_len);
        try mod.scan_namespace(ns, decls, mod.decl_ptr(new_decl_index));
    }

    try mod.finalize_anon_decl(new_decl_index);
    return Air.interned_to_ref(wip_ty.finish(ip, new_decl_index, new_namespace_index));
}

fn create_anonymous_decl_type_named(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    val: Value,
    name_strategy: Zir.Inst.NameStrategy,
    anon_prefix: []const u8,
    inst: ?Zir.Inst.Index,
) !InternPool.DeclIndex {
    const zcu = sema.mod;
    const ip = &zcu.intern_pool;
    const gpa = sema.gpa;
    const namespace = block.namespace;
    const src_decl = zcu.decl_ptr(block.src_decl);
    const src_node = src_decl.relative_to_node_index(src.node_offset.x);
    const new_decl_index = try zcu.allocate_new_decl(namespace, src_node);
    errdefer zcu.destroy_decl(new_decl_index);

    switch (name_strategy) {
        .anon => {
            // It would be neat to have "struct:line:column" but this name has
            // to survive incremental updates, where it may have been shifted down
            // or up to a different line, but unchanged, and thus not unnecessarily
            // semantically analyzed.
            // This name is also used as the key in the parent namespace so it cannot be
            // renamed.

            const name = ip.get_or_put_string_fmt(gpa, "{}__{s}_{d}", .{
                src_decl.name.fmt(ip), anon_prefix, @int_from_enum(new_decl_index),
            }, .no_embedded_nulls) catch unreachable;
            try zcu.init_new_anon_decl(new_decl_index, src_decl.src_line, val, name);
            return new_decl_index;
        },
        .parent => {
            const name = zcu.decl_ptr(block.src_decl).name;
            try zcu.init_new_anon_decl(new_decl_index, src_decl.src_line, val, name);
            return new_decl_index;
        },
        .func => {
            const fn_info = sema.code.get_fn_info(ip.func_zir_body_inst(sema.func_index).resolve(ip));
            const zir_tags = sema.code.instructions.items(.tag);

            var buf = std.ArrayList(u8).init(gpa);
            defer buf.deinit();

            const writer = buf.writer();
            try writer.print("{}(", .{zcu.decl_ptr(block.src_decl).name.fmt(ip)});

            var arg_i: usize = 0;
            for (fn_info.param_body) |zir_inst| switch (zir_tags[@int_from_enum(zir_inst)]) {
                .param, .param_comptime, .param_anytype, .param_anytype_comptime => {
                    const arg = sema.inst_map.get(zir_inst).?;
                    // If this is being called in a generic function then analyze_call will
                    // have already resolved the args and this will work.
                    // If not then this is a struct type being returned from a non-generic
                    // function and the name doesn't matter since it will later
                    // result in a compile error.
                    const arg_val = sema.resolve_const_value(block, .unneeded, arg, undefined) catch
                        return sema.create_anonymous_decl_type_named(block, src, val, .anon, anon_prefix, null);

                    if (arg_i != 0) try writer.write_byte(',');

                    // Limiting the depth here helps avoid type names getting too long, which
                    // in turn helps to avoid unreasonably long symbol names for namespaced
                    // symbols. Such names should ideally be human-readable, and additionally,
                    // some tooling may not support very long symbol names.
                    try writer.print("{}", .{Value.fmt_value_full(.{
                        .val = arg_val,
                        .mod = zcu,
                        .opt_sema = sema,
                        .depth = 1,
                    })});

                    arg_i += 1;
                    continue;
                },
                else => continue,
            };

            try writer.write_byte(')');
            const name = try ip.get_or_put_string(gpa, buf.items, .no_embedded_nulls);
            try zcu.init_new_anon_decl(new_decl_index, src_decl.src_line, val, name);
            return new_decl_index;
        },
        .dbg_var => {
            const ref = inst.?.to_ref();
            const zir_tags = sema.code.instructions.items(.tag);
            const zir_data = sema.code.instructions.items(.data);
            for (@int_from_enum(inst.?)..zir_tags.len) |i| switch (zir_tags[i]) {
                .dbg_var_ptr, .dbg_var_val => {
                    if (zir_data[i].str_op.operand != ref) continue;

                    const name = try ip.get_or_put_string_fmt(gpa, "{}.{s}", .{
                        src_decl.name.fmt(ip), zir_data[i].str_op.get_str(sema.code),
                    }, .no_embedded_nulls);
                    try zcu.init_new_anon_decl(new_decl_index, src_decl.src_line, val, name);
                    return new_decl_index;
                },
                else => {},
            };
            return sema.create_anonymous_decl_type_named(block, src, val, .anon, anon_prefix, null);
        },
    }
}

fn zir_enum_decl(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
    inst: Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;
    const small: Zir.Inst.EnumDecl.Small = @bit_cast(extended.small);
    const extra = sema.code.extra_data(Zir.Inst.EnumDecl, extended.operand);
    var extra_index: usize = extra.end;

    const src = extra.data.src();
    const tag_ty_src: LazySrcLoc = .{ .node_offset_container_tag = src.node_offset.x };

    const tag_type_ref = if (small.has_tag_type) blk: {
        const tag_type_ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
        extra_index += 1;
        break :blk tag_type_ref;
    } else .none;

    const captures_len = if (small.has_captures_len) blk: {
        const captures_len = sema.code.extra[extra_index];
        extra_index += 1;
        break :blk captures_len;
    } else 0;

    const body_len = if (small.has_body_len) blk: {
        const body_len = sema.code.extra[extra_index];
        extra_index += 1;
        break :blk body_len;
    } else 0;

    const fields_len = if (small.has_fields_len) blk: {
        const fields_len = sema.code.extra[extra_index];
        extra_index += 1;
        break :blk fields_len;
    } else 0;

    const decls_len = if (small.has_decls_len) blk: {
        const decls_len = sema.code.extra[extra_index];
        extra_index += 1;
        break :blk decls_len;
    } else 0;

    const captures = try sema.get_captures(block, src, extra_index, captures_len);
    extra_index += captures_len;

    const decls = sema.code.body_slice(extra_index, decls_len);
    extra_index += decls_len;

    const body = sema.code.body_slice(extra_index, body_len);
    extra_index += body.len;

    const bit_bags_count = std.math.div_ceil(usize, fields_len, 32) catch unreachable;
    const body_end = extra_index;
    extra_index += bit_bags_count;

    const any_values = for (sema.code.extra[body_end..][0..bit_bags_count]) |bag| {
        if (bag != 0) break true;
    } else false;

    const enum_init: InternPool.EnumTypeInit = .{
        .has_namespace = true or decls_len > 0, // TODO: see below
        .has_values = any_values,
        .tag_mode = if (small.nonexhaustive)
            .nonexhaustive
        else if (tag_type_ref == .none)
            .auto
        else
            .explicit,
        .fields_len = fields_len,
        .key = .{ .declared = .{
            .zir_index = try mod.intern_pool.track_zir(sema.gpa, block.get_file_scope(mod), inst),
            .captures = captures,
        } },
    };
    const wip_ty = sema.wrap_wip_ty(switch (try ip.get_enum_type(gpa, enum_init)) {
        .existing => |ty| wip: {
            if (!try sema.maybe_remove_outdated_type(ty)) return Air.interned_to_ref(ty);
            break :wip (try ip.get_enum_type(gpa, enum_init)).wip;
        },
        .wip => |wip| wip,
    });

    // Once this is `true`, we will not delete the decl or type even upon failure, since we
    // have finished constructing the type and are in the process of analyzing it.
    var done = false;

    errdefer if (!done) wip_ty.cancel(ip);

    const new_decl_index = try sema.create_anonymous_decl_type_named(
        block,
        src,
        Value.from_interned(wip_ty.index),
        small.name_strategy,
        "enum",
        inst,
    );
    const new_decl = mod.decl_ptr(new_decl_index);
    new_decl.owns_tv = true;
    errdefer if (!done) mod.abort_anon_decl(new_decl_index);

    if (sema.mod.comp.debug_incremental) {
        try mod.intern_pool.add_dependency(
            sema.gpa,
            InternPool.Depender.wrap(.{ .decl = new_decl_index }),
            .{ .src_hash = try mod.intern_pool.track_zir(sema.gpa, block.get_file_scope(mod), inst) },
        );
    }

    // TODO: if AstGen tells us `@This` was not used in the fields, we can elide the namespace.
    const new_namespace_index: InternPool.OptionalNamespaceIndex = if (true or decls_len > 0) (try mod.create_namespace(.{
        .parent = block.namespace.to_optional(),
        .decl_index = new_decl_index,
        .file_scope = block.get_file_scope(mod),
    })).to_optional() else .none;
    errdefer if (!done) if (new_namespace_index.unwrap()) |ns| mod.destroy_namespace(ns);

    if (new_namespace_index.unwrap()) |ns| {
        try mod.scan_namespace(ns, decls, new_decl);
    }

    // We've finished the initial construction of this type, and are about to perform analysis.
    // Set the decl and namespace appropriately, and don't destroy anything on failure.
    wip_ty.prepare(ip, new_decl_index, new_namespace_index);
    done = true;

    const int_tag_ty = ty: {
        // We create a block for the field type instructions because they
        // may need to reference Decls from inside the enum namespace.
        // Within the field type, default value, and alignment expressions, the "owner decl"
        // should be the enum itself.

        const prev_owner_decl = sema.owner_decl;
        const prev_owner_decl_index = sema.owner_decl_index;
        sema.owner_decl = new_decl;
        sema.owner_decl_index = new_decl_index;
        defer {
            sema.owner_decl = prev_owner_decl;
            sema.owner_decl_index = prev_owner_decl_index;
        }

        const prev_owner_func_index = sema.owner_func_index;
        sema.owner_func_index = .none;
        defer sema.owner_func_index = prev_owner_func_index;

        const prev_func_index = sema.func_index;
        sema.func_index = .none;
        defer sema.func_index = prev_func_index;

        var enum_block: Block = .{
            .parent = null,
            .sema = sema,
            .src_decl = new_decl_index,
            .namespace = new_namespace_index.unwrap() orelse block.namespace,
            .instructions = .{},
            .inlining = null,
            .is_comptime = true,
        };
        defer enum_block.instructions.deinit(sema.gpa);

        if (body.len != 0) {
            _ = try sema.analyze_inline_body(&enum_block, body, inst);
        }

        if (tag_type_ref != .none) {
            const ty = try sema.resolve_type(block, tag_ty_src, tag_type_ref);
            if (ty.zig_type_tag(mod) != .Int and ty.zig_type_tag(mod) != .ComptimeInt) {
                return sema.fail(block, tag_ty_src, "expected integer tag type, found '{}'", .{ty.fmt(sema.mod)});
            }
            break :ty ty;
        } else if (fields_len == 0) {
            break :ty try mod.int_type(.unsigned, 0);
        } else {
            const bits = std.math.log2_int_ceil(usize, fields_len);
            break :ty try mod.int_type(.unsigned, bits);
        }
    };

    wip_ty.set_tag_ty(ip, int_tag_ty.to_intern());

    if (small.nonexhaustive and int_tag_ty.to_intern() != .comptime_int_type) {
        if (fields_len > 1 and std.math.log2_int(u64, fields_len) == int_tag_ty.bit_size(mod)) {
            return sema.fail(block, src, "non-exhaustive enum specifies every value", .{});
        }
    }

    var bit_bag_index: usize = body_end;
    var cur_bit_bag: u32 = undefined;
    var field_i: u32 = 0;
    var last_tag_val: ?Value = null;
    while (field_i < fields_len) : (field_i += 1) {
        if (field_i % 32 == 0) {
            cur_bit_bag = sema.code.extra[bit_bag_index];
            bit_bag_index += 1;
        }
        const has_tag_value = @as(u1, @truncate(cur_bit_bag)) != 0;
        cur_bit_bag >>= 1;

        const field_name_index: Zir.NullTerminatedString = @enumFromInt(sema.code.extra[extra_index]);
        const field_name_zir = sema.code.null_terminated_string(field_name_index);
        extra_index += 2; // field name, doc comment

        const field_name = try mod.intern_pool.get_or_put_string(gpa, field_name_zir, .no_embedded_nulls);

        const tag_overflow = if (has_tag_value) overflow: {
            const tag_val_ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
            extra_index += 1;
            const tag_inst = try sema.resolve_inst(tag_val_ref);
            last_tag_val = sema.resolve_const_defined_value(block, .unneeded, tag_inst, undefined) catch |err| switch (err) {
                error.NeededSourceLocation => {
                    const value_src = mod.field_src_loc(new_decl_index, .{
                        .index = field_i,
                        .range = .value,
                    }).lazy;
                    _ = try sema.resolve_const_defined_value(block, value_src, tag_inst, .{
                        .needed_comptime_reason = "enum tag value must be comptime-known",
                    });
                    unreachable;
                },
                else => |e| return e,
            };
            if (!(try sema.int_fits_in_type(last_tag_val.?, int_tag_ty, null))) break :overflow true;
            last_tag_val = try mod.get_coerced(last_tag_val.?, int_tag_ty);
            if (wip_ty.next_field(&mod.intern_pool, field_name, last_tag_val.?.to_intern())) |conflict| {
                assert(conflict.kind == .value); // AstGen validated names are unique
                const value_src = mod.field_src_loc(new_decl_index, .{
                    .index = field_i,
                    .range = .value,
                }).lazy;
                const other_field_src = mod.field_src_loc(new_decl_index, .{ .index = conflict.prev_field_idx }).lazy;
                const msg = msg: {
                    const msg = try sema.err_msg(block, value_src, "enum tag value {} already taken", .{last_tag_val.?.fmt_value(sema.mod, sema)});
                    errdefer msg.destroy(gpa);
                    try sema.err_note(block, other_field_src, msg, "other occurrence here", .{});
                    break :msg msg;
                };
                return sema.fail_with_owned_error_msg(block, msg);
            }
            break :overflow false;
        } else if (any_values) overflow: {
            var overflow: ?usize = null;
            last_tag_val = if (last_tag_val) |val|
                try sema.int_add(val, try mod.int_value(int_tag_ty, 1), int_tag_ty, &overflow)
            else
                try mod.int_value(int_tag_ty, 0);
            if (overflow != null) break :overflow true;
            if (wip_ty.next_field(&mod.intern_pool, field_name, last_tag_val.?.to_intern())) |conflict| {
                assert(conflict.kind == .value); // AstGen validated names are unique
                const field_src = mod.field_src_loc(new_decl_index, .{ .index = field_i }).lazy;
                const other_field_src = mod.field_src_loc(new_decl_index, .{ .index = conflict.prev_field_idx }).lazy;
                const msg = msg: {
                    const msg = try sema.err_msg(block, field_src, "enum tag value {} already taken", .{last_tag_val.?.fmt_value(sema.mod, sema)});
                    errdefer msg.destroy(gpa);
                    try sema.err_note(block, other_field_src, msg, "other occurrence here", .{});
                    break :msg msg;
                };
                return sema.fail_with_owned_error_msg(block, msg);
            }
            break :overflow false;
        } else overflow: {
            assert(wip_ty.next_field(&mod.intern_pool, field_name, .none) == null);
            last_tag_val = try mod.int_value(Type.comptime_int, field_i);
            if (!try sema.int_fits_in_type(last_tag_val.?, int_tag_ty, null)) break :overflow true;
            last_tag_val = try mod.get_coerced(last_tag_val.?, int_tag_ty);
            break :overflow false;
        };

        if (tag_overflow) {
            const value_src = mod.field_src_loc(new_decl_index, .{
                .index = field_i,
                .range = if (has_tag_value) .value else .name,
            }).lazy;
            const msg = try sema.err_msg(block, value_src, "enumeration value '{}' too large for type '{}'", .{
                last_tag_val.?.fmt_value(mod, sema), int_tag_ty.fmt(mod),
            });
            return sema.fail_with_owned_error_msg(block, msg);
        }
    }

    try mod.finalize_anon_decl(new_decl_index);
    return Air.interned_to_ref(wip_ty.index);
}

fn zir_union_decl(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
    inst: Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;
    const small: Zir.Inst.UnionDecl.Small = @bit_cast(extended.small);
    const extra = sema.code.extra_data(Zir.Inst.UnionDecl, extended.operand);
    var extra_index: usize = extra.end;

    const src = extra.data.src();

    extra_index += @int_from_bool(small.has_tag_type);
    const captures_len = if (small.has_captures_len) blk: {
        const captures_len = sema.code.extra[extra_index];
        extra_index += 1;
        break :blk captures_len;
    } else 0;
    extra_index += @int_from_bool(small.has_body_len);
    const fields_len = if (small.has_fields_len) blk: {
        const fields_len = sema.code.extra[extra_index];
        extra_index += 1;
        break :blk fields_len;
    } else 0;

    const decls_len = if (small.has_decls_len) blk: {
        const decls_len = sema.code.extra[extra_index];
        extra_index += 1;
        break :blk decls_len;
    } else 0;

    const captures = try sema.get_captures(block, src, extra_index, captures_len);
    extra_index += captures_len;

    const union_init: InternPool.UnionTypeInit = .{
        .flags = .{
            .layout = small.layout,
            .status = .none,
            .runtime_tag = if (small.has_tag_type or small.auto_enum_tag)
                .tagged
            else if (small.layout != .auto)
                .none
            else switch (block.want_safety()) {
                true => .safety,
                false => .none,
            },
            .any_aligned_fields = small.any_aligned_fields,
            .requires_comptime = .unknown,
            .assumed_runtime_bits = false,
            .assumed_pointer_aligned = false,
            .alignment = .none,
        },
        .has_namespace = true or decls_len != 0, // TODO: see below
        .fields_len = fields_len,
        .enum_tag_ty = .none, // set later
        .field_types = &.{}, // set later
        .field_aligns = &.{}, // set later
        .key = .{ .declared = .{
            .zir_index = try ip.track_zir(gpa, block.get_file_scope(mod), inst),
            .captures = captures,
        } },
    };
    const wip_ty = sema.wrap_wip_ty(switch (try ip.get_union_type(gpa, union_init)) {
        .existing => |ty| wip: {
            if (!try sema.maybe_remove_outdated_type(ty)) return Air.interned_to_ref(ty);
            break :wip (try ip.get_union_type(gpa, union_init)).wip;
        },
        .wip => |wip| wip,
    });
    errdefer wip_ty.cancel(ip);

    const new_decl_index = try sema.create_anonymous_decl_type_named(
        block,
        src,
        Value.from_interned(wip_ty.index),
        small.name_strategy,
        "union",
        inst,
    );
    mod.decl_ptr(new_decl_index).owns_tv = true;
    errdefer mod.abort_anon_decl(new_decl_index);

    if (sema.mod.comp.debug_incremental) {
        try mod.intern_pool.add_dependency(
            sema.gpa,
            InternPool.Depender.wrap(.{ .decl = new_decl_index }),
            .{ .src_hash = try mod.intern_pool.track_zir(sema.gpa, block.get_file_scope(mod), inst) },
        );
    }

    // TODO: if AstGen tells us `@This` was not used in the fields, we can elide the namespace.
    const new_namespace_index: InternPool.OptionalNamespaceIndex = if (true or decls_len > 0) (try mod.create_namespace(.{
        .parent = block.namespace.to_optional(),
        .decl_index = new_decl_index,
        .file_scope = block.get_file_scope(mod),
    })).to_optional() else .none;
    errdefer if (new_namespace_index.unwrap()) |ns| mod.destroy_namespace(ns);

    if (new_namespace_index.unwrap()) |ns| {
        const decls = sema.code.body_slice(extra_index, decls_len);
        try mod.scan_namespace(ns, decls, mod.decl_ptr(new_decl_index));
    }

    try mod.finalize_anon_decl(new_decl_index);

    return Air.interned_to_ref(wip_ty.finish(ip, new_decl_index, new_namespace_index));
}

fn zir_opaque_decl(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
    inst: Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;

    const small: Zir.Inst.OpaqueDecl.Small = @bit_cast(extended.small);
    const extra = sema.code.extra_data(Zir.Inst.OpaqueDecl, extended.operand);
    var extra_index: usize = extra.end;

    const src = extra.data.src();

    const captures_len = if (small.has_captures_len) blk: {
        const captures_len = sema.code.extra[extra_index];
        extra_index += 1;
        break :blk captures_len;
    } else 0;

    const decls_len = if (small.has_decls_len) blk: {
        const decls_len = sema.code.extra[extra_index];
        extra_index += 1;
        break :blk decls_len;
    } else 0;

    const captures = try sema.get_captures(block, src, extra_index, captures_len);
    extra_index += captures_len;

    const opaque_init: InternPool.OpaqueTypeInit = .{
        .has_namespace = decls_len != 0,
        .key = .{ .declared = .{
            .zir_index = try ip.track_zir(gpa, block.get_file_scope(mod), inst),
            .captures = captures,
        } },
    };
    // No `wrap_wip_ty` needed as no std.builtin types are opaque.
    const wip_ty = switch (try ip.get_opaque_type(gpa, opaque_init)) {
        .existing => |ty| wip: {
            if (!try sema.maybe_remove_outdated_type(ty)) return Air.interned_to_ref(ty);
            break :wip (try ip.get_opaque_type(gpa, opaque_init)).wip;
        },
        .wip => |wip| wip,
    };
    errdefer wip_ty.cancel(ip);

    const new_decl_index = try sema.create_anonymous_decl_type_named(
        block,
        src,
        Value.from_interned(wip_ty.index),
        small.name_strategy,
        "opaque",
        inst,
    );
    mod.decl_ptr(new_decl_index).owns_tv = true;
    errdefer mod.abort_anon_decl(new_decl_index);

    if (sema.mod.comp.debug_incremental) {
        try ip.add_dependency(
            gpa,
            InternPool.Depender.wrap(.{ .decl = new_decl_index }),
            .{ .src_hash = try ip.track_zir(gpa, block.get_file_scope(mod), inst) },
        );
    }

    const new_namespace_index: InternPool.OptionalNamespaceIndex = if (decls_len > 0) (try mod.create_namespace(.{
        .parent = block.namespace.to_optional(),
        .decl_index = new_decl_index,
        .file_scope = block.get_file_scope(mod),
    })).to_optional() else .none;
    errdefer if (new_namespace_index.unwrap()) |ns| mod.destroy_namespace(ns);

    if (new_namespace_index.unwrap()) |ns| {
        const decls = sema.code.body_slice(extra_index, decls_len);
        try mod.scan_namespace(ns, decls, mod.decl_ptr(new_decl_index));
    }

    try mod.finalize_anon_decl(new_decl_index);

    return Air.interned_to_ref(wip_ty.finish(ip, new_decl_index, new_namespace_index));
}

fn zir_error_set_decl(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    name_strategy: Zir.Inst.NameStrategy,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const gpa = sema.gpa;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const extra = sema.code.extra_data(Zir.Inst.ErrorSetDecl, inst_data.payload_index);

    var names: InferredErrorSet.NameMap = .{};
    try names.ensure_unused_capacity(sema.arena, extra.data.fields_len);

    var extra_index: u32 = @int_cast(extra.end);
    const extra_index_end = extra_index + (extra.data.fields_len * 2);
    while (extra_index < extra_index_end) : (extra_index += 2) { // +2 to skip over doc_string
        const name_index: Zir.NullTerminatedString = @enumFromInt(sema.code.extra[extra_index]);
        const name = sema.code.null_terminated_string(name_index);
        const name_ip = try mod.intern_pool.get_or_put_string(gpa, name, .no_embedded_nulls);
        _ = try mod.get_error_value(name_ip);
        const result = names.get_or_put_assume_capacity(name_ip);
        assert(!result.found_existing); // verified in AstGen
    }

    const error_set_ty = try mod.error_set_from_unsorted_names(names.keys());

    const new_decl_index = try sema.create_anonymous_decl_type_named(
        block,
        src,
        error_set_ty.to_value(),
        name_strategy,
        "error",
        inst,
    );
    const new_decl = mod.decl_ptr(new_decl_index);
    new_decl.owns_tv = true;
    errdefer mod.abort_anon_decl(new_decl_index);

    const decl_val = sema.analyze_decl_val(block, src, new_decl_index);
    try mod.finalize_anon_decl(new_decl_index);
    return decl_val;
}

fn zir_ret_ptr(sema: *Sema, block: *Block) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    if (block.is_comptime or try sema.type_requires_comptime(sema.fn_ret_ty)) {
        try sema.resolve_type_fields(sema.fn_ret_ty);
        return sema.analyze_comptime_alloc(block, sema.fn_ret_ty, .none);
    }

    const target = sema.mod.get_target();
    const ptr_type = try sema.ptr_type(.{
        .child = sema.fn_ret_ty.to_intern(),
        .flags = .{ .address_space = target_util.default_address_space(target, .local) },
    });

    if (block.inlining != null) {
        // We are inlining a function call; this should be emitted as an alloc, not a ret_ptr.
        // TODO when functions gain result location support, the inlining struct in
        // Block should contain the return pointer, and we would pass that through here.
        try sema.queue_full_type_resolution(sema.fn_ret_ty);
        return block.add_ty(.alloc, ptr_type);
    }

    return block.add_ty(.ret_ptr, ptr_type);
}

fn zir_ref(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_tok;
    const operand = try sema.resolve_inst(inst_data.operand);
    return sema.analyze_ref(block, inst_data.src(), operand);
}

fn zir_ensure_result_used(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const operand = try sema.resolve_inst(inst_data.operand);
    const src = inst_data.src();

    return sema.ensure_result_used(block, sema.type_of(operand), src);
}

fn ensure_result_used(
    sema: *Sema,
    block: *Block,
    ty: Type,
    src: LazySrcLoc,
) CompileError!void {
    const mod = sema.mod;
    switch (ty.zig_type_tag(mod)) {
        .Void, .NoReturn => return,
        .ErrorSet => return sema.fail(block, src, "error set is ignored", .{}),
        .ErrorUnion => {
            const msg = msg: {
                const msg = try sema.err_msg(block, src, "error union is ignored", .{});
                errdefer msg.destroy(sema.gpa);
                try sema.err_note(block, src, msg, "consider using 'try', 'catch', or 'if'", .{});
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        },
        else => {
            const msg = msg: {
                const msg = try sema.err_msg(block, src, "value of type '{}' ignored", .{ty.fmt(sema.mod)});
                errdefer msg.destroy(sema.gpa);
                try sema.err_note(block, src, msg, "all non-void values must be used", .{});
                try sema.err_note(block, src, msg, "to discard the value, assign it to '_'", .{});
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        },
    }
}

fn zir_ensure_result_non_error(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const operand = try sema.resolve_inst(inst_data.operand);
    const src = inst_data.src();
    const operand_ty = sema.type_of(operand);
    switch (operand_ty.zig_type_tag(mod)) {
        .ErrorSet => return sema.fail(block, src, "error set is discarded", .{}),
        .ErrorUnion => {
            const msg = msg: {
                const msg = try sema.err_msg(block, src, "error union is discarded", .{});
                errdefer msg.destroy(sema.gpa);
                try sema.err_note(block, src, msg, "consider using 'try', 'catch', or 'if'", .{});
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        },
        else => return,
    }
}

fn zir_ensure_err_union_payload_void(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand = try sema.resolve_inst(inst_data.operand);
    const operand_ty = sema.type_of(operand);
    const err_union_ty = if (operand_ty.zig_type_tag(mod) == .Pointer)
        operand_ty.child_type(mod)
    else
        operand_ty;
    if (err_union_ty.zig_type_tag(mod) != .ErrorUnion) return;
    const payload_ty = err_union_ty.error_union_payload(mod).zig_type_tag(mod);
    if (payload_ty != .Void and payload_ty != .NoReturn) {
        const msg = msg: {
            const msg = try sema.err_msg(block, src, "error union payload is ignored", .{});
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, src, msg, "payload value can be explicitly ignored with '|_|'", .{});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }
}

fn zir_indexable_ptr_len(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const object = try sema.resolve_inst(inst_data.operand);

    return indexable_ptr_len(sema, block, src, object);
}

fn indexable_ptr_len(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    object: Air.Inst.Ref,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const object_ty = sema.type_of(object);
    const is_pointer_to = object_ty.is_single_pointer(mod);
    const indexable_ty = if (is_pointer_to) object_ty.child_type(mod) else object_ty;
    try check_indexable(sema, block, src, indexable_ty);
    const field_name = try mod.intern_pool.get_or_put_string(sema.gpa, "len", .no_embedded_nulls);
    return sema.field_val(block, src, object, field_name, src);
}

fn indexable_ptr_len_or_none(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    operand: Air.Inst.Ref,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const operand_ty = sema.type_of(operand);
    try check_mem_operand(sema, block, src, operand_ty);
    if (operand_ty.ptr_size(mod) == .Many) return .none;
    const field_name = try mod.intern_pool.get_or_put_string(sema.gpa, "len", .no_embedded_nulls);
    return sema.field_val(block, src, operand, field_name, src);
}

fn zir_alloc_extended(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    const gpa = sema.gpa;
    const extra = sema.code.extra_data(Zir.Inst.AllocExtended, extended.operand);
    const ty_src: LazySrcLoc = .{ .node_offset_var_decl_ty = extra.data.src_node };
    const align_src: LazySrcLoc = .{ .node_offset_var_decl_align = extra.data.src_node };
    const small: Zir.Inst.AllocExtended.Small = @bit_cast(extended.small);

    var extra_index: usize = extra.end;

    const var_ty: Type = if (small.has_type) blk: {
        const type_ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
        extra_index += 1;
        break :blk try sema.resolve_type(block, ty_src, type_ref);
    } else undefined;

    const alignment = if (small.has_align) blk: {
        const align_ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
        extra_index += 1;
        const alignment = try sema.resolve_align(block, align_src, align_ref);
        break :blk alignment;
    } else .none;

    if (block.is_comptime or small.is_comptime) {
        if (small.has_type) {
            return sema.analyze_comptime_alloc(block, var_ty, alignment);
        } else {
            try sema.air_instructions.append(gpa, .{
                .tag = .inferred_alloc_comptime,
                .data = .{ .inferred_alloc_comptime = .{
                    .alignment = alignment,
                    .is_const = small.is_const,
                    .ptr = undefined,
                } },
            });
            return @as(Air.Inst.Index, @enumFromInt(sema.air_instructions.len - 1)).to_ref();
        }
    }

    if (small.has_type) {
        if (!small.is_const) {
            try sema.validate_var_type(block, ty_src, var_ty, false);
        }
        const target = sema.mod.get_target();
        try sema.resolve_type_layout(var_ty);
        const ptr_type = try sema.ptr_type(.{
            .child = var_ty.to_intern(),
            .flags = .{
                .alignment = alignment,
                .address_space = target_util.default_address_space(target, .local),
            },
        });
        const ptr = try block.add_ty(.alloc, ptr_type);
        if (small.is_const) {
            const ptr_inst = ptr.to_index().?;
            try sema.maybe_comptime_allocs.put(gpa, ptr_inst, .{ .runtime_index = block.runtime_index });
            try sema.base_allocs.put(gpa, ptr_inst, ptr_inst);
        }
        return ptr;
    }

    const result_index = try block.add_inst_as_index(.{
        .tag = .inferred_alloc,
        .data = .{ .inferred_alloc = .{
            .alignment = alignment,
            .is_const = small.is_const,
        } },
    });
    try sema.unresolved_inferred_allocs.put_no_clobber(gpa, result_index, .{});
    if (small.is_const) {
        try sema.maybe_comptime_allocs.put(gpa, result_index, .{ .runtime_index = block.runtime_index });
        try sema.base_allocs.put(gpa, result_index, result_index);
    }
    return result_index.to_ref();
}

fn zir_alloc_comptime(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const ty_src: LazySrcLoc = .{ .node_offset_var_decl_ty = inst_data.src_node };
    const var_ty = try sema.resolve_type(block, ty_src, inst_data.operand);
    return sema.analyze_comptime_alloc(block, var_ty, .none);
}

fn zir_make_ptr_const(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const alloc = try sema.resolve_inst(inst_data.operand);
    const alloc_ty = sema.type_of(alloc);
    const ptr_info = alloc_ty.ptr_info(mod);
    const elem_ty = Type.from_interned(ptr_info.child);

    // If the alloc was created in a comptime scope, we already created a comptime alloc for it.
    // However, if the final constructed value does not reference comptime-mutable memory, we wish
    // to promote it to an anon decl.
    already_ct: {
        const ptr_val = try sema.resolve_value(alloc) orelse break :already_ct;

        // If this was a comptime inferred alloc, then `store_to_inferred_alloc_comptime`
        // might have already done our job and created an anon decl ref.
        switch (mod.intern_pool.index_to_key(ptr_val.to_intern())) {
            .ptr => |ptr| switch (ptr.base_addr) {
                .anon_decl => {
                    // The comptime-ification was already done for us.
                    // Just make sure the pointer is const.
                    return sema.make_ptr_const(block, alloc);
                },
                else => {},
            },
            else => {},
        }

        if (!sema.is_comptime_mutable_ptr(ptr_val)) break :already_ct;
        const ptr = mod.intern_pool.index_to_key(ptr_val.to_intern()).ptr;
        assert(ptr.byte_offset == 0);
        const alloc_index = ptr.base_addr.comptime_alloc;
        const ct_alloc = sema.get_comptime_alloc(alloc_index);
        const interned = try ct_alloc.val.intern(mod, sema.arena);
        if (interned.can_mutate_comptime_var_state(mod)) {
            // Preserve the comptime alloc, just make the pointer const.
            ct_alloc.val = .{ .interned = interned.to_intern() };
            ct_alloc.is_const = true;
            return sema.make_ptr_const(block, alloc);
        } else {
            // Promote the constant to an anon decl.
            const new_mut_ptr = Air.interned_to_ref(try mod.intern(.{ .ptr = .{
                .ty = alloc_ty.to_intern(),
                .base_addr = .{ .anon_decl = .{
                    .val = interned.to_intern(),
                    .orig_ty = alloc_ty.to_intern(),
                } },
                .byte_offset = 0,
            } }));
            return sema.make_ptr_const(block, new_mut_ptr);
        }
    }

    // Otherwise, check if the alloc is comptime-known despite being in a runtime scope.
    if (try sema.resolve_comptime_known_alloc_ptr(block, alloc, null)) |ptr_val| {
        return sema.make_ptr_const(block, Air.interned_to_ref(ptr_val));
    }

    if (try sema.type_requires_comptime(elem_ty)) {
        // The value was initialized through RLS, so we didn't detect the runtime condition earlier.
        // TODO: source location of runtime control flow
        const init_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
        return sema.fail(block, init_src, "value with comptime-only type '{}' depends on runtime control flow", .{elem_ty.fmt(mod)});
    }

    // This is a runtime value.
    return sema.make_ptr_const(block, alloc);
}

/// If `alloc` is an inferred allocation, `resolved_inferred_ty` is taken to be its resolved
/// type. Otherwise, it may be `null`, and the type will be inferred from `alloc`.
fn resolve_comptime_known_alloc_ptr(sema: *Sema, block: *Block, alloc: Air.Inst.Ref, resolved_alloc_ty: ?Type) CompileError!?InternPool.Index {
    const zcu = sema.mod;

    const alloc_ty = resolved_alloc_ty orelse sema.type_of(alloc);
    const ptr_info = alloc_ty.ptr_info(zcu);
    const elem_ty = Type.from_interned(ptr_info.child);

    const alloc_inst = alloc.to_index() orelse return null;
    const comptime_info = sema.maybe_comptime_allocs.fetch_remove(alloc_inst) orelse return null;
    const stores = comptime_info.value.stores.items(.inst);

    // Since the entry existed in `maybe_comptime_allocs`, the allocation is comptime-known.
    // We will resolve and return its value.

    // We expect to have emitted at least one store, unless the elem type is OPV.
    if (stores.len == 0) {
        const val = (try sema.type_has_one_possible_value(elem_ty)).?.to_intern();
        return sema.finish_resolve_comptime_known_alloc_ptr(block, alloc_ty, val, null, alloc_inst, comptime_info.value);
    }

    // In general, we want to create a comptime alloc of the correct type and
    // apply the stores to that alloc in order. However, before going to all
    // that effort, let's optimize for the common case of a single store.

    simple: {
        if (stores.len != 1) break :simple;
        const store_inst = sema.air_instructions.get(@int_from_enum(stores[0]));
        switch (store_inst.tag) {
            .store, .store_safe => {},
            .set_union_tag, .optional_payload_ptr_set, .errunion_payload_ptr_set => break :simple, // there's OPV stuff going on!
            else => unreachable,
        }
        if (store_inst.data.bin_op.lhs != alloc) break :simple;

        const val = store_inst.data.bin_op.rhs.to_interned().?;
        assert(zcu.intern_pool.type_of(val) == elem_ty.to_intern());
        return sema.finish_resolve_comptime_known_alloc_ptr(block, alloc_ty, val, null, alloc_inst, comptime_info.value);
    }

    // The simple strategy failed: we must create a mutable comptime alloc and
    // perform all of the runtime store operations at comptime.

    const ct_alloc = try sema.new_comptime_alloc(block, elem_ty, ptr_info.flags.alignment);

    const alloc_ptr = try zcu.intern(.{ .ptr = .{
        .ty = alloc_ty.to_intern(),
        .base_addr = .{ .comptime_alloc = ct_alloc },
        .byte_offset = 0,
    } });

    // Maps from pointers into the runtime allocs, to comptime-mutable pointers into the comptime alloc
    var ptr_mapping = std.AutoHashMap(Air.Inst.Index, InternPool.Index).init(sema.arena);
    try ptr_mapping.ensure_total_capacity(@int_cast(stores.len));
    ptr_mapping.put_assume_capacity(alloc_inst, alloc_ptr);

    // Whilst constructing our mapping, we will also initialize optional and error union payloads when
    // we encounter the corresponding pointers. For this reason, the ordering of `to_map` matters.
    var to_map = try std.ArrayList(Air.Inst.Index).init_capacity(sema.arena, stores.len);
    for (stores) |store_inst_idx| {
        const store_inst = sema.air_instructions.get(@int_from_enum(store_inst_idx));
        const ptr_to_map = switch (store_inst.tag) {
            .store, .store_safe => store_inst.data.bin_op.lhs.to_index().?, // Map the pointer being stored to.
            .set_union_tag => continue, // We can completely ignore these: we'll do it implicitly when we get the field pointer.
            .optional_payload_ptr_set, .errunion_payload_ptr_set => store_inst_idx, // Map the generated pointer itself.
            else => unreachable,
        };
        to_map.append_assume_capacity(ptr_to_map);
    }

    const tmp_air = sema.get_tmp_air();

    while (to_map.pop_or_null()) |air_ptr| {
        if (ptr_mapping.contains(air_ptr)) continue;
        const PointerMethod = union(enum) {
            same_addr,
            opt_payload,
            eu_payload,
            field: u32,
            elem: u64,
        };
        const inst_tag = tmp_air.instructions.items(.tag)[@int_from_enum(air_ptr)];
        const air_parent_ptr: Air.Inst.Ref, const method: PointerMethod = switch (inst_tag) {
            .struct_field_ptr => blk: {
                const data = tmp_air.extra_data(
                    Air.StructField,
                    tmp_air.instructions.items(.data)[@int_from_enum(air_ptr)].ty_pl.payload,
                ).data;
                break :blk .{
                    data.struct_operand,
                    .{ .field = data.field_index },
                };
            },
            .struct_field_ptr_index_0,
            .struct_field_ptr_index_1,
            .struct_field_ptr_index_2,
            .struct_field_ptr_index_3,
            => .{
                tmp_air.instructions.items(.data)[@int_from_enum(air_ptr)].ty_op.operand,
                .{ .field = switch (inst_tag) {
                    .struct_field_ptr_index_0 => 0,
                    .struct_field_ptr_index_1 => 1,
                    .struct_field_ptr_index_2 => 2,
                    .struct_field_ptr_index_3 => 3,
                    else => unreachable,
                } },
            },
            .ptr_slice_ptr_ptr => .{
                tmp_air.instructions.items(.data)[@int_from_enum(air_ptr)].ty_op.operand,
                .{ .field = Value.slice_ptr_index },
            },
            .ptr_slice_len_ptr => .{
                tmp_air.instructions.items(.data)[@int_from_enum(air_ptr)].ty_op.operand,
                .{ .field = Value.slice_len_index },
            },
            .ptr_elem_ptr => blk: {
                const data = tmp_air.extra_data(
                    Air.Bin,
                    tmp_air.instructions.items(.data)[@int_from_enum(air_ptr)].ty_pl.payload,
                ).data;
                const idx_val = (try sema.resolve_value(data.rhs)).?;
                break :blk .{
                    data.lhs,
                    .{ .elem = try idx_val.to_unsigned_int_advanced(sema) },
                };
            },
            .bitcast => .{
                tmp_air.instructions.items(.data)[@int_from_enum(air_ptr)].ty_op.operand,
                .same_addr,
            },
            .optional_payload_ptr_set => .{
                tmp_air.instructions.items(.data)[@int_from_enum(air_ptr)].ty_op.operand,
                .opt_payload,
            },
            .errunion_payload_ptr_set => .{
                tmp_air.instructions.items(.data)[@int_from_enum(air_ptr)].ty_op.operand,
                .eu_payload,
            },
            else => unreachable,
        };

        const decl_parent_ptr = ptr_mapping.get(air_parent_ptr.to_index().?) orelse {
            // Resolve the parent pointer first.
            // Note that we add in what seems like the wrong order, because we're popping from the end of this array.
            try to_map.append_slice(&.{ air_ptr, air_parent_ptr.to_index().? });
            continue;
        };
        const new_ptr_ty = tmp_air.type_of_index(air_ptr, &zcu.intern_pool).to_intern();
        const new_ptr = switch (method) {
            .same_addr => try zcu.intern_pool.get_coerced(sema.gpa, decl_parent_ptr, new_ptr_ty),
            .opt_payload => ptr: {
                // Set the optional to non-null at comptime.
                // If the payload is OPV, we must use that value instead of undef.
                const opt_ty = Value.from_interned(decl_parent_ptr).type_of(zcu).child_type(zcu);
                const payload_ty = opt_ty.optional_child(zcu);
                const payload_val = try sema.type_has_one_possible_value(payload_ty) orelse try zcu.undef_value(payload_ty);
                const opt_val = try zcu.intern(.{ .opt = .{
                    .ty = opt_ty.to_intern(),
                    .val = payload_val.to_intern(),
                } });
                try sema.store_ptr_val(block, .unneeded, Value.from_interned(decl_parent_ptr), Value.from_interned(opt_val), opt_ty);
                break :ptr (try Value.from_interned(decl_parent_ptr).ptr_opt_payload(sema)).to_intern();
            },
            .eu_payload => ptr: {
                // Set the error union to non-error at comptime.
                // If the payload is OPV, we must use that value instead of undef.
                const eu_ty = Value.from_interned(decl_parent_ptr).type_of(zcu).child_type(zcu);
                const payload_ty = eu_ty.error_union_payload(zcu);
                const payload_val = try sema.type_has_one_possible_value(payload_ty) orelse try zcu.undef_value(payload_ty);
                const eu_val = try zcu.intern(.{ .error_union = .{
                    .ty = eu_ty.to_intern(),
                    .val = .{ .payload = payload_val.to_intern() },
                } });
                try sema.store_ptr_val(block, .unneeded, Value.from_interned(decl_parent_ptr), Value.from_interned(eu_val), eu_ty);
                break :ptr (try Value.from_interned(decl_parent_ptr).ptr_eu_payload(sema)).to_intern();
            },
            .field => |idx| ptr: {
                const maybe_union_ty = Value.from_interned(decl_parent_ptr).type_of(zcu).child_type(zcu);
                if (zcu.type_to_union(maybe_union_ty)) |union_obj| {
                    // As this is a union field, we must store to the pointer now to set the tag.
                    // If the payload is OPV, there will not be a payload store, so we store that value.
                    // Otherwise, there will be a payload store to process later, so undef will suffice.
                    const payload_ty = Type.from_interned(union_obj.field_types.get(&zcu.intern_pool)[idx]);
                    const payload_val = try sema.type_has_one_possible_value(payload_ty) orelse try zcu.undef_value(payload_ty);
                    const tag_val = try zcu.enum_value_field_index(Type.from_interned(union_obj.enum_tag_ty), idx);
                    const store_val = try zcu.union_value(maybe_union_ty, tag_val, payload_val);
                    try sema.store_ptr_val(block, .unneeded, Value.from_interned(decl_parent_ptr), store_val, maybe_union_ty);
                }
                break :ptr (try Value.from_interned(decl_parent_ptr).ptr_field(idx, sema)).to_intern();
            },
            .elem => |idx| (try Value.from_interned(decl_parent_ptr).ptr_elem(idx, sema)).to_intern(),
        };
        try ptr_mapping.put(air_ptr, new_ptr);
    }

    // We have a correlation between AIR pointers and decl pointers. Perform all stores at comptime.
    // Any implicit stores performed by `optional_payload_ptr_set`, `errunion_payload_ptr_set`, or
    // `set_union_tag` instructions were already done above.

    for (stores) |store_inst_idx| {
        const store_inst = sema.air_instructions.get(@int_from_enum(store_inst_idx));
        switch (store_inst.tag) {
            .set_union_tag => {}, // Handled implicitly by field pointers above
            .optional_payload_ptr_set, .errunion_payload_ptr_set => {}, // Handled explicitly above
            .store, .store_safe => {
                const air_ptr_inst = store_inst.data.bin_op.lhs.to_index().?;
                const store_val = (try sema.resolve_value(store_inst.data.bin_op.rhs)).?;
                const new_ptr = ptr_mapping.get(air_ptr_inst).?;
                try sema.store_ptr_val(block, .unneeded, Value.from_interned(new_ptr), store_val, Type.from_interned(zcu.intern_pool.type_of(store_val.to_intern())));
            },
            else => unreachable,
        }
    }

    // The value is finalized - load it!
    const val = (try sema.pointer_deref(block, .unneeded, Value.from_interned(alloc_ptr), alloc_ty)).?.to_intern();
    return sema.finish_resolve_comptime_known_alloc_ptr(block, alloc_ty, val, ct_alloc, alloc_inst, comptime_info.value);
}

/// Given the resolved comptime-known value, rewrites the dead AIR to not
/// create a runtime stack allocation. Also places the resulting value into
/// either an anon decl ref or a comptime alloc depending on whether it
/// references comptime-mutable memory. If `existing_comptime_alloc` is
/// passed, it is a scratch allocation which already contains `result_val`.
/// Same return type as `resolve_comptime_known_alloc_ptr` so we can tail call.
fn finish_resolve_comptime_known_alloc_ptr(
    sema: *Sema,
    block: *Block,
    alloc_ty: Type,
    result_val: InternPool.Index,
    existing_comptime_alloc: ?ComptimeAllocIndex,
    alloc_inst: Air.Inst.Index,
    comptime_info: MaybeComptimeAlloc,
) CompileError!?InternPool.Index {
    const zcu = sema.mod;

    // We're almost done - we have the resolved comptime value. We just need to
    // eliminate the now-dead runtime instructions.

    // We will rewrite the AIR to eliminate the alloc and all stores to it.
    // This will cause instructions deriving field pointers etc of the alloc to
    // become invalid, however, since we are removing all stores to those pointers,
    // they will be eliminated by Liveness before they reach codegen.

    // The specifics of this instruction aren't really important: we just want
    // Liveness to elide it.
    const nop_inst: Air.Inst = .{ .tag = .bitcast, .data = .{ .ty_op = .{ .ty = .u8_type, .operand = .zero_u8 } } };

    sema.air_instructions.set(@int_from_enum(alloc_inst), nop_inst);
    for (comptime_info.stores.items(.inst)) |store_inst| {
        sema.air_instructions.set(@int_from_enum(store_inst), nop_inst);
    }

    if (Value.from_interned(result_val).can_mutate_comptime_var_state(zcu)) {
        const alloc_index = existing_comptime_alloc orelse a: {
            const idx = try sema.new_comptime_alloc(block, alloc_ty.child_type(zcu), alloc_ty.ptr_alignment(zcu));
            const alloc = sema.get_comptime_alloc(idx);
            alloc.val = .{ .interned = result_val };
            break :a idx;
        };
        sema.get_comptime_alloc(alloc_index).is_const = true;
        return try zcu.intern(.{ .ptr = .{
            .ty = alloc_ty.to_intern(),
            .base_addr = .{ .comptime_alloc = alloc_index },
            .byte_offset = 0,
        } });
    } else {
        return try zcu.intern(.{ .ptr = .{
            .ty = alloc_ty.to_intern(),
            .base_addr = .{ .anon_decl = .{
                .orig_ty = alloc_ty.to_intern(),
                .val = result_val,
            } },
            .byte_offset = 0,
        } });
    }
}

fn make_ptr_ty_const(sema: *Sema, ptr_ty: Type) CompileError!Type {
    var ptr_info = ptr_ty.ptr_info(sema.mod);
    ptr_info.flags.is_const = true;
    return sema.ptr_type(ptr_info);
}

fn make_ptr_const(sema: *Sema, block: *Block, alloc: Air.Inst.Ref) CompileError!Air.Inst.Ref {
    const alloc_ty = sema.type_of(alloc);
    const const_ptr_ty = try sema.make_ptr_ty_const(alloc_ty);

    // Detect if a comptime value simply needs to have its type changed.
    if (try sema.resolve_value(alloc)) |val| {
        return Air.interned_to_ref((try sema.mod.get_coerced(val, const_ptr_ty)).to_intern());
    }

    return block.add_bit_cast(const_ptr_ty, alloc);
}

fn zir_alloc_inferred_comptime(
    sema: *Sema,
    is_const: bool,
) CompileError!Air.Inst.Ref {
    const gpa = sema.gpa;

    try sema.air_instructions.append(gpa, .{
        .tag = .inferred_alloc_comptime,
        .data = .{ .inferred_alloc_comptime = .{
            .alignment = .none,
            .is_const = is_const,
            .ptr = undefined,
        } },
    });
    return @as(Air.Inst.Index, @enumFromInt(sema.air_instructions.len - 1)).to_ref();
}

fn zir_alloc(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const ty_src: LazySrcLoc = .{ .node_offset_var_decl_ty = inst_data.src_node };
    const var_ty = try sema.resolve_type(block, ty_src, inst_data.operand);
    if (block.is_comptime) {
        return sema.analyze_comptime_alloc(block, var_ty, .none);
    }
    const target = sema.mod.get_target();
    const ptr_type = try sema.ptr_type(.{
        .child = var_ty.to_intern(),
        .flags = .{ .address_space = target_util.default_address_space(target, .local) },
    });
    try sema.queue_full_type_resolution(var_ty);
    const ptr = try block.add_ty(.alloc, ptr_type);
    const ptr_inst = ptr.to_index().?;
    try sema.maybe_comptime_allocs.put(sema.gpa, ptr_inst, .{ .runtime_index = block.runtime_index });
    try sema.base_allocs.put(sema.gpa, ptr_inst, ptr_inst);
    return ptr;
}

fn zir_alloc_mut(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const ty_src: LazySrcLoc = .{ .node_offset_var_decl_ty = inst_data.src_node };
    const var_ty = try sema.resolve_type(block, ty_src, inst_data.operand);
    if (block.is_comptime) {
        return sema.analyze_comptime_alloc(block, var_ty, .none);
    }
    try sema.validate_var_type(block, ty_src, var_ty, false);
    const target = sema.mod.get_target();
    const ptr_type = try sema.ptr_type(.{
        .child = var_ty.to_intern(),
        .flags = .{ .address_space = target_util.default_address_space(target, .local) },
    });
    try sema.queue_full_type_resolution(var_ty);
    return block.add_ty(.alloc, ptr_type);
}

fn zir_alloc_inferred(
    sema: *Sema,
    block: *Block,
    is_const: bool,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const gpa = sema.gpa;

    if (block.is_comptime) {
        try sema.air_instructions.append(gpa, .{
            .tag = .inferred_alloc_comptime,
            .data = .{ .inferred_alloc_comptime = .{
                .alignment = .none,
                .is_const = is_const,
                .ptr = undefined,
            } },
        });
        return @as(Air.Inst.Index, @enumFromInt(sema.air_instructions.len - 1)).to_ref();
    }

    const result_index = try block.add_inst_as_index(.{
        .tag = .inferred_alloc,
        .data = .{ .inferred_alloc = .{
            .alignment = .none,
            .is_const = is_const,
        } },
    });
    try sema.unresolved_inferred_allocs.put_no_clobber(gpa, result_index, .{});
    if (is_const) {
        try sema.maybe_comptime_allocs.put(gpa, result_index, .{ .runtime_index = block.runtime_index });
        try sema.base_allocs.put(sema.gpa, result_index, result_index);
    }
    return result_index.to_ref();
}

fn zir_resolve_inferred_alloc(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const gpa = sema.gpa;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const ty_src: LazySrcLoc = .{ .node_offset_var_decl_ty = inst_data.src_node };
    const ptr = try sema.resolve_inst(inst_data.operand);
    const ptr_inst = ptr.to_index().?;
    const target = mod.get_target();

    switch (sema.air_instructions.items(.tag)[@int_from_enum(ptr_inst)]) {
        .inferred_alloc_comptime => {
            // The work was already done for us by `Sema.store_to_inferred_alloc_comptime`.
            // All we need to do is remap the pointer.
            const iac = sema.air_instructions.items(.data)[@int_from_enum(ptr_inst)].inferred_alloc_comptime;
            const resolved_ptr = iac.ptr;

            if (std.debug.runtime_safety) {
                // The inferred_alloc_comptime should never be referenced again
                sema.air_instructions.set(@int_from_enum(ptr_inst), .{ .tag = undefined, .data = undefined });
            }

            const val = switch (mod.intern_pool.index_to_key(resolved_ptr).ptr.base_addr) {
                .anon_decl => |a| a.val,
                .comptime_alloc => |i| val: {
                    const alloc = sema.get_comptime_alloc(i);
                    break :val (try alloc.val.intern(mod, sema.arena)).to_intern();
                },
                else => unreachable,
            };
            if (mod.intern_pool.is_func_body(val)) {
                const ty = Type.from_interned(mod.intern_pool.type_of(val));
                if (try sema.fn_has_runtime_bits(ty)) {
                    try mod.ensure_func_body_analysis_queued(val);
                }
            }

            // Remap the ZIR operand to the resolved pointer value
            sema.inst_map.put_assume_capacity(inst_data.operand.to_index().?, Air.interned_to_ref(resolved_ptr));
        },
        .inferred_alloc => {
            const ia1 = sema.air_instructions.items(.data)[@int_from_enum(ptr_inst)].inferred_alloc;
            const ia2 = sema.unresolved_inferred_allocs.fetch_swap_remove(ptr_inst).?.value;
            const peer_vals = try sema.arena.alloc(Air.Inst.Ref, ia2.prongs.items.len);
            for (peer_vals, ia2.prongs.items) |*peer_val, store_inst| {
                assert(sema.air_instructions.items(.tag)[@int_from_enum(store_inst)] == .store);
                const bin_op = sema.air_instructions.items(.data)[@int_from_enum(store_inst)].bin_op;
                peer_val.* = bin_op.rhs;
            }
            const final_elem_ty = try sema.resolve_peer_types(block, ty_src, peer_vals, .none);

            const final_ptr_ty = try sema.ptr_type(.{
                .child = final_elem_ty.to_intern(),
                .flags = .{
                    .alignment = ia1.alignment,
                    .address_space = target_util.default_address_space(target, .local),
                },
            });

            if (!ia1.is_const) {
                try sema.validate_var_type(block, ty_src, final_elem_ty, false);
            } else if (try sema.resolve_comptime_known_alloc_ptr(block, ptr, final_ptr_ty)) |ptr_val| {
                const const_ptr_ty = try sema.make_ptr_ty_const(final_ptr_ty);
                const new_const_ptr = try mod.get_coerced(Value.from_interned(ptr_val), const_ptr_ty);

                // Remap the ZIR operand to the resolved pointer value
                sema.inst_map.put_assume_capacity(inst_data.operand.to_index().?, Air.interned_to_ref(new_const_ptr.to_intern()));

                // Unless the block is comptime, `alloc_inferred` always produces
                // a runtime constant. The final inferred type needs to be
                // fully resolved so it can be lowered in codegen.
                try sema.resolve_type_fully(final_elem_ty);

                return;
            }

            if (try sema.type_requires_comptime(final_elem_ty)) {
                // The alloc wasn't comptime-known per the above logic, so the
                // type cannot be comptime-only.
                // TODO: source location of runtime control flow
                return sema.fail(block, src, "value with comptime-only type '{}' depends on runtime control flow", .{final_elem_ty.fmt(mod)});
            }

            try sema.queue_full_type_resolution(final_elem_ty);

            // Change it to a normal alloc.
            sema.air_instructions.set(@int_from_enum(ptr_inst), .{
                .tag = .alloc,
                .data = .{ .ty = final_ptr_ty },
            });

            if (ia1.is_const) {
                // Remap the ZIR operand to the pointer const
                sema.inst_map.put_assume_capacity(inst_data.operand.to_index().?, try sema.make_ptr_const(block, ptr));
            }

            // Now we need to go back over all the store instructions, and do the logic as if
            // the new result ptr type was available.

            for (ia2.prongs.items) |placeholder_inst| {
                var replacement_block = block.make_sub_block();
                defer replacement_block.instructions.deinit(gpa);

                assert(sema.air_instructions.items(.tag)[@int_from_enum(placeholder_inst)] == .store);
                const bin_op = sema.air_instructions.items(.data)[@int_from_enum(placeholder_inst)].bin_op;
                try sema.store_ptr2(&replacement_block, src, bin_op.lhs, src, bin_op.rhs, src, .store);

                // If only one instruction is produced then we can replace the store
                // placeholder instruction with this instruction; no need for an entire block.
                if (replacement_block.instructions.items.len == 1) {
                    const only_inst = replacement_block.instructions.items[0];
                    sema.air_instructions.set(@int_from_enum(placeholder_inst), sema.air_instructions.get(@int_from_enum(only_inst)));
                    continue;
                }

                // Here we replace the placeholder store instruction with a block
                // that does the actual store logic.
                _ = try replacement_block.add_br(placeholder_inst, .void_value);
                try sema.air_extra.ensure_unused_capacity(
                    gpa,
                    @typeInfo(Air.Block).Struct.fields.len + replacement_block.instructions.items.len,
                );
                sema.air_instructions.set(@int_from_enum(placeholder_inst), .{
                    .tag = .block,
                    .data = .{ .ty_pl = .{
                        .ty = .void_type,
                        .payload = sema.add_extra_assume_capacity(Air.Block{
                            .body_len = @int_cast(replacement_block.instructions.items.len),
                        }),
                    } },
                });
                sema.air_extra.append_slice_assume_capacity(@ptr_cast(replacement_block.instructions.items));
            }
        },
        else => unreachable,
    }
}

fn zir_for_len(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.MultiOp, inst_data.payload_index);
    const args = sema.code.ref_slice(extra.end, extra.data.operands_len);
    const src = inst_data.src();

    var len: Air.Inst.Ref = .none;
    var len_val: ?Value = null;
    var len_idx: u32 = undefined;
    var any_runtime = false;

    const runtime_arg_lens = try gpa.alloc(Air.Inst.Ref, args.len);
    defer gpa.free(runtime_arg_lens);

    // First pass to look for comptime values.
    for (args, 0..) |zir_arg, i_usize| {
        const i: u32 = @int_cast(i_usize);
        runtime_arg_lens[i] = .none;
        if (zir_arg == .none) continue;
        const object = try sema.resolve_inst(zir_arg);
        const object_ty = sema.type_of(object);
        // Each arg could be an indexable, or a range, in which case the length
        // is passed directly as an integer.
        const is_int = switch (object_ty.zig_type_tag(mod)) {
            .Int, .ComptimeInt => true,
            else => false,
        };
        const arg_src: LazySrcLoc = .{ .for_input = .{
            .for_node_offset = inst_data.src_node,
            .input_index = i,
        } };
        const arg_len_uncoerced = if (is_int) object else l: {
            if (!object_ty.is_indexable(mod)) {
                // Instead of using check_indexable we customize this error.
                const msg = msg: {
                    const msg = try sema.err_msg(block, arg_src, "type '{}' is not indexable and not a range", .{object_ty.fmt(sema.mod)});
                    errdefer msg.destroy(sema.gpa);
                    try sema.err_note(block, arg_src, msg, "for loop operand must be a range, array, slice, tuple, or vector", .{});

                    if (object_ty.zig_type_tag(mod) == .ErrorUnion) {
                        try sema.err_note(block, arg_src, msg, "consider using 'try', 'catch', or 'if'", .{});
                    }

                    break :msg msg;
                };
                return sema.fail_with_owned_error_msg(block, msg);
            }
            if (!object_ty.indexable_has_len(mod)) continue;

            break :l try sema.field_val(block, arg_src, object, try ip.get_or_put_string(gpa, "len", .no_embedded_nulls), arg_src);
        };
        const arg_len = try sema.coerce(block, Type.usize, arg_len_uncoerced, arg_src);
        if (len == .none) {
            len = arg_len;
            len_idx = i;
        }
        if (try sema.resolve_defined_value(block, src, arg_len)) |arg_val| {
            if (len_val) |v| {
                if (!(try sema.values_equal(arg_val, v, Type.usize))) {
                    const msg = msg: {
                        const msg = try sema.err_msg(block, src, "non-matching for loop lengths", .{});
                        errdefer msg.destroy(gpa);
                        const a_src: LazySrcLoc = .{ .for_input = .{
                            .for_node_offset = inst_data.src_node,
                            .input_index = len_idx,
                        } };
                        try sema.err_note(block, a_src, msg, "length {} here", .{
                            v.fmt_value(sema.mod, sema),
                        });
                        try sema.err_note(block, arg_src, msg, "length {} here", .{
                            arg_val.fmt_value(sema.mod, sema),
                        });
                        break :msg msg;
                    };
                    return sema.fail_with_owned_error_msg(block, msg);
                }
            } else {
                len = arg_len;
                len_val = arg_val;
                len_idx = i;
            }
            continue;
        }
        runtime_arg_lens[i] = arg_len;
        any_runtime = true;
    }

    if (len == .none) {
        const msg = msg: {
            const msg = try sema.err_msg(block, src, "unbounded for loop", .{});
            errdefer msg.destroy(gpa);
            for (args, 0..) |zir_arg, i_usize| {
                const i: u32 = @int_cast(i_usize);
                if (zir_arg == .none) continue;
                const object = try sema.resolve_inst(zir_arg);
                const object_ty = sema.type_of(object);
                // Each arg could be an indexable, or a range, in which case the length
                // is passed directly as an integer.
                switch (object_ty.zig_type_tag(mod)) {
                    .Int, .ComptimeInt => continue,
                    else => {},
                }
                const arg_src: LazySrcLoc = .{ .for_input = .{
                    .for_node_offset = inst_data.src_node,
                    .input_index = i,
                } };
                try sema.err_note(block, arg_src, msg, "type '{}' has no upper bound", .{
                    object_ty.fmt(sema.mod),
                });
            }
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    // Now for the runtime checks.
    if (any_runtime and block.want_safety()) {
        for (runtime_arg_lens, 0..) |arg_len, i| {
            if (arg_len == .none) continue;
            if (i == len_idx) continue;
            const ok = try block.add_bin_op(.cmp_eq, len, arg_len);
            try sema.add_safety_check(block, src, ok, .for_len_mismatch);
        }
    }

    return len;
}

/// Given any single pointer, retrieve a pointer to the payload of any optional
/// or error union pointed to, initializing these pointers along the way.
/// Given a `*E!?T`, returns a (valid) `*T`.
/// May invalidate already-stored payload data.
fn opt_eu_base_ptr_init(sema: *Sema, block: *Block, ptr: Air.Inst.Ref, src: LazySrcLoc) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    var base_ptr = ptr;
    while (true) switch (sema.type_of(base_ptr).child_type(mod).zig_type_tag(mod)) {
        .ErrorUnion => base_ptr = try sema.analyze_err_union_payload_ptr(block, src, base_ptr, false, true),
        .Optional => base_ptr = try sema.analyze_optional_payload_ptr(block, src, base_ptr, false, true),
        else => break,
    };
    try sema.check_known_alloc_ptr(block, ptr, base_ptr);
    return base_ptr;
}

fn zir_opt_eu_base_ptr_init(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const un_node = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const ptr = try sema.resolve_inst(un_node.operand);
    return sema.opt_eu_base_ptr_init(block, ptr, un_node.src());
}

fn zir_coerce_ptr_elem_ty(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const pl_node = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = pl_node.src();
    const extra = sema.code.extra_data(Zir.Inst.Bin, pl_node.payload_index).data;
    const uncoerced_val = try sema.resolve_inst(extra.rhs);
    const maybe_wrapped_ptr_ty = sema.resolve_type(block, .unneeded, extra.lhs) catch |err| switch (err) {
        error.GenericPoison => return uncoerced_val,
        else => |e| return e,
    };
    const ptr_ty = maybe_wrapped_ptr_ty.opt_eu_base_type(mod);
    assert(ptr_ty.zig_type_tag(mod) == .Pointer); // validated by a previous instruction
    const elem_ty = ptr_ty.child_type(mod);
    switch (ptr_ty.ptr_size(mod)) {
        .One => {
            const uncoerced_ty = sema.type_of(uncoerced_val);
            if (elem_ty.zig_type_tag(mod) == .Array and elem_ty.child_type(mod).to_intern() == uncoerced_ty.to_intern()) {
                // We're trying to initialize a *[1]T with a reference to a T - don't perform any coercion.
                return uncoerced_val;
            }
            // If the destination type is anyopaque, don't coerce - the pointer will coerce instead.
            if (elem_ty.to_intern() == .anyopaque_type) {
                return uncoerced_val;
            } else {
                return sema.coerce(block, elem_ty, uncoerced_val, src);
            }
        },
        .Slice, .Many => {
            // Our goal is to coerce `uncoerced_val` to an array of `elem_ty`.
            const val_ty = sema.type_of(uncoerced_val);
            switch (val_ty.zig_type_tag(mod)) {
                .Array, .Vector => {},
                else => if (!val_ty.is_tuple(mod)) {
                    return sema.fail(block, src, "expected array of '{}', found '{}'", .{ elem_ty.fmt(mod), val_ty.fmt(mod) });
                },
            }
            const want_ty = try mod.array_type(.{
                .len = val_ty.array_len(mod),
                .child = elem_ty.to_intern(),
                .sentinel = if (ptr_ty.sentinel(mod)) |s| s.to_intern() else .none,
            });
            return sema.coerce(block, want_ty, uncoerced_val, src);
        },
        .C => {
            // There's nothing meaningful to do here, because we don't know if this is meant to be a
            // single-pointer or a many-pointer.
            return uncoerced_val;
        },
    }
}

fn zir_validate_ref_ty(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const mod = sema.mod;
    const un_tok = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_tok;
    const src = un_tok.src();
    // In case of GenericPoison, we don't actually have a type, so this will be
    // treated as an untyped address-of operator.
    const operand_air_inst = sema.resolve_inst(un_tok.operand) catch |err| switch (err) {
        error.GenericPoison => return,
        else => |e| return e,
    };
    const ty_operand = sema.analyze_as_type(block, src, operand_air_inst) catch |err| switch (err) {
        error.GenericPoison => return,
        else => |e| return e,
    };
    if (ty_operand.is_generic_poison()) return;
    if (ty_operand.opt_eu_base_type(mod).zig_type_tag(mod) != .Pointer) {
        return sema.fail_with_owned_error_msg(block, msg: {
            const msg = try sema.err_msg(block, src, "expected type '{}', found pointer", .{ty_operand.fmt(mod)});
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, src, msg, "address-of operator always returns a pointer", .{});
            break :msg msg;
        });
    }
}

fn zir_validate_array_init_ref_ty(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const pl_node = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = pl_node.src();
    const extra = sema.code.extra_data(Zir.Inst.ArrayInitRefTy, pl_node.payload_index).data;
    const maybe_wrapped_ptr_ty = sema.resolve_type(block, .unneeded, extra.ptr_ty) catch |err| switch (err) {
        error.GenericPoison => return .generic_poison_type,
        else => |e| return e,
    };
    const ptr_ty = maybe_wrapped_ptr_ty.opt_eu_base_type(mod);
    assert(ptr_ty.zig_type_tag(mod) == .Pointer); // validated by a previous instruction
    switch (mod.intern_pool.index_to_key(ptr_ty.to_intern())) {
        .ptr_type => |ptr_type| switch (ptr_type.flags.size) {
            .Slice, .Many => {
                // Use array of correct length
                const arr_ty = try mod.array_type(.{
                    .len = extra.elem_count,
                    .child = ptr_ty.child_type(mod).to_intern(),
                    .sentinel = if (ptr_ty.sentinel(mod)) |s| s.to_intern() else .none,
                });
                return Air.interned_to_ref(arr_ty.to_intern());
            },
            else => {},
        },
        else => {},
    }
    // Otherwise, we just want the pointer child type
    const ret_ty = ptr_ty.child_type(mod);
    if (ret_ty.to_intern() == .anyopaque_type) {
        // The actual array type is unknown, which we represent with a generic poison.
        return .generic_poison_type;
    }
    const arr_ty = ret_ty.opt_eu_base_type(mod);
    try sema.validate_array_init_ty(block, src, src, extra.elem_count, arr_ty);
    return Air.interned_to_ref(ret_ty.to_intern());
}

fn zir_validate_array_init_ty(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    is_result_ty: bool,
) CompileError!void {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const ty_src: LazySrcLoc = if (is_result_ty) src else .{ .node_offset_init_ty = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.ArrayInit, inst_data.payload_index).data;
    const ty = sema.resolve_type(block, ty_src, extra.ty) catch |err| switch (err) {
        // It's okay for the type to be unknown: this will result in an anonymous array init.
        error.GenericPoison => return,
        else => |e| return e,
    };
    const arr_ty = if (is_result_ty) ty.opt_eu_base_type(mod) else ty;
    return sema.validate_array_init_ty(block, src, ty_src, extra.init_count, arr_ty);
}

fn validate_array_init_ty(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    ty_src: LazySrcLoc,
    init_count: u32,
    ty: Type,
) CompileError!void {
    const mod = sema.mod;
    switch (ty.zig_type_tag(mod)) {
        .Array => {
            const array_len = ty.array_len(mod);
            if (init_count != array_len) {
                return sema.fail(block, src, "expected {d} array elements; found {d}", .{
                    array_len, init_count,
                });
            }
            return;
        },
        .Vector => {
            const array_len = ty.array_len(mod);
            if (init_count != array_len) {
                return sema.fail(block, src, "expected {d} vector elements; found {d}", .{
                    array_len, init_count,
                });
            }
            return;
        },
        .Struct => if (ty.is_tuple(mod)) {
            try sema.resolve_type_fields(ty);
            const array_len = ty.array_len(mod);
            if (init_count > array_len) {
                return sema.fail(block, src, "expected at most {d} tuple fields; found {d}", .{
                    array_len, init_count,
                });
            }
            return;
        },
        else => {},
    }
    return sema.fail_with_array_init_not_supported(block, ty_src, ty);
}

fn zir_validate_struct_init_ty(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    is_result_ty: bool,
) CompileError!void {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const ty = sema.resolve_type(block, src, inst_data.operand) catch |err| switch (err) {
        // It's okay for the type to be unknown: this will result in an anonymous struct init.
        error.GenericPoison => return,
        else => |e| return e,
    };
    const struct_ty = if (is_result_ty) ty.opt_eu_base_type(mod) else ty;

    switch (struct_ty.zig_type_tag(mod)) {
        .Struct, .Union => return,
        else => {},
    }
    return sema.fail_with_struct_init_not_supported(block, src, struct_ty);
}

fn zir_validate_ptr_struct_init(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const validate_inst = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const init_src = validate_inst.src();
    const validate_extra = sema.code.extra_data(Zir.Inst.Block, validate_inst.payload_index);
    const instrs = sema.code.body_slice(validate_extra.end, validate_extra.data.body_len);
    const field_ptr_data = sema.code.instructions.items(.data)[@int_from_enum(instrs[0])].pl_node;
    const field_ptr_extra = sema.code.extra_data(Zir.Inst.Field, field_ptr_data.payload_index).data;
    const object_ptr = try sema.resolve_inst(field_ptr_extra.lhs);
    const agg_ty = sema.type_of(object_ptr).child_type(mod).opt_eu_base_type(mod);
    switch (agg_ty.zig_type_tag(mod)) {
        .Struct => return sema.validate_struct_init(
            block,
            agg_ty,
            init_src,
            instrs,
        ),
        .Union => return sema.validate_union_init(
            block,
            agg_ty,
            init_src,
            instrs,
            object_ptr,
        ),
        else => unreachable,
    }
}

fn validate_union_init(
    sema: *Sema,
    block: *Block,
    union_ty: Type,
    init_src: LazySrcLoc,
    instrs: []const Zir.Inst.Index,
    union_ptr: Air.Inst.Ref,
) CompileError!void {
    const mod = sema.mod;
    const gpa = sema.gpa;

    if (instrs.len != 1) {
        const msg = msg: {
            const msg = try sema.err_msg(
                block,
                init_src,
                "cannot initialize multiple union fields at once; unions can only have one active field",
                .{},
            );
            errdefer msg.destroy(gpa);

            for (instrs[1..]) |inst| {
                const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
                const inst_src: LazySrcLoc = .{ .node_offset_initializer = inst_data.src_node };
                try sema.err_note(block, inst_src, msg, "additional initializer here", .{});
            }
            try sema.add_declared_here_note(msg, union_ty);
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    if (block.is_comptime and
        (try sema.resolve_defined_value(block, init_src, union_ptr)) != null)
    {
        // In this case, comptime machinery already did everything. No work to do here.
        return;
    }

    const field_ptr = instrs[0];
    const field_ptr_data = sema.code.instructions.items(.data)[@int_from_enum(field_ptr)].pl_node;
    const field_src: LazySrcLoc = .{ .node_offset_initializer = field_ptr_data.src_node };
    const field_ptr_extra = sema.code.extra_data(Zir.Inst.Field, field_ptr_data.payload_index).data;
    const field_name = try mod.intern_pool.get_or_put_string(
        gpa,
        sema.code.null_terminated_string(field_ptr_extra.field_name_start),
        .no_embedded_nulls,
    );
    const field_index = try sema.union_field_index(block, union_ty, field_name, field_src);
    const air_tags = sema.air_instructions.items(.tag);
    const air_datas = sema.air_instructions.items(.data);
    const field_ptr_ref = sema.inst_map.get(field_ptr).?;

    // Our task here is to determine if the union is comptime-known. In such case,
    // we erase the runtime AIR instructions for initializing the union, and replace
    // the mapping with the comptime value. Either way, we will need to populate the tag.

    // We expect to see something like this in the current block AIR:
    //   %a = alloc(*const U)
    //   %b = bitcast(*U, %a)
    //   %c = field_ptr(..., %b)
    //   %e!= store(%c!, %d!)
    // If %d is a comptime operand, the union is comptime.
    // If the union is comptime, we want `first_block_index`
    // to point at %c so that the bitcast becomes the last instruction in the block.
    //
    // Store instruction may be missing; if field type has only one possible value, this case is handled below.
    //
    // In the case of a comptime-known pointer to a union, the
    // the field_ptr instruction is missing, so we have to pattern-match
    // based only on the store instructions.
    // `first_block_index` needs to point to the `field_ptr` if it exists;
    // the `store` otherwise.
    var first_block_index = block.instructions.items.len;
    var block_index = block.instructions.items.len - 1;
    var init_val: ?Value = null;
    var init_ref: ?Air.Inst.Ref = null;
    while (block_index > 0) : (block_index -= 1) {
        const store_inst = block.instructions.items[block_index];
        if (store_inst.to_ref() == field_ptr_ref) {
            first_block_index = block_index;
            break;
        }
        switch (air_tags[@int_from_enum(store_inst)]) {
            .store, .store_safe => {},
            else => continue,
        }
        const bin_op = air_datas[@int_from_enum(store_inst)].bin_op;
        var ptr_ref = bin_op.lhs;
        if (ptr_ref.to_index()) |ptr_inst| if (air_tags[@int_from_enum(ptr_inst)] == .bitcast) {
            ptr_ref = air_datas[@int_from_enum(ptr_inst)].ty_op.operand;
        };
        if (ptr_ref != field_ptr_ref) continue;
        first_block_index = @min(if (field_ptr_ref.to_index()) |field_ptr_inst|
            std.mem.last_index_of_scalar(
                Air.Inst.Index,
                block.instructions.items[0..block_index],
                field_ptr_inst,
            ).?
        else
            block_index, first_block_index);
        init_ref = bin_op.rhs;
        init_val = try sema.resolve_value(bin_op.rhs);
        break;
    }

    const tag_ty = union_ty.union_tag_type_hypothetical(mod);
    const tag_val = try mod.enum_value_field_index(tag_ty, field_index);
    const field_type = union_ty.union_field_type(tag_val, mod).?;

    if (try sema.type_has_one_possible_value(field_type)) |field_only_value| {
        init_val = field_only_value;
    }

    if (init_val) |val| {
        // Our task is to delete all the `field_ptr` and `store` instructions, and insert
        // instead a single `store` to the result ptr with a comptime union value.
        block_index = first_block_index;
        for (block.instructions.items[first_block_index..]) |cur_inst| {
            switch (air_tags[@int_from_enum(cur_inst)]) {
                .struct_field_ptr,
                .struct_field_ptr_index_0,
                .struct_field_ptr_index_1,
                .struct_field_ptr_index_2,
                .struct_field_ptr_index_3,
                => if (cur_inst.to_ref() == field_ptr_ref) continue,
                .bitcast => if (air_datas[@int_from_enum(cur_inst)].ty_op.operand == field_ptr_ref) continue,
                .store, .store_safe => {
                    var ptr_ref = air_datas[@int_from_enum(cur_inst)].bin_op.lhs;
                    if (ptr_ref.to_index()) |ptr_inst| if (air_tags[@int_from_enum(ptr_inst)] == .bitcast) {
                        ptr_ref = air_datas[@int_from_enum(ptr_inst)].ty_op.operand;
                    };
                    if (ptr_ref == field_ptr_ref) continue;
                },
                else => {},
            }
            block.instructions.items[block_index] = cur_inst;
            block_index += 1;
        }
        block.instructions.shrink_retaining_capacity(block_index);

        const union_val = try mod.intern(.{ .un = .{
            .ty = union_ty.to_intern(),
            .tag = tag_val.to_intern(),
            .val = val.to_intern(),
        } });
        const union_init = Air.interned_to_ref(union_val);
        try sema.store_ptr2(block, init_src, union_ptr, init_src, union_init, init_src, .store);
        return;
    } else if (try sema.type_requires_comptime(union_ty)) {
        return sema.fail_with_needed_comptime(block, field_ptr_data.src(), .{
            .needed_comptime_reason = "initializer of comptime only union must be comptime-known",
        });
    }
    if (init_ref) |v| try sema.validate_runtime_value(block, field_ptr_data.src(), v);

    const new_tag = Air.interned_to_ref(tag_val.to_intern());
    const set_tag_inst = try block.add_bin_op(.set_union_tag, union_ptr, new_tag);
    try sema.check_comptime_known_store(block, set_tag_inst, .unneeded); // `unneeded` since this isn't a "proper" store
}

fn validate_struct_init(
    sema: *Sema,
    block: *Block,
    struct_ty: Type,
    init_src: LazySrcLoc,
    instrs: []const Zir.Inst.Index,
) CompileError!void {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;

    const field_indices = try gpa.alloc(u32, instrs.len);
    defer gpa.free(field_indices);

    // Maps field index to field_ptr index of where it was already initialized.
    const found_fields = try gpa.alloc(Zir.Inst.OptionalIndex, struct_ty.struct_field_count(mod));
    defer gpa.free(found_fields);
    @memset(found_fields, .none);

    var struct_ptr_zir_ref: Zir.Inst.Ref = undefined;

    for (instrs, field_indices) |field_ptr, *field_index| {
        const field_ptr_data = sema.code.instructions.items(.data)[@int_from_enum(field_ptr)].pl_node;
        const field_src: LazySrcLoc = .{ .node_offset_initializer = field_ptr_data.src_node };
        const field_ptr_extra = sema.code.extra_data(Zir.Inst.Field, field_ptr_data.payload_index).data;
        struct_ptr_zir_ref = field_ptr_extra.lhs;
        const field_name = try ip.get_or_put_string(
            gpa,
            sema.code.null_terminated_string(field_ptr_extra.field_name_start),
            .no_embedded_nulls,
        );
        field_index.* = if (struct_ty.is_tuple(mod))
            try sema.tuple_field_index(block, struct_ty, field_name, field_src)
        else
            try sema.struct_field_index(block, struct_ty, field_name, field_src);
        assert(found_fields[field_index.*] == .none);
        found_fields[field_index.*] = field_ptr.to_optional();
    }

    var root_msg: ?*Module.ErrorMsg = null;
    errdefer if (root_msg) |msg| msg.destroy(sema.gpa);

    const struct_ptr = try sema.resolve_inst(struct_ptr_zir_ref);
    if (block.is_comptime and
        (try sema.resolve_defined_value(block, init_src, struct_ptr)) != null)
    {
        try sema.resolve_struct_layout(struct_ty);
        // In this case the only thing we need to do is evaluate the implicit
        // store instructions for default field values, and report any missing fields.
        // Avoid the cost of the extra machinery for detecting a comptime struct init value.
        for (found_fields, 0..) |field_ptr, i_usize| {
            const i: u32 = @int_cast(i_usize);
            if (field_ptr != .none) continue;

            try sema.resolve_struct_field_inits(struct_ty);
            const default_val = struct_ty.struct_field_default_value(i, mod);
            if (default_val.to_intern() == .unreachable_value) {
                const field_name = struct_ty.struct_field_name(i, mod).unwrap() orelse {
                    const template = "missing tuple field with index {d}";
                    if (root_msg) |msg| {
                        try sema.err_note(block, init_src, msg, template, .{i});
                    } else {
                        root_msg = try sema.err_msg(block, init_src, template, .{i});
                    }
                    continue;
                };
                const template = "missing struct field: {}";
                const args = .{field_name.fmt(ip)};
                if (root_msg) |msg| {
                    try sema.err_note(block, init_src, msg, template, args);
                } else {
                    root_msg = try sema.err_msg(block, init_src, template, args);
                }
                continue;
            }

            const field_src = init_src; // TODO better source location
            const default_field_ptr = if (struct_ty.is_tuple(mod))
                try sema.tuple_field_ptr(block, init_src, struct_ptr, field_src, @int_cast(i), true)
            else
                try sema.struct_field_ptr_by_index(block, init_src, struct_ptr, @int_cast(i), field_src, struct_ty, true);
            const init = Air.interned_to_ref(default_val.to_intern());
            try sema.store_ptr2(block, init_src, default_field_ptr, init_src, init, field_src, .store);
        }

        if (root_msg) |msg| {
            if (mod.type_to_struct(struct_ty)) |struct_type| {
                const decl = mod.decl_ptr(struct_type.decl.unwrap().?);
                const fqn = try decl.fully_qualified_name(mod);
                try mod.err_note_non_lazy(
                    decl.src_loc(mod),
                    msg,
                    "struct '{}' declared here",
                    .{fqn.fmt(ip)},
                );
            }
            root_msg = null;
            return sema.fail_with_owned_error_msg(block, msg);
        }

        return;
    }

    var fields_allow_runtime = true;

    var struct_is_comptime = true;
    var first_block_index = block.instructions.items.len;

    const require_comptime = try sema.type_requires_comptime(struct_ty);
    const air_tags = sema.air_instructions.items(.tag);
    const air_datas = sema.air_instructions.items(.data);

    try sema.resolve_struct_field_inits(struct_ty);

    // We collect the comptime field values in case the struct initialization
    // ends up being comptime-known.
    const field_values = try sema.arena.alloc(InternPool.Index, struct_ty.struct_field_count(mod));

    field: for (found_fields, 0..) |opt_field_ptr, i_usize| {
        const i: u32 = @int_cast(i_usize);
        if (opt_field_ptr.unwrap()) |field_ptr| {
            // Determine whether the value stored to this pointer is comptime-known.
            const field_ty = struct_ty.struct_field_type(i, mod);
            if (try sema.type_has_one_possible_value(field_ty)) |opv| {
                field_values[i] = opv.to_intern();
                continue;
            }

            const field_ptr_ref = sema.inst_map.get(field_ptr).?;

            //std.debug.print("validate_struct_init (field_ptr_ref=%{d}):\n", .{field_ptr_ref});
            //for (block.instructions.items) |item| {
            //    std.debug.print("  %{d} = {s}\n", .{item, @tag_name(air_tags[@int_from_enum(item)])});
            //}

            // We expect to see something like this in the current block AIR:
            //   %a = field_ptr(...)
            //   store(%a, %b)
            // With an optional bitcast between the store and the field_ptr.
            // If %b is a comptime operand, this field is comptime.
            //
            // However, in the case of a comptime-known pointer to a struct, the
            // the field_ptr instruction is missing, so we have to pattern-match
            // based only on the store instructions.
            // `first_block_index` needs to point to the `field_ptr` if it exists;
            // the `store` otherwise.

            // Possible performance enhancement: save the `block_index` between iterations
            // of the for loop.
            var block_index = block.instructions.items.len;
            while (block_index > 0) {
                block_index -= 1;
                const store_inst = block.instructions.items[block_index];
                if (store_inst.to_ref() == field_ptr_ref) {
                    struct_is_comptime = false;
                    continue :field;
                }
                switch (air_tags[@int_from_enum(store_inst)]) {
                    .store, .store_safe => {},
                    else => continue,
                }
                const bin_op = air_datas[@int_from_enum(store_inst)].bin_op;
                var ptr_ref = bin_op.lhs;
                if (ptr_ref.to_index()) |ptr_inst| if (air_tags[@int_from_enum(ptr_inst)] == .bitcast) {
                    ptr_ref = air_datas[@int_from_enum(ptr_inst)].ty_op.operand;
                };
                if (ptr_ref != field_ptr_ref) continue;
                first_block_index = @min(if (field_ptr_ref.to_index()) |field_ptr_inst|
                    std.mem.last_index_of_scalar(
                        Air.Inst.Index,
                        block.instructions.items[0..block_index],
                        field_ptr_inst,
                    ).?
                else
                    block_index, first_block_index);
                if (!sema.check_runtime_value(bin_op.rhs)) fields_allow_runtime = false;
                if (try sema.resolve_value(bin_op.rhs)) |val| {
                    field_values[i] = val.to_intern();
                } else if (require_comptime) {
                    const field_ptr_data = sema.code.instructions.items(.data)[@int_from_enum(field_ptr)].pl_node;
                    return sema.fail_with_needed_comptime(block, field_ptr_data.src(), .{
                        .needed_comptime_reason = "initializer of comptime only struct must be comptime-known",
                    });
                } else {
                    struct_is_comptime = false;
                }
                continue :field;
            }
            struct_is_comptime = false;
            continue :field;
        }

        const default_val = struct_ty.struct_field_default_value(i, mod);
        if (default_val.to_intern() == .unreachable_value) {
            const field_name = struct_ty.struct_field_name(i, mod).unwrap() orelse {
                const template = "missing tuple field with index {d}";
                if (root_msg) |msg| {
                    try sema.err_note(block, init_src, msg, template, .{i});
                } else {
                    root_msg = try sema.err_msg(block, init_src, template, .{i});
                }
                continue;
            };
            const template = "missing struct field: {}";
            const args = .{field_name.fmt(ip)};
            if (root_msg) |msg| {
                try sema.err_note(block, init_src, msg, template, args);
            } else {
                root_msg = try sema.err_msg(block, init_src, template, args);
            }
            continue;
        }
        field_values[i] = default_val.to_intern();
    }

    if (!struct_is_comptime and !fields_allow_runtime and root_msg == null) {
        root_msg = try sema.err_msg(block, init_src, "runtime value contains reference to comptime var", .{});
        try sema.err_note(block, init_src, root_msg.?, "comptime var pointers are not available at runtime", .{});
    }

    if (root_msg) |msg| {
        if (mod.type_to_struct(struct_ty)) |struct_type| {
            const decl = mod.decl_ptr(struct_type.decl.unwrap().?);
            const fqn = try decl.fully_qualified_name(mod);
            try mod.err_note_non_lazy(
                decl.src_loc(mod),
                msg,
                "struct '{}' declared here",
                .{fqn.fmt(ip)},
            );
        }
        root_msg = null;
        return sema.fail_with_owned_error_msg(block, msg);
    }

    if (struct_is_comptime) {
        // Our task is to delete all the `field_ptr` and `store` instructions, and insert
        // instead a single `store` to the struct_ptr with a comptime struct value.
        var init_index: usize = 0;
        var field_ptr_ref = Air.Inst.Ref.none;
        var block_index = first_block_index;
        for (block.instructions.items[first_block_index..]) |cur_inst| {
            while (field_ptr_ref == .none and init_index < instrs.len) : (init_index += 1) {
                const field_ty = struct_ty.struct_field_type(field_indices[init_index], mod);
                if (try field_ty.one_possible_value(mod)) |_| continue;
                field_ptr_ref = sema.inst_map.get(instrs[init_index]).?;
            }
            switch (air_tags[@int_from_enum(cur_inst)]) {
                .struct_field_ptr,
                .struct_field_ptr_index_0,
                .struct_field_ptr_index_1,
                .struct_field_ptr_index_2,
                .struct_field_ptr_index_3,
                => if (cur_inst.to_ref() == field_ptr_ref) continue,
                .bitcast => if (air_datas[@int_from_enum(cur_inst)].ty_op.operand == field_ptr_ref) continue,
                .store, .store_safe => {
                    var ptr_ref = air_datas[@int_from_enum(cur_inst)].bin_op.lhs;
                    if (ptr_ref.to_index()) |ptr_inst| if (air_tags[@int_from_enum(ptr_inst)] == .bitcast) {
                        ptr_ref = air_datas[@int_from_enum(ptr_inst)].ty_op.operand;
                    };
                    if (ptr_ref == field_ptr_ref) {
                        field_ptr_ref = .none;
                        continue;
                    }
                },
                else => {},
            }
            block.instructions.items[block_index] = cur_inst;
            block_index += 1;
        }
        block.instructions.shrink_retaining_capacity(block_index);

        const struct_val = try mod.intern(.{ .aggregate = .{
            .ty = struct_ty.to_intern(),
            .storage = .{ .elems = field_values },
        } });
        const struct_init = Air.interned_to_ref(struct_val);
        try sema.store_ptr2(block, init_src, struct_ptr, init_src, struct_init, init_src, .store);
        return;
    }
    try sema.resolve_struct_layout(struct_ty);

    // Our task is to insert `store` instructions for all the default field values.
    for (found_fields, 0..) |field_ptr, i| {
        if (field_ptr != .none) continue;

        const field_src = init_src; // TODO better source location
        const default_field_ptr = if (struct_ty.is_tuple(mod))
            try sema.tuple_field_ptr(block, init_src, struct_ptr, field_src, @int_cast(i), true)
        else
            try sema.struct_field_ptr_by_index(block, init_src, struct_ptr, @int_cast(i), field_src, struct_ty, true);
        try sema.check_known_alloc_ptr(block, struct_ptr, default_field_ptr);
        const init = Air.interned_to_ref(field_values[i]);
        try sema.store_ptr2(block, init_src, default_field_ptr, init_src, init, field_src, .store);
    }
}

fn zir_validate_ptr_array_init(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
) CompileError!void {
    const mod = sema.mod;
    const validate_inst = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const init_src = validate_inst.src();
    const validate_extra = sema.code.extra_data(Zir.Inst.Block, validate_inst.payload_index);
    const instrs = sema.code.body_slice(validate_extra.end, validate_extra.data.body_len);
    const first_elem_ptr_data = sema.code.instructions.items(.data)[@int_from_enum(instrs[0])].pl_node;
    const elem_ptr_extra = sema.code.extra_data(Zir.Inst.ElemPtrImm, first_elem_ptr_data.payload_index).data;
    const array_ptr = try sema.resolve_inst(elem_ptr_extra.ptr);
    const array_ty = sema.type_of(array_ptr).child_type(mod).opt_eu_base_type(mod);
    const array_len = array_ty.array_len(mod);

    // Collect the comptime element values in case the array literal ends up
    // being comptime-known.
    const element_vals = try sema.arena.alloc(
        InternPool.Index,
        try sema.usize_cast(block, init_src, array_len),
    );

    if (instrs.len != array_len) switch (array_ty.zig_type_tag(mod)) {
        .Struct => {
            var root_msg: ?*Module.ErrorMsg = null;
            errdefer if (root_msg) |msg| msg.destroy(sema.gpa);

            try sema.resolve_struct_field_inits(array_ty);
            var i = instrs.len;
            while (i < array_len) : (i += 1) {
                const default_val = array_ty.struct_field_default_value(i, mod).to_intern();
                if (default_val == .unreachable_value) {
                    const template = "missing tuple field with index {d}";
                    if (root_msg) |msg| {
                        try sema.err_note(block, init_src, msg, template, .{i});
                    } else {
                        root_msg = try sema.err_msg(block, init_src, template, .{i});
                    }
                    continue;
                }

                element_vals[i] = default_val;
            }

            if (root_msg) |msg| {
                root_msg = null;
                return sema.fail_with_owned_error_msg(block, msg);
            }
        },
        .Array => {
            return sema.fail(block, init_src, "expected {d} array elements; found {d}", .{
                array_len, instrs.len,
            });
        },
        .Vector => {
            return sema.fail(block, init_src, "expected {d} vector elements; found {d}", .{
                array_len, instrs.len,
            });
        },
        else => unreachable,
    };

    if (block.is_comptime and
        (try sema.resolve_defined_value(block, init_src, array_ptr)) != null)
    {
        // In this case the comptime machinery will have evaluated the store instructions
        // at comptime so we have almost nothing to do here. However, in case of a
        // sentinel-terminated array, the sentinel will not have been populated by
        // any ZIR instructions at comptime; we need to do that here.
        if (array_ty.sentinel(mod)) |sentinel_val| {
            const array_len_ref = try mod.int_ref(Type.usize, array_len);
            const sentinel_ptr = try sema.elem_ptr_array(block, init_src, init_src, array_ptr, init_src, array_len_ref, true, true);
            const sentinel = Air.interned_to_ref(sentinel_val.to_intern());
            try sema.store_ptr2(block, init_src, sentinel_ptr, init_src, sentinel, init_src, .store);
        }
        return;
    }

    // If the array has one possible value, the value is always comptime-known.
    if (try sema.type_has_one_possible_value(array_ty)) |array_opv| {
        const array_init = Air.interned_to_ref(array_opv.to_intern());
        try sema.store_ptr2(block, init_src, array_ptr, init_src, array_init, init_src, .store);
        return;
    }

    var array_is_comptime = true;
    var first_block_index = block.instructions.items.len;

    const air_tags = sema.air_instructions.items(.tag);
    const air_datas = sema.air_instructions.items(.data);

    outer: for (instrs, 0..) |elem_ptr, i| {
        // Determine whether the value stored to this pointer is comptime-known.

        if (array_ty.is_tuple(mod)) {
            if (array_ty.struct_field_is_comptime(i, mod))
                try sema.resolve_struct_field_inits(array_ty);
            if (try array_ty.struct_field_value_comptime(mod, i)) |opv| {
                element_vals[i] = opv.to_intern();
                continue;
            }
        }

        const elem_ptr_ref = sema.inst_map.get(elem_ptr).?;

        // We expect to see something like this in the current block AIR:
        //   %a = elem_ptr(...)
        //   store(%a, %b)
        // With an optional bitcast between the store and the elem_ptr.
        // If %b is a comptime operand, this element is comptime.
        //
        // However, in the case of a comptime-known pointer to an array, the
        // the elem_ptr instruction is missing, so we have to pattern-match
        // based only on the store instructions.
        // `first_block_index` needs to point to the `elem_ptr` if it exists;
        // the `store` otherwise.
        //
        // This is nearly identical to similar logic in `validate_struct_init`.

        // Possible performance enhancement: save the `block_index` between iterations
        // of the for loop.
        var block_index = block.instructions.items.len;
        while (block_index > 0) {
            block_index -= 1;
            const store_inst = block.instructions.items[block_index];
            if (store_inst.to_ref() == elem_ptr_ref) {
                array_is_comptime = false;
                continue :outer;
            }
            switch (air_tags[@int_from_enum(store_inst)]) {
                .store, .store_safe => {},
                else => continue,
            }
            const bin_op = air_datas[@int_from_enum(store_inst)].bin_op;
            var ptr_ref = bin_op.lhs;
            if (ptr_ref.to_index()) |ptr_inst| if (air_tags[@int_from_enum(ptr_inst)] == .bitcast) {
                ptr_ref = air_datas[@int_from_enum(ptr_inst)].ty_op.operand;
            };
            if (ptr_ref != elem_ptr_ref) continue;
            first_block_index = @min(if (elem_ptr_ref.to_index()) |elem_ptr_inst|
                std.mem.last_index_of_scalar(
                    Air.Inst.Index,
                    block.instructions.items[0..block_index],
                    elem_ptr_inst,
                ).?
            else
                block_index, first_block_index);
            if (try sema.resolve_value(bin_op.rhs)) |val| {
                element_vals[i] = val.to_intern();
            } else {
                array_is_comptime = false;
            }
            continue :outer;
        }
        array_is_comptime = false;
        continue :outer;
    }

    if (array_is_comptime) {
        if (try sema.resolve_defined_value(block, init_src, array_ptr)) |ptr_val| {
            switch (mod.intern_pool.index_to_key(ptr_val.to_intern())) {
                .ptr => |ptr| switch (ptr.base_addr) {
                    .comptime_field => return, // This store was validated by the individual elem ptrs.
                    else => {},
                },
                else => {},
            }
        }

        // Our task is to delete all the `elem_ptr` and `store` instructions, and insert
        // instead a single `store` to the array_ptr with a comptime struct value.
        var elem_index: usize = 0;
        var elem_ptr_ref = Air.Inst.Ref.none;
        var block_index = first_block_index;
        for (block.instructions.items[first_block_index..]) |cur_inst| {
            while (elem_ptr_ref == .none and elem_index < instrs.len) : (elem_index += 1) {
                if (array_ty.is_tuple(mod) and array_ty.struct_field_is_comptime(elem_index, mod)) continue;
                elem_ptr_ref = sema.inst_map.get(instrs[elem_index]).?;
            }
            switch (air_tags[@int_from_enum(cur_inst)]) {
                .ptr_elem_ptr => if (cur_inst.to_ref() == elem_ptr_ref) continue,
                .bitcast => if (air_datas[@int_from_enum(cur_inst)].ty_op.operand == elem_ptr_ref) continue,
                .store, .store_safe => {
                    var ptr_ref = air_datas[@int_from_enum(cur_inst)].bin_op.lhs;
                    if (ptr_ref.to_index()) |ptr_inst| if (air_tags[@int_from_enum(ptr_inst)] == .bitcast) {
                        ptr_ref = air_datas[@int_from_enum(ptr_inst)].ty_op.operand;
                    };
                    if (ptr_ref == elem_ptr_ref) {
                        elem_ptr_ref = .none;
                        continue;
                    }
                },
                else => {},
            }
            block.instructions.items[block_index] = cur_inst;
            block_index += 1;
        }
        block.instructions.shrink_retaining_capacity(block_index);

        const array_val = try mod.intern(.{ .aggregate = .{
            .ty = array_ty.to_intern(),
            .storage = .{ .elems = element_vals },
        } });
        const array_init = Air.interned_to_ref(array_val);
        try sema.store_ptr2(block, init_src, array_ptr, init_src, array_init, init_src, .store);
    }
}

fn zir_validate_deref(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand = try sema.resolve_inst(inst_data.operand);
    const operand_ty = sema.type_of(operand);

    if (operand_ty.zig_type_tag(mod) != .Pointer) {
        return sema.fail(block, src, "cannot dereference non-pointer type '{}'", .{operand_ty.fmt(mod)});
    } else switch (operand_ty.ptr_size(mod)) {
        .One, .C => {},
        .Many => return sema.fail(block, src, "index syntax required for unknown-length pointer type '{}'", .{operand_ty.fmt(mod)}),
        .Slice => return sema.fail(block, src, "index syntax required for slice type '{}'", .{operand_ty.fmt(mod)}),
    }

    if ((try sema.type_has_one_possible_value(operand_ty.child_type(mod))) != null) {
        // No need to validate the actual pointer value, we don't need it!
        return;
    }

    const elem_ty = operand_ty.elem_type2(mod);
    if (try sema.resolve_value(operand)) |val| {
        if (val.is_undef(mod)) {
            return sema.fail(block, src, "cannot dereference undefined value", .{});
        }
    } else if (try sema.type_requires_comptime(elem_ty)) {
        const msg = msg: {
            const msg = try sema.err_msg(
                block,
                src,
                "values of type '{}' must be comptime-known, but operand value is runtime-known",
                .{elem_ty.fmt(mod)},
            );
            errdefer msg.destroy(sema.gpa);

            const src_decl = mod.decl_ptr(block.src_decl);
            try sema.explain_why_type_is_comptime(msg, src_decl.to_src_loc(src, mod), elem_ty);
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }
}

fn zir_validate_destructure(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.ValidateDestructure, inst_data.payload_index).data;
    const src = inst_data.src();
    const destructure_src = LazySrcLoc.nodeOffset(extra.destructure_node);
    const operand = try sema.resolve_inst(extra.operand);
    const operand_ty = sema.type_of(operand);

    const can_destructure = switch (operand_ty.zig_type_tag(mod)) {
        .Array, .Vector => true,
        .Struct => operand_ty.is_tuple(mod),
        else => false,
    };

    if (!can_destructure) {
        return sema.fail_with_owned_error_msg(block, msg: {
            const msg = try sema.err_msg(block, src, "type '{}' cannot be destructured", .{operand_ty.fmt(mod)});
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, destructure_src, msg, "result destructured here", .{});
            break :msg msg;
        });
    }

    if (operand_ty.array_len(mod) != extra.expect_len) {
        return sema.fail_with_owned_error_msg(block, msg: {
            const msg = try sema.err_msg(block, src, "expected {} elements for destructure, found {}", .{
                extra.expect_len,
                operand_ty.array_len(mod),
            });
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, destructure_src, msg, "result destructured here", .{});
            break :msg msg;
        });
    }
}

fn fail_with_bad_member_access(
    sema: *Sema,
    block: *Block,
    agg_ty: Type,
    field_src: LazySrcLoc,
    field_name: InternPool.NullTerminatedString,
) CompileError {
    const mod = sema.mod;
    const kw_name = switch (agg_ty.zig_type_tag(mod)) {
        .Union => "union",
        .Struct => "struct",
        .Opaque => "opaque",
        .Enum => "enum",
        else => unreachable,
    };
    if (agg_ty.get_owner_decl_or_null(mod)) |some| if (mod.decl_is_root(some)) {
        return sema.fail(block, field_src, "root struct of file '{}' has no member named '{}'", .{
            agg_ty.fmt(mod), field_name.fmt(&mod.intern_pool),
        });
    };

    return sema.fail(block, field_src, "{s} '{}' has no member named '{}'", .{
        kw_name, agg_ty.fmt(mod), field_name.fmt(&mod.intern_pool),
    });
}

fn fail_with_bad_struct_field_access(
    sema: *Sema,
    block: *Block,
    struct_type: InternPool.LoadedStructType,
    field_src: LazySrcLoc,
    field_name: InternPool.NullTerminatedString,
) CompileError {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const decl = mod.decl_ptr(struct_type.decl.unwrap().?);
    const fqn = try decl.fully_qualified_name(mod);

    const msg = msg: {
        const msg = try sema.err_msg(
            block,
            field_src,
            "no field named '{}' in struct '{}'",
            .{ field_name.fmt(&mod.intern_pool), fqn.fmt(&mod.intern_pool) },
        );
        errdefer msg.destroy(gpa);
        try mod.err_note_non_lazy(decl.src_loc(mod), msg, "struct declared here", .{});
        break :msg msg;
    };
    return sema.fail_with_owned_error_msg(block, msg);
}

fn fail_with_bad_union_field_access(
    sema: *Sema,
    block: *Block,
    union_obj: InternPool.LoadedUnionType,
    field_src: LazySrcLoc,
    field_name: InternPool.NullTerminatedString,
) CompileError {
    const mod = sema.mod;
    const gpa = sema.gpa;

    const decl = mod.decl_ptr(union_obj.decl);
    const fqn = try decl.fully_qualified_name(mod);

    const msg = msg: {
        const msg = try sema.err_msg(
            block,
            field_src,
            "no field named '{}' in union '{}'",
            .{ field_name.fmt(&mod.intern_pool), fqn.fmt(&mod.intern_pool) },
        );
        errdefer msg.destroy(gpa);
        try mod.err_note_non_lazy(decl.src_loc(mod), msg, "union declared here", .{});
        break :msg msg;
    };
    return sema.fail_with_owned_error_msg(block, msg);
}

fn add_declared_here_note(sema: *Sema, parent: *Module.ErrorMsg, decl_ty: Type) !void {
    const mod = sema.mod;
    const src_loc = decl_ty.decl_src_loc_or_null(mod) orelse return;
    const category = switch (decl_ty.zig_type_tag(mod)) {
        .Union => "union",
        .Struct => "struct",
        .Enum => "enum",
        .Opaque => "opaque",
        .ErrorSet => "error set",
        else => unreachable,
    };
    try mod.err_note_non_lazy(src_loc, parent, "{s} declared here", .{category});
}

fn zir_store_to_inferred_ptr(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const pl_node = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = pl_node.src();
    const bin = sema.code.extra_data(Zir.Inst.Bin, pl_node.payload_index).data;
    const ptr = try sema.resolve_inst(bin.lhs);
    const operand = try sema.resolve_inst(bin.rhs);
    const ptr_inst = ptr.to_index().?;
    const air_datas = sema.air_instructions.items(.data);

    switch (sema.air_instructions.items(.tag)[@int_from_enum(ptr_inst)]) {
        .inferred_alloc_comptime => {
            const iac = &air_datas[@int_from_enum(ptr_inst)].inferred_alloc_comptime;
            return sema.store_to_inferred_alloc_comptime(block, src, operand, iac);
        },
        .inferred_alloc => {
            const ia = sema.unresolved_inferred_allocs.get_ptr(ptr_inst).?;
            return sema.store_to_inferred_alloc(block, src, ptr, operand, ia);
        },
        else => unreachable,
    }
}

fn store_to_inferred_alloc(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    ptr: Air.Inst.Ref,
    operand: Air.Inst.Ref,
    inferred_alloc: *InferredAlloc,
) CompileError!void {
    // Create a store instruction as a placeholder.  This will be replaced by a
    // proper store sequence once we know the stored type.
    const dummy_store = try block.add_bin_op(.store, ptr, operand);
    try sema.check_comptime_known_store(block, dummy_store, src);
    // Add the stored instruction to the set we will use to resolve peer types
    // for the inferred allocation.
    try inferred_alloc.prongs.append(sema.arena, dummy_store.to_index().?);
}

fn store_to_inferred_alloc_comptime(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    operand: Air.Inst.Ref,
    iac: *Air.Inst.Data.InferredAllocComptime,
) CompileError!void {
    const zcu = sema.mod;
    const operand_ty = sema.type_of(operand);
    // There will be only one store_to_inferred_ptr because we are running at comptime.
    // The alloc will turn into a Decl or a ComptimeAlloc.
    const operand_val = try sema.resolve_value(operand) orelse {
        return sema.fail_with_needed_comptime(block, src, .{
            .needed_comptime_reason = "value being stored to a comptime variable must be comptime-known",
        });
    };
    const alloc_ty = try sema.ptr_type(.{
        .child = operand_ty.to_intern(),
        .flags = .{
            .alignment = iac.alignment,
            .is_const = iac.is_const,
        },
    });
    if (iac.is_const and !operand_val.can_mutate_comptime_var_state(zcu)) {
        iac.ptr = try zcu.intern(.{ .ptr = .{
            .ty = alloc_ty.to_intern(),
            .base_addr = .{ .anon_decl = .{
                .val = operand_val.to_intern(),
                .orig_ty = alloc_ty.to_intern(),
            } },
            .byte_offset = 0,
        } });
    } else {
        const alloc_index = try sema.new_comptime_alloc(block, operand_ty, iac.alignment);
        sema.get_comptime_alloc(alloc_index).val = .{ .interned = operand_val.to_intern() };
        iac.ptr = try zcu.intern(.{ .ptr = .{
            .ty = alloc_ty.to_intern(),
            .base_addr = .{ .comptime_alloc = alloc_index },
            .byte_offset = 0,
        } });
    }
}

fn zir_set_eval_branch_quota(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const quota: u32 = @int_cast(try sema.resolve_int(block, src, inst_data.operand, Type.u32, .{
        .needed_comptime_reason = "eval branch quota must be comptime-known",
    }));
    sema.branch_quota = @max(sema.branch_quota, quota);
}

fn zir_store_node(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const zir_tags = sema.code.instructions.items(.tag);
    const zir_datas = sema.code.instructions.items(.data);
    const inst_data = zir_datas[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const ptr = try sema.resolve_inst(extra.lhs);
    const operand = try sema.resolve_inst(extra.rhs);

    const is_ret = if (extra.lhs.to_index()) |ptr_index|
        zir_tags[@int_from_enum(ptr_index)] == .ret_ptr
    else
        false;

    // Check for the possibility of this pattern:
    //   %a = ret_ptr
    //   %b = store(%a, %c)
    // Where %c is an error union or error set. In such case we need to add
    // to the current function's inferred error set, if any.
    if (is_ret and sema.fn_ret_ty_ies != null) switch (sema.type_of(operand).zig_type_tag(mod)) {
        .ErrorUnion, .ErrorSet => try sema.add_to_inferred_error_set(operand),
        else => {},
    };

    const ptr_src: LazySrcLoc = .{ .node_offset_store_ptr = inst_data.src_node };
    const operand_src: LazySrcLoc = .{ .node_offset_store_operand = inst_data.src_node };
    const air_tag: Air.Inst.Tag = if (is_ret)
        .ret_ptr
    else if (block.want_safety())
        .store_safe
    else
        .store;
    return sema.store_ptr2(block, src, ptr, ptr_src, operand, operand_src, air_tag);
}

fn zir_str(sema: *Sema, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const bytes = sema.code.instructions.items(.data)[@int_from_enum(inst)].str.get(sema.code);
    return sema.add_str_lit(
        try sema.mod.intern_pool.get_or_put_string(sema.gpa, bytes, .maybe_embedded_nulls),
        bytes.len,
    );
}

fn add_null_terminated_str_lit(sema: *Sema, string: InternPool.NullTerminatedString) CompileError!Air.Inst.Ref {
    return sema.add_str_lit(string.to_string(), string.length(&sema.mod.intern_pool));
}

fn add_str_lit(sema: *Sema, string: InternPool.String, len: u64) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const array_ty = try mod.array_type(.{
        .len = len,
        .sentinel = .zero_u8,
        .child = .u8_type,
    });
    const val = try mod.intern(.{ .aggregate = .{
        .ty = array_ty.to_intern(),
        .storage = .{ .bytes = string },
    } });
    return anon_decl_ref(sema, val);
}

fn anon_decl_ref(sema: *Sema, val: InternPool.Index) CompileError!Air.Inst.Ref {
    return Air.interned_to_ref(try ref_value(sema, val));
}

fn ref_value(sema: *Sema, val: InternPool.Index) CompileError!InternPool.Index {
    const mod = sema.mod;
    const ptr_ty = (try sema.ptr_type(.{
        .child = mod.intern_pool.type_of(val),
        .flags = .{
            .alignment = .none,
            .is_const = true,
            .address_space = .generic,
        },
    })).to_intern();
    return mod.intern(.{ .ptr = .{
        .ty = ptr_ty,
        .base_addr = .{ .anon_decl = .{
            .val = val,
            .orig_ty = ptr_ty,
        } },
        .byte_offset = 0,
    } });
}

fn zir_int(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    _ = block;
    const tracy = trace(@src());
    defer tracy.end();

    const int = sema.code.instructions.items(.data)[@int_from_enum(inst)].int;
    return sema.mod.int_ref(Type.comptime_int, int);
}

fn zir_int_big(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    _ = block;
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const int = sema.code.instructions.items(.data)[@int_from_enum(inst)].str;
    const byte_count = int.len * @size_of(std.math.big.Limb);
    const limb_bytes = sema.code.string_bytes[@int_from_enum(int.start)..][0..byte_count];

    // TODO: this allocation and copy is only needed because the limbs may be unaligned.
    // If ZIR is adjusted so that big int limbs are guaranteed to be aligned, these
    // two lines can be removed.
    const limbs = try sema.arena.alloc(std.math.big.Limb, int.len);
    @memcpy(mem.slice_as_bytes(limbs), limb_bytes);

    return Air.interned_to_ref((try mod.int_value_big(Type.comptime_int, .{
        .limbs = limbs,
        .positive = true,
    })).to_intern());
}

fn zir_float(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    _ = block;
    const number = sema.code.instructions.items(.data)[@int_from_enum(inst)].float;
    return Air.interned_to_ref((try sema.mod.float_value(
        Type.comptime_float,
        number,
    )).to_intern());
}

fn zir_float128(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    _ = block;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Float128, inst_data.payload_index).data;
    const number = extra.get();
    return Air.interned_to_ref((try sema.mod.float_value(Type.comptime_float, number)).to_intern());
}

fn zir_compile_error(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const msg = try sema.resolve_const_string(block, operand_src, inst_data.operand, .{
        .needed_comptime_reason = "compile error string must be comptime-known",
    });
    return sema.fail(block, src, "{s}", .{msg});
}

fn zir_compile_log(
    sema: *Sema,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;

    var managed = mod.compile_log_text.to_managed(sema.gpa);
    defer sema.mod.compile_log_text = managed.move_to_unmanaged();
    const writer = managed.writer();

    const extra = sema.code.extra_data(Zir.Inst.NodeMultiOp, extended.operand);
    const src_node = extra.data.src_node;
    const args = sema.code.ref_slice(extra.end, extended.small);

    for (args, 0..) |arg_ref, i| {
        if (i != 0) try writer.print(", ", .{});

        const arg = try sema.resolve_inst(arg_ref);
        const arg_ty = sema.type_of(arg);
        if (try sema.resolve_value_resolve_lazy(arg)) |val| {
            try writer.print("@as({}, {})", .{
                arg_ty.fmt(mod), val.fmt_value(mod, sema),
            });
        } else {
            try writer.print("@as({}, [runtime value])", .{arg_ty.fmt(mod)});
        }
    }
    try writer.print("\n", .{});

    const decl_index = if (sema.func_index != .none)
        mod.func_owner_decl_index(sema.func_index)
    else
        sema.owner_decl_index;
    const gop = try mod.compile_log_decls.get_or_put(sema.gpa, decl_index);
    if (!gop.found_existing) {
        gop.value_ptr.* = src_node;
    }
    return .void_value;
}

fn zir_panic(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const msg_inst = try sema.resolve_inst(inst_data.operand);

    // `panic_with_msg` would perform this coercion for us, but we can get a better
    // source location if we do it here.
    const coerced_msg = try sema.coerce(block, Type.slice_const_u8, msg_inst, .{ .node_offset_builtin_call_arg0 = inst_data.src_node });

    if (block.is_comptime) {
        return sema.fail(block, src, "encountered @panic at comptime", .{});
    }
    try sema.panic_with_msg(block, src, coerced_msg, .@"@panic");
}

fn zir_trap(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const src_node = sema.code.instructions.items(.data)[@int_from_enum(inst)].node;
    const src = LazySrcLoc.nodeOffset(src_node);
    if (block.is_comptime)
        return sema.fail(block, src, "encountered @trap at comptime", .{});
    _ = try block.add_no_op(.trap);
}

fn zir_loop(sema: *Sema, parent_block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const extra = sema.code.extra_data(Zir.Inst.Block, inst_data.payload_index);
    const body = sema.code.body_slice(extra.end, extra.data.body_len);
    const gpa = sema.gpa;

    // AIR expects a block outside the loop block too.
    // Reserve space for a Loop instruction so that generated Break instructions can
    // point to it, even if it doesn't end up getting used because the code ends up being
    // comptime evaluated.
    const block_inst: Air.Inst.Index = @enumFromInt(sema.air_instructions.len);
    const loop_inst: Air.Inst.Index = @enumFromInt(@int_from_enum(block_inst) + 1);
    try sema.air_instructions.ensure_unused_capacity(gpa, 2);
    sema.air_instructions.append_assume_capacity(.{
        .tag = .block,
        .data = undefined,
    });
    sema.air_instructions.append_assume_capacity(.{
        .tag = .loop,
        .data = .{ .ty_pl = .{
            .ty = .noreturn_type,
            .payload = undefined,
        } },
    });
    var label: Block.Label = .{
        .zir_block = inst,
        .merges = .{
            .src_locs = .{},
            .results = .{},
            .br_list = .{},
            .block_inst = block_inst,
        },
    };
    var child_block = parent_block.make_sub_block();
    child_block.label = &label;
    child_block.runtime_cond = null;
    child_block.runtime_loop = mod.decl_ptr(child_block.src_decl).to_src_loc(src, mod);
    child_block.runtime_index.increment();
    const merges = &child_block.label.?.merges;

    defer child_block.instructions.deinit(gpa);
    defer merges.deinit(gpa);

    var loop_block = child_block.make_sub_block();
    defer loop_block.instructions.deinit(gpa);

    // Use `analyze_body_inner` directly to push any comptime control flow up the stack.
    try sema.analyze_body_inner(&loop_block, body);

    const loop_block_len = loop_block.instructions.items.len;
    if (loop_block_len > 0 and sema.type_of(loop_block.instructions.items[loop_block_len - 1].to_ref()).is_no_return(mod)) {
        // If the loop ended with a noreturn terminator, then there is no way for it to loop,
        // so we can just use the block instead.
        try child_block.instructions.append_slice(gpa, loop_block.instructions.items);
    } else {
        try child_block.instructions.append(gpa, loop_inst);

        try sema.air_extra.ensure_unused_capacity(gpa, @typeInfo(Air.Block).Struct.fields.len + loop_block_len);
        sema.air_instructions.items(.data)[@int_from_enum(loop_inst)].ty_pl.payload = sema.add_extra_assume_capacity(
            Air.Block{ .body_len = @int_cast(loop_block_len) },
        );
        sema.air_extra.append_slice_assume_capacity(@ptr_cast(loop_block.instructions.items));
    }
    return sema.resolve_analyzed_block(parent_block, src, &child_block, merges, false);
}

fn zir_cimport(sema: *Sema, parent_block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const comp = mod.comp;
    const gpa = sema.gpa;
    const pl_node = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = pl_node.src();
    const extra = sema.code.extra_data(Zir.Inst.Block, pl_node.payload_index);
    const body = sema.code.body_slice(extra.end, extra.data.body_len);

    // we check this here to avoid undefined symbols
    if (!build_options.have_llvm)
        return sema.fail(parent_block, src, "C import unavailable; Zig compiler built without LLVM extensions", .{});

    var c_import_buf = std.ArrayList(u8).init(gpa);
    defer c_import_buf.deinit();

    var comptime_reason: Block.ComptimeReason = .{ .c_import = .{
        .block = parent_block,
        .src = src,
    } };
    var child_block: Block = .{
        .parent = parent_block,
        .sema = sema,
        .src_decl = parent_block.src_decl,
        .namespace = parent_block.namespace,
        .instructions = .{},
        .inlining = parent_block.inlining,
        .is_comptime = true,
        .comptime_reason = &comptime_reason,
        .c_import_buf = &c_import_buf,
        .runtime_cond = parent_block.runtime_cond,
        .runtime_loop = parent_block.runtime_loop,
        .runtime_index = parent_block.runtime_index,
    };
    defer child_block.instructions.deinit(gpa);

    _ = try sema.analyze_inline_body(&child_block, body, inst);

    var c_import_res = comp.c_import(c_import_buf.items, parent_block.owner_module()) catch |err|
        return sema.fail(&child_block, src, "C import failed: {s}", .{@errorName(err)});
    defer c_import_res.deinit(gpa);

    if (c_import_res.errors.error_message_count() != 0) {
        const msg = msg: {
            const msg = try sema.err_msg(&child_block, src, "C import failed", .{});
            errdefer msg.destroy(gpa);

            if (!comp.config.link_libc)
                try sema.err_note(&child_block, src, msg, "libc headers not available; compilation does not link against libc", .{});

            const gop = try mod.cimport_errors.get_or_put(gpa, sema.owner_decl_index);
            if (!gop.found_existing) {
                gop.value_ptr.* = c_import_res.errors;
                c_import_res.errors = std.zig.ErrorBundle.empty;
            }
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(&child_block, msg);
    }
    const parent_mod = parent_block.owner_module();
    const c_import_mod = Package.Module.create(comp.arena, .{
        .global_cache_directory = comp.global_cache_directory,
        .paths = .{
            .root = .{
                .root_dir = Compilation.Directory.cwd(),
                .sub_path = std.fs.path.dirname(c_import_res.out_zig_path) orelse "",
            },
            .root_src_path = std.fs.path.basename(c_import_res.out_zig_path),
        },
        .fully_qualified_name = c_import_res.out_zig_path,
        .cc_argv = parent_mod.cc_argv,
        .inherited = .{},
        .global = comp.config,
        .parent = parent_mod,
        .builtin_mod = parent_mod.get_builtin_dependency(),
        .builtin_modules = null, // `builtin_mod` is set
    }) catch |err| switch (err) {
        // None of these are possible because we are creating a package with
        // the exact same configuration as the parent package, which already
        // passed these checks.
        error.ValgrindUnsupportedOnTarget => unreachable,
        error.TargetRequiresSingleThreaded => unreachable,
        error.BackendRequiresSingleThreaded => unreachable,
        error.TargetRequiresPic => unreachable,
        error.PieRequiresPic => unreachable,
        error.DynamicLinkingRequiresPic => unreachable,
        error.TargetHasNoRedZone => unreachable,
        error.StackCheckUnsupportedByTarget => unreachable,
        error.StackProtectorUnsupportedByTarget => unreachable,
        error.StackProtectorUnavailableWithoutLibC => unreachable,

        else => |e| return e,
    };

    const result = mod.import_pkg(c_import_mod) catch |err|
        return sema.fail(&child_block, src, "C import failed: {s}", .{@errorName(err)});

    mod.ast_gen_file(result.file) catch |err|
        return sema.fail(&child_block, src, "C import failed: {s}", .{@errorName(err)});

    try mod.ensure_file_analyzed(result.file);
    const file_root_decl_index = result.file.root_decl.unwrap().?;
    return sema.analyze_decl_val(parent_block, src, file_root_decl_index);
}

fn zir_suspend_block(sema: *Sema, parent_block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    return sema.fail_with_use_of_async(parent_block, src);
}

fn zir_block(sema: *Sema, parent_block: *Block, inst: Zir.Inst.Index, force_comptime: bool) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const pl_node = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = pl_node.src();
    const extra = sema.code.extra_data(Zir.Inst.Block, pl_node.payload_index);
    const body = sema.code.body_slice(extra.end, extra.data.body_len);
    const gpa = sema.gpa;

    // Reserve space for a Block instruction so that generated Break instructions can
    // point to it, even if it doesn't end up getting used because the code ends up being
    // comptime evaluated or is an unlabeled block.
    const block_inst: Air.Inst.Index = @enumFromInt(sema.air_instructions.len);
    try sema.air_instructions.append(gpa, .{
        .tag = .block,
        .data = undefined,
    });

    var label: Block.Label = .{
        .zir_block = inst,
        .merges = .{
            .src_locs = .{},
            .results = .{},
            .br_list = .{},
            .block_inst = block_inst,
        },
    };

    var child_block: Block = .{
        .parent = parent_block,
        .sema = sema,
        .src_decl = parent_block.src_decl,
        .namespace = parent_block.namespace,
        .instructions = .{},
        .label = &label,
        .inlining = parent_block.inlining,
        .is_comptime = parent_block.is_comptime or force_comptime,
        .comptime_reason = parent_block.comptime_reason,
        .is_typeof = parent_block.is_typeof,
        .want_safety = parent_block.want_safety,
        .float_mode = parent_block.float_mode,
        .c_import_buf = parent_block.c_import_buf,
        .runtime_cond = parent_block.runtime_cond,
        .runtime_loop = parent_block.runtime_loop,
        .runtime_index = parent_block.runtime_index,
        .error_return_trace_index = parent_block.error_return_trace_index,
    };

    defer child_block.instructions.deinit(gpa);
    defer label.merges.deinit(gpa);

    return sema.resolve_block_body(parent_block, src, &child_block, body, inst, &label.merges);
}

/// Semantically analyze the given ZIR body, emitting any resulting runtime code into the AIR block
/// specified by `child_block` if necessary (and emitting this block into `parent_block`).
/// TODO: `merges` is known from `child_block`, remove this parameter.
fn resolve_block_body(
    sema: *Sema,
    parent_block: *Block,
    src: LazySrcLoc,
    child_block: *Block,
    body: []const Zir.Inst.Index,
    /// This is the instruction that a break instruction within `body` can
    /// use to return from the body.
    body_inst: Zir.Inst.Index,
    merges: *Block.Merges,
) CompileError!Air.Inst.Ref {
    if (child_block.is_comptime) {
        return sema.resolve_inline_body(child_block, body, body_inst);
    } else {
        assert(sema.air_instructions.items(.tag)[@int_from_enum(merges.block_inst)] == .block);
        var need_debug_scope = false;
        child_block.need_debug_scope = &need_debug_scope;
        if (sema.analyze_body_inner(child_block, body)) |_| {
            return sema.resolve_analyzed_block(parent_block, src, child_block, merges, need_debug_scope);
        } else |err| switch (err) {
            error.ComptimeBreak => {
                // Comptime control flow is happening, however child_block may still contain
                // runtime instructions which need to be copied to the parent block.
                if (need_debug_scope and child_block.instructions.items.len > 0) {
                    // We need a runtime block for scoping reasons.
                    _ = try child_block.add_br(merges.block_inst, .void_value);
                    try parent_block.instructions.append(sema.gpa, merges.block_inst);
                    try sema.air_extra.ensure_unused_capacity(sema.gpa, @typeInfo(Air.Block).Struct.fields.len +
                        child_block.instructions.items.len);
                    sema.air_instructions.items(.data)[@int_from_enum(merges.block_inst)] = .{ .ty_pl = .{
                        .ty = .void_type,
                        .payload = sema.add_extra_assume_capacity(Air.Block{
                            .body_len = @int_cast(child_block.instructions.items.len),
                        }),
                    } };
                    sema.air_extra.append_slice_assume_capacity(@ptr_cast(child_block.instructions.items));
                } else {
                    // We can copy instructions directly to the parent block.
                    try parent_block.instructions.append_slice(sema.gpa, child_block.instructions.items);
                }

                const break_inst = sema.comptime_break_inst;
                const break_data = sema.code.instructions.items(.data)[@int_from_enum(break_inst)].@"break";
                const extra = sema.code.extra_data(Zir.Inst.Break, break_data.payload_index).data;
                if (extra.block_inst == body_inst) {
                    return try sema.resolve_inst(break_data.operand);
                } else {
                    return error.ComptimeBreak;
                }
            },
            else => |e| return e,
        }
    }
}

/// After a body corresponding to an AIR `block` has been analyzed, this function places them into
/// the block pointed at by `merges.block_inst` if necessary, or the block may be elided in favor of
/// inlining the instructions directly into the parent block. Either way, it considers all merges of
/// this block, and combines them appropriately using peer type resolution, returning the final
/// value of the block.
fn resolve_analyzed_block(
    sema: *Sema,
    parent_block: *Block,
    src: LazySrcLoc,
    child_block: *Block,
    merges: *Block.Merges,
    need_debug_scope: bool,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const gpa = sema.gpa;
    const mod = sema.mod;

    // Blocks must terminate with noreturn instruction.
    assert(child_block.instructions.items.len != 0);
    assert(sema.type_of(child_block.instructions.items[child_block.instructions.items.len - 1].to_ref()).is_no_return(mod));

    const block_tag = sema.air_instructions.items(.tag)[@int_from_enum(merges.block_inst)];
    switch (block_tag) {
        .block => {},
        .dbg_inline_block => assert(need_debug_scope),
        else => unreachable,
    }
    if (merges.results.items.len == 0) {
        switch (block_tag) {
            .block => {
                // No need for a block instruction. We can put the new instructions
                // directly into the parent block.
                if (need_debug_scope) {
                    // The code following this block is unreachable, as the block has no
                    // merges, so we don't necessarily need to emit this as an AIR block.
                    // However, we need a block *somewhere* to make the scoping correct,
                    // so forward this request to the parent block.
                    if (parent_block.need_debug_scope) |ptr| ptr.* = true;
                }
                try parent_block.instructions.append_slice(gpa, child_block.instructions.items);
                return child_block.instructions.items[child_block.instructions.items.len - 1].to_ref();
            },
            .dbg_inline_block => {
                // Create a block containing all instruction from the body.
                try parent_block.instructions.append(gpa, merges.block_inst);
                try sema.air_extra.ensure_unused_capacity(gpa, @typeInfo(Air.DbgInlineBlock).Struct.fields.len +
                    child_block.instructions.items.len);
                sema.air_instructions.items(.data)[@int_from_enum(merges.block_inst)] = .{ .ty_pl = .{
                    .ty = .noreturn_type,
                    .payload = sema.add_extra_assume_capacity(Air.DbgInlineBlock{
                        .func = child_block.inlining.?.func,
                        .body_len = @int_cast(child_block.instructions.items.len),
                    }),
                } };
                sema.air_extra.append_slice_assume_capacity(@ptr_cast(child_block.instructions.items));
                return merges.block_inst.to_ref();
            },
            else => unreachable,
        }
    }
    if (merges.results.items.len == 1) {
        // If the `break` is trailing, we may be able to elide the AIR block here
        // by appending the new instructions directly to the parent block.
        if (!need_debug_scope) {
            const last_inst_index = child_block.instructions.items.len - 1;
            const last_inst = child_block.instructions.items[last_inst_index];
            if (sema.get_break_block(last_inst)) |br_block| {
                if (br_block == merges.block_inst) {
                    // Great, the last instruction is the break! Put the instructions
                    // directly into the parent block.
                    try parent_block.instructions.append_slice(gpa, child_block.instructions.items[0..last_inst_index]);
                    return merges.results.items[0];
                }
            }
        }
        // Okay, we need a runtime block. If the value is comptime-known, the
        // block should just return void, and we return the merge result
        // directly. Otherwise, we can defer to the logic below.
        if (try sema.resolve_value(merges.results.items[0])) |result_val| {
            // Create a block containing all instruction from the body.
            try parent_block.instructions.append(gpa, merges.block_inst);
            switch (block_tag) {
                .block => {
                    try sema.air_extra.ensure_unused_capacity(gpa, @typeInfo(Air.Block).Struct.fields.len +
                        child_block.instructions.items.len);
                    sema.air_instructions.items(.data)[@int_from_enum(merges.block_inst)] = .{ .ty_pl = .{
                        .ty = .void_type,
                        .payload = sema.add_extra_assume_capacity(Air.Block{
                            .body_len = @int_cast(child_block.instructions.items.len),
                        }),
                    } };
                },
                .dbg_inline_block => {
                    try sema.air_extra.ensure_unused_capacity(gpa, @typeInfo(Air.DbgInlineBlock).Struct.fields.len +
                        child_block.instructions.items.len);
                    sema.air_instructions.items(.data)[@int_from_enum(merges.block_inst)] = .{ .ty_pl = .{
                        .ty = .void_type,
                        .payload = sema.add_extra_assume_capacity(Air.DbgInlineBlock{
                            .func = child_block.inlining.?.func,
                            .body_len = @int_cast(child_block.instructions.items.len),
                        }),
                    } };
                },
                else => unreachable,
            }
            sema.air_extra.append_slice_assume_capacity(@ptr_cast(child_block.instructions.items));
            // Rewrite the break to just give value {}; the value is
            // comptime-known and will be returned directly.
            sema.air_instructions.items(.data)[@int_from_enum(merges.br_list.items[0])].br.operand = .void_value;
            return Air.interned_to_ref(result_val.to_intern());
        }
    }
    // It is impossible to have the number of results be > 1 in a comptime scope.
    assert(!child_block.is_comptime); // Should already got a compile error in the condbr condition.

    // Note that we'll always create an AIR block here, so `need_debug_scope` is irrelevant.

    // Need to set the type and emit the Block instruction. This allows machine code generation
    // to emit a jump instruction to after the block when it encounters the break.
    try parent_block.instructions.append(gpa, merges.block_inst);
    const resolved_ty = try sema.resolve_peer_types(parent_block, src, merges.results.items, .{ .override = merges.src_locs.items });
    // TODO add note "missing else causes void value"

    const type_src = src; // TODO: better source location
    if (try sema.type_requires_comptime(resolved_ty)) {
        const msg = msg: {
            const msg = try sema.err_msg(child_block, type_src, "value with comptime-only type '{}' depends on runtime control flow", .{resolved_ty.fmt(mod)});
            errdefer msg.destroy(sema.gpa);

            const runtime_src = child_block.runtime_cond orelse child_block.runtime_loop.?;
            try mod.err_note_non_lazy(runtime_src, msg, "runtime control flow here", .{});

            const child_src_decl = mod.decl_ptr(child_block.src_decl);
            try sema.explain_why_type_is_comptime(msg, child_src_decl.to_src_loc(type_src, mod), resolved_ty);

            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(child_block, msg);
    }
    for (merges.results.items, merges.src_locs.items) |merge_inst, merge_src| {
        try sema.validate_runtime_value(child_block, merge_src orelse src, merge_inst);
    }
    const ty_inst = Air.interned_to_ref(resolved_ty.to_intern());
    switch (block_tag) {
        .block => {
            try sema.air_extra.ensure_unused_capacity(gpa, @typeInfo(Air.Block).Struct.fields.len +
                child_block.instructions.items.len);
            sema.air_instructions.items(.data)[@int_from_enum(merges.block_inst)] = .{ .ty_pl = .{
                .ty = ty_inst,
                .payload = sema.add_extra_assume_capacity(Air.Block{
                    .body_len = @int_cast(child_block.instructions.items.len),
                }),
            } };
        },
        .dbg_inline_block => {
            try sema.air_extra.ensure_unused_capacity(gpa, @typeInfo(Air.DbgInlineBlock).Struct.fields.len +
                child_block.instructions.items.len);
            sema.air_instructions.items(.data)[@int_from_enum(merges.block_inst)] = .{ .ty_pl = .{
                .ty = ty_inst,
                .payload = sema.add_extra_assume_capacity(Air.DbgInlineBlock{
                    .func = child_block.inlining.?.func,
                    .body_len = @int_cast(child_block.instructions.items.len),
                }),
            } };
        },
        else => unreachable,
    }
    sema.air_extra.append_slice_assume_capacity(@ptr_cast(child_block.instructions.items));
    // Now that the block has its type resolved, we need to go back into all the break
    // instructions, and insert type coercion on the operands.
    for (merges.br_list.items) |br| {
        const br_operand = sema.air_instructions.items(.data)[@int_from_enum(br)].br.operand;
        const br_operand_src = src;
        const br_operand_ty = sema.type_of(br_operand);
        if (br_operand_ty.eql(resolved_ty, mod)) {
            // No type coercion needed.
            continue;
        }
        var coerce_block = parent_block.make_sub_block();
        defer coerce_block.instructions.deinit(gpa);
        const coerced_operand = try sema.coerce(&coerce_block, resolved_ty, br_operand, br_operand_src);
        // If no instructions were produced, such as in the case of a coercion of a
        // constant value to a new type, we can simply point the br operand to it.
        if (coerce_block.instructions.items.len == 0) {
            sema.air_instructions.items(.data)[@int_from_enum(br)].br.operand = coerced_operand;
            continue;
        }
        assert(coerce_block.instructions.items[coerce_block.instructions.items.len - 1].to_ref() == coerced_operand);

        // Convert the br instruction to a block instruction that has the coercion
        // and then a new br inside that returns the coerced instruction.
        const sub_block_len: u32 = @int_cast(coerce_block.instructions.items.len + 1);
        try sema.air_extra.ensure_unused_capacity(gpa, @typeInfo(Air.Block).Struct.fields.len +
            sub_block_len);
        try sema.air_instructions.ensure_unused_capacity(gpa, 1);
        const sub_br_inst: Air.Inst.Index = @enumFromInt(sema.air_instructions.len);

        sema.air_instructions.items(.tag)[@int_from_enum(br)] = .block;
        sema.air_instructions.items(.data)[@int_from_enum(br)] = .{ .ty_pl = .{
            .ty = .noreturn_type,
            .payload = sema.add_extra_assume_capacity(Air.Block{
                .body_len = sub_block_len,
            }),
        } };
        sema.air_extra.append_slice_assume_capacity(@ptr_cast(coerce_block.instructions.items));
        sema.air_extra.append_assume_capacity(@int_from_enum(sub_br_inst));

        sema.air_instructions.append_assume_capacity(.{
            .tag = .br,
            .data = .{ .br = .{
                .block_inst = merges.block_inst,
                .operand = coerced_operand,
            } },
        });
    }
    return merges.block_inst.to_ref();
}

fn zir_export(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Export, inst_data.payload_index).data;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const options_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const decl_name = try mod.intern_pool.get_or_put_string(
        mod.gpa,
        sema.code.null_terminated_string(extra.decl_name),
        .no_embedded_nulls,
    );
    const decl_index = if (extra.namespace != .none) index_blk: {
        const container_ty = try sema.resolve_type(block, operand_src, extra.namespace);
        const container_namespace = container_ty.get_namespace_index(mod);

        const maybe_index = try sema.lookup_in_namespace(block, operand_src, container_namespace, decl_name, false);
        break :index_blk maybe_index orelse
            return sema.fail_with_bad_member_access(block, container_ty, operand_src, decl_name);
    } else try sema.lookup_identifier(block, operand_src, decl_name);
    const options = sema.resolve_export_options(block, .unneeded, extra.options) catch |err| switch (err) {
        error.NeededSourceLocation => {
            _ = try sema.resolve_export_options(block, options_src, extra.options);
            unreachable;
        },
        else => |e| return e,
    };
    {
        try sema.ensure_decl_analyzed(decl_index);
        const exported_decl = mod.decl_ptr(decl_index);
        if (exported_decl.val.get_function(mod)) |function| {
            return sema.analyze_export(block, src, options, function.owner_decl);
        }
    }
    try sema.analyze_export(block, src, options, decl_index);
}

fn zir_export_value(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.ExportValue, inst_data.payload_index).data;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const options_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const operand = try sema.resolve_inst_const(block, operand_src, extra.operand, .{
        .needed_comptime_reason = "export target must be comptime-known",
    });
    const options = try sema.resolve_export_options(block, options_src, extra.options);
    if (options.linkage == .internal)
        return;
    if (operand.get_function(mod)) |function| {
        const decl_index = function.owner_decl;
        return sema.analyze_export(block, src, options, decl_index);
    }

    try add_export(mod, .{
        .opts = options,
        .src = src,
        .owner_decl = sema.owner_decl_index,
        .src_decl = block.src_decl,
        .exported = .{ .value = operand.to_intern() },
        .status = .in_progress,
    });
}

pub fn analyze_export(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    options: Module.Export.Options,
    exported_decl_index: InternPool.DeclIndex,
) !void {
    const gpa = sema.gpa;
    const mod = sema.mod;

    if (options.linkage == .internal)
        return;

    try sema.ensure_decl_analyzed(exported_decl_index);
    const exported_decl = mod.decl_ptr(exported_decl_index);
    const export_ty = exported_decl.type_of(mod);

    if (!try sema.validate_extern_type(export_ty, .other)) {
        const msg = msg: {
            const msg = try sema.err_msg(block, src, "unable to export type '{}'", .{export_ty.fmt(mod)});
            errdefer msg.destroy(gpa);

            const src_decl = mod.decl_ptr(block.src_decl);
            try sema.explain_why_type_is_not_extern(msg, src_decl.to_src_loc(src, mod), export_ty, .other);

            try sema.add_declared_here_note(msg, export_ty);
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    // TODO: some backends might support re-exporting extern decls
    if (exported_decl.is_extern(mod)) {
        return sema.fail(block, src, "export target cannot be extern", .{});
    }

    try sema.maybe_queue_func_body_analysis(exported_decl_index);

    try add_export(mod, .{
        .opts = options,
        .src = src,
        .owner_decl = sema.owner_decl_index,
        .src_decl = block.src_decl,
        .exported = .{ .decl_index = exported_decl_index },
        .status = .in_progress,
    });
}

fn add_export(mod: *Module, export_init: Module.Export) error{OutOfMemory}!void {
    const gpa = mod.gpa;

    try mod.decl_exports.ensure_unused_capacity(gpa, 1);
    try mod.value_exports.ensure_unused_capacity(gpa, 1);
    try mod.export_owners.ensure_unused_capacity(gpa, 1);

    const new_export = try gpa.create(Module.Export);
    errdefer gpa.destroy(new_export);

    new_export.* = export_init;

    const eo_gop = mod.export_owners.get_or_put_assume_capacity(export_init.owner_decl);
    if (!eo_gop.found_existing) eo_gop.value_ptr.* = .{};
    try eo_gop.value_ptr.append(gpa, new_export);
    errdefer _ = eo_gop.value_ptr.pop();

    switch (export_init.exported) {
        .decl_index => |decl_index| {
            const de_gop = mod.decl_exports.get_or_put_assume_capacity(decl_index);
            if (!de_gop.found_existing) de_gop.value_ptr.* = .{};
            try de_gop.value_ptr.append(gpa, new_export);
        },
        .value => |value| {
            const ve_gop = mod.value_exports.get_or_put_assume_capacity(value);
            if (!ve_gop.found_existing) ve_gop.value_ptr.* = .{};
            try ve_gop.value_ptr.append(gpa, new_export);
        },
    }
}

fn zir_set_align_stack(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!void {
    const mod = sema.mod;
    const extra = sema.code.extra_data(Zir.Inst.UnNode, extended.operand).data;
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const src = LazySrcLoc.nodeOffset(extra.node);
    const alignment = try sema.resolve_align(block, operand_src, extra.operand);
    if (alignment.order(Alignment.from_nonzero_byte_units(256)).compare(.gt)) {
        return sema.fail(block, src, "attempt to @setAlignStack({d}); maximum is 256", .{
            alignment.to_byte_units().?,
        });
    }

    const fn_owner_decl = mod.func_owner_decl_ptr(sema.func_index);
    switch (fn_owner_decl.type_of(mod).fn_calling_convention(mod)) {
        .Naked => return sema.fail(block, src, "@setAlignStack in naked function", .{}),
        .Inline => return sema.fail(block, src, "@setAlignStack in inline function", .{}),
        else => if (block.inlining != null) {
            return sema.fail(block, src, "@setAlignStack in inline call", .{});
        },
    }

    if (sema.prev_stack_alignment_src) |prev_src| {
        const msg = msg: {
            const msg = try sema.err_msg(block, src, "multiple @setAlignStack in the same function body", .{});
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, prev_src, msg, "other instance here", .{});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }
    sema.prev_stack_alignment_src = src;

    const ip = &mod.intern_pool;
    const a = ip.func_analysis(sema.func_index);
    if (a.stack_alignment != .none) {
        a.stack_alignment = @enumFromInt(@max(
            @int_from_enum(alignment),
            @int_from_enum(a.stack_alignment),
        ));
    }
}

fn zir_set_cold(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const extra = sema.code.extra_data(Zir.Inst.UnNode, extended.operand).data;
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const is_cold = try sema.resolve_const_bool(block, operand_src, extra.operand, .{
        .needed_comptime_reason = "operand to @setCold must be comptime-known",
    });
    if (sema.func_index == .none) return; // does nothing outside a function
    ip.func_analysis(sema.func_index).is_cold = is_cold;
}

fn zir_set_float_mode(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!void {
    const extra = sema.code.extra_data(Zir.Inst.UnNode, extended.operand).data;
    const src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    block.float_mode = try sema.resolve_builtin_enum(block, src, extra.operand, "FloatMode", .{
        .needed_comptime_reason = "operand to @setFloatMode must be comptime-known",
    });
}

fn zir_set_runtime_safety(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    block.want_safety = try sema.resolve_const_bool(block, operand_src, inst_data.operand, .{
        .needed_comptime_reason = "operand to @setRuntimeSafety must be comptime-known",
    });
}

fn zir_fence(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!void {
    if (block.is_comptime) return;

    const extra = sema.code.extra_data(Zir.Inst.UnNode, extended.operand).data;
    const order_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const order = try sema.resolve_atomic_order(block, order_src, extra.operand, .{
        .needed_comptime_reason = "atomic order of @fence must be comptime-known",
    });

    if (@int_from_enum(order) < @int_from_enum(std.builtin.AtomicOrder.acquire)) {
        return sema.fail(block, order_src, "atomic ordering must be acquire or stricter", .{});
    }

    _ = try block.add_inst(.{
        .tag = .fence,
        .data = .{ .fence = order },
    });
}

fn zir_break(sema: *Sema, start_block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].@"break";
    const extra = sema.code.extra_data(Zir.Inst.Break, inst_data.payload_index).data;
    const operand = try sema.resolve_inst(inst_data.operand);
    const zir_block = extra.block_inst;

    var block = start_block;
    while (true) {
        if (block.label) |label| {
            if (label.zir_block == zir_block) {
                const br_ref = try start_block.add_br(label.merges.block_inst, operand);
                const src_loc = if (extra.operand_src_node != Zir.Inst.Break.no_src_node)
                    LazySrcLoc.nodeOffset(extra.operand_src_node)
                else
                    null;
                try label.merges.src_locs.append(sema.gpa, src_loc);
                try label.merges.results.append(sema.gpa, operand);
                try label.merges.br_list.append(sema.gpa, br_ref.to_index().?);
                block.runtime_index.increment();
                if (block.runtime_cond == null and block.runtime_loop == null) {
                    block.runtime_cond = start_block.runtime_cond orelse start_block.runtime_loop;
                    block.runtime_loop = start_block.runtime_loop;
                }
                return;
            }
        }
        block = block.parent.?;
    }
}

fn zir_dbg_stmt(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    if (block.is_comptime or block.owner_module().strip) return;

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].dbg_stmt;

    if (block.instructions.items.len != 0) {
        const idx = block.instructions.items[block.instructions.items.len - 1];
        if (sema.air_instructions.items(.tag)[@int_from_enum(idx)] == .dbg_stmt) {
            // The previous dbg_stmt didn't correspond to any actual code, so replace it.
            sema.air_instructions.items(.data)[@int_from_enum(idx)].dbg_stmt = .{
                .line = inst_data.line,
                .column = inst_data.column,
            };
            return;
        }
    }

    _ = try block.add_inst(.{
        .tag = .dbg_stmt,
        .data = .{ .dbg_stmt = .{
            .line = inst_data.line,
            .column = inst_data.column,
        } },
    });
}

fn zir_dbg_var(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    air_tag: Air.Inst.Tag,
) CompileError!void {
    const str_op = sema.code.instructions.items(.data)[@int_from_enum(inst)].str_op;
    const operand = try sema.resolve_inst(str_op.operand);
    const name = str_op.get_str(sema.code);
    try sema.add_dbg_var(block, operand, air_tag, name);
}

fn add_dbg_var(
    sema: *Sema,
    block: *Block,
    operand: Air.Inst.Ref,
    air_tag: Air.Inst.Tag,
    name: []const u8,
) CompileError!void {
    if (block.is_comptime or block.owner_module().strip) return;

    const mod = sema.mod;
    const operand_ty = sema.type_of(operand);
    const val_ty = switch (air_tag) {
        .dbg_var_ptr => operand_ty.child_type(mod),
        .dbg_var_val => operand_ty,
        else => unreachable,
    };
    if (try sema.type_requires_comptime(val_ty)) return;
    if (!(try sema.type_has_runtime_bits(val_ty))) return;
    if (try sema.resolve_value(operand)) |operand_val| {
        if (operand_val.can_mutate_comptime_var_state(mod)) return;
    }

    // To ensure the lexical scoping is known to backends, this alloc must be
    // within a real runtime block. We set a flag which communicates information
    // to the closest lexically enclosing block:
    // * If it is a `block_inline`, communicates to logic in `analyze_body_inner`
    //   to create a post-hoc block.
    // * Otherwise, communicates to logic in `resolve_block_body` to create a
    //   real `block` instruction.
    if (block.need_debug_scope) |ptr| ptr.* = true;

    try sema.queue_full_type_resolution(operand_ty);

    // Add the name to the AIR.
    const name_extra_index: u32 = @int_cast(sema.air_extra.items.len);
    const elements_used = name.len / 4 + 1;
    try sema.air_extra.ensure_unused_capacity(sema.gpa, elements_used);
    const buffer = mem.slice_as_bytes(sema.air_extra.unused_capacity_slice());
    @memcpy(buffer[0..name.len], name);
    buffer[name.len] = 0;
    sema.air_extra.items.len += elements_used;

    _ = try block.add_inst(.{
        .tag = air_tag,
        .data = .{ .pl_op = .{
            .payload = name_extra_index,
            .operand = operand,
        } },
    });
}

fn zir_decl_ref(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].str_tok;
    const src = inst_data.src();
    const decl_name = try mod.intern_pool.get_or_put_string(
        sema.gpa,
        inst_data.get(sema.code),
        .no_embedded_nulls,
    );
    const decl_index = try sema.lookup_identifier(block, src, decl_name);
    try sema.add_referenced_by(block, src, decl_index);
    return sema.analyze_decl_ref(decl_index);
}

fn zir_decl_val(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].str_tok;
    const src = inst_data.src();
    const decl_name = try mod.intern_pool.get_or_put_string(
        sema.gpa,
        inst_data.get(sema.code),
        .no_embedded_nulls,
    );
    const decl = try sema.lookup_identifier(block, src, decl_name);
    return sema.analyze_decl_val(block, src, decl);
}

fn lookup_identifier(sema: *Sema, block: *Block, src: LazySrcLoc, name: InternPool.NullTerminatedString) !InternPool.DeclIndex {
    const mod = sema.mod;
    var namespace = block.namespace;
    while (true) {
        if (try sema.lookup_in_namespace(block, src, namespace.to_optional(), name, false)) |decl_index| {
            return decl_index;
        }
        namespace = mod.namespace_ptr(namespace).parent.unwrap() orelse break;
    }
    unreachable; // AstGen detects use of undeclared identifiers.
}

/// This looks up a member of a specific namespace. It is affected by `usingnamespace` but
/// only for ones in the specified namespace.
fn lookup_in_namespace(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    opt_namespace_index: InternPool.OptionalNamespaceIndex,
    ident_name: InternPool.NullTerminatedString,
    observe_usingnamespace: bool,
) CompileError!?InternPool.DeclIndex {
    const mod = sema.mod;

    const namespace_index = opt_namespace_index.unwrap() orelse return null;
    const namespace = mod.namespace_ptr(namespace_index);
    const namespace_decl = mod.decl_ptr(namespace.decl_index);
    if (namespace_decl.analysis == .file_failure) {
        return error.AnalysisFail;
    }

    if (observe_usingnamespace and namespace.usingnamespace_set.count() != 0) {
        const src_file = mod.namespace_ptr(block.namespace).file_scope;

        const gpa = sema.gpa;
        var checked_namespaces: std.AutoArrayHashMapUnmanaged(*Namespace, bool) = .{};
        defer checked_namespaces.deinit(gpa);

        // Keep track of name conflicts for error notes.
        var candidates: std.ArrayListUnmanaged(InternPool.DeclIndex) = .{};
        defer candidates.deinit(gpa);

        try checked_namespaces.put(gpa, namespace, namespace.file_scope == src_file);
        var check_i: usize = 0;

        while (check_i < checked_namespaces.count()) : (check_i += 1) {
            const check_ns = checked_namespaces.keys()[check_i];
            if (check_ns.decls.get_key_adapted(ident_name, Module.DeclAdapter{ .zcu = mod })) |decl_index| {
                // Skip decls which are not marked pub, which are in a different
                // file than the `a.b`/`@hasDecl` syntax.
                const decl = mod.decl_ptr(decl_index);
                if (decl.is_pub or (src_file == decl.get_file_scope(mod) and checked_namespaces.values()[check_i])) {
                    try candidates.append(gpa, decl_index);
                }
            }
            var it = check_ns.usingnamespace_set.iterator();
            while (it.next()) |entry| {
                const sub_usingnamespace_decl_index = entry.key_ptr.*;
                // Skip the decl we're currently analysing.
                if (sub_usingnamespace_decl_index == sema.owner_decl_index) continue;
                const sub_usingnamespace_decl = mod.decl_ptr(sub_usingnamespace_decl_index);
                const sub_is_pub = entry.value_ptr.*;
                if (!sub_is_pub and src_file != sub_usingnamespace_decl.get_file_scope(mod)) {
                    // Skip usingnamespace decls which are not marked pub, which are in
                    // a different file than the `a.b`/`@hasDecl` syntax.
                    continue;
                }
                try sema.ensure_decl_analyzed(sub_usingnamespace_decl_index);
                const ns_ty = sub_usingnamespace_decl.val.to_type();
                const sub_ns = mod.namespace_ptr_unwrap(ns_ty.get_namespace_index(mod)) orelse continue;
                try checked_namespaces.put(gpa, sub_ns, src_file == sub_usingnamespace_decl.get_file_scope(mod));
            }
        }

        {
            var i: usize = 0;
            while (i < candidates.items.len) {
                if (candidates.items[i] == sema.owner_decl_index) {
                    _ = candidates.ordered_remove(i);
                } else {
                    i += 1;
                }
            }
        }

        switch (candidates.items.len) {
            0 => {},
            1 => {
                const decl_index = candidates.items[0];
                return decl_index;
            },
            else => {
                const msg = msg: {
                    const msg = try sema.err_msg(block, src, "ambiguous reference", .{});
                    errdefer msg.destroy(gpa);
                    for (candidates.items) |candidate_index| {
                        const candidate = mod.decl_ptr(candidate_index);
                        const src_loc = candidate.src_loc(mod);
                        try mod.err_note_non_lazy(src_loc, msg, "declared here", .{});
                    }
                    break :msg msg;
                };
                return sema.fail_with_owned_error_msg(block, msg);
            },
        }
    } else if (namespace.decls.get_key_adapted(ident_name, Module.DeclAdapter{ .zcu = mod })) |decl_index| {
        return decl_index;
    }

    return null;
}

fn func_decl_src(sema: *Sema, func_inst: Air.Inst.Ref) !?*Decl {
    const mod = sema.mod;
    const func_val = (try sema.resolve_value(func_inst)) orelse return null;
    if (func_val.is_undef(mod)) return null;
    const owner_decl_index = switch (mod.intern_pool.index_to_key(func_val.to_intern())) {
        .extern_func => |extern_func| extern_func.decl,
        .func => |func| func.owner_decl,
        .ptr => |ptr| switch (ptr.base_addr) {
            .decl => |decl| if (ptr.byte_offset == 0) mod.decl_ptr(decl).val.get_function(mod).?.owner_decl else return null,
            else => return null,
        },
        else => return null,
    };
    return mod.decl_ptr(owner_decl_index);
}

pub fn analyze_save_err_ret_index(sema: *Sema, block: *Block) SemaError!Air.Inst.Ref {
    const mod = sema.mod;
    const gpa = sema.gpa;

    if (block.is_comptime or block.is_typeof) {
        const index_val = try mod.int_value_u64(Type.usize, sema.comptime_err_ret_trace.items.len);
        return Air.interned_to_ref(index_val.to_intern());
    }

    if (!block.owner_module().error_tracing) return .none;

    const stack_trace_ty = sema.get_builtin_type("StackTrace") catch |err| switch (err) {
        error.NeededSourceLocation, error.GenericPoison, error.ComptimeReturn, error.ComptimeBreak => unreachable,
        else => |e| return e,
    };
    sema.resolve_type_fields(stack_trace_ty) catch |err| switch (err) {
        error.NeededSourceLocation, error.GenericPoison, error.ComptimeReturn, error.ComptimeBreak => unreachable,
        else => |e| return e,
    };
    const field_name = try mod.intern_pool.get_or_put_string(gpa, "index", .no_embedded_nulls);
    const field_index = sema.struct_field_index(block, stack_trace_ty, field_name, .unneeded) catch |err| switch (err) {
        error.AnalysisFail, error.NeededSourceLocation => @panic("std.builtin.StackTrace is corrupt"),
        error.GenericPoison, error.ComptimeReturn, error.ComptimeBreak => unreachable,
        error.OutOfMemory => |e| return e,
    };

    return try block.add_inst(.{
        .tag = .save_err_return_trace_index,
        .data = .{ .ty_pl = .{
            .ty = Air.interned_to_ref(stack_trace_ty.to_intern()),
            .payload = @int_cast(field_index),
        } },
    });
}

/// Add instructions to block to "pop" the error return trace.
/// If `operand` is provided, only pops if operand is non-error.
fn pop_error_return_trace(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    operand: Air.Inst.Ref,
    saved_error_trace_index: Air.Inst.Ref,
) CompileError!void {
    const mod = sema.mod;
    const gpa = sema.gpa;
    var is_non_error: ?bool = null;
    var is_non_error_inst: Air.Inst.Ref = undefined;
    if (operand != .none) {
        is_non_error_inst = try sema.analyze_is_non_err(block, src, operand);
        if (try sema.resolve_defined_value(block, src, is_non_error_inst)) |cond_val|
            is_non_error = cond_val.to_bool();
    } else is_non_error = true; // no operand means pop unconditionally

    if (is_non_error == true) {
        // AstGen determined this result does not go to an error-handling expr (try/catch/return etc.), or
        // the result is comptime-known to be a non-error. Either way, pop unconditionally.

        const stack_trace_ty = try sema.get_builtin_type("StackTrace");
        try sema.resolve_type_fields(stack_trace_ty);
        const ptr_stack_trace_ty = try mod.single_mut_ptr_type(stack_trace_ty);
        const err_return_trace = try block.add_ty(.err_return_trace, ptr_stack_trace_ty);
        const field_name = try mod.intern_pool.get_or_put_string(gpa, "index", .no_embedded_nulls);
        const field_ptr = try sema.struct_field_ptr(block, src, err_return_trace, field_name, src, stack_trace_ty, true);
        try sema.store_ptr2(block, src, field_ptr, src, saved_error_trace_index, src, .store);
    } else if (is_non_error == null) {
        // The result might be an error. If it is, we leave the error trace alone. If it isn't, we need
        // to pop any error trace that may have been propagated from our arguments.

        try sema.air_extra.ensure_unused_capacity(gpa, @typeInfo(Air.Block).Struct.fields.len);
        const cond_block_inst = try block.add_inst_as_index(.{
            .tag = .block,
            .data = .{
                .ty_pl = .{
                    .ty = .void_type,
                    .payload = undefined, // updated below
                },
            },
        });

        var then_block = block.make_sub_block();
        defer then_block.instructions.deinit(gpa);

        // If non-error, then pop the error return trace by restoring the index.
        const stack_trace_ty = try sema.get_builtin_type("StackTrace");
        try sema.resolve_type_fields(stack_trace_ty);
        const ptr_stack_trace_ty = try mod.single_mut_ptr_type(stack_trace_ty);
        const err_return_trace = try then_block.add_ty(.err_return_trace, ptr_stack_trace_ty);
        const field_name = try mod.intern_pool.get_or_put_string(gpa, "index", .no_embedded_nulls);
        const field_ptr = try sema.struct_field_ptr(&then_block, src, err_return_trace, field_name, src, stack_trace_ty, true);
        try sema.store_ptr2(&then_block, src, field_ptr, src, saved_error_trace_index, src, .store);
        _ = try then_block.add_br(cond_block_inst, .void_value);

        // Otherwise, do nothing
        var else_block = block.make_sub_block();
        defer else_block.instructions.deinit(gpa);
        _ = try else_block.add_br(cond_block_inst, .void_value);

        try sema.air_extra.ensure_unused_capacity(gpa, @typeInfo(Air.CondBr).Struct.fields.len +
            then_block.instructions.items.len + else_block.instructions.items.len +
            @typeInfo(Air.Block).Struct.fields.len + 1); // +1 for the sole .cond_br instruction in the .block

        const cond_br_inst: Air.Inst.Index = @enumFromInt(sema.air_instructions.len);
        try sema.air_instructions.append(gpa, .{ .tag = .cond_br, .data = .{ .pl_op = .{
            .operand = is_non_error_inst,
            .payload = sema.add_extra_assume_capacity(Air.CondBr{
                .then_body_len = @int_cast(then_block.instructions.items.len),
                .else_body_len = @int_cast(else_block.instructions.items.len),
            }),
        } } });
        sema.air_extra.append_slice_assume_capacity(@ptr_cast(then_block.instructions.items));
        sema.air_extra.append_slice_assume_capacity(@ptr_cast(else_block.instructions.items));

        sema.air_instructions.items(.data)[@int_from_enum(cond_block_inst)].ty_pl.payload = sema.add_extra_assume_capacity(Air.Block{ .body_len = 1 });
        sema.air_extra.append_assume_capacity(@int_from_enum(cond_br_inst));
    }
}

fn zir_call(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    comptime kind: enum { direct, field },
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const callee_src: LazySrcLoc = .{ .node_offset_call_func = inst_data.src_node };
    const call_src = inst_data.src();
    const ExtraType = switch (kind) {
        .direct => Zir.Inst.Call,
        .field => Zir.Inst.FieldCall,
    };
    const extra = sema.code.extra_data(ExtraType, inst_data.payload_index);
    const args_len = extra.data.flags.args_len;

    const modifier: std.builtin.CallModifier = @enumFromInt(extra.data.flags.packed_modifier);
    const ensure_result_used = extra.data.flags.ensure_result_used;
    const pop_error_return_trace = extra.data.flags.pop_error_return_trace;

    const callee: ResolvedFieldCallee = switch (kind) {
        .direct => .{ .direct = try sema.resolve_inst(extra.data.callee) },
        .field => blk: {
            const object_ptr = try sema.resolve_inst(extra.data.obj_ptr);
            const field_name = try mod.intern_pool.get_or_put_string(
                sema.gpa,
                sema.code.null_terminated_string(extra.data.field_name_start),
                .no_embedded_nulls,
            );
            const field_name_src: LazySrcLoc = .{ .node_offset_field_name = inst_data.src_node };
            break :blk try sema.field_call_bind(block, callee_src, object_ptr, field_name, field_name_src);
        },
    };
    const func: Air.Inst.Ref = switch (callee) {
        .direct => |func_inst| func_inst,
        .method => |method| method.func_inst,
    };

    const callee_ty = sema.type_of(func);
    const total_args = args_len + @int_from_bool(callee == .method);
    const func_ty = try sema.check_call_argument_count(block, func, callee_src, callee_ty, total_args, callee == .method);

    // The block index before the call, so we can potentially insert an error trace save here later.
    const block_index: Air.Inst.Index = @enumFromInt(block.instructions.items.len);

    // This will be set by `analyze_call` to indicate whether any parameter was an error (making the
    // error trace potentially dirty).
    var input_is_error = false;

    const args_info: CallArgsInfo = .{ .zir_call = .{
        .bound_arg = switch (callee) {
            .direct => .none,
            .method => |method| method.arg0_inst,
        },
        .bound_arg_src = callee_src,
        .call_inst = inst,
        .call_node_offset = inst_data.src_node,
        .num_args = args_len,
        .args_body = @ptr_cast(sema.code.extra[extra.end..]),
        .any_arg_is_error = &input_is_error,
    } };

    // AstGen ensures that a call instruction is always preceded by a dbg_stmt instruction.
    const call_dbg_node: Zir.Inst.Index = @enumFromInt(@int_from_enum(inst) - 1);
    const call_inst = try sema.analyze_call(block, func, func_ty, callee_src, call_src, modifier, ensure_result_used, args_info, call_dbg_node, .call);

    if (sema.owner_func_index == .none or
        !mod.intern_pool.func_analysis(sema.owner_func_index).calls_or_awaits_errorable_fn)
    {
        // No errorable fn actually called; we have no error return trace
        input_is_error = false;
    }

    if (block.owner_module().error_tracing and
        !block.is_comptime and !block.is_typeof and (input_is_error or pop_error_return_trace))
    {
        const return_ty = sema.type_of(call_inst);
        if (modifier != .always_tail and return_ty.is_no_return(mod))
            return call_inst; // call to "fn (...) noreturn", don't pop

        // TODO: we don't fix up the error trace for always_tail correctly, we should be doing it
        // *before* the recursive call. This will be a bit tricky to do and probably requires
        // moving this logic into analyze_call. But that's probably a good idea anyway.
        if (modifier == .always_tail)
            return call_inst;

        // If any input is an error-type, we might need to pop any trace it generated. Otherwise, we only
        // need to clean-up our own trace if we were passed to a non-error-handling expression.
        if (input_is_error or (pop_error_return_trace and return_ty.is_error(mod))) {
            const stack_trace_ty = try sema.get_builtin_type("StackTrace");
            try sema.resolve_type_fields(stack_trace_ty);
            const field_name = try mod.intern_pool.get_or_put_string(sema.gpa, "index", .no_embedded_nulls);
            const field_index = try sema.struct_field_index(block, stack_trace_ty, field_name, call_src);

            // Insert a save instruction before the arg resolution + call instructions we just generated
            const save_inst = try block.insert_inst(block_index, .{
                .tag = .save_err_return_trace_index,
                .data = .{ .ty_pl = .{
                    .ty = Air.interned_to_ref(stack_trace_ty.to_intern()),
                    .payload = @int_cast(field_index),
                } },
            });

            // Pop the error return trace, testing the result for non-error if necessary
            const operand = if (pop_error_return_trace or modifier == .always_tail) .none else call_inst;
            try sema.pop_error_return_trace(block, call_src, operand, save_inst);
        }

        return call_inst;
    } else {
        return call_inst;
    }
}

fn check_call_argument_count(
    sema: *Sema,
    block: *Block,
    func: Air.Inst.Ref,
    func_src: LazySrcLoc,
    callee_ty: Type,
    total_args: usize,
    member_fn: bool,
) !Type {
    const mod = sema.mod;
    const func_ty = func_ty: {
        switch (callee_ty.zig_type_tag(mod)) {
            .Fn => break :func_ty callee_ty,
            .Pointer => {
                const ptr_info = callee_ty.ptr_info(mod);
                if (ptr_info.flags.size == .One and Type.from_interned(ptr_info.child).zig_type_tag(mod) == .Fn) {
                    break :func_ty Type.from_interned(ptr_info.child);
                }
            },
            .Optional => {
                const opt_child = callee_ty.optional_child(mod);
                if (opt_child.zig_type_tag(mod) == .Fn or (opt_child.is_single_pointer(mod) and
                    opt_child.child_type(mod).zig_type_tag(mod) == .Fn))
                {
                    const msg = msg: {
                        const msg = try sema.err_msg(block, func_src, "cannot call optional type '{}'", .{
                            callee_ty.fmt(mod),
                        });
                        errdefer msg.destroy(sema.gpa);
                        try sema.err_note(block, func_src, msg, "consider using '.?', 'orelse' or 'if'", .{});
                        break :msg msg;
                    };
                    return sema.fail_with_owned_error_msg(block, msg);
                }
            },
            else => {},
        }
        return sema.fail(block, func_src, "type '{}' not a function", .{callee_ty.fmt(mod)});
    };

    const func_ty_info = mod.type_to_func(func_ty).?;
    const fn_params_len = func_ty_info.param_types.len;
    const args_len = total_args - @int_from_bool(member_fn);
    if (func_ty_info.is_var_args) {
        assert(call_conv_supports_var_args(func_ty_info.cc));
        if (total_args >= fn_params_len) return func_ty;
    } else if (fn_params_len == total_args) {
        return func_ty;
    }

    const maybe_decl = try sema.func_decl_src(func);
    const member_str = if (member_fn) "member function " else "";
    const variadic_str = if (func_ty_info.is_var_args) "at least " else "";
    const msg = msg: {
        const msg = try sema.err_msg(
            block,
            func_src,
            "{s}expected {s}{d} argument(s), found {d}",
            .{
                member_str,
                variadic_str,
                fn_params_len - @int_from_bool(member_fn),
                args_len,
            },
        );
        errdefer msg.destroy(sema.gpa);

        if (maybe_decl) |fn_decl| try mod.err_note_non_lazy(fn_decl.src_loc(mod), msg, "function declared here", .{});
        break :msg msg;
    };
    return sema.fail_with_owned_error_msg(block, msg);
}

fn call_builtin(
    sema: *Sema,
    block: *Block,
    call_src: LazySrcLoc,
    builtin_fn: Air.Inst.Ref,
    modifier: std.builtin.CallModifier,
    args: []const Air.Inst.Ref,
    operation: CallOperation,
) !void {
    const mod = sema.mod;
    const callee_ty = sema.type_of(builtin_fn);
    const func_ty = func_ty: {
        switch (callee_ty.zig_type_tag(mod)) {
            .Fn => break :func_ty callee_ty,
            .Pointer => {
                const ptr_info = callee_ty.ptr_info(mod);
                if (ptr_info.flags.size == .One and Type.from_interned(ptr_info.child).zig_type_tag(mod) == .Fn) {
                    break :func_ty Type.from_interned(ptr_info.child);
                }
            },
            else => {},
        }
        std.debug.panic("type '{}' is not a function calling builtin fn", .{callee_ty.fmt(mod)});
    };

    const func_ty_info = mod.type_to_func(func_ty).?;
    const fn_params_len = func_ty_info.param_types.len;
    if (args.len != fn_params_len or (func_ty_info.is_var_args and args.len < fn_params_len)) {
        std.debug.panic("parameter count mismatch calling builtin fn, expected {d}, found {d}", .{ fn_params_len, args.len });
    }

    _ = try sema.analyze_call(
        block,
        builtin_fn,
        func_ty,
        call_src,
        call_src,
        modifier,
        false,
        .{ .resolved = .{ .src = call_src, .args = args } },
        null,
        operation,
    );
}

const CallOperation = enum {
    call,
    @"@call",
    @"@panic",
    @"safety check",
    @"error return",
};

const CallArgsInfo = union(enum) {
    /// The full list of resolved (but uncoerced) arguments is known ahead of time.
    resolved: struct {
        src: LazySrcLoc,
        args: []const Air.Inst.Ref,
    },

    /// The list of resolved (but uncoerced) arguments is known ahead of time, but
    /// originated from a usage of the @call builtin at the given node offset.
    call_builtin: struct {
        call_node_offset: i32,
        args: []const Air.Inst.Ref,
    },

    /// This call corresponds to a ZIR call instruction. The arguments have not yet been
    /// resolved. They must be resolved by `analyze_call` so that argument resolution and
    /// generic instantiation may be interleaved. This is required for RLS to work on
    /// generic parameters.
    zir_call: struct {
        /// This may be `none`, in which case it is ignored. Otherwise, it is the
        /// already-resolved value of the first argument, from method call syntax.
        bound_arg: Air.Inst.Ref,
        /// The source location of `bound_arg` if it is not `null`. Otherwise `undefined`.
        bound_arg_src: LazySrcLoc,
        /// The ZIR call instruction. The parameter type is placed at this index while
        /// analyzing arguments.
        call_inst: Zir.Inst.Index,
        /// The node offset of `call_inst`.
        call_node_offset: i32,
        /// The number of arguments to this call, not including `bound_arg`.
        num_args: u32,
        /// The ZIR corresponding to all function arguments (other than `bound_arg`, if it
        /// is not `none`). Format is precisely the same as trailing data of ZIR `call`.
        args_body: []const Zir.Inst.Index,
        /// This bool will be set to true if any argument evaluated turns out to have an error set or error union type.
        /// This is used by the caller to restore the error return trace when necessary.
        any_arg_is_error: *bool,
    },

    fn count(cai: CallArgsInfo) usize {
        return switch (cai) {
            inline .resolved, .call_builtin => |resolved| resolved.args.len,
            .zir_call => |zir_call| zir_call.num_args + @int_from_bool(zir_call.bound_arg != .none),
        };
    }

    fn arg_src(cai: CallArgsInfo, block: *Block, arg_index: usize) LazySrcLoc {
        return switch (cai) {
            .resolved => |resolved| resolved.src,
            .call_builtin => |call_builtin| .{ .call_arg = .{
                .decl = block.src_decl,
                .call_node_offset = call_builtin.call_node_offset,
                .arg_index = @int_cast(arg_index),
            } },
            .zir_call => |zir_call| if (arg_index == 0 and zir_call.bound_arg != .none) {
                return zir_call.bound_arg_src;
            } else .{ .call_arg = .{
                .decl = block.src_decl,
                .call_node_offset = zir_call.call_node_offset,
                .arg_index = @int_cast(arg_index - @int_from_bool(zir_call.bound_arg != .none)),
            } },
        };
    }

    /// Analyzes the arg at `arg_index` and coerces it to `param_ty`.
    /// `param_ty` may be `generic_poison`. A value of `null` indicates a varargs parameter.
    /// `func_ty_info` may be the type before instantiation, even if a generic
    /// instantiation has been partially completed.
    fn analyze_arg(
        cai: CallArgsInfo,
        sema: *Sema,
        block: *Block,
        arg_index: usize,
        maybe_param_ty: ?Type,
        func_ty_info: InternPool.Key.FuncType,
        func_inst: Air.Inst.Ref,
    ) CompileError!Air.Inst.Ref {
        const mod = sema.mod;
        const param_count = func_ty_info.param_types.len;
        if (maybe_param_ty) |param_ty| switch (param_ty.to_intern()) {
            .generic_poison_type => {},
            else => try sema.queue_full_type_resolution(param_ty),
        };
        const uncoerced_arg: Air.Inst.Ref = switch (cai) {
            inline .resolved, .call_builtin => |resolved| resolved.args[arg_index],
            .zir_call => |zir_call| arg_val: {
                const has_bound_arg = zir_call.bound_arg != .none;
                if (arg_index == 0 and has_bound_arg) {
                    break :arg_val zir_call.bound_arg;
                }
                const real_arg_idx = arg_index - @int_from_bool(has_bound_arg);

                const arg_body = if (real_arg_idx == 0) blk: {
                    const start = zir_call.num_args;
                    const end = @int_from_enum(zir_call.args_body[0]);
                    break :blk zir_call.args_body[start..end];
                } else blk: {
                    const start = @int_from_enum(zir_call.args_body[real_arg_idx - 1]);
                    const end = @int_from_enum(zir_call.args_body[real_arg_idx]);
                    break :blk zir_call.args_body[start..end];
                };

                // Generate args to comptime params in comptime block
                const parent_comptime = block.is_comptime;
                defer block.is_comptime = parent_comptime;
                // Note that we are indexing into parameters, not arguments, so use `arg_index` instead of `real_arg_idx`
                if (arg_index < @min(param_count, 32) and func_ty_info.param_is_comptime(@int_cast(arg_index))) {
                    block.is_comptime = true;
                    // TODO set comptime_reason
                }
                // Give the arg its result type
                const provide_param_ty = if (maybe_param_ty) |t| t else Type.generic_poison;
                sema.inst_map.put_assume_capacity(zir_call.call_inst, Air.interned_to_ref(provide_param_ty.to_intern()));
                // Resolve the arg!
                const uncoerced_arg = try sema.resolve_inline_body(block, arg_body, zir_call.call_inst);

                if (sema.type_of(uncoerced_arg).zig_type_tag(mod) == .NoReturn) {
                    // This terminates resolution of arguments. The caller should
                    // propagate this.
                    return uncoerced_arg;
                }

                if (sema.type_of(uncoerced_arg).is_error(mod)) {
                    zir_call.any_arg_is_error.* = true;
                }

                break :arg_val uncoerced_arg;
            },
        };
        const param_ty = maybe_param_ty orelse {
            return sema.coerce_var_arg_param(block, uncoerced_arg, cai.arg_src(block, arg_index));
        };
        switch (param_ty.to_intern()) {
            .generic_poison_type => return uncoerced_arg,
            else => return sema.coerce_extra(
                block,
                param_ty,
                uncoerced_arg,
                cai.arg_src(block, arg_index),
                .{ .param_src = .{
                    .func_inst = func_inst,
                    .param_i = @int_cast(arg_index),
                } },
            ) catch |err| switch (err) {
                error.NotCoercible => unreachable,
                else => |e| return e,
            },
        }
    }
};

/// While performing an inline call, we need to switch between two Sema states a few times: the
/// state for the caller (with the callee's `code`, `fn_ret_ty`, etc), and the state for the callee.
/// These cannot be two separate Sema instances as they must share AIR.
/// Therefore, this struct acts as a helper to switch between the two.
/// This switching is required during argument evaluation, where function argument analysis must be
/// interleaved with resolving generic parameter types.
const InlineCallSema = struct {
    sema: *Sema,
    cur: enum {
        caller,
        callee,
    },

    other_code: Zir,
    other_func_index: InternPool.Index,
    other_fn_ret_ty: Type,
    other_fn_ret_ty_ies: ?*InferredErrorSet,
    other_inst_map: InstMap,
    other_error_return_trace_index_on_fn_entry: Air.Inst.Ref,
    other_generic_owner: InternPool.Index,
    other_generic_call_src: LazySrcLoc,
    other_generic_call_decl: InternPool.OptionalDeclIndex,

    /// Sema should currently be set up for the caller (i.e. unchanged yet). This init will not
    /// change that. The other parameters contain data for the callee Sema. The other modified
    /// Sema fields are all initialized to default values for the callee.
    /// Must call deinit on the result.
    fn init(
        sema: *Sema,
        callee_code: Zir,
        callee_func_index: InternPool.Index,
        callee_error_return_trace_index_on_fn_entry: Air.Inst.Ref,
    ) InlineCallSema {
        return .{
            .sema = sema,
            .cur = .caller,
            .other_code = callee_code,
            .other_func_index = callee_func_index,
            .other_fn_ret_ty = Type.void,
            .other_fn_ret_ty_ies = null,
            .other_inst_map = .{},
            .other_error_return_trace_index_on_fn_entry = callee_error_return_trace_index_on_fn_entry,
            .other_generic_owner = .none,
            .other_generic_call_src = .unneeded,
            .other_generic_call_decl = .none,
        };
    }

    /// Switch back to the caller Sema if necessary and free all temporary state of the callee Sema.
    fn deinit(ics: *InlineCallSema) void {
        switch (ics.cur) {
            .caller => {},
            .callee => ics.swap(),
        }
        // Callee Sema owns the inst_map memory
        ics.other_inst_map.deinit(ics.sema.gpa);
        ics.* = undefined;
    }

    /// Returns a Sema instance suitable for usage from the caller context.
    fn caller(ics: *InlineCallSema) *Sema {
        switch (ics.cur) {
            .caller => {},
            .callee => ics.swap(),
        }
        return ics.sema;
    }

    /// Returns a Sema instance suitable for usage from the callee context.
    fn callee(ics: *InlineCallSema) *Sema {
        switch (ics.cur) {
            .caller => ics.swap(),
            .callee => {},
        }
        return ics.sema;
    }

    /// Internal use only. Swaps to the other Sema state.
    fn swap(ics: *InlineCallSema) void {
        ics.cur = switch (ics.cur) {
            .caller => .callee,
            .callee => .caller,
        };
        // zig fmt: off
        std.mem.swap(Zir,                &ics.sema.code,              &ics.other_code);
        std.mem.swap(InternPool.Index,   &ics.sema.func_index,        &ics.other_func_index);
        std.mem.swap(Type,               &ics.sema.fn_ret_ty,         &ics.other_fn_ret_ty);
        std.mem.swap(?*InferredErrorSet, &ics.sema.fn_ret_ty_ies,     &ics.other_fn_ret_ty_ies);
        std.mem.swap(InstMap,            &ics.sema.inst_map,          &ics.other_inst_map);
        std.mem.swap(InternPool.Index,   &ics.sema.generic_owner,     &ics.other_generic_owner);
        std.mem.swap(LazySrcLoc,         &ics.sema.generic_call_src,  &ics.other_generic_call_src);
        std.mem.swap(InternPool.OptionalDeclIndex, &ics.sema.generic_call_decl, &ics.other_generic_call_decl);
        std.mem.swap(Air.Inst.Ref,       &ics.sema.error_return_trace_index_on_fn_entry, &ics.other_error_return_trace_index_on_fn_entry);
        // zig fmt: on
    }
};

fn analyze_call(
    sema: *Sema,
    block: *Block,
    func: Air.Inst.Ref,
    func_ty: Type,
    func_src: LazySrcLoc,
    call_src: LazySrcLoc,
    modifier: std.builtin.CallModifier,
    ensure_result_used: bool,
    args_info: CallArgsInfo,
    call_dbg_node: ?Zir.Inst.Index,
    operation: CallOperation,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;

    const callee_ty = sema.type_of(func);
    const func_ty_info = mod.type_to_func(func_ty).?;
    const cc = func_ty_info.cc;
    if (try sema.resolve_value(func)) |func_val|
        if (func_val.is_undef(mod))
            return sema.fail_with_use_of_undef(block, call_src);
    if (cc == .Naked) {
        const maybe_decl = try sema.func_decl_src(func);
        const msg = msg: {
            const msg = try sema.err_msg(
                block,
                func_src,
                "unable to call function with naked calling convention",
                .{},
            );
            errdefer msg.destroy(sema.gpa);

            if (maybe_decl) |fn_decl| try mod.err_note_non_lazy(fn_decl.src_loc(mod), msg, "function declared here", .{});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    const call_tag: Air.Inst.Tag = switch (modifier) {
        .auto,
        .always_inline,
        .compile_time,
        .no_async,
        => Air.Inst.Tag.call,

        .never_tail => Air.Inst.Tag.call_never_tail,
        .never_inline => Air.Inst.Tag.call_never_inline,
        .always_tail => Air.Inst.Tag.call_always_tail,

        .async_kw => return sema.fail_with_use_of_async(block, call_src),
    };

    if (modifier == .never_inline and func_ty_info.cc == .Inline) {
        return sema.fail(block, call_src, "'never_inline' call of inline function", .{});
    }
    if (modifier == .always_inline and func_ty_info.is_noinline) {
        return sema.fail(block, call_src, "'always_inline' call of noinline function", .{});
    }

    const gpa = sema.gpa;

    var is_generic_call = func_ty_info.is_generic;
    var is_comptime_call = block.is_comptime or modifier == .compile_time;
    var is_inline_call = is_comptime_call or modifier == .always_inline or func_ty_info.cc == .Inline;
    var comptime_reason: ?*const Block.ComptimeReason = null;
    if (!is_inline_call and !is_comptime_call) {
        if (sema.type_requires_comptime(Type.from_interned(func_ty_info.return_type))) |ct| {
            is_comptime_call = ct;
            is_inline_call = ct;
            if (ct) {
                comptime_reason = &.{ .comptime_ret_ty = .{
                    .block = block,
                    .func = func,
                    .func_src = func_src,
                    .return_ty = Type.from_interned(func_ty_info.return_type),
                } };
            }
        } else |err| switch (err) {
            error.GenericPoison => is_generic_call = true,
            else => |e| return e,
        }
    }

    if (sema.func_is_naked and !is_inline_call and !is_comptime_call) {
        const msg = msg: {
            const msg = try sema.err_msg(block, call_src, "runtime {s} not allowed in naked function", .{@tag_name(operation)});
            errdefer msg.destroy(sema.gpa);

            switch (operation) {
                .call, .@"@call", .@"@panic", .@"error return" => {},
                .@"safety check" => try sema.err_note(block, call_src, msg, "use @setRuntimeSafety to disable runtime safety", .{}),
            }
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    if (!is_inline_call and is_generic_call) {
        if (sema.instantiate_generic_call(
            block,
            func,
            func_src,
            call_src,
            ensure_result_used,
            args_info,
            call_tag,
            call_dbg_node,
        )) |some| {
            return some;
        } else |err| switch (err) {
            error.GenericPoison => {
                is_inline_call = true;
            },
            error.ComptimeReturn => {
                is_inline_call = true;
                is_comptime_call = true;
                comptime_reason = &.{ .comptime_ret_ty = .{
                    .block = block,
                    .func = func,
                    .func_src = func_src,
                    .return_ty = Type.from_interned(func_ty_info.return_type),
                } };
            },
            else => |e| return e,
        }
    }

    if (is_comptime_call and modifier == .never_inline) {
        return sema.fail(block, call_src, "unable to perform 'never_inline' call at compile-time", .{});
    }

    const result: Air.Inst.Ref = if (is_inline_call) res: {
        const func_val = try sema.resolve_const_defined_value(block, func_src, func, .{
            .needed_comptime_reason = "function being called at comptime must be comptime-known",
            .block_comptime_reason = comptime_reason,
        });
        const module_fn_index = switch (mod.intern_pool.index_to_key(func_val.to_intern())) {
            .extern_func => return sema.fail(block, call_src, "{s} call of extern function", .{
                @as([]const u8, if (is_comptime_call) "comptime" else "inline"),
            }),
            .func => func_val.to_intern(),
            .ptr => |ptr| blk: {
                switch (ptr.base_addr) {
                    .decl => |decl| if (ptr.byte_offset == 0) {
                        const func_val_ptr = mod.decl_ptr(decl).val.to_intern();
                        const intern_index = mod.intern_pool.index_to_key(func_val_ptr);
                        if (intern_index == .extern_func or (intern_index == .variable and intern_index.variable.is_extern))
                            return sema.fail(block, call_src, "{s} call of extern function pointer", .{
                                @as([]const u8, if (is_comptime_call) "comptime" else "inline"),
                            });
                        break :blk func_val_ptr;
                    },
                    else => {},
                }
                assert(callee_ty.is_ptr_at_runtime(mod));
                return sema.fail(block, call_src, "{s} call of function pointer", .{
                    @as([]const u8, if (is_comptime_call) "comptime" else "inline"),
                });
            },
            else => unreachable,
        };
        if (func_ty_info.is_var_args) {
            return sema.fail(block, call_src, "{s} call of variadic function", .{
                @as([]const u8, if (is_comptime_call) "comptime" else "inline"),
            });
        }

        // Analyze the ZIR. The same ZIR gets analyzed into a runtime function
        // or an inlined call depending on what union tag the `label` field is
        // set to in the `Block`.
        // This block instruction will be used to capture the return value from the
        // inlined function.
        const need_debug_scope = !is_comptime_call and !block.is_typeof and !block.owner_module().strip;
        const block_inst: Air.Inst.Index = @enumFromInt(sema.air_instructions.len);
        try sema.air_instructions.append(gpa, .{
            .tag = if (need_debug_scope) .dbg_inline_block else .block,
            .data = undefined,
        });
        // This one is shared among sub-blocks within the same callee, but not
        // shared among the entire inline/comptime call stack.
        var inlining: Block.Inlining = .{
            .call_block = block,
            .call_src = call_src,
            .has_comptime_args = false,
            .func = module_fn_index,
            .comptime_result = undefined,
            .merges = .{
                .src_locs = .{},
                .results = .{},
                .br_list = .{},
                .block_inst = block_inst,
            },
        };

        const module_fn = mod.func_info(module_fn_index);
        const fn_owner_decl = mod.decl_ptr(module_fn.owner_decl);

        // We effectively want a child Sema here, but can't literally do that, because we need AIR
        // to be shared. InlineCallSema is a wrapper which handles this for us. While `ics` is in
        // scope, we should use its `caller`/`callee` methods rather than using `sema` directly
        // whenever performing an operation where the difference matters.
        var ics = InlineCallSema.init(
            sema,
            fn_owner_decl.get_file_scope(mod).zir,
            module_fn_index,
            block.error_return_trace_index,
        );
        defer ics.deinit();

        var child_block: Block = .{
            .parent = null,
            .sema = sema,
            .src_decl = module_fn.owner_decl,
            .namespace = fn_owner_decl.src_namespace,
            .instructions = .{},
            .label = null,
            .inlining = &inlining,
            .is_typeof = block.is_typeof,
            .is_comptime = is_comptime_call,
            .comptime_reason = comptime_reason,
            .error_return_trace_index = block.error_return_trace_index,
            .runtime_cond = block.runtime_cond,
            .runtime_loop = block.runtime_loop,
            .runtime_index = block.runtime_index,
        };

        const merges = &child_block.inlining.?.merges;

        defer child_block.instructions.deinit(gpa);
        defer merges.deinit(gpa);

        try sema.emit_backward_branch(block, call_src);

        // Whether this call should be memoized, set to false if the call can
        // mutate comptime state.
        var should_memoize = true;

        // If it's a comptime function call, we need to memoize it as long as no external
        // comptime memory is mutated.
        const memoized_arg_values = try sema.arena.alloc(InternPool.Index, func_ty_info.param_types.len);

        const owner_info = mod.type_to_func(fn_owner_decl.type_of(mod)).?;
        const new_param_types = try sema.arena.alloc(InternPool.Index, owner_info.param_types.len);
        var new_fn_info: InternPool.GetFuncTypeKey = .{
            .param_types = new_param_types,
            .return_type = owner_info.return_type,
            .noalias_bits = owner_info.noalias_bits,
            .cc = if (owner_info.cc_is_generic) null else owner_info.cc,
            .is_var_args = owner_info.is_var_args,
            .is_noinline = owner_info.is_noinline,
            .section_is_generic = owner_info.section_is_generic,
            .addrspace_is_generic = owner_info.addrspace_is_generic,
            .is_generic = owner_info.is_generic,
        };

        // This will have return instructions analyzed as break instructions to
        // the block_inst above. Here we are performing "comptime/inline semantic analysis"
        // for a function body, which means we must map the parameter ZIR instructions to
        // the AIR instructions of the callsite. The callee could be a generic function
        // which means its parameter type expressions must be resolved in order and used
        // to successively coerce the arguments.
        const fn_info = ics.callee().code.get_fn_info(module_fn.zir_body_inst.resolve(ip));
        try ics.callee().inst_map.ensure_space_for_instructions(gpa, fn_info.param_body);

        var arg_i: u32 = 0;
        for (fn_info.param_body) |inst| {
            const opt_noreturn_ref = try analyze_inline_call_arg(
                &ics,
                block,
                &child_block,
                inst,
                new_param_types,
                &arg_i,
                args_info,
                is_comptime_call,
                &should_memoize,
                memoized_arg_values,
                func_ty_info,
                func,
            );
            if (opt_noreturn_ref) |ref| {
                // Analyzing this argument gave a ref of a noreturn type. Terminate argument analysis here.
                return ref;
            }
        }

        // From here, we only really need to use the callee Sema. Make it the active one, then we
        // can just use `sema` directly.
        _ = ics.callee();

        if (!inlining.has_comptime_args) {
            if (module_fn.analysis(ip).state == .sema_failure)
                return error.AnalysisFail;

            var block_it = block;
            while (block_it.inlining) |parent_inlining| {
                if (!parent_inlining.has_comptime_args and parent_inlining.func == module_fn_index) {
                    const err_msg = try sema.err_msg(block, call_src, "inline call is recursive", .{});
                    return sema.fail_with_owned_error_msg(null, err_msg);
                }
                block_it = parent_inlining.call_block;
            }
        }

        // In case it is a generic function with an expression for the return type that depends
        // on parameters, we must now do the same for the return type as we just did with
        // each of the parameters, resolving the return type and providing it to the child
        // `Sema` so that it can be used for the `ret_ptr` instruction.
        const ret_ty_inst = if (fn_info.ret_ty_body.len != 0)
            try sema.resolve_inline_body(&child_block, fn_info.ret_ty_body, module_fn.zir_body_inst.resolve(ip))
        else
            try sema.resolve_inst(fn_info.ret_ty_ref);
        const ret_ty_src: LazySrcLoc = .{ .node_offset_fn_type_ret_ty = 0 };
        sema.fn_ret_ty = try sema.analyze_as_type(&child_block, ret_ty_src, ret_ty_inst);
        if (module_fn.analysis(ip).inferred_error_set) {
            // Create a fresh inferred error set type for inline/comptime calls.
            const ies = try sema.arena.create(InferredErrorSet);
            ies.* = .{ .func = .none };
            sema.fn_ret_ty_ies = ies;
            sema.fn_ret_ty = Type.from_interned((try ip.get(gpa, .{ .error_union_type = .{
                .error_set_type = .adhoc_inferred_error_set_type,
                .payload_type = sema.fn_ret_ty.to_intern(),
            } })));
        }

        // This `res2` is here instead of directly breaking from `res` due to a stage1
        // bug generating invalid LLVM IR.
        const res2: Air.Inst.Ref = res2: {
            if (should_memoize and is_comptime_call) {
                if (mod.intern_pool.get_if_exists(.{ .memoized_call = .{
                    .func = module_fn_index,
                    .arg_values = memoized_arg_values,
                    .result = .none,
                } })) |memoized_call_index| {
                    const memoized_call = mod.intern_pool.index_to_key(memoized_call_index).memoized_call;
                    break :res2 Air.interned_to_ref(memoized_call.result);
                }
            }

            new_fn_info.return_type = sema.fn_ret_ty.to_intern();
            if (!is_comptime_call and !block.is_typeof) {
                const zir_tags = sema.code.instructions.items(.tag);
                for (fn_info.param_body) |param| switch (zir_tags[@int_from_enum(param)]) {
                    .param, .param_comptime => {
                        const inst_data = sema.code.instructions.items(.data)[@int_from_enum(param)].pl_tok;
                        const extra = sema.code.extra_data(Zir.Inst.Param, inst_data.payload_index);
                        const param_name = sema.code.null_terminated_string(extra.data.name);
                        const inst = sema.inst_map.get(param).?;

                        try sema.add_dbg_var(&child_block, inst, .dbg_var_val, param_name);
                    },
                    .param_anytype, .param_anytype_comptime => {
                        const inst_data = sema.code.instructions.items(.data)[@int_from_enum(param)].str_tok;
                        const param_name = inst_data.get(sema.code);
                        const inst = sema.inst_map.get(param).?;

                        try sema.add_dbg_var(&child_block, inst, .dbg_var_val, param_name);
                    },
                    else => continue,
                };
            }

            if (is_comptime_call and ensure_result_used) {
                try sema.ensure_result_used(block, sema.fn_ret_ty, call_src);
            }

            if (is_comptime_call or block.is_typeof) {
                // Save the error trace as our first action in the function
                // to match the behavior of runtime function calls.
                const error_return_trace_index = try sema.analyze_save_err_ret_index(&child_block);
                sema.error_return_trace_index_on_fn_entry = error_return_trace_index;
                child_block.error_return_trace_index = error_return_trace_index;
            }

            const result = result: {
                sema.analyze_fn_body(&child_block, fn_info.body) catch |err| switch (err) {
                    error.ComptimeReturn => break :result inlining.comptime_result,
                    else => |e| return e,
                };
                break :result try sema.resolve_analyzed_block(block, call_src, &child_block, merges, need_debug_scope);
            };

            if (is_comptime_call) {
                const result_val = try sema.resolve_const_value(block, .unneeded, result, undefined);
                const result_interned = result_val.to_intern();

                // Transform ad-hoc inferred error set types into concrete error sets.
                const result_transformed = try sema.resolve_ad_hoc_inferred_error_set(block, call_src, result_interned);

                // If the result can mutate comptime vars, we must not memoize it, as it contains
                // a reference to `comptime_allocs` so is not stable across instances of `Sema`.
                // TODO: check whether any external comptime memory was mutated by the
                // comptime function call. If so, then do not memoize the call here.
                if (should_memoize and !Value.from_interned(result_interned).can_mutate_comptime_var_state(mod)) {
                    _ = try mod.intern(.{ .memoized_call = .{
                        .func = module_fn_index,
                        .arg_values = memoized_arg_values,
                        .result = result_transformed,
                    } });
                }

                break :res2 Air.interned_to_ref(result_transformed);
            }

            if (try sema.resolve_value(result)) |result_val| {
                const result_transformed = try sema.resolve_ad_hoc_inferred_error_set(block, call_src, result_val.to_intern());
                break :res2 Air.interned_to_ref(result_transformed);
            }

            const new_ty = try sema.resolve_ad_hoc_inferred_error_set_ty(block, call_src, sema.type_of(result).to_intern());
            if (new_ty != .none) {
                // TODO: mutate in place the previous instruction if possible
                // rather than adding a bitcast instruction.
                break :res2 try block.add_bit_cast(Type.from_interned(new_ty), result);
            }

            break :res2 result;
        };

        break :res res2;
    } else res: {
        assert(!func_ty_info.is_generic);

        const args = try sema.arena.alloc(Air.Inst.Ref, args_info.count());
        for (args, 0..) |*arg_out, arg_idx| {
            // Non-generic, so param types are already resolved
            const param_ty: ?Type = if (arg_idx < func_ty_info.param_types.len) ty: {
                break :ty Type.from_interned(func_ty_info.param_types.get(ip)[arg_idx]);
            } else null;
            if (param_ty) |t| assert(!t.is_generic_poison());
            arg_out.* = try args_info.analyze_arg(sema, block, arg_idx, param_ty, func_ty_info, func);
            try sema.validate_runtime_value(block, args_info.arg_src(block, arg_idx), arg_out.*);
            if (sema.type_of(arg_out.*).zig_type_tag(mod) == .NoReturn) {
                return arg_out.*;
            }
        }

        if (call_dbg_node) |some| try sema.zir_dbg_stmt(block, some);

        try sema.queue_full_type_resolution(Type.from_interned(func_ty_info.return_type));
        if (sema.owner_func_index != .none and Type.from_interned(func_ty_info.return_type).is_error(mod)) {
            ip.func_analysis(sema.owner_func_index).calls_or_awaits_errorable_fn = true;
        }

        if (try sema.resolve_value(func)) |func_val| {
            if (mod.intern_pool.is_func_body(func_val.to_intern())) {
                try mod.ensure_func_body_analysis_queued(func_val.to_intern());
            }
        }

        try sema.air_extra.ensure_unused_capacity(gpa, @typeInfo(Air.Call).Struct.fields.len +
            args.len);
        const func_inst = try block.add_inst(.{
            .tag = call_tag,
            .data = .{ .pl_op = .{
                .operand = func,
                .payload = sema.add_extra_assume_capacity(Air.Call{
                    .args_len = @int_cast(args.len),
                }),
            } },
        });
        sema.append_refs_assume_capacity(args);

        if (call_tag == .call_always_tail) {
            if (ensure_result_used) {
                try sema.ensure_result_used(block, sema.type_of(func_inst), call_src);
            }
            return sema.handle_tail_call(block, call_src, func_ty, func_inst);
        }
        if (block.want_safety() and func_ty_info.return_type == .noreturn_type) skip_safety: {
            // Function pointers and extern functions aren't guaranteed to
            // actually be noreturn so we add a safety check for them.
            if (try sema.resolve_value(func)) |func_val| {
                switch (mod.intern_pool.index_to_key(func_val.to_intern())) {
                    .func => break :skip_safety,
                    .ptr => |ptr| if (ptr.byte_offset == 0) switch (ptr.base_addr) {
                        .decl => |decl| if (!mod.decl_ptr(decl).is_extern(mod)) break :skip_safety,
                        else => {},
                    },
                    else => {},
                }
            }
            try sema.safety_panic(block, call_src, .noreturn_returned);
            return .unreachable_value;
        }
        if (func_ty_info.return_type == .noreturn_type) {
            _ = try block.add_no_op(.unreach);
            return .unreachable_value;
        }
        break :res func_inst;
    };

    if (ensure_result_used) {
        try sema.ensure_result_used(block, sema.type_of(result), call_src);
    }
    return result;
}

fn handle_tail_call(sema: *Sema, block: *Block, call_src: LazySrcLoc, func_ty: Type, result: Air.Inst.Ref) !Air.Inst.Ref {
    const mod = sema.mod;
    const target = mod.get_target();
    const backend = mod.comp.get_zig_backend();
    if (!target_util.supports_tail_call(target, backend)) {
        return sema.fail(block, call_src, "unable to perform tail call: compiler backend '{s}' does not support tail calls on target architecture '{s}' with the selected CPU feature flags", .{
            @tag_name(backend), @tag_name(target.cpu.arch),
        });
    }
    const func_decl = mod.func_owner_decl_ptr(sema.owner_func_index);
    if (!func_ty.eql(func_decl.type_of(mod), mod)) {
        return sema.fail(block, call_src, "unable to perform tail call: type of function being called '{}' does not match type of calling function '{}'", .{
            func_ty.fmt(mod), func_decl.type_of(mod).fmt(mod),
        });
    }
    _ = try block.add_un_op(.ret, result);
    return .unreachable_value;
}

/// Usually, returns null. If an argument was noreturn, returns that ref (which should become the call result).
fn analyze_inline_call_arg(
    ics: *InlineCallSema,
    arg_block: *Block,
    param_block: *Block,
    inst: Zir.Inst.Index,
    new_param_types: []InternPool.Index,
    arg_i: *u32,
    args_info: CallArgsInfo,
    is_comptime_call: bool,
    should_memoize: *bool,
    memoized_arg_values: []InternPool.Index,
    func_ty_info: InternPool.Key.FuncType,
    func_inst: Air.Inst.Ref,
) !?Air.Inst.Ref {
    const mod = ics.sema.mod;
    const ip = &mod.intern_pool;
    const zir_tags = ics.callee().code.instructions.items(.tag);
    switch (zir_tags[@int_from_enum(inst)]) {
        .param_comptime, .param_anytype_comptime => param_block.inlining.?.has_comptime_args = true,
        else => {},
    }
    switch (zir_tags[@int_from_enum(inst)]) {
        .param, .param_comptime => {
            // Evaluate the parameter type expression now that previous ones have
            // been mapped, and coerce the corresponding argument to it.
            const pl_tok = ics.callee().code.instructions.items(.data)[@int_from_enum(inst)].pl_tok;
            const param_src = pl_tok.src();
            const extra = ics.callee().code.extra_data(Zir.Inst.Param, pl_tok.payload_index);
            const param_body = ics.callee().code.body_slice(extra.end, extra.data.body_len);
            const param_ty = param_ty: {
                const raw_param_ty = func_ty_info.param_types.get(ip)[arg_i.*];
                if (raw_param_ty != .generic_poison_type) break :param_ty raw_param_ty;
                const param_ty_inst = try ics.callee().resolve_inline_body(param_block, param_body, inst);
                const param_ty = try ics.callee().analyze_as_type(param_block, param_src, param_ty_inst);
                break :param_ty param_ty.to_intern();
            };
            new_param_types[arg_i.*] = param_ty;
            const casted_arg = try args_info.analyze_arg(ics.caller(), arg_block, arg_i.*, Type.from_interned(param_ty), func_ty_info, func_inst);
            if (ics.caller().type_of(casted_arg).zig_type_tag(mod) == .NoReturn) {
                return casted_arg;
            }
            const arg_src = args_info.arg_src(arg_block, arg_i.*);
            if (try ics.callee().type_requires_comptime(Type.from_interned(param_ty))) {
                _ = try ics.caller().resolve_const_value(arg_block, arg_src, casted_arg, .{
                    .needed_comptime_reason = "argument to parameter with comptime-only type must be comptime-known",
                    .block_comptime_reason = param_block.comptime_reason,
                });
            } else if (!is_comptime_call and zir_tags[@int_from_enum(inst)] == .param_comptime) {
                _ = try ics.caller().resolve_const_value(arg_block, arg_src, casted_arg, .{
                    .needed_comptime_reason = "parameter is comptime",
                });
            }

            if (is_comptime_call) {
                ics.callee().inst_map.put_assume_capacity_no_clobber(inst, casted_arg);
                const arg_val = try ics.caller().resolve_const_value(arg_block, arg_src, casted_arg, .{
                    .needed_comptime_reason = "argument to function being called at comptime must be comptime-known",
                    .block_comptime_reason = param_block.comptime_reason,
                });
                switch (arg_val.to_intern()) {
                    .generic_poison, .generic_poison_type => {
                        // This function is currently evaluated as part of an as-of-yet unresolvable
                        // parameter or return type.
                        return error.GenericPoison;
                    },
                    else => {},
                }
                // Needed so that lazy values do not trigger
                // assertion due to type not being resolved
                // when the hash function is called.
                const resolved_arg_val = try ics.caller().resolve_lazy_value(arg_val);
                should_memoize.* = should_memoize.* and !resolved_arg_val.can_mutate_comptime_var_state(mod);
                memoized_arg_values[arg_i.*] = resolved_arg_val.to_intern();
            } else {
                ics.callee().inst_map.put_assume_capacity_no_clobber(inst, casted_arg);
            }

            if (try ics.caller().resolve_value(casted_arg)) |_| {
                param_block.inlining.?.has_comptime_args = true;
            }

            arg_i.* += 1;
        },
        .param_anytype, .param_anytype_comptime => {
            // No coercion needed.
            const uncasted_arg = try args_info.analyze_arg(ics.caller(), arg_block, arg_i.*, Type.generic_poison, func_ty_info, func_inst);
            if (ics.caller().type_of(uncasted_arg).zig_type_tag(mod) == .NoReturn) {
                return uncasted_arg;
            }
            const arg_src = args_info.arg_src(arg_block, arg_i.*);
            new_param_types[arg_i.*] = ics.caller().type_of(uncasted_arg).to_intern();

            if (is_comptime_call) {
                ics.callee().inst_map.put_assume_capacity_no_clobber(inst, uncasted_arg);
                const arg_val = try ics.caller().resolve_const_value(arg_block, arg_src, uncasted_arg, .{
                    .needed_comptime_reason = "argument to function being called at comptime must be comptime-known",
                    .block_comptime_reason = param_block.comptime_reason,
                });
                switch (arg_val.to_intern()) {
                    .generic_poison, .generic_poison_type => {
                        // This function is currently evaluated as part of an as-of-yet unresolvable
                        // parameter or return type.
                        return error.GenericPoison;
                    },
                    else => {},
                }
                // Needed so that lazy values do not trigger
                // assertion due to type not being resolved
                // when the hash function is called.
                const resolved_arg_val = try ics.caller().resolve_lazy_value(arg_val);
                should_memoize.* = should_memoize.* and !resolved_arg_val.can_mutate_comptime_var_state(mod);
                memoized_arg_values[arg_i.*] = resolved_arg_val.to_intern();
            } else {
                if (zir_tags[@int_from_enum(inst)] == .param_anytype_comptime) {
                    _ = try ics.caller().resolve_const_value(arg_block, arg_src, uncasted_arg, .{
                        .needed_comptime_reason = "parameter is comptime",
                    });
                }
                ics.callee().inst_map.put_assume_capacity_no_clobber(inst, uncasted_arg);
            }

            if (try ics.caller().resolve_value(uncasted_arg)) |_| {
                param_block.inlining.?.has_comptime_args = true;
            }

            arg_i.* += 1;
        },
        else => {},
    }

    return null;
}

fn instantiate_generic_call(
    sema: *Sema,
    block: *Block,
    func: Air.Inst.Ref,
    func_src: LazySrcLoc,
    call_src: LazySrcLoc,
    ensure_result_used: bool,
    args_info: CallArgsInfo,
    call_tag: Air.Inst.Tag,
    call_dbg_node: ?Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;

    const func_val = try sema.resolve_const_defined_value(block, func_src, func, .{
        .needed_comptime_reason = "generic function being called must be comptime-known",
    });
    const generic_owner = switch (mod.intern_pool.index_to_key(func_val.to_intern())) {
        .func => func_val.to_intern(),
        .ptr => |ptr| mod.decl_ptr(ptr.base_addr.decl).val.to_intern(),
        else => unreachable,
    };
    const generic_owner_func = mod.intern_pool.index_to_key(generic_owner).func;
    const generic_owner_ty_info = mod.type_to_func(Type.from_interned(generic_owner_func.ty)).?;

    try sema.declare_dependency(.{ .src_hash = generic_owner_func.zir_body_inst });

    // Even though there may already be a generic instantiation corresponding
    // to this callsite, we must evaluate the expressions of the generic
    // function signature with the values of the callsite plugged in.
    // Importantly, this may include type coercions that determine whether the
    // instantiation is a match of a previous instantiation.
    // The actual monomorphization happens via adding `func_instance` to
    // `InternPool`.

    const fn_owner_decl = mod.decl_ptr(generic_owner_func.owner_decl);
    const namespace_index = fn_owner_decl.src_namespace;
    const namespace = mod.namespace_ptr(namespace_index);
    const fn_zir = namespace.file_scope.zir;
    const fn_info = fn_zir.get_fn_info(generic_owner_func.zir_body_inst.resolve(ip));

    const comptime_args = try sema.arena.alloc(InternPool.Index, args_info.count());
    @memset(comptime_args, .none);

    // We may overestimate the number of runtime args, but this will definitely be sufficient.
    const max_runtime_args = args_info.count() - @pop_count(generic_owner_ty_info.comptime_bits);
    var runtime_args = try std.ArrayListUnmanaged(Air.Inst.Ref).init_capacity(sema.arena, max_runtime_args);

    // Re-run the block that creates the function, with the comptime parameters
    // pre-populated inside `inst_map`. This causes `param_comptime` and
    // `param_anytype_comptime` ZIR instructions to be ignored, resulting in a
    // new, monomorphized function, with the comptime parameters elided.
    var child_sema: Sema = .{
        .mod = mod,
        .gpa = gpa,
        .arena = sema.arena,
        .code = fn_zir,
        // We pass the generic callsite's owner decl here because whatever `Decl`
        // dependencies are chased at this point should be attached to the
        // callsite, not the `Decl` associated with the `func_instance`.
        .owner_decl = sema.owner_decl,
        .owner_decl_index = sema.owner_decl_index,
        .func_index = sema.owner_func_index,
        // This may not be known yet, since the calling convention could be generic, but there
        // should be no illegal instructions encountered while creating the function anyway.
        .func_is_naked = false,
        .fn_ret_ty = Type.void,
        .fn_ret_ty_ies = null,
        .owner_func_index = .none,
        .comptime_args = comptime_args,
        .generic_owner = generic_owner,
        .generic_call_src = call_src,
        .generic_call_decl = block.src_decl.to_optional(),
        .branch_quota = sema.branch_quota,
        .branch_count = sema.branch_count,
        .comptime_err_ret_trace = sema.comptime_err_ret_trace,
    };
    defer child_sema.deinit();

    var child_block: Block = .{
        .parent = null,
        .sema = &child_sema,
        .src_decl = generic_owner_func.owner_decl,
        .namespace = namespace_index,
        .instructions = .{},
        .inlining = null,
        .is_comptime = true,
    };
    defer child_block.instructions.deinit(gpa);

    try child_sema.inst_map.ensure_space_for_instructions(gpa, fn_info.param_body);

    for (fn_info.param_body[0..args_info.count()], 0..) |param_inst, arg_index| {
        const param_tag = fn_zir.instructions.items(.tag)[@int_from_enum(param_inst)];

        const param_ty = switch (generic_owner_ty_info.param_types.get(ip)[arg_index]) {
            else => |ty| Type.from_interned(ty), // parameter is not generic, so type is already resolved
            .generic_poison_type => param_ty: {
                // We have every parameter before this one, so can resolve this parameter's type now.
                // However, first check the param type, since it may be anytype.
                switch (param_tag) {
                    .param_anytype, .param_anytype_comptime => {
                        // The parameter doesn't have a type.
                        break :param_ty Type.generic_poison;
                    },
                    .param, .param_comptime => {
                        // We now know every prior parameter, so can resolve this
                        // parameter's type. The child sema has these types.
                        const param_data = fn_zir.instructions.items(.data)[@int_from_enum(param_inst)].pl_tok;
                        const param_extra = fn_zir.extra_data(Zir.Inst.Param, param_data.payload_index);
                        const param_ty_body = fn_zir.body_slice(param_extra.end, param_extra.data.body_len);

                        // Make sure any nested instructions don't clobber our work.
                        const prev_params = child_block.params;
                        const prev_no_partial_func_ty = child_sema.no_partial_func_ty;
                        const prev_generic_owner = child_sema.generic_owner;
                        const prev_generic_call_src = child_sema.generic_call_src;
                        const prev_generic_call_decl = child_sema.generic_call_decl;
                        child_block.params = .{};
                        child_sema.no_partial_func_ty = true;
                        child_sema.generic_owner = .none;
                        child_sema.generic_call_src = .unneeded;
                        child_sema.generic_call_decl = .none;
                        defer {
                            child_block.params = prev_params;
                            child_sema.no_partial_func_ty = prev_no_partial_func_ty;
                            child_sema.generic_owner = prev_generic_owner;
                            child_sema.generic_call_src = prev_generic_call_src;
                            child_sema.generic_call_decl = prev_generic_call_decl;
                        }

                        const param_ty_inst = try child_sema.resolve_inline_body(&child_block, param_ty_body, param_inst);
                        break :param_ty try child_sema.analyze_as_type(&child_block, param_data.src(), param_ty_inst);
                    },
                    else => unreachable,
                }
            },
        };
        const arg_ref = try args_info.analyze_arg(sema, block, arg_index, param_ty, generic_owner_ty_info, func);
        try sema.validate_runtime_value(block, args_info.arg_src(block, arg_index), arg_ref);
        const arg_ty = sema.type_of(arg_ref);
        if (arg_ty.zig_type_tag(mod) == .NoReturn) {
            // This terminates argument analysis.
            return arg_ref;
        }

        const arg_is_comptime = switch (param_tag) {
            .param_comptime, .param_anytype_comptime => true,
            .param, .param_anytype => try sema.type_requires_comptime(arg_ty),
            else => unreachable,
        };

        if (arg_is_comptime) {
            if (try sema.resolve_value(arg_ref)) |arg_val| {
                comptime_args[arg_index] = arg_val.to_intern();
                child_sema.inst_map.put_assume_capacity_no_clobber(
                    param_inst,
                    Air.interned_to_ref(arg_val.to_intern()),
                );
            } else switch (param_tag) {
                .param_comptime,
                .param_anytype_comptime,
                => return sema.fail_with_owned_error_msg(block, msg: {
                    const arg_src = args_info.arg_src(block, arg_index);
                    const msg = try sema.err_msg(block, arg_src, "runtime-known argument passed to comptime parameter", .{});
                    errdefer msg.destroy(sema.gpa);
                    const param_src = switch (param_tag) {
                        .param_comptime => fn_zir.instructions.items(.data)[@int_from_enum(param_inst)].pl_tok.src(),
                        .param_anytype_comptime => fn_zir.instructions.items(.data)[@int_from_enum(param_inst)].str_tok.src(),
                        else => unreachable,
                    };
                    try child_sema.err_note(&child_block, param_src, msg, "declared comptime here", .{});
                    break :msg msg;
                }),

                .param,
                .param_anytype,
                => return sema.fail_with_owned_error_msg(block, msg: {
                    const arg_src = args_info.arg_src(block, arg_index);
                    const msg = try sema.err_msg(block, arg_src, "runtime-known argument passed to parameter of comptime-only type", .{});
                    errdefer msg.destroy(sema.gpa);
                    const param_src = switch (param_tag) {
                        .param => fn_zir.instructions.items(.data)[@int_from_enum(param_inst)].pl_tok.src(),
                        .param_anytype => fn_zir.instructions.items(.data)[@int_from_enum(param_inst)].str_tok.src(),
                        else => unreachable,
                    };
                    try child_sema.err_note(&child_block, param_src, msg, "declared here", .{});
                    const src_decl = mod.decl_ptr(block.src_decl);
                    try sema.explain_why_type_is_comptime(msg, src_decl.to_src_loc(arg_src, mod), arg_ty);
                    break :msg msg;
                }),

                else => unreachable,
            }
        } else {
            // The parameter is runtime-known.
            try sema.queue_full_type_resolution(arg_ty);
            child_sema.inst_map.put_assume_capacity_no_clobber(param_inst, try child_block.add_inst(.{
                .tag = .arg,
                .data = .{ .arg = .{
                    .ty = Air.interned_to_ref(arg_ty.to_intern()),
                    .src_index = @int_cast(arg_index),
                } },
            }));
            const param_name: Zir.NullTerminatedString = switch (param_tag) {
                .param_anytype => fn_zir.instructions.items(.data)[@int_from_enum(param_inst)].str_tok.start,
                .param => name: {
                    const inst_data = fn_zir.instructions.items(.data)[@int_from_enum(param_inst)].pl_tok;
                    const extra = fn_zir.extra_data(Zir.Inst.Param, inst_data.payload_index);
                    break :name extra.data.name;
                },
                else => unreachable,
            };
            try child_block.params.append(sema.arena, .{
                .ty = arg_ty.to_intern(), // This is the type after coercion
                .is_comptime = false, // We're adding only runtime args to the instantiation
                .name = param_name,
            });
            runtime_args.append_assume_capacity(arg_ref);
        }
    }

    // We've already handled parameters, so don't resolve the whole body. Instead, just
    // do the instructions after the params (i.e. the func itself).
    const new_func_inst = try child_sema.resolve_inline_body(&child_block, fn_info.param_body[args_info.count()..], fn_info.param_body_inst);
    const callee_index = (child_sema.resolve_const_defined_value(&child_block, .unneeded, new_func_inst, undefined) catch unreachable).to_intern();

    const callee = mod.func_info(callee_index);
    callee.branch_quota(ip).* = @max(callee.branch_quota(ip).*, sema.branch_quota);

    try sema.add_referenced_by(block, call_src, callee.owner_decl);

    // Make a runtime call to the new function, making sure to omit the comptime args.
    const func_ty = Type.from_interned(callee.ty);
    const func_ty_info = mod.type_to_func(func_ty).?;

    // If the call evaluated to a return type that requires comptime, never mind
    // our generic instantiation. Instead we need to perform a comptime call.
    if (try sema.type_requires_comptime(Type.from_interned(func_ty_info.return_type))) {
        return error.ComptimeReturn;
    }
    // Similarly, if the call evaluated to a generic type we need to instead
    // call it inline.
    if (func_ty_info.is_generic or func_ty_info.cc == .Inline) {
        return error.GenericPoison;
    }

    try sema.queue_full_type_resolution(Type.from_interned(func_ty_info.return_type));

    if (call_dbg_node) |some| try sema.zir_dbg_stmt(block, some);

    if (sema.owner_func_index != .none and
        Type.from_interned(func_ty_info.return_type).is_error(mod))
    {
        ip.func_analysis(sema.owner_func_index).calls_or_awaits_errorable_fn = true;
    }

    try mod.ensure_func_body_analysis_queued(callee_index);

    try sema.air_extra.ensure_unused_capacity(sema.gpa, @typeInfo(Air.Call).Struct.fields.len + runtime_args.items.len);
    const result = try block.add_inst(.{
        .tag = call_tag,
        .data = .{ .pl_op = .{
            .operand = Air.interned_to_ref(callee_index),
            .payload = sema.add_extra_assume_capacity(Air.Call{
                .args_len = @int_cast(runtime_args.items.len),
            }),
        } },
    });
    sema.append_refs_assume_capacity(runtime_args.items);

    if (ensure_result_used) {
        try sema.ensure_result_used(block, sema.type_of(result), call_src);
    }
    if (call_tag == .call_always_tail) {
        return sema.handle_tail_call(block, call_src, func_ty, result);
    }
    if (func_ty.fn_return_type(mod).is_no_return(mod)) {
        _ = try block.add_no_op(.unreach);
        return .unreachable_value;
    }
    return result;
}

fn resolve_tuple_lazy_values(sema: *Sema, block: *Block, src: LazySrcLoc, ty: Type) CompileError!void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const tuple = switch (ip.index_to_key(ty.to_intern())) {
        .anon_struct_type => |tuple| tuple,
        else => return,
    };
    for (tuple.types.get(ip), tuple.values.get(ip)) |field_ty, field_val| {
        try sema.resolve_tuple_lazy_values(block, src, Type.from_interned(field_ty));
        if (field_val == .none) continue;
        // TODO: mutate in intern pool
        _ = try sema.resolve_lazy_value(Value.from_interned(field_val));
    }
}

fn zir_int_type(sema: *Sema, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const int_type = sema.code.instructions.items(.data)[@int_from_enum(inst)].int_type;
    const ty = try mod.int_type(int_type.signedness, int_type.bit_count);
    return Air.interned_to_ref(ty.to_intern());
}

fn zir_optional_type(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const operand_src: LazySrcLoc = .{ .node_offset_un_op = inst_data.src_node };
    const child_type = try sema.resolve_type(block, operand_src, inst_data.operand);
    if (child_type.zig_type_tag(mod) == .Opaque) {
        return sema.fail(block, operand_src, "opaque type '{}' cannot be optional", .{child_type.fmt(mod)});
    } else if (child_type.zig_type_tag(mod) == .Null) {
        return sema.fail(block, operand_src, "type '{}' cannot be optional", .{child_type.fmt(mod)});
    }
    const opt_type = try mod.optional_type(child_type.to_intern());

    return Air.interned_to_ref(opt_type.to_intern());
}

fn zir_array_init_elem_type(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const bin = sema.code.instructions.items(.data)[@int_from_enum(inst)].bin;
    const maybe_wrapped_indexable_ty = sema.resolve_type(block, .unneeded, bin.lhs) catch |err| switch (err) {
        // Since this is a ZIR instruction that returns a type, encountering
        // generic poison should not result in a failed compilation, but the
        // generic poison type. This prevents unnecessary failures when
        // constructing types at compile-time.
        error.GenericPoison => return .generic_poison_type,
        else => |e| return e,
    };
    const indexable_ty = maybe_wrapped_indexable_ty.opt_eu_base_type(mod);
    try sema.resolve_type_fields(indexable_ty);
    assert(indexable_ty.is_indexable(mod)); // validated by a previous instruction
    if (indexable_ty.zig_type_tag(mod) == .Struct) {
        const elem_type = indexable_ty.struct_field_type(@int_from_enum(bin.rhs), mod);
        return Air.interned_to_ref(elem_type.to_intern());
    } else {
        const elem_type = indexable_ty.elem_type2(mod);
        return Air.interned_to_ref(elem_type.to_intern());
    }
}

fn zir_elem_type(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const un_node = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const maybe_wrapped_ptr_ty = sema.resolve_type(block, .unneeded, un_node.operand) catch |err| switch (err) {
        error.GenericPoison => return .generic_poison_type,
        else => |e| return e,
    };
    const ptr_ty = maybe_wrapped_ptr_ty.opt_eu_base_type(mod);
    assert(ptr_ty.zig_type_tag(mod) == .Pointer); // validated by a previous instruction
    const elem_ty = ptr_ty.child_type(mod);
    if (elem_ty.to_intern() == .anyopaque_type) {
        // The pointer's actual child type is effectively unknown, so it makes
        // sense to represent it with a generic poison.
        return .generic_poison_type;
    }
    return Air.interned_to_ref(ptr_ty.child_type(mod).to_intern());
}

fn zir_indexable_ptr_elem_type(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const un_node = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = un_node.src();
    const ptr_ty = sema.resolve_type(block, src, un_node.operand) catch |err| switch (err) {
        error.GenericPoison => return .generic_poison_type,
        else => |e| return e,
    };
    try sema.check_mem_operand(block, src, ptr_ty);
    const elem_ty = switch (ptr_ty.ptr_size(mod)) {
        .Slice, .Many, .C => ptr_ty.child_type(mod),
        .One => ptr_ty.child_type(mod).child_type(mod),
    };
    return Air.interned_to_ref(elem_ty.to_intern());
}

fn zir_vector_elem_type(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const un_node = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const vec_ty = sema.resolve_type(block, .unneeded, un_node.operand) catch |err| switch (err) {
        // Since this is a ZIR instruction that returns a type, encountering
        // generic poison should not result in a failed compilation, but the
        // generic poison type. This prevents unnecessary failures when
        // constructing types at compile-time.
        error.GenericPoison => return .generic_poison_type,
        else => |e| return e,
    };
    if (!vec_ty.is_vector(mod)) {
        return sema.fail(block, un_node.src(), "expected vector type, found '{}'", .{vec_ty.fmt(mod)});
    }
    return Air.interned_to_ref(vec_ty.child_type(mod).to_intern());
}

fn zir_vector_type(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const len_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const elem_type_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const len: u32 = @int_cast(try sema.resolve_int(block, len_src, extra.lhs, Type.u32, .{
        .needed_comptime_reason = "vector length must be comptime-known",
    }));
    const elem_type = try sema.resolve_type(block, elem_type_src, extra.rhs);
    try sema.check_vector_elem_type(block, elem_type_src, elem_type);
    const vector_type = try mod.vector_type(.{
        .len = len,
        .child = elem_type.to_intern(),
    });
    return Air.interned_to_ref(vector_type.to_intern());
}

fn zir_array_type(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const len_src: LazySrcLoc = .{ .node_offset_array_type_len = inst_data.src_node };
    const elem_src: LazySrcLoc = .{ .node_offset_array_type_elem = inst_data.src_node };
    const len = try sema.resolve_int(block, len_src, extra.lhs, Type.usize, .{
        .needed_comptime_reason = "array length must be comptime-known",
    });
    const elem_type = try sema.resolve_type(block, elem_src, extra.rhs);
    try sema.validate_array_elem_type(block, elem_type, elem_src);
    const array_ty = try sema.mod.array_type(.{
        .len = len,
        .child = elem_type.to_intern(),
    });

    return Air.interned_to_ref(array_ty.to_intern());
}

fn zir_array_type_sentinel(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.ArrayTypeSentinel, inst_data.payload_index).data;
    const len_src: LazySrcLoc = .{ .node_offset_array_type_len = inst_data.src_node };
    const sentinel_src: LazySrcLoc = .{ .node_offset_array_type_sentinel = inst_data.src_node };
    const elem_src: LazySrcLoc = .{ .node_offset_array_type_elem = inst_data.src_node };
    const len = try sema.resolve_int(block, len_src, extra.len, Type.usize, .{
        .needed_comptime_reason = "array length must be comptime-known",
    });
    const elem_type = try sema.resolve_type(block, elem_src, extra.elem_type);
    try sema.validate_array_elem_type(block, elem_type, elem_src);
    const uncasted_sentinel = try sema.resolve_inst(extra.sentinel);
    const sentinel = try sema.coerce(block, elem_type, uncasted_sentinel, sentinel_src);
    const sentinel_val = try sema.resolve_const_defined_value(block, sentinel_src, sentinel, .{
        .needed_comptime_reason = "array sentinel value must be comptime-known",
    });
    const array_ty = try sema.mod.array_type(.{
        .len = len,
        .sentinel = sentinel_val.to_intern(),
        .child = elem_type.to_intern(),
    });

    return Air.interned_to_ref(array_ty.to_intern());
}

fn validate_array_elem_type(sema: *Sema, block: *Block, elem_type: Type, elem_src: LazySrcLoc) !void {
    const mod = sema.mod;
    if (elem_type.zig_type_tag(mod) == .Opaque) {
        return sema.fail(block, elem_src, "array of opaque type '{}' not allowed", .{elem_type.fmt(mod)});
    } else if (elem_type.zig_type_tag(mod) == .NoReturn) {
        return sema.fail(block, elem_src, "array of 'noreturn' not allowed", .{});
    }
}

fn zir_anyframe_type(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    if (true) {
        return sema.fail_with_use_of_async(block, inst_data.src());
    }
    const mod = sema.mod;
    const operand_src: LazySrcLoc = .{ .node_offset_anyframe_type = inst_data.src_node };
    const return_type = try sema.resolve_type(block, operand_src, inst_data.operand);
    const anyframe_type = try mod.anyframe_type(return_type);

    return Air.interned_to_ref(anyframe_type.to_intern());
}

fn zir_error_union_type(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
    const error_set = try sema.resolve_type(block, lhs_src, extra.lhs);
    const payload = try sema.resolve_type(block, rhs_src, extra.rhs);

    if (error_set.zig_type_tag(mod) != .ErrorSet) {
        return sema.fail(block, lhs_src, "expected error set type, found '{}'", .{
            error_set.fmt(mod),
        });
    }
    try sema.validate_error_union_payload_type(block, payload, rhs_src);
    const err_union_ty = try mod.error_union_type(error_set, payload);
    return Air.interned_to_ref(err_union_ty.to_intern());
}

fn validate_error_union_payload_type(sema: *Sema, block: *Block, payload_ty: Type, payload_src: LazySrcLoc) !void {
    const mod = sema.mod;
    if (payload_ty.zig_type_tag(mod) == .Opaque) {
        return sema.fail(block, payload_src, "error union with payload of opaque type '{}' not allowed", .{
            payload_ty.fmt(mod),
        });
    } else if (payload_ty.zig_type_tag(mod) == .ErrorSet) {
        return sema.fail(block, payload_src, "error union with payload of error set type '{}' not allowed", .{
            payload_ty.fmt(mod),
        });
    }
}

fn zir_error_value(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    _ = block;
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].str_tok;
    const name = try mod.intern_pool.get_or_put_string(
        sema.gpa,
        inst_data.get(sema.code),
        .no_embedded_nulls,
    );
    _ = try mod.get_error_value(name);
    // Create an error set type with only this error value, and return the value.
    const error_set_type = try mod.single_error_set_type(name);
    return Air.interned_to_ref((try mod.intern(.{ .err = .{
        .ty = error_set_type.to_intern(),
        .name = name,
    } })));
}

fn zir_int_from_error(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const extra = sema.code.extra_data(Zir.Inst.UnNode, extended.operand).data;
    const src = LazySrcLoc.nodeOffset(extra.node);
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const uncasted_operand = try sema.resolve_inst(extra.operand);
    const operand = try sema.coerce(block, Type.anyerror, uncasted_operand, operand_src);
    const err_int_ty = try mod.error_int_type();

    if (try sema.resolve_value(operand)) |val| {
        if (val.is_undef(mod)) {
            return mod.undef_ref(err_int_ty);
        }
        const err_name = ip.index_to_key(val.to_intern()).err.name;
        return Air.interned_to_ref((try mod.int_value(
            err_int_ty,
            try mod.get_error_value(err_name),
        )).to_intern());
    }

    const op_ty = sema.type_of(uncasted_operand);
    switch (try sema.resolve_inferred_error_set_ty(block, src, op_ty.to_intern())) {
        .anyerror_type => {},
        else => |err_set_ty_index| {
            const names = ip.index_to_key(err_set_ty_index).error_set_type.names;
            switch (names.len) {
                0 => return Air.interned_to_ref((try mod.int_value(err_int_ty, 0)).to_intern()),
                1 => {
                    const int: Module.ErrorInt = @int_cast(mod.global_error_set.get_index(names.get(ip)[0]).?);
                    return mod.int_ref(err_int_ty, int);
                },
                else => {},
            }
        },
    }

    try sema.require_runtime_block(block, src, operand_src);
    return block.add_bit_cast(err_int_ty, operand);
}

fn zir_error_from_int(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const extra = sema.code.extra_data(Zir.Inst.UnNode, extended.operand).data;
    const src = LazySrcLoc.nodeOffset(extra.node);
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const uncasted_operand = try sema.resolve_inst(extra.operand);
    const err_int_ty = try mod.error_int_type();
    const operand = try sema.coerce(block, err_int_ty, uncasted_operand, operand_src);

    if (try sema.resolve_defined_value(block, operand_src, operand)) |value| {
        const int = try sema.usize_cast(block, operand_src, try value.to_unsigned_int_advanced(sema));
        if (int > mod.global_error_set.count() or int == 0)
            return sema.fail(block, operand_src, "integer value '{d}' represents no error", .{int});
        return Air.interned_to_ref((try mod.intern(.{ .err = .{
            .ty = .anyerror_type,
            .name = mod.global_error_set.keys()[int],
        } })));
    }
    try sema.require_runtime_block(block, src, operand_src);
    if (block.want_safety()) {
        const is_lt_len = try block.add_un_op(.cmp_lt_errors_len, operand);
        const zero_val = Air.interned_to_ref((try mod.int_value(err_int_ty, 0)).to_intern());
        const is_non_zero = try block.add_bin_op(.cmp_neq, operand, zero_val);
        const ok = try block.add_bin_op(.bool_and, is_lt_len, is_non_zero);
        try sema.add_safety_check(block, src, ok, .invalid_error_code);
    }
    return block.add_inst(.{
        .tag = .bitcast,
        .data = .{ .ty_op = .{
            .ty = .anyerror_type,
            .operand = operand,
        } },
    });
}

fn zir_merge_error_sets(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const src: LazySrcLoc = .{ .node_offset_bin_op = inst_data.src_node };
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
    const lhs = try sema.resolve_inst(extra.lhs);
    const rhs = try sema.resolve_inst(extra.rhs);
    if (sema.type_of(lhs).zig_type_tag(mod) == .Bool and sema.type_of(rhs).zig_type_tag(mod) == .Bool) {
        const msg = msg: {
            const msg = try sema.err_msg(block, lhs_src, "expected error set type, found 'bool'", .{});
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, src, msg, "'||' merges error sets; 'or' performs boolean OR", .{});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }
    const lhs_ty = try sema.analyze_as_type(block, lhs_src, lhs);
    const rhs_ty = try sema.analyze_as_type(block, rhs_src, rhs);
    if (lhs_ty.zig_type_tag(mod) != .ErrorSet)
        return sema.fail(block, lhs_src, "expected error set type, found '{}'", .{lhs_ty.fmt(mod)});
    if (rhs_ty.zig_type_tag(mod) != .ErrorSet)
        return sema.fail(block, rhs_src, "expected error set type, found '{}'", .{rhs_ty.fmt(mod)});

    // Anything merged with anyerror is anyerror.
    if (lhs_ty.to_intern() == .anyerror_type or rhs_ty.to_intern() == .anyerror_type) {
        return .anyerror_type;
    }

    if (ip.is_inferred_error_set_type(lhs_ty.to_intern())) {
        switch (try sema.resolve_inferred_error_set(block, src, lhs_ty.to_intern())) {
            // is_any_error might have changed from a false negative to a true
            // positive after resolution.
            .anyerror_type => return .anyerror_type,
            else => {},
        }
    }
    if (ip.is_inferred_error_set_type(rhs_ty.to_intern())) {
        switch (try sema.resolve_inferred_error_set(block, src, rhs_ty.to_intern())) {
            // is_any_error might have changed from a false negative to a true
            // positive after resolution.
            .anyerror_type => return .anyerror_type,
            else => {},
        }
    }

    const err_set_ty = try sema.error_set_merge(lhs_ty, rhs_ty);
    return Air.interned_to_ref(err_set_ty.to_intern());
}

fn zir_enum_literal(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    _ = block;
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].str_tok;
    const name = inst_data.get(sema.code);
    return Air.interned_to_ref((try mod.intern(.{
        .enum_literal = try mod.intern_pool.get_or_put_string(sema.gpa, name, .no_embedded_nulls),
    })));
}

fn zir_int_from_enum(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const operand = try sema.resolve_inst(inst_data.operand);
    const operand_ty = sema.type_of(operand);

    const enum_tag: Air.Inst.Ref = switch (operand_ty.zig_type_tag(mod)) {
        .Enum => operand,
        .Union => blk: {
            try sema.resolve_type_fields(operand_ty);
            const tag_ty = operand_ty.union_tag_type(mod) orelse {
                return sema.fail(
                    block,
                    operand_src,
                    "untagged union '{}' cannot be converted to integer",
                    .{src},
                );
            };

            break :blk try sema.union_to_tag(block, tag_ty, operand, operand_src);
        },
        else => {
            return sema.fail(block, operand_src, "expected enum or tagged union, found '{}'", .{
                operand_ty.fmt(mod),
            });
        },
    };
    const enum_tag_ty = sema.type_of(enum_tag);
    const int_tag_ty = enum_tag_ty.int_tag_type(mod);

    // TODO: use correct solution
    // https://github.com/ziglang/zig/issues/15909
    if (enum_tag_ty.enum_field_count(mod) == 0 and !enum_tag_ty.is_nonexhaustive_enum(mod)) {
        return sema.fail(block, operand_src, "cannot use @int_from_enum on empty enum '{}'", .{
            enum_tag_ty.fmt(mod),
        });
    }

    if (try sema.type_has_one_possible_value(enum_tag_ty)) |opv| {
        return Air.interned_to_ref((try mod.get_coerced(opv, int_tag_ty)).to_intern());
    }

    if (try sema.resolve_value(enum_tag)) |enum_tag_val| {
        if (enum_tag_val.is_undef(mod)) {
            return mod.undef_ref(int_tag_ty);
        }

        const val = try enum_tag_val.int_from_enum(enum_tag_ty, mod);
        return Air.interned_to_ref(val.to_intern());
    }

    try sema.require_runtime_block(block, src, operand_src);
    return block.add_bit_cast(int_tag_ty, enum_tag);
}

fn zir_enum_from_int(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const dest_ty = try sema.resolve_dest_type(block, src, extra.lhs, .remove_eu_opt, "@enumFromInt");
    const operand = try sema.resolve_inst(extra.rhs);

    if (dest_ty.zig_type_tag(mod) != .Enum) {
        return sema.fail(block, src, "expected enum, found '{}'", .{dest_ty.fmt(mod)});
    }
    _ = try sema.check_int_type(block, operand_src, sema.type_of(operand));

    if (try sema.resolve_value(operand)) |int_val| {
        if (dest_ty.is_nonexhaustive_enum(mod)) {
            const int_tag_ty = dest_ty.int_tag_type(mod);
            if (try sema.int_fits_in_type(int_val, int_tag_ty, null)) {
                return Air.interned_to_ref((try mod.get_coerced(int_val, dest_ty)).to_intern());
            }
            return sema.fail(block, src, "int value '{}' out of range of non-exhaustive enum '{}'", .{
                int_val.fmt_value(mod, sema), dest_ty.fmt(mod),
            });
        }
        if (int_val.is_undef(mod)) {
            return sema.fail_with_use_of_undef(block, operand_src);
        }
        if (!(try sema.enum_has_int(dest_ty, int_val))) {
            return sema.fail(block, src, "enum '{}' has no tag with value '{}'", .{
                dest_ty.fmt(mod), int_val.fmt_value(mod, sema),
            });
        }
        return Air.interned_to_ref((try mod.get_coerced(int_val, dest_ty)).to_intern());
    }

    if (dest_ty.int_tag_type(mod).zig_type_tag(mod) == .ComptimeInt) {
        return sema.fail_with_needed_comptime(block, operand_src, .{
            .needed_comptime_reason = "value being casted to enum with 'comptime_int' tag type must be comptime-known",
        });
    }

    if (try sema.type_has_one_possible_value(dest_ty)) |opv| {
        const result = Air.interned_to_ref(opv.to_intern());
        // The operand is runtime-known but the result is comptime-known. In
        // this case we still need a safety check.
        // TODO add a safety check here. we can't use is_named_enum_value -
        // it needs to convert the enum back to int and make sure it equals the operand int.
        return result;
    }

    try sema.require_runtime_block(block, src, operand_src);
    const result = try block.add_ty_op(.intcast, dest_ty, operand);
    if (block.want_safety() and !dest_ty.is_nonexhaustive_enum(mod) and
        mod.backend_supports_feature(.is_named_enum_value))
    {
        const ok = try block.add_un_op(.is_named_enum_value, result);
        try sema.add_safety_check(block, src, ok, .invalid_enum_value);
    }
    return result;
}

/// Pointer in, pointer out.
fn zir_optional_payload_ptr(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    safety_check: bool,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const optional_ptr = try sema.resolve_inst(inst_data.operand);
    const src = inst_data.src();

    return sema.analyze_optional_payload_ptr(block, src, optional_ptr, safety_check, false);
}

fn analyze_optional_payload_ptr(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    optional_ptr: Air.Inst.Ref,
    safety_check: bool,
    initializing: bool,
) CompileError!Air.Inst.Ref {
    const zcu = sema.mod;
    const optional_ptr_ty = sema.type_of(optional_ptr);
    assert(optional_ptr_ty.zig_type_tag(zcu) == .Pointer);

    const opt_type = optional_ptr_ty.child_type(zcu);
    if (opt_type.zig_type_tag(zcu) != .Optional) {
        return sema.fail_with_expected_optional_type(block, src, opt_type);
    }

    const child_type = opt_type.optional_child(zcu);
    const child_pointer = try sema.ptr_type(.{
        .child = child_type.to_intern(),
        .flags = .{
            .is_const = optional_ptr_ty.is_const_ptr(zcu),
            .address_space = optional_ptr_ty.ptr_address_space(zcu),
        },
    });

    if (try sema.resolve_defined_value(block, src, optional_ptr)) |ptr_val| {
        if (initializing) {
            if (sema.is_comptime_mutable_ptr(ptr_val)) {
                // Set the optional to non-null at comptime.
                // If the payload is OPV, we must use that value instead of undef.
                const payload_val = try sema.type_has_one_possible_value(child_type) orelse try zcu.undef_value(child_type);
                const opt_val = try zcu.intern(.{ .opt = .{
                    .ty = opt_type.to_intern(),
                    .val = payload_val.to_intern(),
                } });
                try sema.store_ptr_val(block, src, ptr_val, Value.from_interned(opt_val), opt_type);
            } else {
                // Emit runtime instructions to set the optional non-null bit.
                const opt_payload_ptr = try block.add_ty_op(.optional_payload_ptr_set, child_pointer, optional_ptr);
                try sema.check_known_alloc_ptr(block, optional_ptr, opt_payload_ptr);
            }
            return Air.interned_to_ref((try ptr_val.ptr_opt_payload(sema)).to_intern());
        }
        if (try sema.pointer_deref(block, src, ptr_val, optional_ptr_ty)) |val| {
            if (val.is_null(zcu)) {
                return sema.fail(block, src, "unable to unwrap null", .{});
            }
            return Air.interned_to_ref((try ptr_val.ptr_opt_payload(sema)).to_intern());
        }
    }

    try sema.require_runtime_block(block, src, null);
    if (safety_check and block.want_safety()) {
        const is_non_null = try block.add_un_op(.is_non_null_ptr, optional_ptr);
        try sema.add_safety_check(block, src, is_non_null, .unwrap_null);
    }

    if (initializing) {
        const opt_payload_ptr = try block.add_ty_op(.optional_payload_ptr_set, child_pointer, optional_ptr);
        try sema.check_known_alloc_ptr(block, optional_ptr, opt_payload_ptr);
        return opt_payload_ptr;
    } else {
        return block.add_ty_op(.optional_payload_ptr, child_pointer, optional_ptr);
    }
}

/// Value in, value out.
fn zir_optional_payload(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    safety_check: bool,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand = try sema.resolve_inst(inst_data.operand);
    const operand_ty = sema.type_of(operand);
    const result_ty = switch (operand_ty.zig_type_tag(mod)) {
        .Optional => operand_ty.optional_child(mod),
        .Pointer => t: {
            if (operand_ty.ptr_size(mod) != .C) {
                return sema.fail_with_expected_optional_type(block, src, operand_ty);
            }
            // TODO https://github.com/ziglang/zig/issues/6597
            if (true) break :t operand_ty;
            const ptr_info = operand_ty.ptr_info(mod);
            break :t try sema.ptr_type(.{
                .child = ptr_info.child,
                .flags = .{
                    .alignment = ptr_info.flags.alignment,
                    .is_const = ptr_info.flags.is_const,
                    .is_volatile = ptr_info.flags.is_volatile,
                    .is_allowzero = ptr_info.flags.is_allowzero,
                    .address_space = ptr_info.flags.address_space,
                },
            });
        },
        else => return sema.fail_with_expected_optional_type(block, src, operand_ty),
    };

    if (try sema.resolve_defined_value(block, src, operand)) |val| {
        return if (val.optional_value(mod)) |payload|
            Air.interned_to_ref(payload.to_intern())
        else
            sema.fail(block, src, "unable to unwrap null", .{});
    }

    try sema.require_runtime_block(block, src, null);
    if (safety_check and block.want_safety()) {
        const is_non_null = try block.add_un_op(.is_non_null, operand);
        try sema.add_safety_check(block, src, is_non_null, .unwrap_null);
    }
    return block.add_ty_op(.optional_payload, result_ty, operand);
}

/// Value in, value out
fn zir_err_union_payload(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand = try sema.resolve_inst(inst_data.operand);
    const operand_src = src;
    const err_union_ty = sema.type_of(operand);
    if (err_union_ty.zig_type_tag(mod) != .ErrorUnion) {
        return sema.fail(block, operand_src, "expected error union type, found '{}'", .{
            err_union_ty.fmt(mod),
        });
    }
    return sema.analyze_err_union_payload(block, src, err_union_ty, operand, operand_src, false);
}

fn analyze_err_union_payload(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    err_union_ty: Type,
    operand: Air.Inst.Ref,
    operand_src: LazySrcLoc,
    safety_check: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const payload_ty = err_union_ty.error_union_payload(mod);
    if (try sema.resolve_defined_value(block, operand_src, operand)) |val| {
        if (val.get_error_name(mod).unwrap()) |name| {
            return sema.fail_with_comptime_error_ret_trace(block, src, name);
        }
        return Air.interned_to_ref(mod.intern_pool.index_to_key(val.to_intern()).error_union.val.payload);
    }

    try sema.require_runtime_block(block, src, null);

    // If the error set has no fields then no safety check is needed.
    if (safety_check and block.want_safety() and
        !err_union_ty.error_union_set(mod).error_set_is_empty(mod))
    {
        try sema.panic_unwrap_error(block, src, operand, .unwrap_errunion_err, .is_non_err);
    }

    return block.add_ty_op(.unwrap_errunion_payload, payload_ty, operand);
}

/// Pointer in, pointer out.
fn zir_err_union_payload_ptr(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const operand = try sema.resolve_inst(inst_data.operand);
    const src = inst_data.src();

    return sema.analyze_err_union_payload_ptr(block, src, operand, false, false);
}

fn analyze_err_union_payload_ptr(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    operand: Air.Inst.Ref,
    safety_check: bool,
    initializing: bool,
) CompileError!Air.Inst.Ref {
    const zcu = sema.mod;
    const operand_ty = sema.type_of(operand);
    assert(operand_ty.zig_type_tag(zcu) == .Pointer);

    if (operand_ty.child_type(zcu).zig_type_tag(zcu) != .ErrorUnion) {
        return sema.fail(block, src, "expected error union type, found '{}'", .{
            operand_ty.child_type(zcu).fmt(zcu),
        });
    }

    const err_union_ty = operand_ty.child_type(zcu);
    const payload_ty = err_union_ty.error_union_payload(zcu);
    const operand_pointer_ty = try sema.ptr_type(.{
        .child = payload_ty.to_intern(),
        .flags = .{
            .is_const = operand_ty.is_const_ptr(zcu),
            .address_space = operand_ty.ptr_address_space(zcu),
        },
    });

    if (try sema.resolve_defined_value(block, src, operand)) |ptr_val| {
        if (initializing) {
            if (sema.is_comptime_mutable_ptr(ptr_val)) {
                // Set the error union to non-error at comptime.
                // If the payload is OPV, we must use that value instead of undef.
                const payload_val = try sema.type_has_one_possible_value(payload_ty) orelse try zcu.undef_value(payload_ty);
                const eu_val = try zcu.intern(.{ .error_union = .{
                    .ty = err_union_ty.to_intern(),
                    .val = .{ .payload = payload_val.to_intern() },
                } });
                try sema.store_ptr_val(block, src, ptr_val, Value.from_interned(eu_val), err_union_ty);
            } else {
                // Emit runtime instructions to set the error union error code.
                try sema.require_runtime_block(block, src, null);
                const eu_payload_ptr = try block.add_ty_op(.errunion_payload_ptr_set, operand_pointer_ty, operand);
                try sema.check_known_alloc_ptr(block, operand, eu_payload_ptr);
            }
            return Air.interned_to_ref((try ptr_val.ptr_eu_payload(sema)).to_intern());
        }
        if (try sema.pointer_deref(block, src, ptr_val, operand_ty)) |val| {
            if (val.get_error_name(zcu).unwrap()) |name| {
                return sema.fail_with_comptime_error_ret_trace(block, src, name);
            }
            return Air.interned_to_ref((try ptr_val.ptr_eu_payload(sema)).to_intern());
        }
    }

    try sema.require_runtime_block(block, src, null);

    // If the error set has no fields then no safety check is needed.
    if (safety_check and block.want_safety() and
        !err_union_ty.error_union_set(zcu).error_set_is_empty(zcu))
    {
        try sema.panic_unwrap_error(block, src, operand, .unwrap_errunion_err_ptr, .is_non_err_ptr);
    }

    if (initializing) {
        const eu_payload_ptr = try block.add_ty_op(.errunion_payload_ptr_set, operand_pointer_ty, operand);
        try sema.check_known_alloc_ptr(block, operand, eu_payload_ptr);
        return eu_payload_ptr;
    } else {
        return block.add_ty_op(.unwrap_errunion_payload_ptr, operand_pointer_ty, operand);
    }
}

/// Value in, value out
fn zir_err_union_code(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand = try sema.resolve_inst(inst_data.operand);
    return sema.analyze_err_union_code(block, src, operand);
}

fn analyze_err_union_code(sema: *Sema, block: *Block, src: LazySrcLoc, operand: Air.Inst.Ref) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const operand_ty = sema.type_of(operand);
    if (operand_ty.zig_type_tag(mod) != .ErrorUnion) {
        return sema.fail(block, src, "expected error union type, found '{}'", .{
            operand_ty.fmt(mod),
        });
    }

    const result_ty = operand_ty.error_union_set(mod);

    if (try sema.resolve_defined_value(block, src, operand)) |val| {
        return Air.interned_to_ref((try mod.intern(.{ .err = .{
            .ty = result_ty.to_intern(),
            .name = mod.intern_pool.index_to_key(val.to_intern()).error_union.val.err_name,
        } })));
    }

    try sema.require_runtime_block(block, src, null);
    return block.add_ty_op(.unwrap_errunion_err, result_ty, operand);
}

/// Pointer in, value out
fn zir_err_union_code_ptr(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand = try sema.resolve_inst(inst_data.operand);
    return sema.analyze_err_union_code_ptr(block, src, operand);
}

fn analyze_err_union_code_ptr(sema: *Sema, block: *Block, src: LazySrcLoc, operand: Air.Inst.Ref) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const operand_ty = sema.type_of(operand);
    assert(operand_ty.zig_type_tag(mod) == .Pointer);

    if (operand_ty.child_type(mod).zig_type_tag(mod) != .ErrorUnion) {
        return sema.fail(block, src, "expected error union type, found '{}'", .{
            operand_ty.child_type(mod).fmt(mod),
        });
    }

    const result_ty = operand_ty.child_type(mod).error_union_set(mod);

    if (try sema.resolve_defined_value(block, src, operand)) |pointer_val| {
        if (try sema.pointer_deref(block, src, pointer_val, operand_ty)) |val| {
            assert(val.get_error_name(mod) != .none);
            return Air.interned_to_ref((try mod.intern(.{ .err = .{
                .ty = result_ty.to_intern(),
                .name = mod.intern_pool.index_to_key(val.to_intern()).error_union.val.err_name,
            } })));
        }
    }

    try sema.require_runtime_block(block, src, null);
    return block.add_ty_op(.unwrap_errunion_err_ptr, result_ty, operand);
}

fn zir_func(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    inferred_error_set: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Func, inst_data.payload_index);
    const target = sema.mod.get_target();
    const ret_ty_src: LazySrcLoc = .{ .node_offset_fn_type_ret_ty = inst_data.src_node };

    var extra_index = extra.end;

    const ret_ty: Type = switch (extra.data.ret_body_len) {
        0 => Type.void,
        1 => blk: {
            const ret_ty_ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
            extra_index += 1;
            if (sema.resolve_type(block, ret_ty_src, ret_ty_ref)) |ret_ty| {
                break :blk ret_ty;
            } else |err| switch (err) {
                error.GenericPoison => {
                    break :blk Type.generic_poison;
                },
                else => |e| return e,
            }
        },
        else => blk: {
            const ret_ty_body = sema.code.body_slice(extra_index, extra.data.ret_body_len);
            extra_index += ret_ty_body.len;

            const ret_ty_val = try sema.resolve_generic_body(block, ret_ty_src, ret_ty_body, inst, Type.type, .{
                .needed_comptime_reason = "return type must be comptime-known",
            });
            break :blk ret_ty_val.to_type();
        },
    };

    var src_locs: Zir.Inst.Func.SrcLocs = undefined;
    const has_body = extra.data.body_len != 0;
    if (has_body) {
        extra_index += extra.data.body_len;
        src_locs = sema.code.extra_data(Zir.Inst.Func.SrcLocs, extra_index).data;
    }

    // If this instruction has a body it means it's the type of the `owner_decl`
    // otherwise it's a function type without a `callconv` attribute and should
    // never be `.C`.
    const cc: std.builtin.CallingConvention = if (has_body and mod.decl_ptr(block.src_decl).is_exported)
        .C
    else
        .Unspecified;

    return sema.func_common(
        block,
        inst_data.src_node,
        inst,
        .none,
        target_util.default_address_space(target, .function),
        .default,
        cc,
        ret_ty,
        false,
        inferred_error_set,
        false,
        has_body,
        src_locs,
        null,
        0,
        false,
    );
}

fn resolve_generic_body(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    body: []const Zir.Inst.Index,
    func_inst: Zir.Inst.Index,
    dest_ty: Type,
    reason: NeededComptimeReason,
) !Value {
    assert(body.len != 0);

    const err = err: {
        // Make sure any nested param instructions don't clobber our work.
        const prev_params = block.params;
        const prev_no_partial_func_type = sema.no_partial_func_ty;
        const prev_generic_owner = sema.generic_owner;
        const prev_generic_call_src = sema.generic_call_src;
        const prev_generic_call_decl = sema.generic_call_decl;
        block.params = .{};
        sema.no_partial_func_ty = true;
        sema.generic_owner = .none;
        sema.generic_call_src = .unneeded;
        sema.generic_call_decl = .none;
        defer {
            block.params = prev_params;
            sema.no_partial_func_ty = prev_no_partial_func_type;
            sema.generic_owner = prev_generic_owner;
            sema.generic_call_src = prev_generic_call_src;
            sema.generic_call_decl = prev_generic_call_decl;
        }

        const uncasted = sema.resolve_inline_body(block, body, func_inst) catch |err| break :err err;
        const result = sema.coerce(block, dest_ty, uncasted, src) catch |err| break :err err;
        const val = sema.resolve_const_defined_value(block, src, result, reason) catch |err| break :err err;
        return val;
    };
    switch (err) {
        error.GenericPoison => {
            if (dest_ty.to_intern() == .type_type) {
                return Value.generic_poison_type;
            } else {
                return Value.generic_poison;
            }
        },
        else => |e| return e,
    }
}

/// Given a library name, examines if the library name should end up in
/// `link.File.Options.system_libs` table (for example, libc is always
/// specified via dedicated flag `link_libc` instead),
/// and puts it there if it doesn't exist.
/// It also dupes the library name which can then be saved as part of the
/// respective `Decl` (either `ExternFn` or `Var`).
/// The liveness of the duped library name is tied to liveness of `Module`.
/// To deallocate, call `deinit` on the respective `Decl` (`ExternFn` or `Var`).
fn handle_extern_lib_name(
    sema: *Sema,
    block: *Block,
    src_loc: LazySrcLoc,
    lib_name: []const u8,
) CompileError!void {
    blk: {
        const mod = sema.mod;
        const comp = mod.comp;
        const target = mod.get_target();
        log.debug("extern fn symbol expected in lib '{s}'", .{lib_name});
        if (target.is_libc_lib_name(lib_name)) {
            if (!comp.config.link_libc) {
                return sema.fail(
                    block,
                    src_loc,
                    "dependency on libc must be explicitly specified in the build command",
                    .{},
                );
            }
            break :blk;
        }
        if (target.is_libcpp_lib_name(lib_name)) {
            if (!comp.config.link_libcpp) return sema.fail(
                block,
                src_loc,
                "dependency on libc++ must be explicitly specified in the build command",
                .{},
            );
            break :blk;
        }
        if (mem.eql(u8, lib_name, "unwind")) {
            if (!comp.config.link_libunwind) return sema.fail(
                block,
                src_loc,
                "dependency on libunwind must be explicitly specified in the build command",
                .{},
            );
            break :blk;
        }
        if (!target.is_wasm() and !block.owner_module().pic) {
            return sema.fail(
                block,
                src_loc,
                "dependency on dynamic library '{s}' requires enabling Position Independent Code; fixed by '-l{s}' or '-fPIC'",
                .{ lib_name, lib_name },
            );
        }
        comp.add_link_lib(lib_name) catch |err| {
            return sema.fail(block, src_loc, "unable to add link lib '{s}': {s}", .{
                lib_name, @errorName(err),
            });
        };
    }
}

/// These are calling conventions that are confirmed to work with variadic functions.
/// Any calling conventions not included here are either not yet verified to work with variadic
/// functions or there are no more other calling conventions that support variadic functions.
const calling_conventions_supporting_var_args = [_]std.builtin.CallingConvention{
    .C,
};
fn call_conv_supports_var_args(cc: std.builtin.CallingConvention) bool {
    return for (calling_conventions_supporting_var_args) |supported_cc| {
        if (cc == supported_cc) return true;
    } else false;
}
fn check_call_conv_supports_var_args(sema: *Sema, block: *Block, src: LazySrcLoc, cc: std.builtin.CallingConvention) CompileError!void {
    const CallingConventionsSupportingVarArgsList = struct {
        pub fn format(_: @This(), comptime fmt: []const u8, options: std.fmt.FormatOptions, writer: anytype) !void {
            _ = fmt;
            _ = options;
            for (calling_conventions_supporting_var_args, 0..) |cc_inner, i| {
                if (i != 0)
                    try writer.write_all(", ");
                try writer.print("'.{s}'", .{@tag_name(cc_inner)});
            }
        }
    };

    if (!call_conv_supports_var_args(cc)) {
        const msg = msg: {
            const msg = try sema.err_msg(block, src, "variadic function does not support '.{s}' calling convention", .{@tag_name(cc)});
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, src, msg, "supported calling conventions: {}", .{CallingConventionsSupportingVarArgsList{}});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }
}

const Section = union(enum) {
    generic,
    default,
    explicit: InternPool.NullTerminatedString,
};

fn func_common(
    sema: *Sema,
    block: *Block,
    src_node_offset: i32,
    func_inst: Zir.Inst.Index,
    /// null means generic poison
    alignment: ?Alignment,
    /// null means generic poison
    address_space: ?std.builtin.AddressSpace,
    section: Section,
    /// null means generic poison
    cc: ?std.builtin.CallingConvention,
    /// this might be Type.generic_poison
    bare_return_type: Type,
    var_args: bool,
    inferred_error_set: bool,
    is_extern: bool,
    has_body: bool,
    src_locs: Zir.Inst.Func.SrcLocs,
    opt_lib_name: ?[]const u8,
    noalias_bits: u32,
    is_noinline: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const target = mod.get_target();
    const ip = &mod.intern_pool;
    const ret_ty_src: LazySrcLoc = .{ .node_offset_fn_type_ret_ty = src_node_offset };
    const cc_src: LazySrcLoc = .{ .node_offset_fn_type_cc = src_node_offset };
    const func_src = LazySrcLoc.nodeOffset(src_node_offset);

    var is_generic = bare_return_type.is_generic_poison() or
        alignment == null or
        address_space == null or
        section == .generic or
        cc == null;

    if (var_args) {
        if (is_generic) {
            return sema.fail(block, func_src, "generic function cannot be variadic", .{});
        }
        try sema.check_call_conv_supports_var_args(block, cc_src, cc.?);
    }

    const is_source_decl = sema.generic_owner == .none;

    // In the case of generic calling convention, or generic alignment, we use
    // default values which are only meaningful for the generic function, *not*
    // the instantiation, which can depend on comptime parameters.
    // Related proposal: https://github.com/ziglang/zig/issues/11834
    const cc_resolved = cc orelse .Unspecified;
    var comptime_bits: u32 = 0;
    for (block.params.items(.ty), block.params.items(.is_comptime), 0..) |param_ty_ip, param_is_comptime, i| {
        const param_ty = Type.from_interned(param_ty_ip);
        const is_noalias = blk: {
            const index = std.math.cast(u5, i) orelse break :blk false;
            break :blk @as(u1, @truncate(noalias_bits >> index)) != 0;
        };
        const param_src: LazySrcLoc = .{ .fn_proto_param = .{
            .decl = block.src_decl,
            .fn_proto_node_offset = src_node_offset,
            .param_index = @int_cast(i),
        } };
        const requires_comptime = try sema.type_requires_comptime(param_ty);
        if (param_is_comptime or requires_comptime) {
            comptime_bits |= @as(u32, 1) << @int_cast(i); // TODO: handle cast error
        }
        const this_generic = param_ty.is_generic_poison();
        is_generic = is_generic or this_generic;
        if (param_is_comptime and !target_util.fn_call_conv_allows_zig_types(target, cc_resolved)) {
            return sema.fail(block, param_src, "comptime parameters not allowed in function with calling convention '{s}'", .{@tag_name(cc_resolved)});
        }
        if (this_generic and !sema.no_partial_func_ty and !target_util.fn_call_conv_allows_zig_types(target, cc_resolved)) {
            return sema.fail(block, param_src, "generic parameters not allowed in function with calling convention '{s}'", .{@tag_name(cc_resolved)});
        }
        if (!param_ty.is_valid_param_type(mod)) {
            const opaque_str = if (param_ty.zig_type_tag(mod) == .Opaque) "opaque " else "";
            return sema.fail(block, param_src, "parameter of {s}type '{}' not allowed", .{
                opaque_str, param_ty.fmt(mod),
            });
        }
        if (!this_generic and !target_util.fn_call_conv_allows_zig_types(target, cc_resolved) and !try sema.validate_extern_type(param_ty, .param_ty)) {
            const msg = msg: {
                const msg = try sema.err_msg(block, param_src, "parameter of type '{}' not allowed in function with calling convention '{s}'", .{
                    param_ty.fmt(mod), @tag_name(cc_resolved),
                });
                errdefer msg.destroy(sema.gpa);

                const src_decl = mod.decl_ptr(block.src_decl);
                try sema.explain_why_type_is_not_extern(msg, src_decl.to_src_loc(param_src, mod), param_ty, .param_ty);

                try sema.add_declared_here_note(msg, param_ty);
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        }
        if (is_source_decl and requires_comptime and !param_is_comptime and has_body and !block.is_comptime) {
            const msg = msg: {
                const msg = try sema.err_msg(block, param_src, "parameter of type '{}' must be declared comptime", .{
                    param_ty.fmt(mod),
                });
                errdefer msg.destroy(sema.gpa);

                const src_decl = mod.decl_ptr(block.src_decl);
                try sema.explain_why_type_is_comptime(msg, src_decl.to_src_loc(param_src, mod), param_ty);

                try sema.add_declared_here_note(msg, param_ty);
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        }
        if (is_source_decl and !this_generic and is_noalias and
            !(param_ty.zig_type_tag(mod) == .Pointer or param_ty.is_ptr_like_optional(mod)))
        {
            return sema.fail(block, param_src, "non-pointer parameter declared noalias", .{});
        }
        switch (cc_resolved) {
            .Interrupt => if (target.cpu.arch.is_x86()) {
                const err_code_size = target.ptr_bit_width();
                switch (i) {
                    0 => if (param_ty.zig_type_tag(mod) != .Pointer) return sema.fail(block, param_src, "first parameter of function with 'Interrupt' calling convention must be a pointer type", .{}),
                    1 => if (param_ty.bit_size(mod) != err_code_size) return sema.fail(block, param_src, "second parameter of function with 'Interrupt' calling convention must be a {d}-bit integer", .{err_code_size}),
                    else => return sema.fail(block, param_src, "'Interrupt' calling convention supports up to 2 parameters, found {d}", .{i + 1}),
                }
            } else return sema.fail(block, param_src, "parameters are not allowed with 'Interrupt' calling convention", .{}),
            .Signal => return sema.fail(block, param_src, "parameters are not allowed with 'Signal' calling convention", .{}),
            else => {},
        }
    }

    var ret_ty_requires_comptime = false;
    const ret_poison = if (sema.type_requires_comptime(bare_return_type)) |ret_comptime| rp: {
        ret_ty_requires_comptime = ret_comptime;
        break :rp bare_return_type.is_generic_poison();
    } else |err| switch (err) {
        error.GenericPoison => rp: {
            is_generic = true;
            break :rp true;
        },
        else => |e| return e,
    };
    const final_is_generic = is_generic or comptime_bits != 0 or ret_ty_requires_comptime;

    const param_types = block.params.items(.ty);

    if (!is_source_decl) {
        assert(has_body);
        assert(!is_generic);
        assert(comptime_bits == 0);
        assert(cc != null);
        assert(section != .generic);
        assert(address_space != null);
        assert(!var_args);
        if (inferred_error_set) {
            try sema.validate_error_union_payload_type(block, bare_return_type, ret_ty_src);
        }
        const func_index = try ip.get_func_instance(gpa, .{
            .param_types = param_types,
            .noalias_bits = noalias_bits,
            .bare_return_type = bare_return_type.to_intern(),
            .cc = cc_resolved,
            .alignment = alignment.?,
            .section = switch (section) {
                .generic => unreachable,
                .default => .none,
                .explicit => |x| x.to_optional(),
            },
            .is_noinline = is_noinline,
            .inferred_error_set = inferred_error_set,
            .generic_owner = sema.generic_owner,
            .comptime_args = sema.comptime_args,
        });
        return finish_func(
            sema,
            block,
            func_index,
            .none,
            ret_poison,
            bare_return_type,
            ret_ty_src,
            cc_resolved,
            is_source_decl,
            ret_ty_requires_comptime,
            func_inst,
            cc_src,
            is_noinline,
            is_generic,
            final_is_generic,
        );
    }

    // extern_func and func_decl functions take ownership of `sema.owner_decl`.
    sema.owner_decl.@"linksection" = switch (section) {
        .generic => .none,
        .default => .none,
        .explicit => |section_name| section_name.to_optional(),
    };
    sema.owner_decl.alignment = alignment orelse .none;
    sema.owner_decl.@"addrspace" = address_space orelse .generic;

    if (inferred_error_set) {
        assert(!is_extern);
        assert(has_body);
        if (!ret_poison)
            try sema.validate_error_union_payload_type(block, bare_return_type, ret_ty_src);
        const func_index = try ip.get_func_decl_ies(gpa, .{
            .owner_decl = sema.owner_decl_index,

            .param_types = param_types,
            .noalias_bits = noalias_bits,
            .comptime_bits = comptime_bits,
            .bare_return_type = bare_return_type.to_intern(),
            .cc = cc,
            .alignment = alignment,
            .section_is_generic = section == .generic,
            .addrspace_is_generic = address_space == null,
            .is_var_args = var_args,
            .is_generic = final_is_generic,
            .is_noinline = is_noinline,

            .zir_body_inst = try ip.track_zir(gpa, block.get_file_scope(mod), func_inst),
            .lbrace_line = src_locs.lbrace_line,
            .rbrace_line = src_locs.rbrace_line,
            .lbrace_column = @as(u16, @truncate(src_locs.columns)),
            .rbrace_column = @as(u16, @truncate(src_locs.columns >> 16)),
        });
        return finish_func(
            sema,
            block,
            func_index,
            .none,
            ret_poison,
            bare_return_type,
            ret_ty_src,
            cc_resolved,
            is_source_decl,
            ret_ty_requires_comptime,
            func_inst,
            cc_src,
            is_noinline,
            is_generic,
            final_is_generic,
        );
    }

    const func_ty = try ip.get_func_type(gpa, .{
        .param_types = param_types,
        .noalias_bits = noalias_bits,
        .comptime_bits = comptime_bits,
        .return_type = bare_return_type.to_intern(),
        .cc = cc,
        .section_is_generic = section == .generic,
        .addrspace_is_generic = address_space == null,
        .is_var_args = var_args,
        .is_generic = final_is_generic,
        .is_noinline = is_noinline,
    });

    if (is_extern) {
        assert(comptime_bits == 0);
        assert(cc != null);
        assert(alignment != null);
        assert(section != .generic);
        assert(address_space != null);
        assert(!is_generic);
        if (opt_lib_name) |lib_name| try sema.handle_extern_lib_name(block, .{
            .node_offset_lib_name = src_node_offset,
        }, lib_name);
        const func_index = try ip.get_extern_func(gpa, .{
            .ty = func_ty,
            .decl = sema.owner_decl_index,
            .lib_name = try mod.intern_pool.get_or_put_string_opt(gpa, opt_lib_name, .no_embedded_nulls),
        });
        return finish_func(
            sema,
            block,
            func_index,
            func_ty,
            ret_poison,
            bare_return_type,
            ret_ty_src,
            cc_resolved,
            is_source_decl,
            ret_ty_requires_comptime,
            func_inst,
            cc_src,
            is_noinline,
            is_generic,
            final_is_generic,
        );
    }

    if (has_body) {
        const func_index = try ip.get_func_decl(gpa, .{
            .owner_decl = sema.owner_decl_index,
            .ty = func_ty,
            .cc = cc,
            .is_noinline = is_noinline,
            .zir_body_inst = try ip.track_zir(gpa, block.get_file_scope(mod), func_inst),
            .lbrace_line = src_locs.lbrace_line,
            .rbrace_line = src_locs.rbrace_line,
            .lbrace_column = @as(u16, @truncate(src_locs.columns)),
            .rbrace_column = @as(u16, @truncate(src_locs.columns >> 16)),
        });
        return finish_func(
            sema,
            block,
            func_index,
            func_ty,
            ret_poison,
            bare_return_type,
            ret_ty_src,
            cc_resolved,
            is_source_decl,
            ret_ty_requires_comptime,
            func_inst,
            cc_src,
            is_noinline,
            is_generic,
            final_is_generic,
        );
    }

    return finish_func(
        sema,
        block,
        .none,
        func_ty,
        ret_poison,
        bare_return_type,
        ret_ty_src,
        cc_resolved,
        is_source_decl,
        ret_ty_requires_comptime,
        func_inst,
        cc_src,
        is_noinline,
        is_generic,
        final_is_generic,
    );
}

fn finish_func(
    sema: *Sema,
    block: *Block,
    opt_func_index: InternPool.Index,
    func_ty: InternPool.Index,
    ret_poison: bool,
    bare_return_type: Type,
    ret_ty_src: LazySrcLoc,
    cc_resolved: std.builtin.CallingConvention,
    is_source_decl: bool,
    ret_ty_requires_comptime: bool,
    func_inst: Zir.Inst.Index,
    cc_src: LazySrcLoc,
    is_noinline: bool,
    is_generic: bool,
    final_is_generic: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const gpa = sema.gpa;
    const target = mod.get_target();

    const return_type: Type = if (opt_func_index == .none or ret_poison)
        bare_return_type
    else
        Type.from_interned(ip.func_type_return_type(ip.type_of(opt_func_index)));

    if (!return_type.is_valid_return_type(mod)) {
        const opaque_str = if (return_type.zig_type_tag(mod) == .Opaque) "opaque " else "";
        return sema.fail(block, ret_ty_src, "{s}return type '{}' not allowed", .{
            opaque_str, return_type.fmt(mod),
        });
    }
    if (!ret_poison and !target_util.fn_call_conv_allows_zig_types(target, cc_resolved) and
        !try sema.validate_extern_type(return_type, .ret_ty))
    {
        const msg = msg: {
            const msg = try sema.err_msg(block, ret_ty_src, "return type '{}' not allowed in function with calling convention '{s}'", .{
                return_type.fmt(mod), @tag_name(cc_resolved),
            });
            errdefer msg.destroy(gpa);

            const src_decl = mod.decl_ptr(block.src_decl);
            try sema.explain_why_type_is_not_extern(msg, src_decl.to_src_loc(ret_ty_src, mod), return_type, .ret_ty);

            try sema.add_declared_here_note(msg, return_type);
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    // If the return type is comptime-only but not dependent on parameters then
    // all parameter types also need to be comptime.
    if (is_source_decl and opt_func_index != .none and ret_ty_requires_comptime and !block.is_comptime) comptime_check: {
        for (block.params.items(.is_comptime)) |is_comptime| {
            if (!is_comptime) break;
        } else break :comptime_check;

        const msg = try sema.err_msg(
            block,
            ret_ty_src,
            "function with comptime-only return type '{}' requires all parameters to be comptime",
            .{return_type.fmt(mod)},
        );
        try sema.explain_why_type_is_comptime(msg, sema.owner_decl.to_src_loc(ret_ty_src, mod), return_type);

        const tags = sema.code.instructions.items(.tag);
        const data = sema.code.instructions.items(.data);
        const param_body = sema.code.get_param_body(func_inst);
        for (
            block.params.items(.is_comptime),
            block.params.items(.name),
            param_body[0..block.params.len],
        ) |is_comptime, name_nts, param_index| {
            if (!is_comptime) {
                const param_src = switch (tags[@int_from_enum(param_index)]) {
                    .param => data[@int_from_enum(param_index)].pl_tok.src(),
                    .param_anytype => data[@int_from_enum(param_index)].str_tok.src(),
                    else => unreachable,
                };
                const name = sema.code.null_terminated_string(name_nts);
                if (name.len != 0) {
                    try sema.err_note(block, param_src, msg, "param '{s}' is required to be comptime", .{name});
                } else {
                    try sema.err_note(block, param_src, msg, "param is required to be comptime", .{});
                }
            }
        }
        return sema.fail_with_owned_error_msg(block, msg);
    }

    switch (cc_resolved) {
        .Interrupt, .Signal => if (return_type.zig_type_tag(mod) != .Void and return_type.zig_type_tag(mod) != .NoReturn) {
            return sema.fail(block, ret_ty_src, "function with calling convention '{s}' must return 'void' or 'noreturn'", .{@tag_name(cc_resolved)});
        },
        .Inline => if (is_noinline) {
            return sema.fail(block, cc_src, "'noinline' function cannot have callconv 'Inline'", .{});
        },
        else => {},
    }

    const arch = target.cpu.arch;
    if (@as(?[]const u8, switch (cc_resolved) {
        .Unspecified, .C, .Naked, .Async, .Inline => null,
        .Interrupt => switch (arch) {
            .x86, .x86_64, .avr, .msp430 => null,
            else => "x86, x86_64, AVR, and MSP430",
        },
        .Signal => switch (arch) {
            .avr => null,
            else => "AVR",
        },
        .Stdcall, .Fastcall, .Thiscall => switch (arch) {
            .x86 => null,
            else => "x86",
        },
        .Vectorcall => switch (arch) {
            .x86, .aarch64, .aarch64_be, .aarch64_32 => null,
            else => "x86 and AArch64",
        },
        .APCS, .AAPCS, .AAPCSVFP => switch (arch) {
            .arm, .armeb, .aarch64, .aarch64_be, .aarch64_32, .thumb, .thumbeb => null,
            else => "ARM",
        },
        .SysV, .Win64 => switch (arch) {
            .x86_64 => null,
            else => "x86_64",
        },
        .Kernel => switch (arch) {
            .nvptx, .nvptx64, .amdgcn, .spirv32, .spirv64 => null,
            else => "nvptx, amdgcn and SPIR-V",
        },
        .Fragment, .Vertex => switch (arch) {
            .spirv32, .spirv64 => null,
            else => "SPIR-V",
        },
    })) |allowed_platform| {
        return sema.fail(block, cc_src, "callconv '{s}' is only available on {s}, not {s}", .{
            @tag_name(cc_resolved),
            allowed_platform,
            @tag_name(arch),
        });
    }

    if (is_generic and sema.no_partial_func_ty) return error.GenericPoison;
    if (!final_is_generic and sema.want_error_return_tracing(return_type)) {
        // Make sure that StackTrace's fields are resolved so that the backend can
        // lower this fn type.
        const unresolved_stack_trace_ty = try sema.get_builtin_type("StackTrace");
        try sema.resolve_type_fields(unresolved_stack_trace_ty);
    }

    return Air.interned_to_ref(if (opt_func_index != .none) opt_func_index else func_ty);
}

fn zir_param(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    comptime_syntax: bool,
) CompileError!void {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_tok;
    const src = inst_data.src();
    const extra = sema.code.extra_data(Zir.Inst.Param, inst_data.payload_index);
    const param_name: Zir.NullTerminatedString = extra.data.name;
    const body = sema.code.body_slice(extra.end, extra.data.body_len);

    const param_ty = param_ty: {
        const err = err: {
            // Make sure any nested param instructions don't clobber our work.
            const prev_params = block.params;
            const prev_no_partial_func_type = sema.no_partial_func_ty;
            const prev_generic_owner = sema.generic_owner;
            const prev_generic_call_src = sema.generic_call_src;
            const prev_generic_call_decl = sema.generic_call_decl;
            block.params = .{};
            sema.no_partial_func_ty = true;
            sema.generic_owner = .none;
            sema.generic_call_src = .unneeded;
            sema.generic_call_decl = .none;
            defer {
                block.params = prev_params;
                sema.no_partial_func_ty = prev_no_partial_func_type;
                sema.generic_owner = prev_generic_owner;
                sema.generic_call_src = prev_generic_call_src;
                sema.generic_call_decl = prev_generic_call_decl;
            }

            if (sema.resolve_inline_body(block, body, inst)) |param_ty_inst| {
                if (sema.analyze_as_type(block, src, param_ty_inst)) |param_ty| {
                    break :param_ty param_ty;
                } else |err| break :err err;
            } else |err| break :err err;
        };
        switch (err) {
            error.GenericPoison => {
                // The type is not available until the generic instantiation.
                // We result the param instruction with a poison value and
                // insert an anytype parameter.
                try block.params.append(sema.arena, .{
                    .ty = .generic_poison_type,
                    .is_comptime = comptime_syntax,
                    .name = param_name,
                });
                sema.inst_map.put_assume_capacity(inst, .generic_poison);
                return;
            },
            else => |e| return e,
        }
    };

    const is_comptime = sema.type_requires_comptime(param_ty) catch |err| switch (err) {
        error.GenericPoison => {
            // The type is not available until the generic instantiation.
            // We result the param instruction with a poison value and
            // insert an anytype parameter.
            try block.params.append(sema.arena, .{
                .ty = .generic_poison_type,
                .is_comptime = comptime_syntax,
                .name = param_name,
            });
            sema.inst_map.put_assume_capacity(inst, .generic_poison);
            return;
        },
        else => |e| return e,
    } or comptime_syntax;

    try block.params.append(sema.arena, .{
        .ty = param_ty.to_intern(),
        .is_comptime = comptime_syntax,
        .name = param_name,
    });

    if (is_comptime) {
        // If this is a comptime parameter we can add a constant generic_poison
        // since this is also a generic parameter.
        sema.inst_map.put_assume_capacity(inst, .generic_poison);
    } else {
        // Otherwise we need a dummy runtime instruction.
        const result_index: Air.Inst.Index = @enumFromInt(sema.air_instructions.len);
        try sema.air_instructions.append(sema.gpa, .{
            .tag = .alloc,
            .data = .{ .ty = param_ty },
        });
        sema.inst_map.put_assume_capacity(inst, result_index.to_ref());
    }
}

fn zir_param_anytype(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    comptime_syntax: bool,
) CompileError!void {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].str_tok;
    const param_name: Zir.NullTerminatedString = inst_data.start;

    // We are evaluating a generic function without any comptime args provided.

    try block.params.append(sema.arena, .{
        .ty = .generic_poison_type,
        .is_comptime = comptime_syntax,
        .name = param_name,
    });
    sema.inst_map.put_assume_capacity(inst, .generic_poison);
}

fn zir_as_node(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const extra = sema.code.extra_data(Zir.Inst.As, inst_data.payload_index).data;
    return sema.analyze_as(block, src, extra.dest_type, extra.operand, false);
}

fn zir_as_shift_operand(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const extra = sema.code.extra_data(Zir.Inst.As, inst_data.payload_index).data;
    return sema.analyze_as(block, src, extra.dest_type, extra.operand, true);
}

fn analyze_as(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    zir_dest_type: Zir.Inst.Ref,
    zir_operand: Zir.Inst.Ref,
    no_cast_to_comptime_int: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const operand = try sema.resolve_inst(zir_operand);
    const operand_air_inst = sema.resolve_inst(zir_dest_type) catch |err| switch (err) {
        error.GenericPoison => return operand,
        else => |e| return e,
    };
    const dest_ty = sema.analyze_as_type(block, src, operand_air_inst) catch |err| switch (err) {
        error.GenericPoison => return operand,
        else => |e| return e,
    };
    const dest_ty_tag = dest_ty.zig_type_tag_or_poison(mod) catch |err| switch (err) {
        error.GenericPoison => return operand,
    };
    if (dest_ty_tag == .NoReturn) {
        return sema.fail(block, src, "cannot cast to noreturn", .{});
    }
    const is_ret = if (zir_dest_type.to_index()) |ptr_index|
        sema.code.instructions.items(.tag)[@int_from_enum(ptr_index)] == .ret_type
    else
        false;
    return sema.coerce_extra(block, dest_ty, operand, src, .{ .is_ret = is_ret, .no_cast_to_comptime_int = no_cast_to_comptime_int }) catch |err| switch (err) {
        error.NotCoercible => unreachable,
        else => |e| return e,
    };
}

fn zir_int_from_ptr(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const zcu = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const ptr_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const operand = try sema.resolve_inst(inst_data.operand);
    const operand_ty = sema.type_of(operand);
    const ptr_ty = operand_ty.scalar_type(zcu);
    const is_vector = operand_ty.zig_type_tag(zcu) == .Vector;
    if (!ptr_ty.is_ptr_at_runtime(zcu)) {
        return sema.fail(block, ptr_src, "expected pointer, found '{}'", .{ptr_ty.fmt(zcu)});
    }
    const pointee_ty = ptr_ty.child_type(zcu);
    if (try sema.type_requires_comptime(ptr_ty)) {
        const msg = msg: {
            const msg = try sema.err_msg(block, ptr_src, "comptime-only type '{}' has no pointer address", .{pointee_ty.fmt(zcu)});
            errdefer msg.destroy(sema.gpa);
            const src_decl = zcu.decl_ptr(block.src_decl);
            try sema.explain_why_type_is_comptime(msg, src_decl.to_src_loc(ptr_src, zcu), pointee_ty);
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }
    if (try sema.resolve_value_intable(operand)) |operand_val| ct: {
        if (!is_vector) {
            if (operand_val.is_undef(zcu)) {
                return Air.interned_to_ref((try zcu.undef_value(Type.usize)).to_intern());
            }
            return Air.interned_to_ref((try zcu.int_value(
                Type.usize,
                (try operand_val.get_unsigned_int_advanced(zcu, sema)).?,
            )).to_intern());
        }
        const len = operand_ty.vector_len(zcu);
        const dest_ty = try zcu.vector_type(.{ .child = .usize_type, .len = len });
        const new_elems = try sema.arena.alloc(InternPool.Index, len);
        for (new_elems, 0..) |*new_elem, i| {
            const ptr_val = try operand_val.elem_value(zcu, i);
            if (ptr_val.is_undef(zcu)) {
                new_elem.* = (try zcu.undef_value(Type.usize)).to_intern();
                continue;
            }
            const addr = try ptr_val.get_unsigned_int_advanced(zcu, sema) orelse {
                // A vector element wasn't an integer pointer. This is a runtime operation.
                break :ct;
            };
            new_elem.* = (try zcu.int_value(
                Type.usize,
                addr,
            )).to_intern();
        }
        return Air.interned_to_ref(try zcu.intern(.{ .aggregate = .{
            .ty = dest_ty.to_intern(),
            .storage = .{ .elems = new_elems },
        } }));
    }
    try sema.require_runtime_block(block, inst_data.src(), ptr_src);
    try sema.validate_runtime_value(block, ptr_src, operand);
    if (!is_vector) {
        return block.add_un_op(.int_from_ptr, operand);
    }
    const len = operand_ty.vector_len(zcu);
    const dest_ty = try zcu.vector_type(.{ .child = .usize_type, .len = len });
    const new_elems = try sema.arena.alloc(Air.Inst.Ref, len);
    for (new_elems, 0..) |*new_elem, i| {
        const idx_ref = try zcu.int_ref(Type.usize, i);
        const old_elem = try block.add_bin_op(.array_elem_val, operand, idx_ref);
        new_elem.* = try block.add_un_op(.int_from_ptr, old_elem);
    }
    return block.add_aggregate_init(dest_ty, new_elems);
}

fn zir_field_val(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const field_name_src: LazySrcLoc = .{ .node_offset_field_name = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Field, inst_data.payload_index).data;
    const field_name = try mod.intern_pool.get_or_put_string(
        sema.gpa,
        sema.code.null_terminated_string(extra.field_name_start),
        .no_embedded_nulls,
    );
    const object = try sema.resolve_inst(extra.lhs);
    return sema.field_val(block, src, object, field_name, field_name_src);
}

fn zir_field_ptr(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const field_name_src: LazySrcLoc = .{ .node_offset_field_name = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Field, inst_data.payload_index).data;
    const field_name = try mod.intern_pool.get_or_put_string(
        sema.gpa,
        sema.code.null_terminated_string(extra.field_name_start),
        .no_embedded_nulls,
    );
    const object_ptr = try sema.resolve_inst(extra.lhs);
    return sema.field_ptr(block, src, object_ptr, field_name, field_name_src, false);
}

fn zir_struct_init_field_ptr(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const field_name_src: LazySrcLoc = .{ .node_offset_field_name_init = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Field, inst_data.payload_index).data;
    const field_name = try mod.intern_pool.get_or_put_string(
        sema.gpa,
        sema.code.null_terminated_string(extra.field_name_start),
        .no_embedded_nulls,
    );
    const object_ptr = try sema.resolve_inst(extra.lhs);
    const struct_ty = sema.type_of(object_ptr).child_type(mod);
    switch (struct_ty.zig_type_tag(mod)) {
        .Struct, .Union => {
            return sema.field_ptr(block, src, object_ptr, field_name, field_name_src, true);
        },
        else => {
            return sema.fail_with_struct_init_not_supported(block, src, struct_ty);
        },
    }
}

fn zir_field_val_named(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const field_name_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.FieldNamed, inst_data.payload_index).data;
    const object = try sema.resolve_inst(extra.lhs);
    const field_name = try sema.resolve_const_string_intern(block, field_name_src, extra.field_name, .{
        .needed_comptime_reason = "field name must be comptime-known",
    });
    return sema.field_val(block, src, object, field_name, field_name_src);
}

fn zir_field_ptr_named(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const field_name_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.FieldNamed, inst_data.payload_index).data;
    const object_ptr = try sema.resolve_inst(extra.lhs);
    const field_name = try sema.resolve_const_string_intern(block, field_name_src, extra.field_name, .{
        .needed_comptime_reason = "field name must be comptime-known",
    });
    return sema.field_ptr(block, src, object_ptr, field_name, field_name_src, false);
}

fn zir_int_cast(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;

    const dest_ty = try sema.resolve_dest_type(block, src, extra.lhs, .remove_eu_opt, "@int_cast");
    const operand = try sema.resolve_inst(extra.rhs);

    return sema.int_cast(block, inst_data.src(), dest_ty, src, operand, operand_src, true);
}

fn int_cast(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    dest_ty: Type,
    dest_ty_src: LazySrcLoc,
    operand: Air.Inst.Ref,
    operand_src: LazySrcLoc,
    runtime_safety: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const operand_ty = sema.type_of(operand);
    const dest_scalar_ty = try sema.check_int_or_vector_allow_comptime(block, dest_ty, dest_ty_src);
    const operand_scalar_ty = try sema.check_int_or_vector_allow_comptime(block, operand_ty, operand_src);

    if (try sema.is_comptime_known(operand)) {
        return sema.coerce(block, dest_ty, operand, operand_src);
    } else if (dest_scalar_ty.zig_type_tag(mod) == .ComptimeInt) {
        return sema.fail(block, operand_src, "unable to cast runtime value to 'comptime_int'", .{});
    }

    try sema.check_vectorizable_binary_operands(block, operand_src, dest_ty, operand_ty, dest_ty_src, operand_src);
    const is_vector = dest_ty.zig_type_tag(mod) == .Vector;

    if ((try sema.type_has_one_possible_value(dest_ty))) |opv| {
        // requirement: int_cast(u0, input) iff input == 0
        if (runtime_safety and block.want_safety()) {
            try sema.require_runtime_block(block, src, operand_src);
            const wanted_info = dest_scalar_ty.int_info(mod);
            const wanted_bits = wanted_info.bits;

            if (wanted_bits == 0) {
                const ok = if (is_vector) ok: {
                    const zeros = try sema.splat(operand_ty, try mod.int_value(operand_scalar_ty, 0));
                    const zero_inst = Air.interned_to_ref(zeros.to_intern());
                    const is_in_range = try block.add_cmp_vector(operand, zero_inst, .eq);
                    const all_in_range = try block.add_inst(.{
                        .tag = .reduce,
                        .data = .{ .reduce = .{ .operand = is_in_range, .operation = .And } },
                    });
                    break :ok all_in_range;
                } else ok: {
                    const zero_inst = Air.interned_to_ref((try mod.int_value(operand_ty, 0)).to_intern());
                    const is_in_range = try block.add_bin_op(.cmp_lte, operand, zero_inst);
                    break :ok is_in_range;
                };
                try sema.add_safety_check(block, src, ok, .cast_truncated_data);
            }
        }

        return Air.interned_to_ref(opv.to_intern());
    }

    try sema.require_runtime_block(block, src, operand_src);
    if (runtime_safety and block.want_safety()) {
        const actual_info = operand_scalar_ty.int_info(mod);
        const wanted_info = dest_scalar_ty.int_info(mod);
        const actual_bits = actual_info.bits;
        const wanted_bits = wanted_info.bits;
        const actual_value_bits = actual_bits - @int_from_bool(actual_info.signedness == .signed);
        const wanted_value_bits = wanted_bits - @int_from_bool(wanted_info.signedness == .signed);

        // range shrinkage
        // requirement: int value fits into target type
        if (wanted_value_bits < actual_value_bits) {
            const dest_max_val_scalar = try dest_scalar_ty.max_int_scalar(mod, operand_scalar_ty);
            const dest_max_val = try sema.splat(operand_ty, dest_max_val_scalar);
            const dest_max = Air.interned_to_ref(dest_max_val.to_intern());

            if (actual_info.signedness == .signed) {
                const diff = try block.add_bin_op(.sub_wrap, dest_max, operand);

                // Reinterpret the sign-bit as part of the value. This will make
                // negative differences (`operand` > `dest_max`) appear too big.
                const unsigned_scalar_operand_ty = try mod.int_type(.unsigned, actual_bits);
                const unsigned_operand_ty = if (is_vector) try mod.vector_type(.{
                    .len = dest_ty.vector_len(mod),
                    .child = unsigned_scalar_operand_ty.to_intern(),
                }) else unsigned_scalar_operand_ty;
                const diff_unsigned = try block.add_bit_cast(unsigned_operand_ty, diff);

                // If the destination type is signed, then we need to double its
                // range to account for negative values.
                const dest_range_val = if (wanted_info.signedness == .signed) range_val: {
                    const one_scalar = try mod.int_value(unsigned_scalar_operand_ty, 1);
                    const one = if (is_vector) Value.from_interned((try mod.intern(.{ .aggregate = .{
                        .ty = unsigned_operand_ty.to_intern(),
                        .storage = .{ .repeated_elem = one_scalar.to_intern() },
                    } }))) else one_scalar;
                    const range_minus_one = try dest_max_val.shl(one, unsigned_operand_ty, sema.arena, mod);
                    break :range_val try sema.int_add(range_minus_one, one, unsigned_operand_ty, undefined);
                } else try mod.get_coerced(dest_max_val, unsigned_operand_ty);
                const dest_range = Air.interned_to_ref(dest_range_val.to_intern());

                const ok = if (is_vector) ok: {
                    const is_in_range = try block.add_cmp_vector(diff_unsigned, dest_range, .lte);
                    const all_in_range = try block.add_inst(.{
                        .tag = if (block.float_mode == .optimized) .reduce_optimized else .reduce,
                        .data = .{ .reduce = .{
                            .operand = is_in_range,
                            .operation = .And,
                        } },
                    });
                    break :ok all_in_range;
                } else ok: {
                    const is_in_range = try block.add_bin_op(.cmp_lte, diff_unsigned, dest_range);
                    break :ok is_in_range;
                };
                // TODO negative_to_unsigned?
                try sema.add_safety_check(block, src, ok, .cast_truncated_data);
            } else {
                const ok = if (is_vector) ok: {
                    const is_in_range = try block.add_cmp_vector(operand, dest_max, .lte);
                    const all_in_range = try block.add_inst(.{
                        .tag = if (block.float_mode == .optimized) .reduce_optimized else .reduce,
                        .data = .{ .reduce = .{
                            .operand = is_in_range,
                            .operation = .And,
                        } },
                    });
                    break :ok all_in_range;
                } else ok: {
                    const is_in_range = try block.add_bin_op(.cmp_lte, operand, dest_max);
                    break :ok is_in_range;
                };
                try sema.add_safety_check(block, src, ok, .cast_truncated_data);
            }
        } else if (actual_info.signedness == .signed and wanted_info.signedness == .unsigned) {
            // no shrinkage, yes sign loss
            // requirement: signed to unsigned >= 0
            const ok = if (is_vector) ok: {
                const scalar_zero = try mod.int_value(operand_scalar_ty, 0);
                const zero_val = try sema.splat(operand_ty, scalar_zero);
                const zero_inst = Air.interned_to_ref(zero_val.to_intern());
                const is_in_range = try block.add_cmp_vector(operand, zero_inst, .gte);
                const all_in_range = try block.add_inst(.{
                    .tag = if (block.float_mode == .optimized) .reduce_optimized else .reduce,
                    .data = .{ .reduce = .{
                        .operand = is_in_range,
                        .operation = .And,
                    } },
                });
                break :ok all_in_range;
            } else ok: {
                const zero_inst = Air.interned_to_ref((try mod.int_value(operand_ty, 0)).to_intern());
                const is_in_range = try block.add_bin_op(.cmp_gte, operand, zero_inst);
                break :ok is_in_range;
            };
            try sema.add_safety_check(block, src, ok, .negative_to_unsigned);
        }
    }
    return block.add_ty_op(.intcast, dest_ty, operand);
}

fn zir_bitcast(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;

    const dest_ty = try sema.resolve_dest_type(block, src, extra.lhs, .remove_eu_opt, "@bit_cast");
    const operand = try sema.resolve_inst(extra.rhs);
    const operand_ty = sema.type_of(operand);
    switch (dest_ty.zig_type_tag(mod)) {
        .AnyFrame,
        .ComptimeFloat,
        .ComptimeInt,
        .EnumLiteral,
        .ErrorSet,
        .ErrorUnion,
        .Fn,
        .Frame,
        .NoReturn,
        .Null,
        .Opaque,
        .Optional,
        .Type,
        .Undefined,
        .Void,
        => return sema.fail(block, src, "cannot @bit_cast to '{}'", .{dest_ty.fmt(mod)}),

        .Enum => {
            const msg = msg: {
                const msg = try sema.err_msg(block, src, "cannot @bit_cast to '{}'", .{dest_ty.fmt(mod)});
                errdefer msg.destroy(sema.gpa);
                switch (operand_ty.zig_type_tag(mod)) {
                    .Int, .ComptimeInt => try sema.err_note(block, src, msg, "use @enumFromInt to cast from '{}'", .{operand_ty.fmt(mod)}),
                    else => {},
                }

                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        },

        .Pointer => {
            const msg = msg: {
                const msg = try sema.err_msg(block, src, "cannot @bit_cast to '{}'", .{dest_ty.fmt(mod)});
                errdefer msg.destroy(sema.gpa);
                switch (operand_ty.zig_type_tag(mod)) {
                    .Int, .ComptimeInt => try sema.err_note(block, src, msg, "use @ptrFromInt to cast from '{}'", .{operand_ty.fmt(mod)}),
                    .Pointer => try sema.err_note(block, src, msg, "use @ptr_cast to cast from '{}'", .{operand_ty.fmt(mod)}),
                    else => {},
                }

                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        },
        .Struct, .Union => if (dest_ty.container_layout(mod) == .auto) {
            const container = switch (dest_ty.zig_type_tag(mod)) {
                .Struct => "struct",
                .Union => "union",
                else => unreachable,
            };
            return sema.fail(block, src, "cannot @bit_cast to '{}'; {s} does not have a guaranteed in-memory layout", .{
                dest_ty.fmt(mod), container,
            });
        },

        .Array,
        .Bool,
        .Float,
        .Int,
        .Vector,
        => {},
    }
    switch (operand_ty.zig_type_tag(mod)) {
        .AnyFrame,
        .ComptimeFloat,
        .ComptimeInt,
        .EnumLiteral,
        .ErrorSet,
        .ErrorUnion,
        .Fn,
        .Frame,
        .NoReturn,
        .Null,
        .Opaque,
        .Optional,
        .Type,
        .Undefined,
        .Void,
        => return sema.fail(block, operand_src, "cannot @bit_cast from '{}'", .{operand_ty.fmt(mod)}),

        .Enum => {
            const msg = msg: {
                const msg = try sema.err_msg(block, operand_src, "cannot @bit_cast from '{}'", .{operand_ty.fmt(mod)});
                errdefer msg.destroy(sema.gpa);
                switch (dest_ty.zig_type_tag(mod)) {
                    .Int, .ComptimeInt => try sema.err_note(block, operand_src, msg, "use @int_from_enum to cast to '{}'", .{dest_ty.fmt(mod)}),
                    else => {},
                }

                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        },
        .Pointer => {
            const msg = msg: {
                const msg = try sema.err_msg(block, operand_src, "cannot @bit_cast from '{}'", .{operand_ty.fmt(mod)});
                errdefer msg.destroy(sema.gpa);
                switch (dest_ty.zig_type_tag(mod)) {
                    .Int, .ComptimeInt => try sema.err_note(block, operand_src, msg, "use @int_from_ptr to cast to '{}'", .{dest_ty.fmt(mod)}),
                    .Pointer => try sema.err_note(block, operand_src, msg, "use @ptr_cast to cast to '{}'", .{dest_ty.fmt(mod)}),
                    else => {},
                }

                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        },
        .Struct, .Union => if (operand_ty.container_layout(mod) == .auto) {
            const container = switch (operand_ty.zig_type_tag(mod)) {
                .Struct => "struct",
                .Union => "union",
                else => unreachable,
            };
            return sema.fail(block, operand_src, "cannot @bit_cast from '{}'; {s} does not have a guaranteed in-memory layout", .{
                operand_ty.fmt(mod), container,
            });
        },

        .Array,
        .Bool,
        .Float,
        .Int,
        .Vector,
        => {},
    }
    return sema.bit_cast(block, dest_ty, operand, inst_data.src(), operand_src);
}

fn zir_float_cast(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;

    const dest_ty = try sema.resolve_dest_type(block, src, extra.lhs, .remove_eu_opt, "@float_cast");
    const dest_scalar_ty = dest_ty.scalar_type(mod);

    const operand = try sema.resolve_inst(extra.rhs);
    const operand_ty = sema.type_of(operand);
    const operand_scalar_ty = operand_ty.scalar_type(mod);

    try sema.check_vectorizable_binary_operands(block, operand_src, dest_ty, operand_ty, src, operand_src);
    const is_vector = dest_ty.zig_type_tag(mod) == .Vector;

    const target = mod.get_target();
    const dest_is_comptime_float = switch (dest_scalar_ty.zig_type_tag(mod)) {
        .ComptimeFloat => true,
        .Float => false,
        else => return sema.fail(
            block,
            src,
            "expected float or vector type, found '{}'",
            .{dest_ty.fmt(mod)},
        ),
    };

    switch (operand_scalar_ty.zig_type_tag(mod)) {
        .ComptimeFloat, .Float, .ComptimeInt => {},
        else => return sema.fail(
            block,
            operand_src,
            "expected float or vector type, found '{}'",
            .{operand_ty.fmt(mod)},
        ),
    }

    if (try sema.resolve_value(operand)) |operand_val| {
        if (!is_vector) {
            return Air.interned_to_ref((try operand_val.float_cast(dest_ty, mod)).to_intern());
        }
        const vec_len = operand_ty.vector_len(mod);
        const new_elems = try sema.arena.alloc(InternPool.Index, vec_len);
        for (new_elems, 0..) |*new_elem, i| {
            const old_elem = try operand_val.elem_value(mod, i);
            new_elem.* = (try old_elem.float_cast(dest_scalar_ty, mod)).to_intern();
        }
        return Air.interned_to_ref(try mod.intern(.{ .aggregate = .{
            .ty = dest_ty.to_intern(),
            .storage = .{ .elems = new_elems },
        } }));
    }
    if (dest_is_comptime_float) {
        return sema.fail(block, operand_src, "unable to cast runtime value to 'comptime_float'", .{});
    }
    try sema.require_runtime_block(block, inst_data.src(), operand_src);

    const src_bits = operand_scalar_ty.float_bits(target);
    const dst_bits = dest_scalar_ty.float_bits(target);
    if (dst_bits >= src_bits) {
        return sema.coerce(block, dest_ty, operand, operand_src);
    }
    if (!is_vector) {
        return block.add_ty_op(.fptrunc, dest_ty, operand);
    }
    const vec_len = operand_ty.vector_len(mod);
    const new_elems = try sema.arena.alloc(Air.Inst.Ref, vec_len);
    for (new_elems, 0..) |*new_elem, i| {
        const idx_ref = try mod.int_ref(Type.usize, i);
        const old_elem = try block.add_bin_op(.array_elem_val, operand, idx_ref);
        new_elem.* = try block.add_ty_op(.fptrunc, dest_scalar_ty, old_elem);
    }
    return block.add_aggregate_init(dest_ty, new_elems);
}

fn zir_elem_val(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const array = try sema.resolve_inst(extra.lhs);
    const elem_index = try sema.resolve_inst(extra.rhs);
    return sema.elem_val(block, src, array, elem_index, src, false);
}

fn zir_elem_val_node(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const elem_index_src: LazySrcLoc = .{ .node_offset_array_access_index = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const array = try sema.resolve_inst(extra.lhs);
    const uncoerced_elem_index = try sema.resolve_inst(extra.rhs);
    const elem_index = try sema.coerce(block, Type.usize, uncoerced_elem_index, elem_index_src);
    return sema.elem_val(block, src, array, elem_index, elem_index_src, true);
}

fn zir_elem_val_imm(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].elem_val_imm;
    const array = try sema.resolve_inst(inst_data.operand);
    const elem_index = try mod.int_ref(Type.usize, inst_data.idx);
    return sema.elem_val(block, .unneeded, array, elem_index, .unneeded, false);
}

fn zir_elem_ptr(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const array_ptr = try sema.resolve_inst(extra.lhs);
    const elem_index = try sema.resolve_inst(extra.rhs);
    const indexable_ty = sema.type_of(array_ptr);
    if (indexable_ty.zig_type_tag(mod) != .Pointer) {
        const capture_src: LazySrcLoc = .{ .for_capture_from_input = inst_data.src_node };
        const msg = msg: {
            const msg = try sema.err_msg(block, capture_src, "pointer capture of non pointer type '{}'", .{
                indexable_ty.fmt(mod),
            });
            errdefer msg.destroy(sema.gpa);
            if (indexable_ty.is_indexable(mod)) {
                try sema.err_note(block, src, msg, "consider using '&' here", .{});
            }
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }
    return sema.elem_ptr_one_layer_only(block, src, array_ptr, elem_index, src, false, false);
}

fn zir_elem_ptr_node(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const elem_index_src: LazySrcLoc = .{ .node_offset_array_access_index = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const array_ptr = try sema.resolve_inst(extra.lhs);
    const uncoerced_elem_index = try sema.resolve_inst(extra.rhs);
    const elem_index = try sema.coerce(block, Type.usize, uncoerced_elem_index, elem_index_src);
    return sema.elem_ptr(block, src, array_ptr, elem_index, elem_index_src, false, true);
}

fn zir_array_init_elem_ptr(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const extra = sema.code.extra_data(Zir.Inst.ElemPtrImm, inst_data.payload_index).data;
    const array_ptr = try sema.resolve_inst(extra.ptr);
    const elem_index = try sema.mod.int_ref(Type.usize, extra.index);
    const array_ty = sema.type_of(array_ptr).child_type(mod);
    switch (array_ty.zig_type_tag(mod)) {
        .Array, .Vector => {},
        else => if (!array_ty.is_tuple(mod)) {
            return sema.fail_with_array_init_not_supported(block, src, array_ty);
        },
    }
    return sema.elem_ptr(block, src, array_ptr, elem_index, src, true, true);
}

fn zir_slice_start(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const extra = sema.code.extra_data(Zir.Inst.SliceStart, inst_data.payload_index).data;
    const array_ptr = try sema.resolve_inst(extra.lhs);
    const start = try sema.resolve_inst(extra.start);
    const ptr_src: LazySrcLoc = .{ .node_offset_slice_ptr = inst_data.src_node };
    const start_src: LazySrcLoc = .{ .node_offset_slice_start = inst_data.src_node };
    const end_src: LazySrcLoc = .{ .node_offset_slice_end = inst_data.src_node };

    return sema.analyze_slice(block, src, array_ptr, start, .none, .none, .unneeded, ptr_src, start_src, end_src, false);
}

fn zir_slice_end(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const extra = sema.code.extra_data(Zir.Inst.SliceEnd, inst_data.payload_index).data;
    const array_ptr = try sema.resolve_inst(extra.lhs);
    const start = try sema.resolve_inst(extra.start);
    const end = try sema.resolve_inst(extra.end);
    const ptr_src: LazySrcLoc = .{ .node_offset_slice_ptr = inst_data.src_node };
    const start_src: LazySrcLoc = .{ .node_offset_slice_start = inst_data.src_node };
    const end_src: LazySrcLoc = .{ .node_offset_slice_end = inst_data.src_node };

    return sema.analyze_slice(block, src, array_ptr, start, end, .none, .unneeded, ptr_src, start_src, end_src, false);
}

fn zir_slice_sentinel(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const sentinel_src: LazySrcLoc = .{ .node_offset_slice_sentinel = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.SliceSentinel, inst_data.payload_index).data;
    const array_ptr = try sema.resolve_inst(extra.lhs);
    const start = try sema.resolve_inst(extra.start);
    const end: Air.Inst.Ref = if (extra.end == .none) .none else try sema.resolve_inst(extra.end);
    const sentinel = try sema.resolve_inst(extra.sentinel);
    const ptr_src: LazySrcLoc = .{ .node_offset_slice_ptr = inst_data.src_node };
    const start_src: LazySrcLoc = .{ .node_offset_slice_start = inst_data.src_node };
    const end_src: LazySrcLoc = .{ .node_offset_slice_end = inst_data.src_node };

    return sema.analyze_slice(block, src, array_ptr, start, end, sentinel, sentinel_src, ptr_src, start_src, end_src, false);
}

fn zir_slice_length(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const extra = sema.code.extra_data(Zir.Inst.SliceLength, inst_data.payload_index).data;
    const array_ptr = try sema.resolve_inst(extra.lhs);
    const start = try sema.resolve_inst(extra.start);
    const len = try sema.resolve_inst(extra.len);
    const sentinel = if (extra.sentinel == .none) .none else try sema.resolve_inst(extra.sentinel);
    const ptr_src: LazySrcLoc = .{ .node_offset_slice_ptr = inst_data.src_node };
    const start_src: LazySrcLoc = .{ .node_offset_slice_start = extra.start_src_node_offset };
    const end_src: LazySrcLoc = .{ .node_offset_slice_end = inst_data.src_node };
    const sentinel_src: LazySrcLoc = if (sentinel == .none)
        .unneeded
    else
        .{ .node_offset_slice_sentinel = inst_data.src_node };

    return sema.analyze_slice(block, src, array_ptr, start, len, sentinel, sentinel_src, ptr_src, start_src, end_src, true);
}

/// Holds common data used when analyzing or resolving switch prong bodies,
/// including setting up captures.
const SwitchProngAnalysis = struct {
    sema: *Sema,
    /// The block containing the `switch_block` itself.
    parent_block: *Block,
    /// The raw switch operand value (*not* the condition). Always defined.
    operand: Air.Inst.Ref,
    /// May be `undefined` if no prong has a by-ref capture.
    operand_ptr: Air.Inst.Ref,
    /// The switch condition value. For unions, `operand` is the union and `cond` is its tag.
    cond: Air.Inst.Ref,
    /// If this switch is on an error set, this is the type to assign to the
    /// `else` prong. If `null`, the prong should be unreachable.
    else_error_ty: ?Type,
    /// The index of the `switch_block` instruction itself.
    switch_block_inst: Zir.Inst.Index,
    /// The dummy index into which inline tag captures should be placed. May be
    /// undefined if no prong has a tag capture.
    tag_capture_inst: Zir.Inst.Index,

    /// Resolve a switch prong which is determined at comptime to have no peers.
    /// Uses `resolve_block_body`. Sets up captures as needed.
    fn resolve_prong_comptime(
        spa: SwitchProngAnalysis,
        child_block: *Block,
        prong_type: enum { normal, special },
        prong_body: []const Zir.Inst.Index,
        capture: Zir.Inst.SwitchBlock.ProngInfo.Capture,
        /// Must use the `scalar_capture`, `special_capture`, or `multi_capture` union field.
        raw_capture_src: Module.SwitchProngSrc,
        /// The set of all values which can reach this prong. May be undefined
        /// if the prong is special or contains ranges.
        case_vals: []const Air.Inst.Ref,
        /// The inline capture of this prong. If this is not an inline prong,
        /// this is `.none`.
        inline_case_capture: Air.Inst.Ref,
        /// Whether this prong has an inline tag capture. If `true`, then
        /// `inline_case_capture` cannot be `.none`.
        has_tag_capture: bool,
        merges: *Block.Merges,
    ) CompileError!Air.Inst.Ref {
        const sema = spa.sema;
        const src = sema.code.instructions.items(.data)[@int_from_enum(spa.switch_block_inst)].pl_node.src();

        if (has_tag_capture) {
            const tag_ref = try spa.analyze_tag_capture(child_block, raw_capture_src, inline_case_capture);
            sema.inst_map.put_assume_capacity(spa.tag_capture_inst, tag_ref);
        }
        defer if (has_tag_capture) assert(sema.inst_map.remove(spa.tag_capture_inst));

        switch (capture) {
            .none => {
                return sema.resolve_block_body(spa.parent_block, src, child_block, prong_body, spa.switch_block_inst, merges);
            },

            .by_val, .by_ref => {
                const capture_ref = try spa.analyze_capture(
                    child_block,
                    capture == .by_ref,
                    prong_type == .special,
                    raw_capture_src,
                    case_vals,
                    inline_case_capture,
                );

                if (sema.type_of(capture_ref).is_no_return(sema.mod)) {
                    // This prong should be unreachable!
                    return .unreachable_value;
                }

                sema.inst_map.put_assume_capacity(spa.switch_block_inst, capture_ref);
                defer assert(sema.inst_map.remove(spa.switch_block_inst));

                return sema.resolve_block_body(spa.parent_block, src, child_block, prong_body, spa.switch_block_inst, merges);
            },
        }
    }

    /// Analyze a switch prong which may have peers at runtime.
    /// Uses `analyze_body_runtime_break`. Sets up captures as needed.
    fn analyze_prong_runtime(
        spa: SwitchProngAnalysis,
        case_block: *Block,
        prong_type: enum { normal, special },
        prong_body: []const Zir.Inst.Index,
        capture: Zir.Inst.SwitchBlock.ProngInfo.Capture,
        /// Must use the `scalar`, `special`, or `multi_capture` union field.
        raw_capture_src: Module.SwitchProngSrc,
        /// The set of all values which can reach this prong. May be undefined
        /// if the prong is special or contains ranges.
        case_vals: []const Air.Inst.Ref,
        /// The inline capture of this prong. If this is not an inline prong,
        /// this is `.none`.
        inline_case_capture: Air.Inst.Ref,
        /// Whether this prong has an inline tag capture. If `true`, then
        /// `inline_case_capture` cannot be `.none`.
        has_tag_capture: bool,
    ) CompileError!void {
        const sema = spa.sema;

        if (has_tag_capture) {
            const tag_ref = try spa.analyze_tag_capture(case_block, raw_capture_src, inline_case_capture);
            sema.inst_map.put_assume_capacity(spa.tag_capture_inst, tag_ref);
        }
        defer if (has_tag_capture) assert(sema.inst_map.remove(spa.tag_capture_inst));

        switch (capture) {
            .none => {
                return sema.analyze_body_runtime_break(case_block, prong_body);
            },

            .by_val, .by_ref => {
                const capture_ref = try spa.analyze_capture(
                    case_block,
                    capture == .by_ref,
                    prong_type == .special,
                    raw_capture_src,
                    case_vals,
                    inline_case_capture,
                );

                if (sema.type_of(capture_ref).is_no_return(sema.mod)) {
                    // No need to analyze any further, the prong is unreachable
                    return;
                }

                sema.inst_map.put_assume_capacity(spa.switch_block_inst, capture_ref);
                defer assert(sema.inst_map.remove(spa.switch_block_inst));

                return sema.analyze_body_runtime_break(case_block, prong_body);
            },
        }
    }

    fn analyze_tag_capture(
        spa: SwitchProngAnalysis,
        block: *Block,
        raw_capture_src: Module.SwitchProngSrc,
        inline_case_capture: Air.Inst.Ref,
    ) CompileError!Air.Inst.Ref {
        const sema = spa.sema;
        const mod = sema.mod;
        const operand_ty = sema.type_of(spa.operand);
        if (operand_ty.zig_type_tag(mod) != .Union) {
            const zir_datas = sema.code.instructions.items(.data);
            const switch_node_offset = zir_datas[@int_from_enum(spa.switch_block_inst)].pl_node.src_node;
            const raw_tag_capture_src: Module.SwitchProngSrc = switch (raw_capture_src) {
                .scalar_capture => |i| .{ .scalar_tag_capture = i },
                .multi_capture => |i| .{ .multi_tag_capture = i },
                .special_capture => .special_tag_capture,
                else => unreachable,
            };
            const capture_src = raw_tag_capture_src.resolve(mod, mod.decl_ptr(block.src_decl), switch_node_offset, .none);
            return sema.fail(block, capture_src, "cannot capture tag of non-union type '{}'", .{
                operand_ty.fmt(mod),
            });
        }
        assert(inline_case_capture != .none);
        return inline_case_capture;
    }

    fn analyze_capture(
        spa: SwitchProngAnalysis,
        block: *Block,
        capture_byref: bool,
        is_special_prong: bool,
        raw_capture_src: Module.SwitchProngSrc,
        case_vals: []const Air.Inst.Ref,
        inline_case_capture: Air.Inst.Ref,
    ) CompileError!Air.Inst.Ref {
        const sema = spa.sema;
        const zcu = sema.mod;
        const ip = &zcu.intern_pool;

        const zir_datas = sema.code.instructions.items(.data);
        const switch_node_offset = zir_datas[@int_from_enum(spa.switch_block_inst)].pl_node.src_node;

        const operand_ty = sema.type_of(spa.operand);
        const operand_ptr_ty = if (capture_byref) sema.type_of(spa.operand_ptr) else undefined;
        const operand_src: LazySrcLoc = .{ .node_offset_switch_operand = switch_node_offset };

        if (inline_case_capture != .none) {
            const item_val = sema.resolve_const_defined_value(block, .unneeded, inline_case_capture, undefined) catch unreachable;
            if (operand_ty.zig_type_tag(zcu) == .Union) {
                const field_index: u32 = @int_cast(operand_ty.union_tag_field_index(item_val, zcu).?);
                const union_obj = zcu.type_to_union(operand_ty).?;
                const field_ty = Type.from_interned(union_obj.field_types.get(ip)[field_index]);
                if (capture_byref) {
                    const ptr_field_ty = try sema.ptr_type(.{
                        .child = field_ty.to_intern(),
                        .flags = .{
                            .is_const = !operand_ptr_ty.ptr_is_mutable(zcu),
                            .is_volatile = operand_ptr_ty.is_volatile_ptr(zcu),
                            .address_space = operand_ptr_ty.ptr_address_space(zcu),
                        },
                    });
                    if (try sema.resolve_defined_value(block, operand_src, spa.operand_ptr)) |union_ptr| {
                        return Air.interned_to_ref((try union_ptr.ptr_field(field_index, sema)).to_intern());
                    }
                    return block.add_struct_field_ptr(spa.operand_ptr, field_index, ptr_field_ty);
                } else {
                    if (try sema.resolve_defined_value(block, operand_src, spa.operand)) |union_val| {
                        const tag_and_val = ip.index_to_key(union_val.to_intern()).un;
                        return Air.interned_to_ref(tag_and_val.val);
                    }
                    return block.add_struct_field_val(spa.operand, field_index, field_ty);
                }
            } else if (capture_byref) {
                return anon_decl_ref(sema, item_val.to_intern());
            } else {
                return inline_case_capture;
            }
        }

        if (is_special_prong) {
            if (capture_byref) {
                return spa.operand_ptr;
            }

            switch (operand_ty.zig_type_tag(zcu)) {
                .ErrorSet => if (spa.else_error_ty) |ty| {
                    return sema.bit_cast(block, ty, spa.operand, operand_src, null);
                } else {
                    try block.add_unreachable(operand_src, false);
                    return .unreachable_value;
                },
                else => return spa.operand,
            }
        }

        switch (operand_ty.zig_type_tag(zcu)) {
            .Union => {
                const union_obj = zcu.type_to_union(operand_ty).?;
                const first_item_val = sema.resolve_const_defined_value(block, .unneeded, case_vals[0], undefined) catch unreachable;

                const first_field_index: u32 = zcu.union_tag_field_index(union_obj, first_item_val).?;
                const first_field_ty = Type.from_interned(union_obj.field_types.get(ip)[first_field_index]);

                const field_indices = try sema.arena.alloc(u32, case_vals.len);
                for (case_vals, field_indices) |item, *field_idx| {
                    const item_val = sema.resolve_const_defined_value(block, .unneeded, item, undefined) catch unreachable;
                    field_idx.* = zcu.union_tag_field_index(union_obj, item_val).?;
                }

                // Fast path: if all the operands are the same type already, we don't need to hit
                // PTR! This will also allow us to emit simpler code.
                const same_types = for (field_indices[1..]) |field_idx| {
                    const field_ty = Type.from_interned(union_obj.field_types.get(ip)[field_idx]);
                    if (!field_ty.eql(first_field_ty, zcu)) break false;
                } else true;

                const capture_ty = if (same_types) first_field_ty else capture_ty: {
                    // We need values to run PTR on, so make a bunch of undef constants.
                    const dummy_captures = try sema.arena.alloc(Air.Inst.Ref, case_vals.len);
                    for (dummy_captures, field_indices) |*dummy, field_idx| {
                        const field_ty = Type.from_interned(union_obj.field_types.get(ip)[field_idx]);
                        dummy.* = try zcu.undef_ref(field_ty);
                    }

                    const case_srcs = try sema.arena.alloc(?LazySrcLoc, case_vals.len);
                    @memset(case_srcs, .unneeded);

                    break :capture_ty sema.resolve_peer_types(block, .unneeded, dummy_captures, .{ .override = case_srcs }) catch |err| switch (err) {
                        error.NeededSourceLocation => {
                            // This must be a multi-prong so this must be a `multi_capture` src
                            const multi_idx = raw_capture_src.multi_capture;
                            const src_decl_ptr = zcu.decl_ptr(block.src_decl);
                            for (case_srcs, 0..) |*case_src, i| {
                                const raw_case_src: Module.SwitchProngSrc = .{ .multi = .{ .prong = multi_idx, .item = @int_cast(i) } };
                                case_src.* = raw_case_src.resolve(zcu, src_decl_ptr, switch_node_offset, .none);
                            }
                            const capture_src = raw_capture_src.resolve(zcu, src_decl_ptr, switch_node_offset, .none);
                            _ = sema.resolve_peer_types(block, capture_src, dummy_captures, .{ .override = case_srcs }) catch |err1| switch (err1) {
                                error.AnalysisFail => {
                                    const msg = sema.err orelse return error.AnalysisFail;
                                    try sema.reparent_owned_error_msg(block, capture_src, msg, "capture group with incompatible types", .{});
                                    return error.AnalysisFail;
                                },
                                else => |e| return e,
                            };
                            unreachable;
                        },
                        else => |e| return e,
                    };
                };

                // By-reference captures have some further restrictions which make them easier to emit
                if (capture_byref) {
                    const operand_ptr_info = operand_ptr_ty.ptr_info(zcu);
                    const capture_ptr_ty = resolve: {
                        // By-ref captures of hetereogeneous types are only allowed if all field
                        // pointer types are peer resolvable to each other.
                        // We need values to run PTR on, so make a bunch of undef constants.
                        const dummy_captures = try sema.arena.alloc(Air.Inst.Ref, case_vals.len);
                        for (field_indices, dummy_captures) |field_idx, *dummy| {
                            const field_ty = Type.from_interned(union_obj.field_types.get(ip)[field_idx]);
                            const field_ptr_ty = try sema.ptr_type(.{
                                .child = field_ty.to_intern(),
                                .flags = .{
                                    .is_const = operand_ptr_info.flags.is_const,
                                    .is_volatile = operand_ptr_info.flags.is_volatile,
                                    .address_space = operand_ptr_info.flags.address_space,
                                    .alignment = union_obj.field_align(ip, field_idx),
                                },
                            });
                            dummy.* = try zcu.undef_ref(field_ptr_ty);
                        }
                        const case_srcs = try sema.arena.alloc(?LazySrcLoc, case_vals.len);
                        @memset(case_srcs, .unneeded);

                        break :resolve sema.resolve_peer_types(block, .unneeded, dummy_captures, .{ .override = case_srcs }) catch |err| switch (err) {
                            error.NeededSourceLocation => {
                                // This must be a multi-prong so this must be a `multi_capture` src
                                const multi_idx = raw_capture_src.multi_capture;
                                const src_decl_ptr = zcu.decl_ptr(block.src_decl);
                                for (case_srcs, 0..) |*case_src, i| {
                                    const raw_case_src: Module.SwitchProngSrc = .{ .multi = .{ .prong = multi_idx, .item = @int_cast(i) } };
                                    case_src.* = raw_case_src.resolve(zcu, src_decl_ptr, switch_node_offset, .none);
                                }
                                const capture_src = raw_capture_src.resolve(zcu, src_decl_ptr, switch_node_offset, .none);
                                _ = sema.resolve_peer_types(block, capture_src, dummy_captures, .{ .override = case_srcs }) catch |err1| switch (err1) {
                                    error.AnalysisFail => {
                                        const msg = sema.err orelse return error.AnalysisFail;
                                        try sema.err_note(block, capture_src, msg, "this coercion is only possible when capturing by value", .{});
                                        try sema.reparent_owned_error_msg(block, capture_src, msg, "capture group with incompatible types", .{});
                                        return error.AnalysisFail;
                                    },
                                    else => |e| return e,
                                };
                                unreachable;
                            },
                            else => |e| return e,
                        };
                    };

                    if (try sema.resolve_defined_value(block, operand_src, spa.operand_ptr)) |op_ptr_val| {
                        if (op_ptr_val.is_undef(zcu)) return zcu.undef_ref(capture_ptr_ty);
                        const field_ptr_val = try op_ptr_val.ptr_field(first_field_index, sema);
                        return Air.interned_to_ref((try zcu.get_coerced(field_ptr_val, capture_ptr_ty)).to_intern());
                    }

                    try sema.require_runtime_block(block, operand_src, null);
                    return block.add_struct_field_ptr(spa.operand_ptr, first_field_index, capture_ptr_ty);
                }

                if (try sema.resolve_defined_value(block, operand_src, spa.operand)) |operand_val| {
                    if (operand_val.is_undef(zcu)) return zcu.undef_ref(capture_ty);
                    const union_val = ip.index_to_key(operand_val.to_intern()).un;
                    if (Value.from_interned(union_val.tag).is_undef(zcu)) return zcu.undef_ref(capture_ty);
                    const uncoerced = Air.interned_to_ref(union_val.val);
                    return sema.coerce(block, capture_ty, uncoerced, operand_src);
                }

                try sema.require_runtime_block(block, operand_src, null);

                if (same_types) {
                    return block.add_struct_field_val(spa.operand, first_field_index, capture_ty);
                }

                // We may have to emit a switch block which coerces the operand to the capture type.
                // If we can, try to avoid that using in-memory coercions.
                const first_non_imc = in_mem: {
                    for (field_indices, 0..) |field_idx, i| {
                        const field_ty = Type.from_interned(union_obj.field_types.get(ip)[field_idx]);
                        if (.ok != try sema.coerce_in_memory_allowed(block, capture_ty, field_ty, false, zcu.get_target(), .unneeded, .unneeded)) {
                            break :in_mem i;
                        }
                    }
                    // All fields are in-memory coercible to the resolved type!
                    // Just take the first field and bitcast the result.
                    const uncoerced = try block.add_struct_field_val(spa.operand, first_field_index, first_field_ty);
                    return block.add_bit_cast(capture_ty, uncoerced);
                };

                // By-val capture with heterogeneous types which are not all in-memory coercible to
                // the resolved capture type. We finally have to fall back to the ugly method.

                // However, let's first track which operands are in-memory coercible. There may well
                // be several, and we can squash all of these cases into the same switch prong using
                // a simple bitcast. We'll make this the 'else' prong.

                var in_mem_coercible = try std.DynamicBitSet.init_full(sema.arena, field_indices.len);
                in_mem_coercible.unset(first_non_imc);
                {
                    const next = first_non_imc + 1;
                    for (field_indices[next..], next..) |field_idx, i| {
                        const field_ty = Type.from_interned(union_obj.field_types.get(ip)[field_idx]);
                        if (.ok != try sema.coerce_in_memory_allowed(block, capture_ty, field_ty, false, zcu.get_target(), .unneeded, .unneeded)) {
                            in_mem_coercible.unset(i);
                        }
                    }
                }

                const capture_block_inst = try block.add_inst_as_index(.{
                    .tag = .block,
                    .data = .{
                        .ty_pl = .{
                            .ty = Air.interned_to_ref(capture_ty.to_intern()),
                            .payload = undefined, // updated below
                        },
                    },
                });

                const prong_count = field_indices.len - in_mem_coercible.count();

                const estimated_extra = prong_count * 6; // 2 for Case, 1 item, probably 3 insts
                var cases_extra = try std.ArrayList(u32).init_capacity(sema.gpa, estimated_extra);
                defer cases_extra.deinit();

                {
                    // Non-bitcast cases
                    var it = in_mem_coercible.iterator(.{ .kind = .unset });
                    while (it.next()) |idx| {
                        var coerce_block = block.make_sub_block();
                        defer coerce_block.instructions.deinit(sema.gpa);

                        const field_idx = field_indices[idx];
                        const field_ty = Type.from_interned(union_obj.field_types.get(ip)[field_idx]);
                        const uncoerced = try coerce_block.add_struct_field_val(spa.operand, field_idx, field_ty);
                        const coerced = sema.coerce(&coerce_block, capture_ty, uncoerced, .unneeded) catch |err| switch (err) {
                            error.NeededSourceLocation => {
                                const multi_idx = raw_capture_src.multi_capture;
                                const src_decl_ptr = zcu.decl_ptr(block.src_decl);
                                const raw_case_src: Module.SwitchProngSrc = .{ .multi = .{ .prong = multi_idx, .item = @int_cast(idx) } };
                                const case_src = raw_case_src.resolve(zcu, src_decl_ptr, switch_node_offset, .none);
                                _ = try sema.coerce(&coerce_block, capture_ty, uncoerced, case_src);
                                unreachable;
                            },
                            else => |e| return e,
                        };
                        _ = try coerce_block.add_br(capture_block_inst, coerced);

                        try cases_extra.ensure_unused_capacity(3 + coerce_block.instructions.items.len);
                        cases_extra.append_assume_capacity(1); // items_len
                        cases_extra.append_assume_capacity(@int_cast(coerce_block.instructions.items.len)); // body_len
                        cases_extra.append_assume_capacity(@int_from_enum(case_vals[idx])); // item
                        cases_extra.append_slice_assume_capacity(@ptr_cast(coerce_block.instructions.items)); // body
                    }
                }
                const else_body_len = len: {
                    // 'else' prong uses a bitcast
                    var coerce_block = block.make_sub_block();
                    defer coerce_block.instructions.deinit(sema.gpa);

                    const first_imc_item_idx = in_mem_coercible.find_first_set().?;
                    const first_imc_field_idx = field_indices[first_imc_item_idx];
                    const first_imc_field_ty = Type.from_interned(union_obj.field_types.get(ip)[first_imc_field_idx]);
                    const uncoerced = try coerce_block.add_struct_field_val(spa.operand, first_imc_field_idx, first_imc_field_ty);
                    const coerced = try coerce_block.add_bit_cast(capture_ty, uncoerced);
                    _ = try coerce_block.add_br(capture_block_inst, coerced);

                    try cases_extra.append_slice(@ptr_cast(coerce_block.instructions.items));
                    break :len coerce_block.instructions.items.len;
                };

                try sema.air_extra.ensure_unused_capacity(sema.gpa, @typeInfo(Air.SwitchBr).Struct.fields.len +
                    cases_extra.items.len +
                    @typeInfo(Air.Block).Struct.fields.len +
                    1);

                const switch_br_inst: u32 = @int_cast(sema.air_instructions.len);
                try sema.air_instructions.append(sema.gpa, .{
                    .tag = .switch_br,
                    .data = .{ .pl_op = .{
                        .operand = spa.cond,
                        .payload = sema.add_extra_assume_capacity(Air.SwitchBr{
                            .cases_len = @int_cast(prong_count),
                            .else_body_len = @int_cast(else_body_len),
                        }),
                    } },
                });
                sema.air_extra.append_slice_assume_capacity(cases_extra.items);

                // Set up block body
                sema.air_instructions.items(.data)[@int_from_enum(capture_block_inst)].ty_pl.payload = sema.add_extra_assume_capacity(Air.Block{
                    .body_len = 1,
                });
                sema.air_extra.append_assume_capacity(switch_br_inst);

                return capture_block_inst.to_ref();
            },
            .ErrorSet => {
                if (capture_byref) {
                    const capture_src = raw_capture_src.resolve(zcu, zcu.decl_ptr(block.src_decl), switch_node_offset, .none);
                    return sema.fail(
                        block,
                        capture_src,
                        "error set cannot be captured by reference",
                        .{},
                    );
                }

                if (case_vals.len == 1) {
                    const item_val = sema.resolve_const_defined_value(block, .unneeded, case_vals[0], undefined) catch unreachable;
                    const item_ty = try zcu.single_error_set_type(item_val.get_error_name(zcu).unwrap().?);
                    return sema.bit_cast(block, item_ty, spa.operand, operand_src, null);
                }

                var names: InferredErrorSet.NameMap = .{};
                try names.ensure_unused_capacity(sema.arena, case_vals.len);
                for (case_vals) |err| {
                    const err_val = sema.resolve_const_defined_value(block, .unneeded, err, undefined) catch unreachable;
                    names.put_assume_capacity_no_clobber(err_val.get_error_name(zcu).unwrap().?, {});
                }
                const error_ty = try zcu.error_set_from_unsorted_names(names.keys());
                return sema.bit_cast(block, error_ty, spa.operand, operand_src, null);
            },
            else => {
                // In this case the capture value is just the passed-through value
                // of the switch condition.
                if (capture_byref) {
                    return spa.operand_ptr;
                } else {
                    return spa.operand;
                }
            },
        }
    }
};

fn switch_cond(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    operand: Air.Inst.Ref,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const operand_ty = sema.type_of(operand);
    switch (operand_ty.zig_type_tag(mod)) {
        .Type,
        .Void,
        .Bool,
        .Int,
        .Float,
        .ComptimeFloat,
        .ComptimeInt,
        .EnumLiteral,
        .Pointer,
        .Fn,
        .ErrorSet,
        .Enum,
        => {
            if (operand_ty.is_slice(mod)) {
                return sema.fail(block, src, "switch on type '{}'", .{operand_ty.fmt(mod)});
            }
            if ((try sema.type_has_one_possible_value(operand_ty))) |opv| {
                return Air.interned_to_ref(opv.to_intern());
            }
            return operand;
        },

        .Union => {
            try sema.resolve_type_fields(operand_ty);
            const enum_ty = operand_ty.union_tag_type(mod) orelse {
                const msg = msg: {
                    const msg = try sema.err_msg(block, src, "switch on union with no attached enum", .{});
                    errdefer msg.destroy(sema.gpa);
                    if (operand_ty.decl_src_loc_or_null(mod)) |union_src| {
                        try mod.err_note_non_lazy(union_src, msg, "consider 'union(enum)' here", .{});
                    }
                    break :msg msg;
                };
                return sema.fail_with_owned_error_msg(block, msg);
            };
            return sema.union_to_tag(block, enum_ty, operand, src);
        },

        .ErrorUnion,
        .NoReturn,
        .Array,
        .Struct,
        .Undefined,
        .Null,
        .Optional,
        .Opaque,
        .Vector,
        .Frame,
        .AnyFrame,
        => return sema.fail(block, src, "switch on type '{}'", .{operand_ty.fmt(mod)}),
    }
}

const SwitchErrorSet = std.AutoHashMap(InternPool.NullTerminatedString, Module.SwitchProngSrc);

fn zir_switch_block_err_union(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const gpa = sema.gpa;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const switch_src = inst_data.src();
    const switch_src_node_offset = inst_data.src_node;
    const switch_operand_src: LazySrcLoc = .{ .node_offset_switch_operand = switch_src_node_offset };
    const else_prong_src: LazySrcLoc = .{ .node_offset_switch_special_prong = switch_src_node_offset };
    const extra = sema.code.extra_data(Zir.Inst.SwitchBlockErrUnion, inst_data.payload_index);
    const main_operand_src: LazySrcLoc = .{ .node_offset_if_cond = extra.data.main_src_node_offset };
    const main_src: LazySrcLoc = .{ .node_offset_main_token = extra.data.main_src_node_offset };

    const raw_operand_val = try sema.resolve_inst(extra.data.operand);

    // AstGen guarantees that the instruction immediately preceding
    // switch_block_err_union is a dbg_stmt
    const cond_dbg_node_index: Zir.Inst.Index = @enumFromInt(@int_from_enum(inst) - 1);

    var header_extra_index: usize = extra.end;

    const scalar_cases_len = extra.data.bits.scalar_cases_len;
    const multi_cases_len = if (extra.data.bits.has_multi_cases) blk: {
        const multi_cases_len = sema.code.extra[header_extra_index];
        header_extra_index += 1;
        break :blk multi_cases_len;
    } else 0;

    const err_capture_inst: Zir.Inst.Index = if (extra.data.bits.any_uses_err_capture) blk: {
        const err_capture_inst: Zir.Inst.Index = @enumFromInt(sema.code.extra[header_extra_index]);
        header_extra_index += 1;
        // SwitchProngAnalysis wants inst_map to have space for the tag capture.
        // Note that the normal capture is referred to via the switch block
        // index, which there is already necessarily space for.
        try sema.inst_map.ensure_space_for_instructions(gpa, &.{err_capture_inst});
        break :blk err_capture_inst;
    } else undefined;

    var case_vals = try std.ArrayListUnmanaged(Air.Inst.Ref).init_capacity(gpa, scalar_cases_len + 2 * multi_cases_len);
    defer case_vals.deinit(gpa);

    const NonError = struct {
        body: []const Zir.Inst.Index,
        end: usize,
        capture: Zir.Inst.SwitchBlock.ProngInfo.Capture,
    };

    const non_error_case: NonError = non_error: {
        const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[header_extra_index]);
        const extra_body_start = header_extra_index + 1;
        break :non_error .{
            .body = sema.code.body_slice(extra_body_start, info.body_len),
            .end = extra_body_start + info.body_len,
            .capture = info.capture,
        };
    };

    const Else = struct {
        body: []const Zir.Inst.Index,
        end: usize,
        is_inline: bool,
        has_capture: bool,
    };

    const else_case: Else = if (!extra.data.bits.has_else) .{
        .body = &.{},
        .end = non_error_case.end,
        .is_inline = false,
        .has_capture = false,
    } else special: {
        const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[non_error_case.end]);
        const extra_body_start = non_error_case.end + 1;
        assert(info.capture != .by_ref);
        assert(!info.has_tag_capture);
        break :special .{
            .body = sema.code.body_slice(extra_body_start, info.body_len),
            .end = extra_body_start + info.body_len,
            .is_inline = info.is_inline,
            .has_capture = info.capture != .none,
        };
    };

    var seen_errors = SwitchErrorSet.init(gpa);
    defer seen_errors.deinit();

    const operand_ty = sema.type_of(raw_operand_val);
    const operand_err_set = if (extra.data.bits.payload_is_ref)
        operand_ty.child_type(mod)
    else
        operand_ty;

    if (operand_err_set.zig_type_tag(mod) != .ErrorUnion) {
        return sema.fail(block, switch_src, "expected error union type, found '{}'", .{
            operand_ty.fmt(mod),
        });
    }

    const operand_err_set_ty = operand_err_set.error_union_set(mod);

    const block_inst: Air.Inst.Index = @enumFromInt(sema.air_instructions.len);
    try sema.air_instructions.append(gpa, .{
        .tag = .block,
        .data = undefined,
    });
    var label: Block.Label = .{
        .zir_block = inst,
        .merges = .{
            .src_locs = .{},
            .results = .{},
            .br_list = .{},
            .block_inst = block_inst,
        },
    };

    var child_block: Block = .{
        .parent = block,
        .sema = sema,
        .src_decl = block.src_decl,
        .namespace = block.namespace,
        .instructions = .{},
        .label = &label,
        .inlining = block.inlining,
        .is_comptime = block.is_comptime,
        .comptime_reason = block.comptime_reason,
        .is_typeof = block.is_typeof,
        .c_import_buf = block.c_import_buf,
        .runtime_cond = block.runtime_cond,
        .runtime_loop = block.runtime_loop,
        .runtime_index = block.runtime_index,
        .error_return_trace_index = block.error_return_trace_index,
        .want_safety = block.want_safety,
    };
    const merges = &child_block.label.?.merges;
    defer child_block.instructions.deinit(gpa);
    defer merges.deinit(gpa);

    const resolved_err_set = try sema.resolve_inferred_error_set_ty(block, main_src, operand_err_set_ty.to_intern());
    if (Type.from_interned(resolved_err_set).error_set_is_empty(mod)) {
        return sema.resolve_block_body(block, main_operand_src, &child_block, non_error_case.body, inst, merges);
    }

    const else_error_ty: ?Type = try validate_err_set_switch(
        sema,
        block,
        &seen_errors,
        &case_vals,
        operand_err_set_ty,
        inst_data,
        scalar_cases_len,
        multi_cases_len,
        .{ .body = else_case.body, .end = else_case.end, .src = else_prong_src },
        extra.data.bits.has_else,
    );

    var spa: SwitchProngAnalysis = .{
        .sema = sema,
        .parent_block = block,
        .operand = undefined, // must be set to the unwrapped error code before use
        .operand_ptr = .none,
        .cond = raw_operand_val,
        .else_error_ty = else_error_ty,
        .switch_block_inst = inst,
        .tag_capture_inst = undefined,
    };

    if (try sema.resolve_defined_value(&child_block, main_src, raw_operand_val)) |ov| {
        const operand_val = if (extra.data.bits.payload_is_ref)
            (try sema.pointer_deref(&child_block, main_src, ov, operand_ty)).?
        else
            ov;

        if (operand_val.error_union_is_payload(mod)) {
            return sema.resolve_block_body(block, main_operand_src, &child_block, non_error_case.body, inst, merges);
        } else {
            const err_val = Value.from_interned(try mod.intern(.{
                .err = .{
                    .ty = operand_err_set_ty.to_intern(),
                    .name = operand_val.get_error_name(mod).unwrap().?,
                },
            }));
            spa.operand = if (extra.data.bits.payload_is_ref)
                try sema.analyze_err_union_code_ptr(block, switch_operand_src, raw_operand_val)
            else
                try sema.analyze_err_union_code(block, switch_operand_src, raw_operand_val);

            if (extra.data.bits.any_uses_err_capture) {
                sema.inst_map.put_assume_capacity(err_capture_inst, spa.operand);
            }
            defer if (extra.data.bits.any_uses_err_capture) assert(sema.inst_map.remove(err_capture_inst));

            return resolve_switch_comptime(
                sema,
                spa,
                &child_block,
                try sema.switch_cond(block, switch_operand_src, spa.operand),
                err_val,
                operand_err_set_ty,
                .{
                    .body = else_case.body,
                    .end = else_case.end,
                    .capture = if (else_case.has_capture) .by_val else .none,
                    .is_inline = else_case.is_inline,
                    .has_tag_capture = false,
                },
                case_vals,
                scalar_cases_len,
                multi_cases_len,
                true,
                false,
            );
        }
    }

    if (scalar_cases_len + multi_cases_len == 0) {
        if (else_error_ty) |ty| if (ty.error_set_is_empty(mod)) {
            return sema.resolve_block_body(block, main_operand_src, &child_block, non_error_case.body, inst, merges);
        };
    }

    if (child_block.is_comptime) {
        _ = try sema.resolve_const_defined_value(&child_block, main_operand_src, raw_operand_val, .{
            .needed_comptime_reason = "condition in comptime switch must be comptime-known",
            .block_comptime_reason = child_block.comptime_reason,
        });
        unreachable;
    }

    const cond = if (extra.data.bits.payload_is_ref) blk: {
        try sema.check_error_type(block, main_src, sema.type_of(raw_operand_val).elem_type2(mod));
        const loaded = try sema.analyze_load(block, main_src, raw_operand_val, main_src);
        break :blk try sema.analyze_is_non_err(block, main_src, loaded);
    } else blk: {
        try sema.check_error_type(block, main_src, sema.type_of(raw_operand_val));
        break :blk try sema.analyze_is_non_err(block, main_src, raw_operand_val);
    };

    var sub_block = child_block.make_sub_block();
    sub_block.runtime_loop = null;
    sub_block.runtime_cond = mod.decl_ptr(child_block.src_decl).to_src_loc(main_operand_src, mod);
    sub_block.runtime_index.increment();
    sub_block.need_debug_scope = null; // this body is emitted regardless
    defer sub_block.instructions.deinit(gpa);

    try sema.analyze_body_runtime_break(&sub_block, non_error_case.body);
    const true_instructions = try sub_block.instructions.to_owned_slice(gpa);
    defer gpa.free(true_instructions);

    spa.operand = if (extra.data.bits.payload_is_ref)
        try sema.analyze_err_union_code_ptr(&sub_block, switch_operand_src, raw_operand_val)
    else
        try sema.analyze_err_union_code(&sub_block, switch_operand_src, raw_operand_val);

    if (extra.data.bits.any_uses_err_capture) {
        sema.inst_map.put_assume_capacity(err_capture_inst, spa.operand);
    }
    defer if (extra.data.bits.any_uses_err_capture) assert(sema.inst_map.remove(err_capture_inst));
    _ = try sema.analyze_switch_runtime_block(
        spa,
        &sub_block,
        switch_src,
        try sema.switch_cond(block, switch_operand_src, spa.operand),
        operand_err_set_ty,
        switch_operand_src,
        case_vals,
        .{
            .body = else_case.body,
            .end = else_case.end,
            .capture = if (else_case.has_capture) .by_val else .none,
            .is_inline = else_case.is_inline,
            .has_tag_capture = false,
        },
        scalar_cases_len,
        multi_cases_len,
        false,
        undefined,
        true,
        switch_src_node_offset,
        else_prong_src,
        undefined,
        seen_errors,
        undefined,
        undefined,
        undefined,
        cond_dbg_node_index,
        true,
    );

    try sema.air_extra.ensure_unused_capacity(gpa, @typeInfo(Air.CondBr).Struct.fields.len +
        true_instructions.len + sub_block.instructions.items.len);

    _ = try child_block.add_inst(.{
        .tag = .cond_br,
        .data = .{ .pl_op = .{
            .operand = cond,
            .payload = sema.add_extra_assume_capacity(Air.CondBr{
                .then_body_len = @int_cast(true_instructions.len),
                .else_body_len = @int_cast(sub_block.instructions.items.len),
            }),
        } },
    });
    sema.air_extra.append_slice_assume_capacity(@ptr_cast(true_instructions));
    sema.air_extra.append_slice_assume_capacity(@ptr_cast(sub_block.instructions.items));

    return sema.resolve_analyzed_block(block, main_src, &child_block, merges, false);
}

fn zir_switch_block(sema: *Sema, block: *Block, inst: Zir.Inst.Index, operand_is_ref: bool) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const gpa = sema.gpa;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const src_node_offset = inst_data.src_node;
    const operand_src: LazySrcLoc = .{ .node_offset_switch_operand = src_node_offset };
    const special_prong_src: LazySrcLoc = .{ .node_offset_switch_special_prong = src_node_offset };
    const extra = sema.code.extra_data(Zir.Inst.SwitchBlock, inst_data.payload_index);

    const raw_operand_val: Air.Inst.Ref, const raw_operand_ptr: Air.Inst.Ref = blk: {
        const maybe_ptr = try sema.resolve_inst(extra.data.operand);
        if (operand_is_ref) {
            const val = try sema.analyze_load(block, src, maybe_ptr, operand_src);
            break :blk .{ val, maybe_ptr };
        } else {
            break :blk .{ maybe_ptr, undefined };
        }
    };

    const operand = try sema.switch_cond(block, operand_src, raw_operand_val);

    // AstGen guarantees that the instruction immediately preceding
    // switch_block(_ref) is a dbg_stmt
    const cond_dbg_node_index: Zir.Inst.Index = @enumFromInt(@int_from_enum(inst) - 1);

    var header_extra_index: usize = extra.end;

    const scalar_cases_len = extra.data.bits.scalar_cases_len;
    const multi_cases_len = if (extra.data.bits.has_multi_cases) blk: {
        const multi_cases_len = sema.code.extra[header_extra_index];
        header_extra_index += 1;
        break :blk multi_cases_len;
    } else 0;

    const tag_capture_inst: Zir.Inst.Index = if (extra.data.bits.any_has_tag_capture) blk: {
        const tag_capture_inst: Zir.Inst.Index = @enumFromInt(sema.code.extra[header_extra_index]);
        header_extra_index += 1;
        // SwitchProngAnalysis wants inst_map to have space for the tag capture.
        // Note that the normal capture is referred to via the switch block
        // index, which there is already necessarily space for.
        try sema.inst_map.ensure_space_for_instructions(gpa, &.{tag_capture_inst});
        break :blk tag_capture_inst;
    } else undefined;

    var case_vals = try std.ArrayListUnmanaged(Air.Inst.Ref).init_capacity(gpa, scalar_cases_len + 2 * multi_cases_len);
    defer case_vals.deinit(gpa);

    const special_prong = extra.data.bits.special_prong();
    const special: SpecialProng = switch (special_prong) {
        .none => .{
            .body = &.{},
            .end = header_extra_index,
            .capture = .none,
            .is_inline = false,
            .has_tag_capture = false,
        },
        .under, .@"else" => blk: {
            const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[header_extra_index]);
            const extra_body_start = header_extra_index + 1;
            break :blk .{
                .body = sema.code.body_slice(extra_body_start, info.body_len),
                .end = extra_body_start + info.body_len,
                .capture = info.capture,
                .is_inline = info.is_inline,
                .has_tag_capture = info.has_tag_capture,
            };
        },
    };

    const maybe_union_ty = sema.type_of(raw_operand_val);
    const union_originally = maybe_union_ty.zig_type_tag(mod) == .Union;

    // Duplicate checking variables later also used for `inline else`.
    var seen_enum_fields: []?Module.SwitchProngSrc = &.{};
    var seen_errors = SwitchErrorSet.init(gpa);
    var range_set = RangeSet.init(gpa, mod);
    var true_count: u8 = 0;
    var false_count: u8 = 0;

    defer {
        range_set.deinit();
        gpa.free(seen_enum_fields);
        seen_errors.deinit();
    }

    var empty_enum = false;

    const operand_ty = sema.type_of(operand);
    const err_set = operand_ty.zig_type_tag(mod) == .ErrorSet;

    var else_error_ty: ?Type = null;

    // Validate usage of '_' prongs.
    if (special_prong == .under and (!operand_ty.is_nonexhaustive_enum(mod) or union_originally)) {
        const msg = msg: {
            const msg = try sema.err_msg(
                block,
                src,
                "'_' prong only allowed when switching on non-exhaustive enums",
                .{},
            );
            errdefer msg.destroy(gpa);
            try sema.err_note(
                block,
                special_prong_src,
                msg,
                "'_' prong here",
                .{},
            );
            try sema.err_note(
                block,
                src,
                msg,
                "consider using 'else'",
                .{},
            );
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    // Validate for duplicate items, missing else prong, and invalid range.
    switch (operand_ty.zig_type_tag(mod)) {
        .Union => unreachable, // handled in `switch_cond`
        .Enum => {
            seen_enum_fields = try gpa.alloc(?Module.SwitchProngSrc, operand_ty.enum_field_count(mod));
            empty_enum = seen_enum_fields.len == 0 and !operand_ty.is_nonexhaustive_enum(mod);
            @memset(seen_enum_fields, null);
            // `range_set` is used for non-exhaustive enum values that do not correspond to any tags.

            var extra_index: usize = special.end;
            {
                var scalar_i: u32 = 0;
                while (scalar_i < scalar_cases_len) : (scalar_i += 1) {
                    const item_ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
                    extra_index += 1;
                    const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[extra_index]);
                    extra_index += 1 + info.body_len;

                    case_vals.append_assume_capacity(try sema.validate_switch_item_enum(
                        block,
                        seen_enum_fields,
                        &range_set,
                        item_ref,
                        operand_ty,
                        src_node_offset,
                        .{ .scalar = scalar_i },
                    ));
                }
            }
            {
                var multi_i: u32 = 0;
                while (multi_i < multi_cases_len) : (multi_i += 1) {
                    const items_len = sema.code.extra[extra_index];
                    extra_index += 1;
                    const ranges_len = sema.code.extra[extra_index];
                    extra_index += 1;
                    const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[extra_index]);
                    extra_index += 1;
                    const items = sema.code.ref_slice(extra_index, items_len);
                    extra_index += items_len + info.body_len;

                    try case_vals.ensure_unused_capacity(gpa, items.len);
                    for (items, 0..) |item_ref, item_i| {
                        case_vals.append_assume_capacity(try sema.validate_switch_item_enum(
                            block,
                            seen_enum_fields,
                            &range_set,
                            item_ref,
                            operand_ty,
                            src_node_offset,
                            .{ .multi = .{ .prong = multi_i, .item = @int_cast(item_i) } },
                        ));
                    }

                    try sema.validate_switch_no_range(block, ranges_len, operand_ty, src_node_offset);
                }
            }
            const all_tags_handled = for (seen_enum_fields) |seen_src| {
                if (seen_src == null) break false;
            } else true;

            if (special_prong == .@"else") {
                if (all_tags_handled and !operand_ty.is_nonexhaustive_enum(mod)) return sema.fail(
                    block,
                    special_prong_src,
                    "unreachable else prong; all cases already handled",
                    .{},
                );
            } else if (!all_tags_handled) {
                const msg = msg: {
                    const msg = try sema.err_msg(
                        block,
                        src,
                        "switch must handle all possibilities",
                        .{},
                    );
                    errdefer msg.destroy(sema.gpa);
                    for (seen_enum_fields, 0..) |seen_src, i| {
                        if (seen_src != null) continue;

                        const field_name = operand_ty.enum_field_name(i, mod);
                        try sema.add_field_err_note(
                            operand_ty,
                            i,
                            msg,
                            "unhandled enumeration value: '{}'",
                            .{field_name.fmt(&mod.intern_pool)},
                        );
                    }
                    try mod.err_note_non_lazy(
                        operand_ty.decl_src_loc(mod),
                        msg,
                        "enum '{}' declared here",
                        .{operand_ty.fmt(mod)},
                    );
                    break :msg msg;
                };
                return sema.fail_with_owned_error_msg(block, msg);
            } else if (special_prong == .none and operand_ty.is_nonexhaustive_enum(mod) and !union_originally) {
                return sema.fail(
                    block,
                    src,
                    "switch on non-exhaustive enum must include 'else' or '_' prong",
                    .{},
                );
            }
        },
        .ErrorSet => else_error_ty = try validate_err_set_switch(
            sema,
            block,
            &seen_errors,
            &case_vals,
            operand_ty,
            inst_data,
            scalar_cases_len,
            multi_cases_len,
            .{ .body = special.body, .end = special.end, .src = special_prong_src },
            special_prong == .@"else",
        ),
        .Int, .ComptimeInt => {
            var extra_index: usize = special.end;
            {
                var scalar_i: u32 = 0;
                while (scalar_i < scalar_cases_len) : (scalar_i += 1) {
                    const item_ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
                    extra_index += 1;
                    const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[extra_index]);
                    extra_index += 1 + info.body_len;

                    case_vals.append_assume_capacity(try sema.validate_switch_item_int(
                        block,
                        &range_set,
                        item_ref,
                        operand_ty,
                        src_node_offset,
                        .{ .scalar = scalar_i },
                    ));
                }
            }
            {
                var multi_i: u32 = 0;
                while (multi_i < multi_cases_len) : (multi_i += 1) {
                    const items_len = sema.code.extra[extra_index];
                    extra_index += 1;
                    const ranges_len = sema.code.extra[extra_index];
                    extra_index += 1;
                    const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[extra_index]);
                    extra_index += 1;
                    const items = sema.code.ref_slice(extra_index, items_len);
                    extra_index += items_len;

                    try case_vals.ensure_unused_capacity(gpa, items.len);
                    for (items, 0..) |item_ref, item_i| {
                        case_vals.append_assume_capacity(try sema.validate_switch_item_int(
                            block,
                            &range_set,
                            item_ref,
                            operand_ty,
                            src_node_offset,
                            .{ .multi = .{ .prong = multi_i, .item = @int_cast(item_i) } },
                        ));
                    }

                    try case_vals.ensure_unused_capacity(gpa, 2 * ranges_len);
                    var range_i: u32 = 0;
                    while (range_i < ranges_len) : (range_i += 1) {
                        const item_first: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
                        extra_index += 1;
                        const item_last: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
                        extra_index += 1;

                        const vals = try sema.validate_switch_range(
                            block,
                            &range_set,
                            item_first,
                            item_last,
                            operand_ty,
                            src_node_offset,
                            .{ .range = .{ .prong = multi_i, .item = range_i } },
                        );
                        case_vals.append_assume_capacity(vals[0]);
                        case_vals.append_assume_capacity(vals[1]);
                    }

                    extra_index += info.body_len;
                }
            }

            check_range: {
                if (operand_ty.zig_type_tag(mod) == .Int) {
                    const min_int = try operand_ty.min_int(mod, operand_ty);
                    const max_int = try operand_ty.max_int(mod, operand_ty);
                    if (try range_set.spans(min_int.to_intern(), max_int.to_intern())) {
                        if (special_prong == .@"else") {
                            return sema.fail(
                                block,
                                special_prong_src,
                                "unreachable else prong; all cases already handled",
                                .{},
                            );
                        }
                        break :check_range;
                    }
                }
                if (special_prong != .@"else") {
                    return sema.fail(
                        block,
                        src,
                        "switch must handle all possibilities",
                        .{},
                    );
                }
            }
        },
        .Bool => {
            var extra_index: usize = special.end;
            {
                var scalar_i: u32 = 0;
                while (scalar_i < scalar_cases_len) : (scalar_i += 1) {
                    const item_ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
                    extra_index += 1;
                    const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[extra_index]);
                    extra_index += 1 + info.body_len;

                    case_vals.append_assume_capacity(try sema.validate_switch_item_bool(
                        block,
                        &true_count,
                        &false_count,
                        item_ref,
                        src_node_offset,
                        .{ .scalar = scalar_i },
                    ));
                }
            }
            {
                var multi_i: u32 = 0;
                while (multi_i < multi_cases_len) : (multi_i += 1) {
                    const items_len = sema.code.extra[extra_index];
                    extra_index += 1;
                    const ranges_len = sema.code.extra[extra_index];
                    extra_index += 1;
                    const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[extra_index]);
                    extra_index += 1;
                    const items = sema.code.ref_slice(extra_index, items_len);
                    extra_index += items_len + info.body_len;

                    try case_vals.ensure_unused_capacity(gpa, items.len);
                    for (items, 0..) |item_ref, item_i| {
                        case_vals.append_assume_capacity(try sema.validate_switch_item_bool(
                            block,
                            &true_count,
                            &false_count,
                            item_ref,
                            src_node_offset,
                            .{ .multi = .{ .prong = multi_i, .item = @int_cast(item_i) } },
                        ));
                    }

                    try sema.validate_switch_no_range(block, ranges_len, operand_ty, src_node_offset);
                }
            }
            switch (special_prong) {
                .@"else" => {
                    if (true_count + false_count == 2) {
                        return sema.fail(
                            block,
                            special_prong_src,
                            "unreachable else prong; all cases already handled",
                            .{},
                        );
                    }
                },
                .under, .none => {
                    if (true_count + false_count < 2) {
                        return sema.fail(
                            block,
                            src,
                            "switch must handle all possibilities",
                            .{},
                        );
                    }
                },
            }
        },
        .EnumLiteral, .Void, .Fn, .Pointer, .Type => {
            if (special_prong != .@"else") {
                return sema.fail(
                    block,
                    src,
                    "else prong required when switching on type '{}'",
                    .{operand_ty.fmt(mod)},
                );
            }

            var seen_values = ValueSrcMap{};
            defer seen_values.deinit(gpa);

            var extra_index: usize = special.end;
            {
                var scalar_i: u32 = 0;
                while (scalar_i < scalar_cases_len) : (scalar_i += 1) {
                    const item_ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
                    extra_index += 1;
                    const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[extra_index]);
                    extra_index += 1;
                    extra_index += info.body_len;

                    case_vals.append_assume_capacity(try sema.validate_switch_item_sparse(
                        block,
                        &seen_values,
                        item_ref,
                        operand_ty,
                        src_node_offset,
                        .{ .scalar = scalar_i },
                    ));
                }
            }
            {
                var multi_i: u32 = 0;
                while (multi_i < multi_cases_len) : (multi_i += 1) {
                    const items_len = sema.code.extra[extra_index];
                    extra_index += 1;
                    const ranges_len = sema.code.extra[extra_index];
                    extra_index += 1;
                    const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[extra_index]);
                    extra_index += 1;
                    const items = sema.code.ref_slice(extra_index, items_len);
                    extra_index += items_len + info.body_len;

                    try case_vals.ensure_unused_capacity(gpa, items.len);
                    for (items, 0..) |item_ref, item_i| {
                        case_vals.append_assume_capacity(try sema.validate_switch_item_sparse(
                            block,
                            &seen_values,
                            item_ref,
                            operand_ty,
                            src_node_offset,
                            .{ .multi = .{ .prong = multi_i, .item = @int_cast(item_i) } },
                        ));
                    }

                    try sema.validate_switch_no_range(block, ranges_len, operand_ty, src_node_offset);
                }
            }
        },

        .ErrorUnion,
        .NoReturn,
        .Array,
        .Struct,
        .Undefined,
        .Null,
        .Optional,
        .Opaque,
        .Vector,
        .Frame,
        .AnyFrame,
        .ComptimeFloat,
        .Float,
        => return sema.fail(block, operand_src, "invalid switch operand type '{}'", .{
            operand_ty.fmt(mod),
        }),
    }

    const spa: SwitchProngAnalysis = .{
        .sema = sema,
        .parent_block = block,
        .operand = raw_operand_val,
        .operand_ptr = raw_operand_ptr,
        .cond = operand,
        .else_error_ty = else_error_ty,
        .switch_block_inst = inst,
        .tag_capture_inst = tag_capture_inst,
    };

    const block_inst: Air.Inst.Index = @enumFromInt(sema.air_instructions.len);
    try sema.air_instructions.append(gpa, .{
        .tag = .block,
        .data = undefined,
    });
    var label: Block.Label = .{
        .zir_block = inst,
        .merges = .{
            .src_locs = .{},
            .results = .{},
            .br_list = .{},
            .block_inst = block_inst,
        },
    };

    var child_block: Block = .{
        .parent = block,
        .sema = sema,
        .src_decl = block.src_decl,
        .namespace = block.namespace,
        .instructions = .{},
        .label = &label,
        .inlining = block.inlining,
        .is_comptime = block.is_comptime,
        .comptime_reason = block.comptime_reason,
        .is_typeof = block.is_typeof,
        .c_import_buf = block.c_import_buf,
        .runtime_cond = block.runtime_cond,
        .runtime_loop = block.runtime_loop,
        .runtime_index = block.runtime_index,
        .want_safety = block.want_safety,
        .error_return_trace_index = block.error_return_trace_index,
    };
    const merges = &child_block.label.?.merges;
    defer child_block.instructions.deinit(gpa);
    defer merges.deinit(gpa);

    if (try sema.resolve_defined_value(&child_block, src, operand)) |operand_val| {
        return resolve_switch_comptime(
            sema,
            spa,
            &child_block,
            operand,
            operand_val,
            operand_ty,
            special,
            case_vals,
            scalar_cases_len,
            multi_cases_len,
            err_set,
            empty_enum,
        );
    }

    if (scalar_cases_len + multi_cases_len == 0 and !special.is_inline) {
        if (empty_enum) {
            return .void_value;
        }
        if (special_prong == .none) {
            return sema.fail(block, src, "switch must handle all possibilities", .{});
        }
        if (err_set and try sema.maybe_error_unwrap(block, special.body, operand, operand_src, false)) {
            return .unreachable_value;
        }
        if (mod.backend_supports_feature(.is_named_enum_value) and block.want_safety() and operand_ty.zig_type_tag(mod) == .Enum and
            (!operand_ty.is_nonexhaustive_enum(mod) or union_originally))
        {
            try sema.zir_dbg_stmt(block, cond_dbg_node_index);
            const ok = try block.add_un_op(.is_named_enum_value, operand);
            try sema.add_safety_check(block, src, ok, .corrupt_switch);
        }

        return spa.resolve_prong_comptime(
            &child_block,
            .special,
            special.body,
            special.capture,
            .special_capture,
            undefined, // case_vals may be undefined for special prongs
            .none,
            false,
            merges,
        );
    }

    if (child_block.is_comptime) {
        _ = try sema.resolve_const_defined_value(&child_block, operand_src, operand, .{
            .needed_comptime_reason = "condition in comptime switch must be comptime-known",
            .block_comptime_reason = child_block.comptime_reason,
        });
        unreachable;
    }

    _ = try sema.analyze_switch_runtime_block(
        spa,
        &child_block,
        src,
        operand,
        operand_ty,
        operand_src,
        case_vals,
        special,
        scalar_cases_len,
        multi_cases_len,
        union_originally,
        maybe_union_ty,
        err_set,
        src_node_offset,
        special_prong_src,
        seen_enum_fields,
        seen_errors,
        range_set,
        true_count,
        false_count,
        cond_dbg_node_index,
        false,
    );

    return sema.resolve_analyzed_block(block, src, &child_block, merges, false);
}

const SpecialProng = struct {
    body: []const Zir.Inst.Index,
    end: usize,
    capture: Zir.Inst.SwitchBlock.ProngInfo.Capture,
    is_inline: bool,
    has_tag_capture: bool,
};

fn analyze_switch_runtime_block(
    sema: *Sema,
    spa: SwitchProngAnalysis,
    child_block: *Block,
    src: LazySrcLoc,
    operand: Air.Inst.Ref,
    operand_ty: Type,
    operand_src: LazySrcLoc,
    case_vals: std.ArrayListUnmanaged(Air.Inst.Ref),
    special: SpecialProng,
    scalar_cases_len: usize,
    multi_cases_len: usize,
    union_originally: bool,
    maybe_union_ty: Type,
    err_set: bool,
    src_node_offset: i32,
    special_prong_src: LazySrcLoc,
    seen_enum_fields: []?Module.SwitchProngSrc,
    seen_errors: SwitchErrorSet,
    range_set: RangeSet,
    true_count: u8,
    false_count: u8,
    cond_dbg_node_index: Zir.Inst.Index,
    allow_err_code_unwrap: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;

    const block = child_block.parent.?;

    const estimated_cases_extra = (scalar_cases_len + multi_cases_len) *
        @typeInfo(Air.SwitchBr.Case).Struct.fields.len + 2;
    var cases_extra = try std.ArrayListUnmanaged(u32).init_capacity(gpa, estimated_cases_extra);
    defer cases_extra.deinit(gpa);

    var case_block = child_block.make_sub_block();
    case_block.runtime_loop = null;
    case_block.runtime_cond = mod.decl_ptr(child_block.src_decl).to_src_loc(operand_src, mod);
    case_block.runtime_index.increment();
    case_block.need_debug_scope = null; // this body is emitted regardless
    defer case_block.instructions.deinit(gpa);

    var extra_index: usize = special.end;

    var scalar_i: usize = 0;
    while (scalar_i < scalar_cases_len) : (scalar_i += 1) {
        extra_index += 1;
        const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[extra_index]);
        extra_index += 1;
        const body = sema.code.body_slice(extra_index, info.body_len);
        extra_index += info.body_len;

        case_block.instructions.shrink_retaining_capacity(0);
        case_block.error_return_trace_index = child_block.error_return_trace_index;

        const item = case_vals.items[scalar_i];
        // `item` is already guaranteed to be constant known.

        const analyze_body = if (union_originally) blk: {
            const unresolved_item_val = sema.resolve_const_defined_value(block, .unneeded, item, undefined) catch unreachable;
            const item_val = sema.resolve_lazy_value(unresolved_item_val) catch unreachable;
            const field_ty = maybe_union_ty.union_field_type(item_val, mod).?;
            break :blk field_ty.zig_type_tag(mod) != .NoReturn;
        } else true;

        if (err_set and try sema.maybe_error_unwrap(&case_block, body, operand, operand_src, allow_err_code_unwrap)) {
            // nothing to do here
        } else if (analyze_body) {
            try spa.analyze_prong_runtime(
                &case_block,
                .normal,
                body,
                info.capture,
                .{ .scalar_capture = @int_cast(scalar_i) },
                &.{item},
                if (info.is_inline) item else .none,
                info.has_tag_capture,
            );
        } else {
            _ = try case_block.add_no_op(.unreach);
        }

        try cases_extra.ensure_unused_capacity(gpa, 3 + case_block.instructions.items.len);
        cases_extra.append_assume_capacity(1); // items_len
        cases_extra.append_assume_capacity(@int_cast(case_block.instructions.items.len));
        cases_extra.append_assume_capacity(@int_from_enum(item));
        cases_extra.append_slice_assume_capacity(@ptr_cast(case_block.instructions.items));
    }

    var is_first = true;
    var prev_cond_br: Air.Inst.Index = undefined;
    var first_else_body: []const Air.Inst.Index = &.{};
    defer gpa.free(first_else_body);
    var prev_then_body: []const Air.Inst.Index = &.{};
    defer gpa.free(prev_then_body);

    var cases_len = scalar_cases_len;
    var case_val_idx: usize = scalar_cases_len;
    var multi_i: u32 = 0;
    while (multi_i < multi_cases_len) : (multi_i += 1) {
        const items_len = sema.code.extra[extra_index];
        extra_index += 1;
        const ranges_len = sema.code.extra[extra_index];
        extra_index += 1;
        const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[extra_index]);
        extra_index += 1 + items_len;

        const items = case_vals.items[case_val_idx..][0..items_len];
        case_val_idx += items_len;

        case_block.instructions.shrink_retaining_capacity(0);
        case_block.error_return_trace_index = child_block.error_return_trace_index;

        // Generate all possible cases as scalar prongs.
        if (info.is_inline) {
            const body_start = extra_index + 2 * ranges_len;
            const body = sema.code.body_slice(body_start, info.body_len);
            var emit_bb = false;

            var range_i: u32 = 0;
            while (range_i < ranges_len) : (range_i += 1) {
                const range_items = case_vals.items[case_val_idx..][0..2];
                extra_index += 2;
                case_val_idx += 2;

                const item_first_ref = range_items[0];
                const item_last_ref = range_items[1];

                var item = sema.resolve_const_defined_value(block, .unneeded, item_first_ref, undefined) catch unreachable;
                const item_last = sema.resolve_const_defined_value(block, .unneeded, item_last_ref, undefined) catch unreachable;

                while (item.compare_scalar(.lte, item_last, operand_ty, mod)) : ({
                    // Previous validation has resolved any possible lazy values.
                    item = sema.int_add_scalar(item, try mod.int_value(operand_ty, 1), operand_ty) catch |err| switch (err) {
                        error.Overflow => unreachable,
                        else => |e| return e,
                    };
                }) {
                    cases_len += 1;

                    const item_ref = Air.interned_to_ref(item.to_intern());

                    case_block.instructions.shrink_retaining_capacity(0);
                    case_block.error_return_trace_index = child_block.error_return_trace_index;

                    if (emit_bb) sema.emit_backward_branch(block, .unneeded) catch |err| switch (err) {
                        error.NeededSourceLocation => {
                            const case_src = Module.SwitchProngSrc{
                                .range = .{ .prong = multi_i, .item = range_i },
                            };
                            const decl = mod.decl_ptr(case_block.src_decl);
                            try sema.emit_backward_branch(block, case_src.resolve(mod, decl, src_node_offset, .none));
                            unreachable;
                        },
                        else => return err,
                    };
                    emit_bb = true;

                    try spa.analyze_prong_runtime(
                        &case_block,
                        .normal,
                        body,
                        info.capture,
                        .{ .multi_capture = multi_i },
                        undefined, // case_vals may be undefined for ranges
                        item_ref,
                        info.has_tag_capture,
                    );

                    try cases_extra.ensure_unused_capacity(gpa, 3 + case_block.instructions.items.len);
                    cases_extra.append_assume_capacity(1); // items_len
                    cases_extra.append_assume_capacity(@int_cast(case_block.instructions.items.len));
                    cases_extra.append_assume_capacity(@int_from_enum(item_ref));
                    cases_extra.append_slice_assume_capacity(@ptr_cast(case_block.instructions.items));

                    if (item.compare_scalar(.eq, item_last, operand_ty, mod)) break;
                }
            }

            for (items, 0..) |item, item_i| {
                cases_len += 1;

                case_block.instructions.shrink_retaining_capacity(0);
                case_block.error_return_trace_index = child_block.error_return_trace_index;

                const analyze_body = if (union_originally) blk: {
                    const item_val = sema.resolve_const_defined_value(block, .unneeded, item, undefined) catch unreachable;
                    const field_ty = maybe_union_ty.union_field_type(item_val, mod).?;
                    break :blk field_ty.zig_type_tag(mod) != .NoReturn;
                } else true;

                if (emit_bb) sema.emit_backward_branch(block, .unneeded) catch |err| switch (err) {
                    error.NeededSourceLocation => {
                        const case_src = Module.SwitchProngSrc{
                            .multi = .{ .prong = multi_i, .item = @int_cast(item_i) },
                        };
                        const decl = mod.decl_ptr(case_block.src_decl);
                        try sema.emit_backward_branch(block, case_src.resolve(mod, decl, src_node_offset, .none));
                        unreachable;
                    },
                    else => return err,
                };
                emit_bb = true;

                if (analyze_body) {
                    try spa.analyze_prong_runtime(
                        &case_block,
                        .normal,
                        body,
                        info.capture,
                        .{ .multi_capture = multi_i },
                        &.{item},
                        item,
                        info.has_tag_capture,
                    );
                } else {
                    _ = try case_block.add_no_op(.unreach);
                }

                try cases_extra.ensure_unused_capacity(gpa, 3 + case_block.instructions.items.len);
                cases_extra.append_assume_capacity(1); // items_len
                cases_extra.append_assume_capacity(@int_cast(case_block.instructions.items.len));
                cases_extra.append_assume_capacity(@int_from_enum(item));
                cases_extra.append_slice_assume_capacity(@ptr_cast(case_block.instructions.items));
            }

            extra_index += info.body_len;
            continue;
        }

        var any_ok: Air.Inst.Ref = .none;

        // If there are any ranges, we have to put all the items into the
        // else prong. Otherwise, we can take advantage of multiple items
        // mapping to the same body.
        if (ranges_len == 0) {
            cases_len += 1;

            const analyze_body = if (union_originally)
                for (items) |item| {
                    const item_val = sema.resolve_const_defined_value(block, .unneeded, item, undefined) catch unreachable;
                    const field_ty = maybe_union_ty.union_field_type(item_val, mod).?;
                    if (field_ty.zig_type_tag(mod) != .NoReturn) break true;
                } else false
            else
                true;

            const body = sema.code.body_slice(extra_index, info.body_len);
            extra_index += info.body_len;
            if (err_set and try sema.maybe_error_unwrap(&case_block, body, operand, operand_src, allow_err_code_unwrap)) {
                // nothing to do here
            } else if (analyze_body) {
                try spa.analyze_prong_runtime(
                    &case_block,
                    .normal,
                    body,
                    info.capture,
                    .{ .multi_capture = multi_i },
                    items,
                    .none,
                    false,
                );
            } else {
                _ = try case_block.add_no_op(.unreach);
            }

            try cases_extra.ensure_unused_capacity(gpa, 2 + items.len +
                case_block.instructions.items.len);

            cases_extra.append_assume_capacity(@int_cast(items.len));
            cases_extra.append_assume_capacity(@int_cast(case_block.instructions.items.len));

            for (items) |item| {
                cases_extra.append_assume_capacity(@int_from_enum(item));
            }

            cases_extra.append_slice_assume_capacity(@ptr_cast(case_block.instructions.items));
        } else {
            for (items) |item| {
                const cmp_ok = try case_block.add_bin_op(if (case_block.float_mode == .optimized) .cmp_eq_optimized else .cmp_eq, operand, item);
                if (any_ok != .none) {
                    any_ok = try case_block.add_bin_op(.bool_or, any_ok, cmp_ok);
                } else {
                    any_ok = cmp_ok;
                }
            }

            var range_i: usize = 0;
            while (range_i < ranges_len) : (range_i += 1) {
                const range_items = case_vals.items[case_val_idx..][0..2];
                extra_index += 2;
                case_val_idx += 2;

                const item_first = range_items[0];
                const item_last = range_items[1];

                // operand >= first and operand <= last
                const range_first_ok = try case_block.add_bin_op(
                    if (case_block.float_mode == .optimized) .cmp_gte_optimized else .cmp_gte,
                    operand,
                    item_first,
                );
                const range_last_ok = try case_block.add_bin_op(
                    if (case_block.float_mode == .optimized) .cmp_lte_optimized else .cmp_lte,
                    operand,
                    item_last,
                );
                const range_ok = try case_block.add_bin_op(
                    .bool_and,
                    range_first_ok,
                    range_last_ok,
                );
                if (any_ok != .none) {
                    any_ok = try case_block.add_bin_op(.bool_or, any_ok, range_ok);
                } else {
                    any_ok = range_ok;
                }
            }

            const new_cond_br = try case_block.add_inst_as_index(.{ .tag = .cond_br, .data = .{
                .pl_op = .{
                    .operand = any_ok,
                    .payload = undefined,
                },
            } });
            var cond_body = try case_block.instructions.to_owned_slice(gpa);
            defer gpa.free(cond_body);

            case_block.instructions.shrink_retaining_capacity(0);
            case_block.error_return_trace_index = child_block.error_return_trace_index;

            const body = sema.code.body_slice(extra_index, info.body_len);
            extra_index += info.body_len;
            if (err_set and try sema.maybe_error_unwrap(&case_block, body, operand, operand_src, allow_err_code_unwrap)) {
                // nothing to do here
            } else {
                try spa.analyze_prong_runtime(
                    &case_block,
                    .normal,
                    body,
                    info.capture,
                    .{ .multi_capture = multi_i },
                    items,
                    .none,
                    false,
                );
            }

            if (is_first) {
                is_first = false;
                first_else_body = cond_body;
                cond_body = &.{};
            } else {
                try sema.air_extra.ensure_unused_capacity(
                    gpa,
                    @typeInfo(Air.CondBr).Struct.fields.len + prev_then_body.len + cond_body.len,
                );

                sema.air_instructions.items(.data)[@int_from_enum(prev_cond_br)].pl_op.payload =
                    sema.add_extra_assume_capacity(Air.CondBr{
                    .then_body_len = @int_cast(prev_then_body.len),
                    .else_body_len = @int_cast(cond_body.len),
                });
                sema.air_extra.append_slice_assume_capacity(@ptr_cast(prev_then_body));
                sema.air_extra.append_slice_assume_capacity(@ptr_cast(cond_body));
            }
            gpa.free(prev_then_body);
            prev_then_body = try case_block.instructions.to_owned_slice(gpa);
            prev_cond_br = new_cond_br;
        }
    }

    var final_else_body: []const Air.Inst.Index = &.{};
    if (special.body.len != 0 or !is_first or case_block.want_safety()) {
        var emit_bb = false;
        if (special.is_inline) switch (operand_ty.zig_type_tag(mod)) {
            .Enum => {
                if (operand_ty.is_nonexhaustive_enum(mod) and !union_originally) {
                    return sema.fail(block, special_prong_src, "cannot enumerate values of type '{}' for 'inline else'", .{
                        operand_ty.fmt(mod),
                    });
                }
                for (seen_enum_fields, 0..) |f, i| {
                    if (f != null) continue;
                    cases_len += 1;

                    const item_val = try mod.enum_value_field_index(operand_ty, @int_cast(i));
                    const item_ref = Air.interned_to_ref(item_val.to_intern());

                    case_block.instructions.shrink_retaining_capacity(0);
                    case_block.error_return_trace_index = child_block.error_return_trace_index;

                    const analyze_body = if (union_originally) blk: {
                        const field_ty = maybe_union_ty.union_field_type(item_val, mod).?;
                        break :blk field_ty.zig_type_tag(mod) != .NoReturn;
                    } else true;

                    if (emit_bb) try sema.emit_backward_branch(block, special_prong_src);
                    emit_bb = true;

                    if (analyze_body) {
                        try spa.analyze_prong_runtime(
                            &case_block,
                            .special,
                            special.body,
                            special.capture,
                            .special_capture,
                            &.{item_ref},
                            item_ref,
                            special.has_tag_capture,
                        );
                    } else {
                        _ = try case_block.add_no_op(.unreach);
                    }

                    try cases_extra.ensure_unused_capacity(gpa, 3 + case_block.instructions.items.len);
                    cases_extra.append_assume_capacity(1); // items_len
                    cases_extra.append_assume_capacity(@int_cast(case_block.instructions.items.len));
                    cases_extra.append_assume_capacity(@int_from_enum(item_ref));
                    cases_extra.append_slice_assume_capacity(@ptr_cast(case_block.instructions.items));
                }
            },
            .ErrorSet => {
                if (operand_ty.is_any_error(mod)) {
                    return sema.fail(block, special_prong_src, "cannot enumerate values of type '{}' for 'inline else'", .{
                        operand_ty.fmt(mod),
                    });
                }
                const error_names = operand_ty.error_set_names(mod);
                for (0..error_names.len) |name_index| {
                    const error_name = error_names.get(ip)[name_index];
                    if (seen_errors.contains(error_name)) continue;
                    cases_len += 1;

                    const item_val = try mod.intern(.{ .err = .{
                        .ty = operand_ty.to_intern(),
                        .name = error_name,
                    } });
                    const item_ref = Air.interned_to_ref(item_val);

                    case_block.instructions.shrink_retaining_capacity(0);
                    case_block.error_return_trace_index = child_block.error_return_trace_index;

                    if (emit_bb) try sema.emit_backward_branch(block, special_prong_src);
                    emit_bb = true;

                    try spa.analyze_prong_runtime(
                        &case_block,
                        .special,
                        special.body,
                        special.capture,
                        .special_capture,
                        &.{item_ref},
                        item_ref,
                        special.has_tag_capture,
                    );

                    try cases_extra.ensure_unused_capacity(gpa, 3 + case_block.instructions.items.len);
                    cases_extra.append_assume_capacity(1); // items_len
                    cases_extra.append_assume_capacity(@int_cast(case_block.instructions.items.len));
                    cases_extra.append_assume_capacity(@int_from_enum(item_ref));
                    cases_extra.append_slice_assume_capacity(@ptr_cast(case_block.instructions.items));
                }
            },
            .Int => {
                var it = try RangeSetUnhandledIterator.init(sema, operand_ty, range_set);
                while (try it.next()) |cur| {
                    cases_len += 1;

                    const item_ref = Air.interned_to_ref(cur);

                    case_block.instructions.shrink_retaining_capacity(0);
                    case_block.error_return_trace_index = child_block.error_return_trace_index;

                    if (emit_bb) try sema.emit_backward_branch(block, special_prong_src);
                    emit_bb = true;

                    try spa.analyze_prong_runtime(
                        &case_block,
                        .special,
                        special.body,
                        special.capture,
                        .special_capture,
                        &.{item_ref},
                        item_ref,
                        special.has_tag_capture,
                    );

                    try cases_extra.ensure_unused_capacity(gpa, 3 + case_block.instructions.items.len);
                    cases_extra.append_assume_capacity(1); // items_len
                    cases_extra.append_assume_capacity(@int_cast(case_block.instructions.items.len));
                    cases_extra.append_assume_capacity(@int_from_enum(item_ref));
                    cases_extra.append_slice_assume_capacity(@ptr_cast(case_block.instructions.items));
                }
            },
            .Bool => {
                if (true_count == 0) {
                    cases_len += 1;

                    case_block.instructions.shrink_retaining_capacity(0);
                    case_block.error_return_trace_index = child_block.error_return_trace_index;

                    if (emit_bb) try sema.emit_backward_branch(block, special_prong_src);
                    emit_bb = true;

                    try spa.analyze_prong_runtime(
                        &case_block,
                        .special,
                        special.body,
                        special.capture,
                        .special_capture,
                        &.{.bool_true},
                        .bool_true,
                        special.has_tag_capture,
                    );

                    try cases_extra.ensure_unused_capacity(gpa, 3 + case_block.instructions.items.len);
                    cases_extra.append_assume_capacity(1); // items_len
                    cases_extra.append_assume_capacity(@int_cast(case_block.instructions.items.len));
                    cases_extra.append_assume_capacity(@int_from_enum(Air.Inst.Ref.bool_true));
                    cases_extra.append_slice_assume_capacity(@ptr_cast(case_block.instructions.items));
                }
                if (false_count == 0) {
                    cases_len += 1;

                    case_block.instructions.shrink_retaining_capacity(0);
                    case_block.error_return_trace_index = child_block.error_return_trace_index;

                    if (emit_bb) try sema.emit_backward_branch(block, special_prong_src);
                    emit_bb = true;

                    try spa.analyze_prong_runtime(
                        &case_block,
                        .special,
                        special.body,
                        special.capture,
                        .special_capture,
                        &.{.bool_false},
                        .bool_false,
                        special.has_tag_capture,
                    );

                    try cases_extra.ensure_unused_capacity(gpa, 3 + case_block.instructions.items.len);
                    cases_extra.append_assume_capacity(1); // items_len
                    cases_extra.append_assume_capacity(@int_cast(case_block.instructions.items.len));
                    cases_extra.append_assume_capacity(@int_from_enum(Air.Inst.Ref.bool_false));
                    cases_extra.append_slice_assume_capacity(@ptr_cast(case_block.instructions.items));
                }
            },
            else => return sema.fail(block, special_prong_src, "cannot enumerate values of type '{}' for 'inline else'", .{
                operand_ty.fmt(mod),
            }),
        };

        case_block.instructions.shrink_retaining_capacity(0);
        case_block.error_return_trace_index = child_block.error_return_trace_index;

        if (mod.backend_supports_feature(.is_named_enum_value) and
            special.body.len != 0 and block.want_safety() and
            operand_ty.zig_type_tag(mod) == .Enum and
            (!operand_ty.is_nonexhaustive_enum(mod) or union_originally))
        {
            try sema.zir_dbg_stmt(&case_block, cond_dbg_node_index);
            const ok = try case_block.add_un_op(.is_named_enum_value, operand);
            try sema.add_safety_check(&case_block, src, ok, .corrupt_switch);
        }

        const analyze_body = if (union_originally and !special.is_inline)
            for (seen_enum_fields, 0..) |seen_field, index| {
                if (seen_field != null) continue;
                const union_obj = mod.type_to_union(maybe_union_ty).?;
                const field_ty = Type.from_interned(union_obj.field_types.get(ip)[index]);
                if (field_ty.zig_type_tag(mod) != .NoReturn) break true;
            } else false
        else
            true;
        if (special.body.len != 0 and err_set and
            try sema.maybe_error_unwrap(&case_block, special.body, operand, operand_src, allow_err_code_unwrap))
        {
            // nothing to do here
        } else if (special.body.len != 0 and analyze_body and !special.is_inline) {
            try spa.analyze_prong_runtime(
                &case_block,
                .special,
                special.body,
                special.capture,
                .special_capture,
                undefined, // case_vals may be undefined for special prongs
                .none,
                false,
            );
        } else {
            // We still need a terminator in this block, but we have proven
            // that it is unreachable.
            if (case_block.want_safety()) {
                try sema.zir_dbg_stmt(&case_block, cond_dbg_node_index);
                try sema.safety_panic(&case_block, src, .corrupt_switch);
            } else {
                _ = try case_block.add_no_op(.unreach);
            }
        }

        if (is_first) {
            final_else_body = case_block.instructions.items;
        } else {
            try sema.air_extra.ensure_unused_capacity(gpa, prev_then_body.len +
                @typeInfo(Air.CondBr).Struct.fields.len + case_block.instructions.items.len);

            sema.air_instructions.items(.data)[@int_from_enum(prev_cond_br)].pl_op.payload =
                sema.add_extra_assume_capacity(Air.CondBr{
                .then_body_len = @int_cast(prev_then_body.len),
                .else_body_len = @int_cast(case_block.instructions.items.len),
            });
            sema.air_extra.append_slice_assume_capacity(@ptr_cast(prev_then_body));
            sema.air_extra.append_slice_assume_capacity(@ptr_cast(case_block.instructions.items));
            final_else_body = first_else_body;
        }
    }

    try sema.air_extra.ensure_unused_capacity(gpa, @typeInfo(Air.SwitchBr).Struct.fields.len +
        cases_extra.items.len + final_else_body.len);

    const payload_index = sema.add_extra_assume_capacity(Air.SwitchBr{
        .cases_len = @int_cast(cases_len),
        .else_body_len = @int_cast(final_else_body.len),
    });

    sema.air_extra.append_slice_assume_capacity(@ptr_cast(cases_extra.items));
    sema.air_extra.append_slice_assume_capacity(@ptr_cast(final_else_body));

    return try child_block.add_inst(.{
        .tag = .switch_br,
        .data = .{ .pl_op = .{
            .operand = operand,
            .payload = payload_index,
        } },
    });
}

fn resolve_switch_comptime(
    sema: *Sema,
    spa: SwitchProngAnalysis,
    child_block: *Block,
    cond_operand: Air.Inst.Ref,
    operand_val: Value,
    operand_ty: Type,
    special: SpecialProng,
    case_vals: std.ArrayListUnmanaged(Air.Inst.Ref),
    scalar_cases_len: u32,
    multi_cases_len: u32,
    err_set: bool,
    empty_enum: bool,
) CompileError!Air.Inst.Ref {
    const merges = &child_block.label.?.merges;
    const resolved_operand_val = try sema.resolve_lazy_value(operand_val);
    var extra_index: usize = special.end;
    {
        var scalar_i: usize = 0;
        while (scalar_i < scalar_cases_len) : (scalar_i += 1) {
            extra_index += 1;
            const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[extra_index]);
            extra_index += 1;
            const body = sema.code.body_slice(extra_index, info.body_len);
            extra_index += info.body_len;

            const item = case_vals.items[scalar_i];
            const item_val = sema.resolve_const_defined_value(child_block, .unneeded, item, undefined) catch unreachable;
            if (operand_val.eql(item_val, operand_ty, sema.mod)) {
                if (err_set) try sema.maybe_error_unwrap_comptime(child_block, body, cond_operand);
                return spa.resolve_prong_comptime(
                    child_block,
                    .normal,
                    body,
                    info.capture,
                    .{ .scalar_capture = @int_cast(scalar_i) },
                    &.{item},
                    if (info.is_inline) cond_operand else .none,
                    info.has_tag_capture,
                    merges,
                );
            }
        }
    }
    {
        var multi_i: usize = 0;
        var case_val_idx: usize = scalar_cases_len;
        while (multi_i < multi_cases_len) : (multi_i += 1) {
            const items_len = sema.code.extra[extra_index];
            extra_index += 1;
            const ranges_len = sema.code.extra[extra_index];
            extra_index += 1;
            const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[extra_index]);
            extra_index += 1 + items_len;
            const body = sema.code.body_slice(extra_index + 2 * ranges_len, info.body_len);

            const items = case_vals.items[case_val_idx..][0..items_len];
            case_val_idx += items_len;

            for (items) |item| {
                // Validation above ensured these will succeed.
                const item_val = sema.resolve_const_defined_value(child_block, .unneeded, item, undefined) catch unreachable;
                if (operand_val.eql(item_val, operand_ty, sema.mod)) {
                    if (err_set) try sema.maybe_error_unwrap_comptime(child_block, body, cond_operand);
                    return spa.resolve_prong_comptime(
                        child_block,
                        .normal,
                        body,
                        info.capture,
                        .{ .multi_capture = @int_cast(multi_i) },
                        items,
                        if (info.is_inline) cond_operand else .none,
                        info.has_tag_capture,
                        merges,
                    );
                }
            }

            var range_i: usize = 0;
            while (range_i < ranges_len) : (range_i += 1) {
                const range_items = case_vals.items[case_val_idx..][0..2];
                extra_index += 2;
                case_val_idx += 2;

                // Validation above ensured these will succeed.
                const first_val = sema.resolve_const_defined_value(child_block, .unneeded, range_items[0], undefined) catch unreachable;
                const last_val = sema.resolve_const_defined_value(child_block, .unneeded, range_items[1], undefined) catch unreachable;
                if ((try sema.compare_all(resolved_operand_val, .gte, first_val, operand_ty)) and
                    (try sema.compare_all(resolved_operand_val, .lte, last_val, operand_ty)))
                {
                    if (err_set) try sema.maybe_error_unwrap_comptime(child_block, body, cond_operand);
                    return spa.resolve_prong_comptime(
                        child_block,
                        .normal,
                        body,
                        info.capture,
                        .{ .multi_capture = @int_cast(multi_i) },
                        undefined, // case_vals may be undefined for ranges
                        if (info.is_inline) cond_operand else .none,
                        info.has_tag_capture,
                        merges,
                    );
                }
            }

            extra_index += info.body_len;
        }
    }
    if (err_set) try sema.maybe_error_unwrap_comptime(child_block, special.body, cond_operand);
    if (empty_enum) {
        return .void_value;
    }

    return spa.resolve_prong_comptime(
        child_block,
        .special,
        special.body,
        special.capture,
        .special_capture,
        undefined, // case_vals may be undefined for special prongs
        if (special.is_inline) cond_operand else .none,
        special.has_tag_capture,
        merges,
    );
}

const RangeSetUnhandledIterator = struct {
    mod: *Module,
    cur: ?InternPool.Index,
    max: InternPool.Index,
    range_i: usize,
    ranges: []const RangeSet.Range,
    limbs: []math.big.Limb,

    const preallocated_limbs = math.big.int.calc_twos_comp_limb_count(128);

    fn init(sema: *Sema, ty: Type, range_set: RangeSet) !RangeSetUnhandledIterator {
        const mod = sema.mod;
        const int_type = mod.intern_pool.index_to_key(ty.to_intern()).int_type;
        const needed_limbs = math.big.int.calc_twos_comp_limb_count(int_type.bits);
        return .{
            .mod = mod,
            .cur = (try ty.min_int(mod, ty)).to_intern(),
            .max = (try ty.max_int(mod, ty)).to_intern(),
            .range_i = 0,
            .ranges = range_set.ranges.items,
            .limbs = if (needed_limbs > preallocated_limbs)
                try sema.arena.alloc(math.big.Limb, needed_limbs)
            else
                &.{},
        };
    }

    fn add_one(it: *const RangeSetUnhandledIterator, val: InternPool.Index) !?InternPool.Index {
        if (val == it.max) return null;
        const int = it.mod.intern_pool.index_to_key(val).int;

        switch (int.storage) {
            inline .u64, .i64 => |val_int| {
                const next_int = @add_with_overflow(val_int, 1);
                if (next_int[1] == 0)
                    return (try it.mod.int_value(Type.from_interned(int.ty), next_int[0])).to_intern();
            },
            .big_int => {},
            .lazy_align, .lazy_size => unreachable,
        }

        var val_space: InternPool.Key.Int.Storage.BigIntSpace = undefined;
        const val_bigint = int.storage.to_big_int(&val_space);

        var result_limbs: [preallocated_limbs]math.big.Limb = undefined;
        var result_bigint = math.big.int.Mutable.init(
            if (it.limbs.len > 0) it.limbs else &result_limbs,
            0,
        );

        result_bigint.add_scalar(val_bigint, 1);
        return (try it.mod.int_value_big(Type.from_interned(int.ty), result_bigint.to_const())).to_intern();
    }

    fn next(it: *RangeSetUnhandledIterator) !?InternPool.Index {
        var cur = it.cur orelse return null;
        while (it.range_i < it.ranges.len and cur == it.ranges[it.range_i].first) {
            defer it.range_i += 1;
            cur = (try it.add_one(it.ranges[it.range_i].last)) orelse {
                it.cur = null;
                return null;
            };
        }
        it.cur = try it.add_one(cur);
        return cur;
    }
};

const ResolvedSwitchItem = struct {
    ref: Air.Inst.Ref,
    val: InternPool.Index,
};
fn resolve_switch_item_val(
    sema: *Sema,
    block: *Block,
    item_ref: Zir.Inst.Ref,
    /// Coerce `item_ref` to this type.
    coerce_ty: Type,
    switch_node_offset: i32,
    switch_prong_src: Module.SwitchProngSrc,
    range_expand: Module.SwitchProngSrc.RangeExpand,
) CompileError!ResolvedSwitchItem {
    const mod = sema.mod;
    const uncoerced_item = try sema.resolve_inst(item_ref);

    // Constructing a LazySrcLoc is costly because we only have the switch AST node.
    // Only if we know for sure we need to report a compile error do we resolve the
    // full source locations.

    const item = sema.coerce(block, coerce_ty, uncoerced_item, .unneeded) catch |err| switch (err) {
        error.NeededSourceLocation => {
            const src = switch_prong_src.resolve(mod, mod.decl_ptr(block.src_decl), switch_node_offset, range_expand);
            _ = try sema.coerce(block, coerce_ty, uncoerced_item, src);
            unreachable;
        },
        else => |e| return e,
    };

    const maybe_lazy = sema.resolve_const_defined_value(block, .unneeded, item, undefined) catch |err| switch (err) {
        error.NeededSourceLocation => {
            const src = switch_prong_src.resolve(mod, mod.decl_ptr(block.src_decl), switch_node_offset, range_expand);
            _ = try sema.resolve_const_defined_value(block, src, item, .{
                .needed_comptime_reason = "switch prong values must be comptime-known",
            });
            unreachable;
        },
        else => |e| return e,
    };

    const val = try sema.resolve_lazy_value(maybe_lazy);
    const new_item = if (val.to_intern() != maybe_lazy.to_intern()) blk: {
        break :blk Air.interned_to_ref(val.to_intern());
    } else item;

    return .{ .ref = new_item, .val = val.to_intern() };
}

fn validate_err_set_switch(
    sema: *Sema,
    block: *Block,
    seen_errors: *SwitchErrorSet,
    case_vals: *std.ArrayListUnmanaged(Air.Inst.Ref),
    operand_ty: Type,
    inst_data: std.meta.FieldType(Zir.Inst.Data, .pl_node),
    scalar_cases_len: u32,
    multi_cases_len: u32,
    else_case: struct { body: []const Zir.Inst.Index, end: usize, src: LazySrcLoc },
    has_else: bool,
) CompileError!?Type {
    const gpa = sema.gpa;
    const mod = sema.mod;
    const ip = &mod.intern_pool;

    const src_node_offset = inst_data.src_node;
    const src = inst_data.src();

    var extra_index: usize = else_case.end;
    {
        var scalar_i: u32 = 0;
        while (scalar_i < scalar_cases_len) : (scalar_i += 1) {
            const item_ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
            extra_index += 1;
            const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[extra_index]);
            extra_index += 1 + info.body_len;

            case_vals.append_assume_capacity(try sema.validate_switch_item_error(
                block,
                seen_errors,
                item_ref,
                operand_ty,
                src_node_offset,
                .{ .scalar = scalar_i },
            ));
        }
    }
    {
        var multi_i: u32 = 0;
        while (multi_i < multi_cases_len) : (multi_i += 1) {
            const items_len = sema.code.extra[extra_index];
            extra_index += 1;
            const ranges_len = sema.code.extra[extra_index];
            extra_index += 1;
            const info: Zir.Inst.SwitchBlock.ProngInfo = @bit_cast(sema.code.extra[extra_index]);
            extra_index += 1;
            const items = sema.code.ref_slice(extra_index, items_len);
            extra_index += items_len + info.body_len;

            try case_vals.ensure_unused_capacity(gpa, items.len);
            for (items, 0..) |item_ref, item_i| {
                case_vals.append_assume_capacity(try sema.validate_switch_item_error(
                    block,
                    seen_errors,
                    item_ref,
                    operand_ty,
                    src_node_offset,
                    .{ .multi = .{ .prong = multi_i, .item = @int_cast(item_i) } },
                ));
            }

            try sema.validate_switch_no_range(block, ranges_len, operand_ty, src_node_offset);
        }
    }

    switch (try sema.resolve_inferred_error_set_ty(block, src, operand_ty.to_intern())) {
        .anyerror_type => {
            if (!has_else) {
                return sema.fail(
                    block,
                    src,
                    "else prong required when switching on type 'anyerror'",
                    .{},
                );
            }
            return Type.anyerror;
        },
        else => |err_set_ty_index| else_validation: {
            const error_names = ip.index_to_key(err_set_ty_index).error_set_type.names;
            var maybe_msg: ?*Module.ErrorMsg = null;
            errdefer if (maybe_msg) |msg| msg.destroy(sema.gpa);

            for (error_names.get(ip)) |error_name| {
                if (!seen_errors.contains(error_name) and !has_else) {
                    const msg = maybe_msg orelse blk: {
                        maybe_msg = try sema.err_msg(
                            block,
                            src,
                            "switch must handle all possibilities",
                            .{},
                        );
                        break :blk maybe_msg.?;
                    };

                    try sema.err_note(
                        block,
                        src,
                        msg,
                        "unhandled error value: 'error.{}'",
                        .{error_name.fmt(ip)},
                    );
                }
            }

            if (maybe_msg) |msg| {
                maybe_msg = null;
                try sema.add_declared_here_note(msg, operand_ty);
                return sema.fail_with_owned_error_msg(block, msg);
            }

            if (has_else and seen_errors.count() == error_names.len) {
                // In order to enable common patterns for generic code allow simple else bodies
                // else => unreachable,
                // else => return,
                // else => |e| return e,
                // even if all the possible errors were already handled.
                const tags = sema.code.instructions.items(.tag);
                const datas = sema.code.instructions.items(.data);
                for (else_case.body) |else_inst| switch (tags[@int_from_enum(else_inst)]) {
                    .dbg_stmt,
                    .dbg_var_val,
                    .ret_type,
                    .as_node,
                    .ret_node,
                    .@"unreachable",
                    .@"defer",
                    .defer_err_code,
                    .err_union_code,
                    .ret_err_value_code,
                    .save_err_ret_index,
                    .restore_err_ret_index_unconditional,
                    .restore_err_ret_index_fn_entry,
                    .is_non_err,
                    .ret_is_non_err,
                    .condbr,
                    => {},
                    .extended => switch (datas[@int_from_enum(else_inst)].extended.opcode) {
                        .restore_err_ret_index => {},
                        else => break,
                    },
                    else => break,
                } else break :else_validation;

                return sema.fail(
                    block,
                    else_case.src,
                    "unreachable else prong; all cases already handled",
                    .{},
                );
            }

            var names: InferredErrorSet.NameMap = .{};
            try names.ensure_unused_capacity(sema.arena, error_names.len);
            for (error_names.get(ip)) |error_name| {
                if (seen_errors.contains(error_name)) continue;

                names.put_assume_capacity_no_clobber(error_name, {});
            }
            // No need to keep the hash map metadata correct; here we
            // extract the (sorted) keys only.
            return try mod.error_set_from_unsorted_names(names.keys());
        },
    }
    return null;
}

fn validate_switch_range(
    sema: *Sema,
    block: *Block,
    range_set: *RangeSet,
    first_ref: Zir.Inst.Ref,
    last_ref: Zir.Inst.Ref,
    operand_ty: Type,
    src_node_offset: i32,
    switch_prong_src: Module.SwitchProngSrc,
) CompileError![2]Air.Inst.Ref {
    const mod = sema.mod;
    const first = try sema.resolve_switch_item_val(block, first_ref, operand_ty, src_node_offset, switch_prong_src, .first);
    const last = try sema.resolve_switch_item_val(block, last_ref, operand_ty, src_node_offset, switch_prong_src, .last);
    if (try Value.from_interned(first.val).compare_all(.gt, Value.from_interned(last.val), operand_ty, mod)) {
        const src = switch_prong_src.resolve(mod, mod.decl_ptr(block.src_decl), src_node_offset, .first);
        return sema.fail(block, src, "range start value is greater than the end value", .{});
    }
    const maybe_prev_src = try range_set.add(first.val, last.val, switch_prong_src);
    try sema.validate_switch_dupe(block, maybe_prev_src, switch_prong_src, src_node_offset);
    return .{ first.ref, last.ref };
}

fn validate_switch_item_int(
    sema: *Sema,
    block: *Block,
    range_set: *RangeSet,
    item_ref: Zir.Inst.Ref,
    operand_ty: Type,
    src_node_offset: i32,
    switch_prong_src: Module.SwitchProngSrc,
) CompileError!Air.Inst.Ref {
    const item = try sema.resolve_switch_item_val(block, item_ref, operand_ty, src_node_offset, switch_prong_src, .none);
    const maybe_prev_src = try range_set.add(item.val, item.val, switch_prong_src);
    try sema.validate_switch_dupe(block, maybe_prev_src, switch_prong_src, src_node_offset);
    return item.ref;
}

fn validate_switch_item_enum(
    sema: *Sema,
    block: *Block,
    seen_fields: []?Module.SwitchProngSrc,
    range_set: *RangeSet,
    item_ref: Zir.Inst.Ref,
    operand_ty: Type,
    src_node_offset: i32,
    switch_prong_src: Module.SwitchProngSrc,
) CompileError!Air.Inst.Ref {
    const ip = &sema.mod.intern_pool;
    const item = try sema.resolve_switch_item_val(block, item_ref, operand_ty, src_node_offset, switch_prong_src, .none);
    const int = ip.index_to_key(item.val).enum_tag.int;
    const field_index = ip.load_enum_type(ip.type_of(item.val)).tag_value_index(ip, int) orelse {
        const maybe_prev_src = try range_set.add(int, int, switch_prong_src);
        try sema.validate_switch_dupe(block, maybe_prev_src, switch_prong_src, src_node_offset);
        return item.ref;
    };
    const maybe_prev_src = seen_fields[field_index];
    seen_fields[field_index] = switch_prong_src;
    try sema.validate_switch_dupe(block, maybe_prev_src, switch_prong_src, src_node_offset);
    return item.ref;
}

fn validate_switch_item_error(
    sema: *Sema,
    block: *Block,
    seen_errors: *SwitchErrorSet,
    item_ref: Zir.Inst.Ref,
    operand_ty: Type,
    src_node_offset: i32,
    switch_prong_src: Module.SwitchProngSrc,
) CompileError!Air.Inst.Ref {
    const ip = &sema.mod.intern_pool;
    const item = try sema.resolve_switch_item_val(block, item_ref, operand_ty, src_node_offset, switch_prong_src, .none);
    const error_name = ip.index_to_key(item.val).err.name;
    const maybe_prev_src = if (try seen_errors.fetch_put(error_name, switch_prong_src)) |prev|
        prev.value
    else
        null;
    try sema.validate_switch_dupe(block, maybe_prev_src, switch_prong_src, src_node_offset);
    return item.ref;
}

fn validate_switch_dupe(
    sema: *Sema,
    block: *Block,
    maybe_prev_src: ?Module.SwitchProngSrc,
    switch_prong_src: Module.SwitchProngSrc,
    src_node_offset: i32,
) CompileError!void {
    const prev_prong_src = maybe_prev_src orelse return;
    const mod = sema.mod;
    const block_src_decl = mod.decl_ptr(block.src_decl);
    const src = switch_prong_src.resolve(mod, block_src_decl, src_node_offset, .none);
    const prev_src = prev_prong_src.resolve(mod, block_src_decl, src_node_offset, .none);
    const msg = msg: {
        const msg = try sema.err_msg(
            block,
            src,
            "duplicate switch value",
            .{},
        );
        errdefer msg.destroy(sema.gpa);
        try sema.err_note(
            block,
            prev_src,
            msg,
            "previous value here",
            .{},
        );
        break :msg msg;
    };
    return sema.fail_with_owned_error_msg(block, msg);
}

fn validate_switch_item_bool(
    sema: *Sema,
    block: *Block,
    true_count: *u8,
    false_count: *u8,
    item_ref: Zir.Inst.Ref,
    src_node_offset: i32,
    switch_prong_src: Module.SwitchProngSrc,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const item = try sema.resolve_switch_item_val(block, item_ref, Type.bool, src_node_offset, switch_prong_src, .none);
    if (Value.from_interned(item.val).to_bool()) {
        true_count.* += 1;
    } else {
        false_count.* += 1;
    }
    if (true_count.* > 1 or false_count.* > 1) {
        const block_src_decl = sema.mod.decl_ptr(block.src_decl);
        const src = switch_prong_src.resolve(mod, block_src_decl, src_node_offset, .none);
        return sema.fail(block, src, "duplicate switch value", .{});
    }
    return item.ref;
}

const ValueSrcMap = std.AutoHashMapUnmanaged(InternPool.Index, Module.SwitchProngSrc);

fn validate_switch_item_sparse(
    sema: *Sema,
    block: *Block,
    seen_values: *ValueSrcMap,
    item_ref: Zir.Inst.Ref,
    operand_ty: Type,
    src_node_offset: i32,
    switch_prong_src: Module.SwitchProngSrc,
) CompileError!Air.Inst.Ref {
    const item = try sema.resolve_switch_item_val(block, item_ref, operand_ty, src_node_offset, switch_prong_src, .none);
    const kv = (try seen_values.fetch_put(sema.gpa, item.val, switch_prong_src)) orelse return item.ref;
    try sema.validate_switch_dupe(block, kv.value, switch_prong_src, src_node_offset);
    unreachable;
}

fn validate_switch_no_range(
    sema: *Sema,
    block: *Block,
    ranges_len: u32,
    operand_ty: Type,
    src_node_offset: i32,
) CompileError!void {
    if (ranges_len == 0)
        return;

    const operand_src: LazySrcLoc = .{ .node_offset_switch_operand = src_node_offset };
    const range_src: LazySrcLoc = .{ .node_offset_switch_range = src_node_offset };

    const msg = msg: {
        const msg = try sema.err_msg(
            block,
            operand_src,
            "ranges not allowed when switching on type '{}'",
            .{operand_ty.fmt(sema.mod)},
        );
        errdefer msg.destroy(sema.gpa);
        try sema.err_note(
            block,
            range_src,
            msg,
            "range here",
            .{},
        );
        break :msg msg;
    };
    return sema.fail_with_owned_error_msg(block, msg);
}

fn maybe_error_unwrap(
    sema: *Sema,
    block: *Block,
    body: []const Zir.Inst.Index,
    operand: Air.Inst.Ref,
    operand_src: LazySrcLoc,
    allow_err_code_inst: bool,
) !bool {
    const mod = sema.mod;
    if (!mod.backend_supports_feature(.panic_unwrap_error)) return false;

    const tags = sema.code.instructions.items(.tag);
    for (body) |inst| {
        switch (tags[@int_from_enum(inst)]) {
            .@"unreachable" => if (!block.want_safety()) return false,
            .err_union_code => if (!allow_err_code_inst) return false,
            .save_err_ret_index,
            .dbg_stmt,
            .str,
            .as_node,
            .panic,
            .field_val,
            => {},
            else => return false,
        }
    }

    for (body) |inst| {
        const air_inst = switch (tags[@int_from_enum(inst)]) {
            .err_union_code => continue,
            .dbg_stmt => {
                try sema.zir_dbg_stmt(block, inst);
                continue;
            },
            .save_err_ret_index => {
                try sema.zir_save_err_ret_index(block, inst);
                continue;
            },
            .str => try sema.zir_str(inst),
            .as_node => try sema.zir_as_node(block, inst),
            .field_val => try sema.zir_field_val(block, inst),
            .@"unreachable" => {
                if (!mod.comp.formatted_panics) {
                    try sema.safety_panic(block, operand_src, .unwrap_error);
                    return true;
                }

                const panic_fn = try sema.get_builtin("panic_unwrap_error");
                const err_return_trace = try sema.get_error_return_trace(block);
                const args: [2]Air.Inst.Ref = .{ err_return_trace, operand };
                try sema.call_builtin(block, operand_src, panic_fn, .auto, &args, .@"safety check");
                return true;
            },
            .panic => {
                const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
                const msg_inst = try sema.resolve_inst(inst_data.operand);

                const panic_fn = try sema.get_builtin("panic");
                const err_return_trace = try sema.get_error_return_trace(block);
                const args: [3]Air.Inst.Ref = .{ msg_inst, err_return_trace, .null_value };
                try sema.call_builtin(block, operand_src, panic_fn, .auto, &args, .@"safety check");
                return true;
            },
            else => unreachable,
        };
        if (sema.type_of(air_inst).is_no_return(mod))
            return true;
        sema.inst_map.put_assume_capacity(inst, air_inst);
    }
    unreachable;
}

fn maybe_error_unwrap_condbr(sema: *Sema, block: *Block, body: []const Zir.Inst.Index, cond: Zir.Inst.Ref, cond_src: LazySrcLoc) !void {
    const mod = sema.mod;
    const index = cond.to_index() orelse return;
    if (sema.code.instructions.items(.tag)[@int_from_enum(index)] != .is_non_err) return;

    const err_inst_data = sema.code.instructions.items(.data)[@int_from_enum(index)].un_node;
    const err_operand = try sema.resolve_inst(err_inst_data.operand);
    const operand_ty = sema.type_of(err_operand);
    if (operand_ty.zig_type_tag(mod) == .ErrorSet) {
        try sema.maybe_error_unwrap_comptime(block, body, err_operand);
        return;
    }
    if (try sema.resolve_defined_value(block, cond_src, err_operand)) |val| {
        if (!operand_ty.is_error(mod)) return;
        if (val.get_error_name(mod) == .none) return;
        try sema.maybe_error_unwrap_comptime(block, body, err_operand);
    }
}

fn maybe_error_unwrap_comptime(sema: *Sema, block: *Block, body: []const Zir.Inst.Index, operand: Air.Inst.Ref) !void {
    const tags = sema.code.instructions.items(.tag);
    const inst = for (body) |inst| {
        switch (tags[@int_from_enum(inst)]) {
            .dbg_stmt,
            .save_err_ret_index,
            => {},
            .@"unreachable" => break inst,
            else => return,
        }
    } else return;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].@"unreachable";
    const src = inst_data.src();

    if (try sema.resolve_defined_value(block, src, operand)) |val| {
        if (val.get_error_name(sema.mod).unwrap()) |name| {
            return sema.fail_with_comptime_error_ret_trace(block, src, name);
        }
    }
}

fn zir_has_field(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const ty_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const name_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const ty = try sema.resolve_type(block, ty_src, extra.lhs);
    const field_name = try sema.resolve_const_string_intern(block, name_src, extra.rhs, .{
        .needed_comptime_reason = "field name must be comptime-known",
    });
    try sema.resolve_type_fields(ty);
    const ip = &mod.intern_pool;

    const has_field = hf: {
        switch (ip.index_to_key(ty.to_intern())) {
            .ptr_type => |ptr_type| switch (ptr_type.flags.size) {
                .Slice => {
                    if (field_name.eql_slice("ptr", ip)) break :hf true;
                    if (field_name.eql_slice("len", ip)) break :hf true;
                    break :hf false;
                },
                else => {},
            },
            .anon_struct_type => |anon_struct| {
                if (anon_struct.names.len != 0) {
                    break :hf mem.index_of_scalar(InternPool.NullTerminatedString, anon_struct.names.get(ip), field_name) != null;
                } else {
                    const field_index = field_name.to_unsigned(ip) orelse break :hf false;
                    break :hf field_index < ty.struct_field_count(mod);
                }
            },
            .struct_type => {
                break :hf ip.load_struct_type(ty.to_intern()).name_index(ip, field_name) != null;
            },
            .union_type => {
                const union_type = ip.load_union_type(ty.to_intern());
                break :hf union_type.load_tag_type(ip).name_index(ip, field_name) != null;
            },
            .enum_type => {
                break :hf ip.load_enum_type(ty.to_intern()).name_index(ip, field_name) != null;
            },
            .array_type => break :hf field_name.eql_slice("len", ip),
            else => {},
        }
        return sema.fail(block, ty_src, "type '{}' does not support '@has_field'", .{
            ty.fmt(mod),
        });
    };
    return if (has_field) .bool_true else .bool_false;
}

fn zir_has_decl(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const src = inst_data.src();
    const lhs_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const container_type = try sema.resolve_type(block, lhs_src, extra.lhs);
    const decl_name = try sema.resolve_const_string_intern(block, rhs_src, extra.rhs, .{
        .needed_comptime_reason = "decl name must be comptime-known",
    });

    try sema.check_namespace_type(block, lhs_src, container_type);
    if (container_type.type_decl_inst(mod)) |type_decl_inst| {
        try sema.declare_dependency(.{ .namespace_name = .{
            .namespace = type_decl_inst,
            .name = decl_name,
        } });
    }

    const namespace = container_type.get_namespace_index(mod);
    if (try sema.lookup_in_namespace(block, src, namespace, decl_name, true)) |decl_index| {
        const decl = mod.decl_ptr(decl_index);
        if (decl.is_pub or decl.get_file_scope(mod) == block.get_file_scope(mod)) {
            return .bool_true;
        }
    }
    return .bool_false;
}

fn zir_import(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].str_tok;
    const operand_src = inst_data.src();
    const operand = inst_data.get(sema.code);

    const result = mod.import_file(block.get_file_scope(mod), operand) catch |err| switch (err) {
        error.ImportOutsideModulePath => {
            return sema.fail(block, operand_src, "import of file outside module path: '{s}'", .{operand});
        },
        error.ModuleNotFound => {
            return sema.fail(block, operand_src, "no module named '{s}' available within module {s}", .{
                operand, block.get_file_scope(mod).mod.fully_qualified_name,
            });
        },
        else => {
            // TODO: these errors are file system errors; make sure an update() will
            // retry this and not cache the file system error, which may be transient.
            return sema.fail(block, operand_src, "unable to open '{s}': {s}", .{ operand, @errorName(err) });
        },
    };
    try mod.ensure_file_analyzed(result.file);
    const file_root_decl_index = result.file.root_decl.unwrap().?;
    return sema.analyze_decl_val(block, operand_src, file_root_decl_index);
}

fn zir_embed_file(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const name = try sema.resolve_const_string(block, operand_src, inst_data.operand, .{
        .needed_comptime_reason = "file path name must be comptime-known",
    });

    if (name.len == 0) {
        return sema.fail(block, operand_src, "file path name cannot be empty", .{});
    }

    const src_loc = mod.decl_ptr(block.src_decl).to_src_loc(operand_src, mod);
    const val = mod.embed_file(block.get_file_scope(mod), name, src_loc) catch |err| switch (err) {
        error.ImportOutsideModulePath => {
            return sema.fail(block, operand_src, "embed of file outside package path: '{s}'", .{name});
        },
        else => {
            // TODO: these errors are file system errors; make sure an update() will
            // retry this and not cache the file system error, which may be transient.
            return sema.fail(block, operand_src, "unable to open '{s}': {s}", .{ name, @errorName(err) });
        },
    };

    return Air.interned_to_ref(val);
}

fn zir_ret_err_value_code(sema: *Sema, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].str_tok;
    const name = try mod.intern_pool.get_or_put_string(
        sema.gpa,
        inst_data.get(sema.code),
        .no_embedded_nulls,
    );
    _ = try mod.get_error_value(name);
    const error_set_type = try mod.single_error_set_type(name);
    return Air.interned_to_ref((try mod.intern(.{ .err = .{
        .ty = error_set_type.to_intern(),
        .name = name,
    } })));
}

fn zir_shl(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    air_tag: Air.Inst.Tag,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const lhs = try sema.resolve_inst(extra.lhs);
    const rhs = try sema.resolve_inst(extra.rhs);
    const lhs_ty = sema.type_of(lhs);
    const rhs_ty = sema.type_of(rhs);
    try sema.check_vectorizable_binary_operands(block, src, lhs_ty, rhs_ty, lhs_src, rhs_src);

    const scalar_ty = lhs_ty.scalar_type(mod);
    const scalar_rhs_ty = rhs_ty.scalar_type(mod);

    // TODO coerce rhs if air_tag is not shl_sat
    const rhs_is_comptime_int = try sema.check_int_type(block, rhs_src, scalar_rhs_ty);

    const maybe_lhs_val = try sema.resolve_value_intable(lhs);
    const maybe_rhs_val = try sema.resolve_value_intable(rhs);

    if (maybe_rhs_val) |rhs_val| {
        if (rhs_val.is_undef(mod)) {
            return mod.undef_ref(sema.type_of(lhs));
        }
        // If rhs is 0, return lhs without doing any calculations.
        if (try rhs_val.compare_all_with_zero_advanced(.eq, sema)) {
            return lhs;
        }
        if (scalar_ty.zig_type_tag(mod) != .ComptimeInt and air_tag != .shl_sat) {
            const bit_value = try mod.int_value(Type.comptime_int, scalar_ty.int_info(mod).bits);
            if (rhs_ty.zig_type_tag(mod) == .Vector) {
                var i: usize = 0;
                while (i < rhs_ty.vector_len(mod)) : (i += 1) {
                    const rhs_elem = try rhs_val.elem_value(mod, i);
                    if (rhs_elem.compare_hetero(.gte, bit_value, mod)) {
                        return sema.fail(block, rhs_src, "shift amount '{}' at index '{d}' is too large for operand type '{}'", .{
                            rhs_elem.fmt_value(mod, sema),
                            i,
                            scalar_ty.fmt(mod),
                        });
                    }
                }
            } else if (rhs_val.compare_hetero(.gte, bit_value, mod)) {
                return sema.fail(block, rhs_src, "shift amount '{}' is too large for operand type '{}'", .{
                    rhs_val.fmt_value(mod, sema),
                    scalar_ty.fmt(mod),
                });
            }
        }
        if (rhs_ty.zig_type_tag(mod) == .Vector) {
            var i: usize = 0;
            while (i < rhs_ty.vector_len(mod)) : (i += 1) {
                const rhs_elem = try rhs_val.elem_value(mod, i);
                if (rhs_elem.compare_hetero(.lt, try mod.int_value(scalar_rhs_ty, 0), mod)) {
                    return sema.fail(block, rhs_src, "shift by negative amount '{}' at index '{d}'", .{
                        rhs_elem.fmt_value(mod, sema),
                        i,
                    });
                }
            }
        } else if (rhs_val.compare_hetero(.lt, try mod.int_value(rhs_ty, 0), mod)) {
            return sema.fail(block, rhs_src, "shift by negative amount '{}'", .{
                rhs_val.fmt_value(mod, sema),
            });
        }
    }

    const runtime_src = if (maybe_lhs_val) |lhs_val| rs: {
        if (lhs_val.is_undef(mod)) return mod.undef_ref(lhs_ty);
        const rhs_val = maybe_rhs_val orelse {
            if (scalar_ty.zig_type_tag(mod) == .ComptimeInt) {
                return sema.fail(block, src, "LHS of shift must be a fixed-width integer type, or RHS must be comptime-known", .{});
            }
            break :rs rhs_src;
        };
        const val = if (scalar_ty.zig_type_tag(mod) == .ComptimeInt)
            try lhs_val.shl(rhs_val, lhs_ty, sema.arena, mod)
        else switch (air_tag) {
            .shl_exact => val: {
                const shifted = try lhs_val.shl_with_overflow(rhs_val, lhs_ty, sema.arena, mod);
                if (shifted.overflow_bit.compare_all_with_zero(.eq, mod)) {
                    break :val shifted.wrapped_result;
                }
                return sema.fail(block, src, "operation caused overflow", .{});
            },
            .shl_sat => try lhs_val.shl_sat(rhs_val, lhs_ty, sema.arena, mod),
            .shl => try lhs_val.shl_trunc(rhs_val, lhs_ty, sema.arena, mod),
            else => unreachable,
        };
        return Air.interned_to_ref(val.to_intern());
    } else lhs_src;

    const new_rhs = if (air_tag == .shl_sat) rhs: {
        // Limit the RHS type for saturating shl to be an integer as small as the LHS.
        if (rhs_is_comptime_int or
            scalar_rhs_ty.int_info(mod).bits > scalar_ty.int_info(mod).bits)
        {
            const max_int = Air.interned_to_ref((try lhs_ty.max_int(mod, lhs_ty)).to_intern());
            const rhs_limited = try sema.analyze_min_max(block, rhs_src, .min, &.{ rhs, max_int }, &.{ rhs_src, rhs_src });
            break :rhs try sema.int_cast(block, src, lhs_ty, rhs_src, rhs_limited, rhs_src, false);
        } else {
            break :rhs rhs;
        }
    } else rhs;

    try sema.require_runtime_block(block, src, runtime_src);
    if (block.want_safety()) {
        const bit_count = scalar_ty.int_info(mod).bits;
        if (!std.math.is_power_of_two(bit_count)) {
            const bit_count_val = try mod.int_value(scalar_rhs_ty, bit_count);
            const ok = if (rhs_ty.zig_type_tag(mod) == .Vector) ok: {
                const bit_count_inst = Air.interned_to_ref((try sema.splat(rhs_ty, bit_count_val)).to_intern());
                const lt = try block.add_cmp_vector(rhs, bit_count_inst, .lt);
                break :ok try block.add_inst(.{
                    .tag = .reduce,
                    .data = .{ .reduce = .{
                        .operand = lt,
                        .operation = .And,
                    } },
                });
            } else ok: {
                const bit_count_inst = Air.interned_to_ref(bit_count_val.to_intern());
                break :ok try block.add_bin_op(.cmp_lt, rhs, bit_count_inst);
            };
            try sema.add_safety_check(block, src, ok, .shift_rhs_too_big);
        }

        if (air_tag == .shl_exact) {
            const op_ov_tuple_ty = try sema.overflow_arithmetic_tuple_type(lhs_ty);
            const op_ov = try block.add_inst(.{
                .tag = .shl_with_overflow,
                .data = .{ .ty_pl = .{
                    .ty = Air.interned_to_ref(op_ov_tuple_ty.to_intern()),
                    .payload = try sema.add_extra(Air.Bin{
                        .lhs = lhs,
                        .rhs = rhs,
                    }),
                } },
            });
            const ov_bit = try sema.tuple_field_val_by_index(block, src, op_ov, 1, op_ov_tuple_ty);
            const any_ov_bit = if (lhs_ty.zig_type_tag(mod) == .Vector)
                try block.add_inst(.{
                    .tag = if (block.float_mode == .optimized) .reduce_optimized else .reduce,
                    .data = .{ .reduce = .{
                        .operand = ov_bit,
                        .operation = .Or,
                    } },
                })
            else
                ov_bit;
            const zero_ov = Air.interned_to_ref((try mod.int_value(Type.u1, 0)).to_intern());
            const no_ov = try block.add_bin_op(.cmp_eq, any_ov_bit, zero_ov);

            try sema.add_safety_check(block, src, no_ov, .shl_overflow);
            return sema.tuple_field_val_by_index(block, src, op_ov, 0, op_ov_tuple_ty);
        }
    }
    return block.add_bin_op(air_tag, lhs, new_rhs);
}

fn zir_shr(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    air_tag: Air.Inst.Tag,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const lhs = try sema.resolve_inst(extra.lhs);
    const rhs = try sema.resolve_inst(extra.rhs);
    const lhs_ty = sema.type_of(lhs);
    const rhs_ty = sema.type_of(rhs);
    try sema.check_vectorizable_binary_operands(block, src, lhs_ty, rhs_ty, lhs_src, rhs_src);
    const scalar_ty = lhs_ty.scalar_type(mod);

    const maybe_lhs_val = try sema.resolve_value_intable(lhs);
    const maybe_rhs_val = try sema.resolve_value_intable(rhs);

    const runtime_src = if (maybe_rhs_val) |rhs_val| rs: {
        if (rhs_val.is_undef(mod)) {
            return mod.undef_ref(lhs_ty);
        }
        // If rhs is 0, return lhs without doing any calculations.
        if (try rhs_val.compare_all_with_zero_advanced(.eq, sema)) {
            return lhs;
        }
        if (scalar_ty.zig_type_tag(mod) != .ComptimeInt) {
            const bit_value = try mod.int_value(Type.comptime_int, scalar_ty.int_info(mod).bits);
            if (rhs_ty.zig_type_tag(mod) == .Vector) {
                var i: usize = 0;
                while (i < rhs_ty.vector_len(mod)) : (i += 1) {
                    const rhs_elem = try rhs_val.elem_value(mod, i);
                    if (rhs_elem.compare_hetero(.gte, bit_value, mod)) {
                        return sema.fail(block, rhs_src, "shift amount '{}' at index '{d}' is too large for operand type '{}'", .{
                            rhs_elem.fmt_value(mod, sema),
                            i,
                            scalar_ty.fmt(mod),
                        });
                    }
                }
            } else if (rhs_val.compare_hetero(.gte, bit_value, mod)) {
                return sema.fail(block, rhs_src, "shift amount '{}' is too large for operand type '{}'", .{
                    rhs_val.fmt_value(mod, sema),
                    scalar_ty.fmt(mod),
                });
            }
        }
        if (rhs_ty.zig_type_tag(mod) == .Vector) {
            var i: usize = 0;
            while (i < rhs_ty.vector_len(mod)) : (i += 1) {
                const rhs_elem = try rhs_val.elem_value(mod, i);
                if (rhs_elem.compare_hetero(.lt, try mod.int_value(rhs_ty.child_type(mod), 0), mod)) {
                    return sema.fail(block, rhs_src, "shift by negative amount '{}' at index '{d}'", .{
                        rhs_elem.fmt_value(mod, sema),
                        i,
                    });
                }
            }
        } else if (rhs_val.compare_hetero(.lt, try mod.int_value(rhs_ty, 0), mod)) {
            return sema.fail(block, rhs_src, "shift by negative amount '{}'", .{
                rhs_val.fmt_value(mod, sema),
            });
        }
        if (maybe_lhs_val) |lhs_val| {
            if (lhs_val.is_undef(mod)) {
                return mod.undef_ref(lhs_ty);
            }
            if (air_tag == .shr_exact) {
                // Detect if any ones would be shifted out.
                const truncated = try lhs_val.int_trunc_bits_as_value(lhs_ty, sema.arena, .unsigned, rhs_val, mod);
                if (!(try truncated.compare_all_with_zero_advanced(.eq, sema))) {
                    return sema.fail(block, src, "exact shift shifted out 1 bits", .{});
                }
            }
            const val = try lhs_val.shr(rhs_val, lhs_ty, sema.arena, mod);
            return Air.interned_to_ref(val.to_intern());
        } else {
            break :rs lhs_src;
        }
    } else rhs_src;

    if (maybe_rhs_val == null and scalar_ty.zig_type_tag(mod) == .ComptimeInt) {
        return sema.fail(block, src, "LHS of shift must be a fixed-width integer type, or RHS must be comptime-known", .{});
    }

    try sema.require_runtime_block(block, src, runtime_src);
    const result = try block.add_bin_op(air_tag, lhs, rhs);
    if (block.want_safety()) {
        const bit_count = scalar_ty.int_info(mod).bits;
        if (!std.math.is_power_of_two(bit_count)) {
            const bit_count_val = try mod.int_value(rhs_ty.scalar_type(mod), bit_count);

            const ok = if (rhs_ty.zig_type_tag(mod) == .Vector) ok: {
                const bit_count_inst = Air.interned_to_ref((try sema.splat(rhs_ty, bit_count_val)).to_intern());
                const lt = try block.add_cmp_vector(rhs, bit_count_inst, .lt);
                break :ok try block.add_inst(.{
                    .tag = .reduce,
                    .data = .{ .reduce = .{
                        .operand = lt,
                        .operation = .And,
                    } },
                });
            } else ok: {
                const bit_count_inst = Air.interned_to_ref(bit_count_val.to_intern());
                break :ok try block.add_bin_op(.cmp_lt, rhs, bit_count_inst);
            };
            try sema.add_safety_check(block, src, ok, .shift_rhs_too_big);
        }

        if (air_tag == .shr_exact) {
            const back = try block.add_bin_op(.shl, result, rhs);

            const ok = if (rhs_ty.zig_type_tag(mod) == .Vector) ok: {
                const eql = try block.add_cmp_vector(lhs, back, .eq);
                break :ok try block.add_inst(.{
                    .tag = if (block.float_mode == .optimized) .reduce_optimized else .reduce,
                    .data = .{ .reduce = .{
                        .operand = eql,
                        .operation = .And,
                    } },
                });
            } else try block.add_bin_op(.cmp_eq, lhs, back);
            try sema.add_safety_check(block, src, ok, .shr_overflow);
        }
    }
    return result;
}

fn zir_bitwise(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    air_tag: Air.Inst.Tag,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src: LazySrcLoc = .{ .node_offset_bin_op = inst_data.src_node };
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const lhs = try sema.resolve_inst(extra.lhs);
    const rhs = try sema.resolve_inst(extra.rhs);
    const lhs_ty = sema.type_of(lhs);
    const rhs_ty = sema.type_of(rhs);
    try sema.check_vectorizable_binary_operands(block, src, lhs_ty, rhs_ty, lhs_src, rhs_src);

    const instructions = &[_]Air.Inst.Ref{ lhs, rhs };
    const resolved_type = try sema.resolve_peer_types(block, src, instructions, .{ .override = &[_]?LazySrcLoc{ lhs_src, rhs_src } });
    const scalar_type = resolved_type.scalar_type(mod);
    const scalar_tag = scalar_type.zig_type_tag(mod);

    const casted_lhs = try sema.coerce(block, resolved_type, lhs, lhs_src);
    const casted_rhs = try sema.coerce(block, resolved_type, rhs, rhs_src);

    const is_int = scalar_tag == .Int or scalar_tag == .ComptimeInt;

    if (!is_int) {
        return sema.fail(block, src, "invalid operands to binary bitwise expression: '{s}' and '{s}'", .{ @tag_name(lhs_ty.zig_type_tag(mod)), @tag_name(rhs_ty.zig_type_tag(mod)) });
    }

    const runtime_src = runtime: {
        // TODO: ask the linker what kind of relocations are available, and
        // in some cases emit a Value that means "this decl's address AND'd with this operand".
        if (try sema.resolve_value_intable(casted_lhs)) |lhs_val| {
            if (try sema.resolve_value_intable(casted_rhs)) |rhs_val| {
                const result_val = switch (air_tag) {
                    .bit_and => try lhs_val.bitwise_and(rhs_val, resolved_type, sema.arena, mod),
                    .bit_or => try lhs_val.bitwise_or(rhs_val, resolved_type, sema.arena, mod),
                    .xor => try lhs_val.bitwise_xor(rhs_val, resolved_type, sema.arena, mod),
                    else => unreachable,
                };
                return Air.interned_to_ref(result_val.to_intern());
            } else {
                break :runtime rhs_src;
            }
        } else {
            break :runtime lhs_src;
        }
    };

    try sema.require_runtime_block(block, src, runtime_src);
    return block.add_bin_op(air_tag, casted_lhs, casted_rhs);
}

fn zir_bit_not(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_un_op = inst_data.src_node };

    const operand = try sema.resolve_inst(inst_data.operand);
    const operand_type = sema.type_of(operand);
    const scalar_type = operand_type.scalar_type(mod);

    if (scalar_type.zig_type_tag(mod) != .Int) {
        return sema.fail(block, src, "unable to perform binary not operation on type '{}'", .{
            operand_type.fmt(mod),
        });
    }

    if (try sema.resolve_value(operand)) |val| {
        if (val.is_undef(mod)) {
            return mod.undef_ref(operand_type);
        } else if (operand_type.zig_type_tag(mod) == .Vector) {
            const vec_len = try sema.usize_cast(block, operand_src, operand_type.vector_len(mod));
            const elems = try sema.arena.alloc(InternPool.Index, vec_len);
            for (elems, 0..) |*elem, i| {
                const elem_val = try val.elem_value(mod, i);
                elem.* = (try elem_val.bitwise_not(scalar_type, sema.arena, mod)).to_intern();
            }
            return Air.interned_to_ref((try mod.intern(.{ .aggregate = .{
                .ty = operand_type.to_intern(),
                .storage = .{ .elems = elems },
            } })));
        } else {
            const result_val = try val.bitwise_not(operand_type, sema.arena, mod);
            return Air.interned_to_ref(result_val.to_intern());
        }
    }

    try sema.require_runtime_block(block, src, null);
    return block.add_ty_op(.not, operand_type, operand);
}

fn analyze_tuple_cat(
    sema: *Sema,
    block: *Block,
    src_node: i32,
    lhs: Air.Inst.Ref,
    rhs: Air.Inst.Ref,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const lhs_ty = sema.type_of(lhs);
    const rhs_ty = sema.type_of(rhs);
    const src = LazySrcLoc.nodeOffset(src_node);

    const lhs_len = lhs_ty.struct_field_count(mod);
    const rhs_len = rhs_ty.struct_field_count(mod);
    const dest_fields = lhs_len + rhs_len;

    if (dest_fields == 0) {
        return Air.interned_to_ref(Value.empty_struct.to_intern());
    }
    if (lhs_len == 0) {
        return rhs;
    }
    if (rhs_len == 0) {
        return lhs;
    }
    const final_len = try sema.usize_cast(block, src, dest_fields);

    const types = try sema.arena.alloc(InternPool.Index, final_len);
    const values = try sema.arena.alloc(InternPool.Index, final_len);

    const opt_runtime_src = rs: {
        var runtime_src: ?LazySrcLoc = null;
        var i: u32 = 0;
        while (i < lhs_len) : (i += 1) {
            types[i] = lhs_ty.struct_field_type(i, mod).to_intern();
            const default_val = lhs_ty.struct_field_default_value(i, mod);
            values[i] = default_val.to_intern();
            const operand_src: LazySrcLoc = .{ .array_cat_lhs = .{
                .array_cat_offset = src_node,
                .elem_index = i,
            } };
            if (default_val.to_intern() == .unreachable_value) {
                runtime_src = operand_src;
                values[i] = .none;
            }
        }
        i = 0;
        while (i < rhs_len) : (i += 1) {
            types[i + lhs_len] = rhs_ty.struct_field_type(i, mod).to_intern();
            const default_val = rhs_ty.struct_field_default_value(i, mod);
            values[i + lhs_len] = default_val.to_intern();
            const operand_src: LazySrcLoc = .{ .array_cat_rhs = .{
                .array_cat_offset = src_node,
                .elem_index = i,
            } };
            if (default_val.to_intern() == .unreachable_value) {
                runtime_src = operand_src;
                values[i + lhs_len] = .none;
            }
        }
        break :rs runtime_src;
    };

    const tuple_ty = try mod.intern_pool.get_anon_struct_type(mod.gpa, .{
        .types = types,
        .values = values,
        .names = &.{},
    });

    const runtime_src = opt_runtime_src orelse {
        const tuple_val = try mod.intern(.{ .aggregate = .{
            .ty = tuple_ty,
            .storage = .{ .elems = values },
        } });
        return Air.interned_to_ref(tuple_val);
    };

    try sema.require_runtime_block(block, src, runtime_src);

    const element_refs = try sema.arena.alloc(Air.Inst.Ref, final_len);
    var i: u32 = 0;
    while (i < lhs_len) : (i += 1) {
        const operand_src: LazySrcLoc = .{ .array_cat_lhs = .{
            .array_cat_offset = src_node,
            .elem_index = i,
        } };
        element_refs[i] = try sema.tuple_field_val_by_index(block, operand_src, lhs, i, lhs_ty);
    }
    i = 0;
    while (i < rhs_len) : (i += 1) {
        const operand_src: LazySrcLoc = .{ .array_cat_rhs = .{
            .array_cat_offset = src_node,
            .elem_index = i,
        } };
        element_refs[i + lhs_len] =
            try sema.tuple_field_val_by_index(block, operand_src, rhs, i, rhs_ty);
    }

    return block.add_aggregate_init(Type.from_interned(tuple_ty), element_refs);
}

fn zir_array_cat(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const lhs = try sema.resolve_inst(extra.lhs);
    const rhs = try sema.resolve_inst(extra.rhs);
    const lhs_ty = sema.type_of(lhs);
    const rhs_ty = sema.type_of(rhs);
    const src = inst_data.src();

    const lhs_is_tuple = lhs_ty.is_tuple(mod);
    const rhs_is_tuple = rhs_ty.is_tuple(mod);
    if (lhs_is_tuple and rhs_is_tuple) {
        return sema.analyze_tuple_cat(block, inst_data.src_node, lhs, rhs);
    }

    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };

    const lhs_info = try sema.get_array_cat_info(block, lhs_src, lhs, rhs_ty) orelse lhs_info: {
        if (lhs_is_tuple) break :lhs_info @as(Type.ArrayInfo, undefined);
        return sema.fail(block, lhs_src, "expected indexable; found '{}'", .{lhs_ty.fmt(mod)});
    };
    const rhs_info = try sema.get_array_cat_info(block, rhs_src, rhs, lhs_ty) orelse {
        assert(!rhs_is_tuple);
        return sema.fail(block, rhs_src, "expected indexable; found '{}'", .{rhs_ty.fmt(mod)});
    };

    const resolved_elem_ty = t: {
        var trash_block = block.make_sub_block();
        trash_block.is_comptime = false;
        defer trash_block.instructions.deinit(sema.gpa);

        const instructions = [_]Air.Inst.Ref{
            try trash_block.add_bit_cast(lhs_info.elem_type, .void_value),
            try trash_block.add_bit_cast(rhs_info.elem_type, .void_value),
        };
        break :t try sema.resolve_peer_types(block, src, &instructions, .{
            .override = &[_]?LazySrcLoc{ lhs_src, rhs_src },
        });
    };

    // When there is a sentinel mismatch, no sentinel on the result.
    // Otherwise, use the sentinel value provided by either operand,
    // coercing it to the peer-resolved element type.
    const res_sent_val: ?Value = s: {
        if (lhs_info.sentinel) |lhs_sent_val| {
            const lhs_sent = Air.interned_to_ref(lhs_sent_val.to_intern());
            if (rhs_info.sentinel) |rhs_sent_val| {
                const rhs_sent = Air.interned_to_ref(rhs_sent_val.to_intern());
                const lhs_sent_casted = try sema.coerce(block, resolved_elem_ty, lhs_sent, lhs_src);
                const rhs_sent_casted = try sema.coerce(block, resolved_elem_ty, rhs_sent, rhs_src);
                const lhs_sent_casted_val = (try sema.resolve_defined_value(block, lhs_src, lhs_sent_casted)).?;
                const rhs_sent_casted_val = (try sema.resolve_defined_value(block, rhs_src, rhs_sent_casted)).?;
                if (try sema.values_equal(lhs_sent_casted_val, rhs_sent_casted_val, resolved_elem_ty)) {
                    break :s lhs_sent_casted_val;
                } else {
                    break :s null;
                }
            } else {
                const lhs_sent_casted = try sema.coerce(block, resolved_elem_ty, lhs_sent, lhs_src);
                const lhs_sent_casted_val = (try sema.resolve_defined_value(block, lhs_src, lhs_sent_casted)).?;
                break :s lhs_sent_casted_val;
            }
        } else {
            if (rhs_info.sentinel) |rhs_sent_val| {
                const rhs_sent = Air.interned_to_ref(rhs_sent_val.to_intern());
                const rhs_sent_casted = try sema.coerce(block, resolved_elem_ty, rhs_sent, rhs_src);
                const rhs_sent_casted_val = (try sema.resolve_defined_value(block, rhs_src, rhs_sent_casted)).?;
                break :s rhs_sent_casted_val;
            } else {
                break :s null;
            }
        }
    };

    const lhs_len = try sema.usize_cast(block, lhs_src, lhs_info.len);
    const rhs_len = try sema.usize_cast(block, rhs_src, rhs_info.len);
    const result_len = std.math.add(usize, lhs_len, rhs_len) catch |err| switch (err) {
        error.Overflow => return sema.fail(
            block,
            src,
            "concatenating arrays of length {d} and {d} produces an array too large for this compiler implementation to handle",
            .{ lhs_len, rhs_len },
        ),
    };

    const result_ty = try mod.array_type(.{
        .len = result_len,
        .sentinel = if (res_sent_val) |v| v.to_intern() else .none,
        .child = resolved_elem_ty.to_intern(),
    });
    const ptr_addrspace = p: {
        if (lhs_ty.zig_type_tag(mod) == .Pointer) break :p lhs_ty.ptr_address_space(mod);
        if (rhs_ty.zig_type_tag(mod) == .Pointer) break :p rhs_ty.ptr_address_space(mod);
        break :p null;
    };

    const runtime_src = if (switch (lhs_ty.zig_type_tag(mod)) {
        .Array, .Struct => try sema.resolve_value(lhs),
        .Pointer => try sema.resolve_defined_value(block, lhs_src, lhs),
        else => unreachable,
    }) |lhs_val| rs: {
        if (switch (rhs_ty.zig_type_tag(mod)) {
            .Array, .Struct => try sema.resolve_value(rhs),
            .Pointer => try sema.resolve_defined_value(block, rhs_src, rhs),
            else => unreachable,
        }) |rhs_val| {
            const lhs_sub_val = if (lhs_ty.is_single_pointer(mod))
                try sema.pointer_deref(block, lhs_src, lhs_val, lhs_ty) orelse break :rs lhs_src
            else if (lhs_ty.is_slice(mod))
                try sema.maybe_deref_slice_as_array(block, lhs_src, lhs_val) orelse break :rs lhs_src
            else
                lhs_val;

            const rhs_sub_val = if (rhs_ty.is_single_pointer(mod))
                try sema.pointer_deref(block, rhs_src, rhs_val, rhs_ty) orelse break :rs rhs_src
            else if (rhs_ty.is_slice(mod))
                try sema.maybe_deref_slice_as_array(block, rhs_src, rhs_val) orelse break :rs rhs_src
            else
                rhs_val;

            const element_vals = try sema.arena.alloc(InternPool.Index, result_len);
            var elem_i: u32 = 0;
            while (elem_i < lhs_len) : (elem_i += 1) {
                const lhs_elem_i = elem_i;
                const elem_default_val = if (lhs_is_tuple) lhs_ty.struct_field_default_value(lhs_elem_i, mod) else Value.@"unreachable";
                const elem_val = if (elem_default_val.to_intern() == .unreachable_value) try lhs_sub_val.elem_value(mod, lhs_elem_i) else elem_default_val;
                const elem_val_inst = Air.interned_to_ref(elem_val.to_intern());
                const operand_src: LazySrcLoc = .{ .array_cat_lhs = .{
                    .array_cat_offset = inst_data.src_node,
                    .elem_index = elem_i,
                } };
                const coerced_elem_val_inst = try sema.coerce(block, resolved_elem_ty, elem_val_inst, operand_src);
                const coerced_elem_val = try sema.resolve_const_value(block, operand_src, coerced_elem_val_inst, undefined);
                element_vals[elem_i] = coerced_elem_val.to_intern();
            }
            while (elem_i < result_len) : (elem_i += 1) {
                const rhs_elem_i = elem_i - lhs_len;
                const elem_default_val = if (rhs_is_tuple) rhs_ty.struct_field_default_value(rhs_elem_i, mod) else Value.@"unreachable";
                const elem_val = if (elem_default_val.to_intern() == .unreachable_value) try rhs_sub_val.elem_value(mod, rhs_elem_i) else elem_default_val;
                const elem_val_inst = Air.interned_to_ref(elem_val.to_intern());
                const operand_src: LazySrcLoc = .{ .array_cat_rhs = .{
                    .array_cat_offset = inst_data.src_node,
                    .elem_index = @int_cast(rhs_elem_i),
                } };
                const coerced_elem_val_inst = try sema.coerce(block, resolved_elem_ty, elem_val_inst, operand_src);
                const coerced_elem_val = try sema.resolve_const_value(block, operand_src, coerced_elem_val_inst, undefined);
                element_vals[elem_i] = coerced_elem_val.to_intern();
            }
            return sema.add_constant_maybe_ref(try mod.intern(.{ .aggregate = .{
                .ty = result_ty.to_intern(),
                .storage = .{ .elems = element_vals },
            } }), ptr_addrspace != null);
        } else break :rs rhs_src;
    } else lhs_src;

    try sema.require_runtime_block(block, src, runtime_src);

    if (ptr_addrspace) |ptr_as| {
        const alloc_ty = try sema.ptr_type(.{
            .child = result_ty.to_intern(),
            .flags = .{ .address_space = ptr_as },
        });
        const alloc = try block.add_ty(.alloc, alloc_ty);
        const elem_ptr_ty = try sema.ptr_type(.{
            .child = resolved_elem_ty.to_intern(),
            .flags = .{ .address_space = ptr_as },
        });

        var elem_i: u32 = 0;
        while (elem_i < lhs_len) : (elem_i += 1) {
            const elem_index = try mod.int_ref(Type.usize, elem_i);
            const elem_ptr = try block.add_ptr_elem_ptr(alloc, elem_index, elem_ptr_ty);
            const operand_src: LazySrcLoc = .{ .array_cat_lhs = .{
                .array_cat_offset = inst_data.src_node,
                .elem_index = elem_i,
            } };
            const init = try sema.elem_val(block, operand_src, lhs, elem_index, src, true);
            try sema.store_ptr2(block, src, elem_ptr, src, init, operand_src, .store);
        }
        while (elem_i < result_len) : (elem_i += 1) {
            const rhs_elem_i = elem_i - lhs_len;
            const elem_index = try mod.int_ref(Type.usize, elem_i);
            const rhs_index = try mod.int_ref(Type.usize, rhs_elem_i);
            const elem_ptr = try block.add_ptr_elem_ptr(alloc, elem_index, elem_ptr_ty);
            const operand_src: LazySrcLoc = .{ .array_cat_rhs = .{
                .array_cat_offset = inst_data.src_node,
                .elem_index = @int_cast(rhs_elem_i),
            } };
            const init = try sema.elem_val(block, operand_src, rhs, rhs_index, src, true);
            try sema.store_ptr2(block, src, elem_ptr, src, init, operand_src, .store);
        }
        if (res_sent_val) |sent_val| {
            const elem_index = try mod.int_ref(Type.usize, result_len);
            const elem_ptr = try block.add_ptr_elem_ptr(alloc, elem_index, elem_ptr_ty);
            const init = Air.interned_to_ref((try mod.get_coerced(sent_val, lhs_info.elem_type)).to_intern());
            try sema.store_ptr2(block, src, elem_ptr, src, init, lhs_src, .store);
        }

        return alloc;
    }

    const element_refs = try sema.arena.alloc(Air.Inst.Ref, result_len);
    {
        var elem_i: u32 = 0;
        while (elem_i < lhs_len) : (elem_i += 1) {
            const index = try mod.int_ref(Type.usize, elem_i);
            const operand_src: LazySrcLoc = .{ .array_cat_lhs = .{
                .array_cat_offset = inst_data.src_node,
                .elem_index = elem_i,
            } };
            const init = try sema.elem_val(block, operand_src, lhs, index, src, true);
            element_refs[elem_i] = try sema.coerce(block, resolved_elem_ty, init, operand_src);
        }
        while (elem_i < result_len) : (elem_i += 1) {
            const rhs_elem_i = elem_i - lhs_len;
            const index = try mod.int_ref(Type.usize, rhs_elem_i);
            const operand_src: LazySrcLoc = .{ .array_cat_rhs = .{
                .array_cat_offset = inst_data.src_node,
                .elem_index = @int_cast(rhs_elem_i),
            } };
            const init = try sema.elem_val(block, operand_src, rhs, index, src, true);
            element_refs[elem_i] = try sema.coerce(block, resolved_elem_ty, init, operand_src);
        }
    }

    return block.add_aggregate_init(result_ty, element_refs);
}

fn get_array_cat_info(sema: *Sema, block: *Block, src: LazySrcLoc, operand: Air.Inst.Ref, peer_ty: Type) !?Type.ArrayInfo {
    const mod = sema.mod;
    const operand_ty = sema.type_of(operand);
    switch (operand_ty.zig_type_tag(mod)) {
        .Array => return operand_ty.array_info(mod),
        .Pointer => {
            const ptr_info = operand_ty.ptr_info(mod);
            switch (ptr_info.flags.size) {
                .Slice => {
                    const val = try sema.resolve_const_defined_value(block, src, operand, .{
                        .needed_comptime_reason = "slice value being concatenated must be comptime-known",
                    });
                    return Type.ArrayInfo{
                        .elem_type = Type.from_interned(ptr_info.child),
                        .sentinel = switch (ptr_info.sentinel) {
                            .none => null,
                            else => Value.from_interned(ptr_info.sentinel),
                        },
                        .len = try val.slice_len(sema),
                    };
                },
                .One => {
                    if (Type.from_interned(ptr_info.child).zig_type_tag(mod) == .Array) {
                        return Type.from_interned(ptr_info.child).array_info(mod);
                    }
                },
                .C, .Many => {},
            }
        },
        .Struct => {
            if (operand_ty.is_tuple(mod) and peer_ty.is_indexable(mod)) {
                assert(!peer_ty.is_tuple(mod));
                return .{
                    .elem_type = peer_ty.elem_type2(mod),
                    .sentinel = null,
                    .len = operand_ty.array_len(mod),
                };
            }
        },
        else => {},
    }
    return null;
}

fn analyze_tuple_mul(
    sema: *Sema,
    block: *Block,
    src_node: i32,
    operand: Air.Inst.Ref,
    factor: usize,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const operand_ty = sema.type_of(operand);
    const src = LazySrcLoc.nodeOffset(src_node);
    const len_src: LazySrcLoc = .{ .node_offset_bin_rhs = src_node };

    const tuple_len = operand_ty.struct_field_count(mod);
    const final_len = std.math.mul(usize, tuple_len, factor) catch
        return sema.fail(block, len_src, "operation results in overflow", .{});

    if (final_len == 0) {
        return Air.interned_to_ref(Value.empty_struct.to_intern());
    }
    const types = try sema.arena.alloc(InternPool.Index, final_len);
    const values = try sema.arena.alloc(InternPool.Index, final_len);

    const opt_runtime_src = rs: {
        var runtime_src: ?LazySrcLoc = null;
        for (0..tuple_len) |i| {
            types[i] = operand_ty.struct_field_type(i, mod).to_intern();
            values[i] = operand_ty.struct_field_default_value(i, mod).to_intern();
            const operand_src: LazySrcLoc = .{ .array_cat_lhs = .{
                .array_cat_offset = src_node,
                .elem_index = @int_cast(i),
            } };
            if (values[i] == .unreachable_value) {
                runtime_src = operand_src;
                values[i] = .none; // TODO don't treat unreachable_value as special
            }
        }
        for (0..factor) |i| {
            mem.copy_forwards(InternPool.Index, types[tuple_len * i ..], types[0..tuple_len]);
            mem.copy_forwards(InternPool.Index, values[tuple_len * i ..], values[0..tuple_len]);
        }
        break :rs runtime_src;
    };

    const tuple_ty = try mod.intern_pool.get_anon_struct_type(mod.gpa, .{
        .types = types,
        .values = values,
        .names = &.{},
    });

    const runtime_src = opt_runtime_src orelse {
        const tuple_val = try mod.intern(.{ .aggregate = .{
            .ty = tuple_ty,
            .storage = .{ .elems = values },
        } });
        return Air.interned_to_ref(tuple_val);
    };

    try sema.require_runtime_block(block, src, runtime_src);

    const element_refs = try sema.arena.alloc(Air.Inst.Ref, final_len);
    var i: u32 = 0;
    while (i < tuple_len) : (i += 1) {
        const operand_src: LazySrcLoc = .{ .array_cat_lhs = .{
            .array_cat_offset = src_node,
            .elem_index = i,
        } };
        element_refs[i] = try sema.tuple_field_val_by_index(block, operand_src, operand, @int_cast(i), operand_ty);
    }
    i = 1;
    while (i < factor) : (i += 1) {
        @memcpy(element_refs[tuple_len * i ..][0..tuple_len], element_refs[0..tuple_len]);
    }

    return block.add_aggregate_init(Type.from_interned(tuple_ty), element_refs);
}

fn zir_array_mul(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.ArrayMul, inst_data.payload_index).data;
    const uncoerced_lhs = try sema.resolve_inst(extra.lhs);
    const uncoerced_lhs_ty = sema.type_of(uncoerced_lhs);
    const src: LazySrcLoc = inst_data.src();
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const operator_src: LazySrcLoc = .{ .node_offset_main_token = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };

    const lhs, const lhs_ty = coerced_lhs: {
        // If we have a result type, we might be able to do this more efficiently
        // by coercing the LHS first. Specifically, if we want an array or vector
        // and have a tuple, coerce the tuple immediately.
        no_coerce: {
            if (extra.res_ty == .none) break :no_coerce;
            const res_ty_inst = try sema.resolve_inst(extra.res_ty);
            const res_ty = try sema.analyze_as_type(block, src, res_ty_inst);
            if (res_ty.is_generic_poison()) break :no_coerce;
            if (!uncoerced_lhs_ty.is_tuple(mod)) break :no_coerce;
            const lhs_len = uncoerced_lhs_ty.struct_field_count(mod);
            const lhs_dest_ty = switch (res_ty.zig_type_tag(mod)) {
                else => break :no_coerce,
                .Array => try mod.array_type(.{
                    .child = res_ty.child_type(mod).to_intern(),
                    .len = lhs_len,
                    .sentinel = if (res_ty.sentinel(mod)) |s| s.to_intern() else .none,
                }),
                .Vector => try mod.vector_type(.{
                    .child = res_ty.child_type(mod).to_intern(),
                    .len = lhs_len,
                }),
            };
            // Attempt to coerce to this type, but don't emit an error if it fails. Instead,
            // just exit out of this path and let the usual error happen later, so that error
            // messages are consistent.
            const coerced = sema.coerce_extra(block, lhs_dest_ty, uncoerced_lhs, lhs_src, .{ .report_err = false }) catch |err| switch (err) {
                error.NotCoercible => break :no_coerce,
                else => |e| return e,
            };
            break :coerced_lhs .{ coerced, lhs_dest_ty };
        }
        break :coerced_lhs .{ uncoerced_lhs, uncoerced_lhs_ty };
    };

    if (lhs_ty.is_tuple(mod)) {
        // In `**` rhs must be comptime-known, but lhs can be runtime-known
        const factor = try sema.resolve_int(block, rhs_src, extra.rhs, Type.usize, .{
            .needed_comptime_reason = "array multiplication factor must be comptime-known",
        });
        const factor_casted = try sema.usize_cast(block, rhs_src, factor);
        return sema.analyze_tuple_mul(block, inst_data.src_node, lhs, factor_casted);
    }

    // Analyze the lhs first, to catch the case that someone tried to do exponentiation
    const lhs_info = try sema.get_array_cat_info(block, lhs_src, lhs, lhs_ty) orelse {
        const msg = msg: {
            const msg = try sema.err_msg(block, lhs_src, "expected indexable; found '{}'", .{lhs_ty.fmt(mod)});
            errdefer msg.destroy(sema.gpa);
            switch (lhs_ty.zig_type_tag(mod)) {
                .Int, .Float, .ComptimeFloat, .ComptimeInt, .Vector => {
                    try sema.err_note(block, operator_src, msg, "this operator multiplies arrays; use std.math.pow for exponentiation", .{});
                },
                else => {},
            }
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    };

    // In `**` rhs must be comptime-known, but lhs can be runtime-known
    const factor = try sema.resolve_int(block, rhs_src, extra.rhs, Type.usize, .{
        .needed_comptime_reason = "array multiplication factor must be comptime-known",
    });

    const result_len_u64 = std.math.mul(u64, lhs_info.len, factor) catch
        return sema.fail(block, rhs_src, "operation results in overflow", .{});
    const result_len = try sema.usize_cast(block, src, result_len_u64);

    const result_ty = try mod.array_type(.{
        .len = result_len,
        .sentinel = if (lhs_info.sentinel) |s| s.to_intern() else .none,
        .child = lhs_info.elem_type.to_intern(),
    });

    const ptr_addrspace = if (lhs_ty.zig_type_tag(mod) == .Pointer) lhs_ty.ptr_address_space(mod) else null;
    const lhs_len = try sema.usize_cast(block, lhs_src, lhs_info.len);

    if (try sema.resolve_defined_value(block, lhs_src, lhs)) |lhs_val| ct: {
        const lhs_sub_val = if (lhs_ty.is_single_pointer(mod))
            try sema.pointer_deref(block, lhs_src, lhs_val, lhs_ty) orelse break :ct
        else if (lhs_ty.is_slice(mod))
            try sema.maybe_deref_slice_as_array(block, lhs_src, lhs_val) orelse break :ct
        else
            lhs_val;

        const val = v: {
            // Optimization for the common pattern of a single element repeated N times, such
            // as zero-filling a byte array.
            if (lhs_len == 1 and lhs_info.sentinel == null) {
                const elem_val = try lhs_sub_val.elem_value(mod, 0);
                break :v try mod.intern(.{ .aggregate = .{
                    .ty = result_ty.to_intern(),
                    .storage = .{ .repeated_elem = elem_val.to_intern() },
                } });
            }

            const element_vals = try sema.arena.alloc(InternPool.Index, result_len);
            var elem_i: usize = 0;
            while (elem_i < result_len) {
                var lhs_i: usize = 0;
                while (lhs_i < lhs_len) : (lhs_i += 1) {
                    const elem_val = try lhs_sub_val.elem_value(mod, lhs_i);
                    element_vals[elem_i] = elem_val.to_intern();
                    elem_i += 1;
                }
            }
            break :v try mod.intern(.{ .aggregate = .{
                .ty = result_ty.to_intern(),
                .storage = .{ .elems = element_vals },
            } });
        };
        return sema.add_constant_maybe_ref(val, ptr_addrspace != null);
    }

    try sema.require_runtime_block(block, src, lhs_src);

    // Grab all the LHS values ahead of time, rather than repeatedly emitting instructions
    // to get the same elem values.
    const lhs_vals = try sema.arena.alloc(Air.Inst.Ref, lhs_len);
    for (lhs_vals, 0..) |*lhs_val, idx| {
        const idx_ref = try mod.int_ref(Type.usize, idx);
        lhs_val.* = try sema.elem_val(block, lhs_src, lhs, idx_ref, src, false);
    }

    if (ptr_addrspace) |ptr_as| {
        const alloc_ty = try sema.ptr_type(.{
            .child = result_ty.to_intern(),
            .flags = .{ .address_space = ptr_as },
        });
        const alloc = try block.add_ty(.alloc, alloc_ty);
        const elem_ptr_ty = try sema.ptr_type(.{
            .child = lhs_info.elem_type.to_intern(),
            .flags = .{ .address_space = ptr_as },
        });

        var elem_i: usize = 0;
        while (elem_i < result_len) {
            for (lhs_vals) |lhs_val| {
                const elem_index = try mod.int_ref(Type.usize, elem_i);
                const elem_ptr = try block.add_ptr_elem_ptr(alloc, elem_index, elem_ptr_ty);
                try sema.store_ptr2(block, src, elem_ptr, src, lhs_val, lhs_src, .store);
                elem_i += 1;
            }
        }
        if (lhs_info.sentinel) |sent_val| {
            const elem_index = try mod.int_ref(Type.usize, result_len);
            const elem_ptr = try block.add_ptr_elem_ptr(alloc, elem_index, elem_ptr_ty);
            const init = Air.interned_to_ref(sent_val.to_intern());
            try sema.store_ptr2(block, src, elem_ptr, src, init, lhs_src, .store);
        }

        return alloc;
    }

    const element_refs = try sema.arena.alloc(Air.Inst.Ref, result_len);
    for (0..try sema.usize_cast(block, rhs_src, factor)) |i| {
        @memcpy(element_refs[i * lhs_len ..][0..lhs_len], lhs_vals);
    }
    return block.add_aggregate_init(result_ty, element_refs);
}

fn zir_negate(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const lhs_src = src;
    const rhs_src: LazySrcLoc = .{ .node_offset_un_op = inst_data.src_node };

    const rhs = try sema.resolve_inst(inst_data.operand);
    const rhs_ty = sema.type_of(rhs);
    const rhs_scalar_ty = rhs_ty.scalar_type(mod);

    if (rhs_scalar_ty.is_unsigned_int(mod) or switch (rhs_scalar_ty.zig_type_tag(mod)) {
        .Int, .ComptimeInt, .Float, .ComptimeFloat => false,
        else => true,
    }) {
        return sema.fail(block, src, "negation of type '{}'", .{rhs_ty.fmt(mod)});
    }

    if (rhs_scalar_ty.is_any_float()) {
        // We handle float negation here to ensure negative zero is represented in the bits.
        if (try sema.resolve_value(rhs)) |rhs_val| {
            if (rhs_val.is_undef(mod)) return mod.undef_ref(rhs_ty);
            return Air.interned_to_ref((try rhs_val.float_neg(rhs_ty, sema.arena, mod)).to_intern());
        }
        try sema.require_runtime_block(block, src, null);
        return block.add_un_op(if (block.float_mode == .optimized) .neg_optimized else .neg, rhs);
    }

    const lhs = Air.interned_to_ref((try sema.splat(rhs_ty, try mod.int_value(rhs_scalar_ty, 0))).to_intern());
    return sema.analyze_arithmetic(block, .sub, lhs, rhs, src, lhs_src, rhs_src, true);
}

fn zir_negate_wrap(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const lhs_src = src;
    const rhs_src: LazySrcLoc = .{ .node_offset_un_op = inst_data.src_node };

    const rhs = try sema.resolve_inst(inst_data.operand);
    const rhs_ty = sema.type_of(rhs);
    const rhs_scalar_ty = rhs_ty.scalar_type(mod);

    switch (rhs_scalar_ty.zig_type_tag(mod)) {
        .Int, .ComptimeInt, .Float, .ComptimeFloat => {},
        else => return sema.fail(block, src, "negation of type '{}'", .{rhs_ty.fmt(mod)}),
    }

    const lhs = Air.interned_to_ref((try sema.splat(rhs_ty, try mod.int_value(rhs_scalar_ty, 0))).to_intern());
    return sema.analyze_arithmetic(block, .subwrap, lhs, rhs, src, lhs_src, rhs_src, true);
}

fn zir_arithmetic(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    zir_tag: Zir.Inst.Tag,
    safety: bool,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src: LazySrcLoc = .{ .node_offset_bin_op = inst_data.src_node };
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const lhs = try sema.resolve_inst(extra.lhs);
    const rhs = try sema.resolve_inst(extra.rhs);

    return sema.analyze_arithmetic(block, zir_tag, lhs, rhs, src, lhs_src, rhs_src, safety);
}

fn zir_div(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src: LazySrcLoc = .{ .node_offset_bin_op = inst_data.src_node };
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const lhs = try sema.resolve_inst(extra.lhs);
    const rhs = try sema.resolve_inst(extra.rhs);
    const lhs_ty = sema.type_of(lhs);
    const rhs_ty = sema.type_of(rhs);
    const lhs_zig_ty_tag = try lhs_ty.zig_type_tag_or_poison(mod);
    const rhs_zig_ty_tag = try rhs_ty.zig_type_tag_or_poison(mod);
    try sema.check_vectorizable_binary_operands(block, src, lhs_ty, rhs_ty, lhs_src, rhs_src);
    try sema.check_invalid_ptr_arithmetic(block, src, lhs_ty);

    const instructions = &[_]Air.Inst.Ref{ lhs, rhs };
    const resolved_type = try sema.resolve_peer_types(block, src, instructions, .{
        .override = &[_]?LazySrcLoc{ lhs_src, rhs_src },
    });

    const casted_lhs = try sema.coerce(block, resolved_type, lhs, lhs_src);
    const casted_rhs = try sema.coerce(block, resolved_type, rhs, rhs_src);

    const lhs_scalar_ty = lhs_ty.scalar_type(mod);
    const rhs_scalar_ty = rhs_ty.scalar_type(mod);
    const scalar_tag = resolved_type.scalar_type(mod).zig_type_tag(mod);

    const is_int = scalar_tag == .Int or scalar_tag == .ComptimeInt;

    try sema.check_arithmetic_op(block, src, scalar_tag, lhs_zig_ty_tag, rhs_zig_ty_tag, .div);

    const maybe_lhs_val = try sema.resolve_value_intable(casted_lhs);
    const maybe_rhs_val = try sema.resolve_value_intable(casted_rhs);

    if ((lhs_ty.zig_type_tag(mod) == .ComptimeFloat and rhs_ty.zig_type_tag(mod) == .ComptimeInt) or
        (lhs_ty.zig_type_tag(mod) == .ComptimeInt and rhs_ty.zig_type_tag(mod) == .ComptimeFloat))
    {
        // If it makes a difference whether we coerce to ints or floats before doing the division, error.
        // If lhs % rhs is 0, it doesn't matter.
        const lhs_val = maybe_lhs_val orelse unreachable;
        const rhs_val = maybe_rhs_val orelse unreachable;
        const rem = lhs_val.float_rem(rhs_val, resolved_type, sema.arena, mod) catch unreachable;
        if (!rem.compare_all_with_zero(.eq, mod)) {
            return sema.fail(
                block,
                src,
                "ambiguous coercion of division operands '{}' and '{}'; non-zero remainder '{}'",
                .{ lhs_ty.fmt(mod), rhs_ty.fmt(mod), rem.fmt_value(mod, sema) },
            );
        }
    }

    // TODO: emit compile error when .div is used on integers and there would be an
    // ambiguous result between div_floor and div_trunc.

    // For integers:
    // If the lhs is zero, then zero is returned regardless of rhs.
    // If the rhs is zero, compile error for division by zero.
    // If the rhs is undefined, compile error because there is a possible
    // value (zero) for which the division would be illegal behavior.
    // If the lhs is undefined:
    //   * if lhs type is signed:
    //     * if rhs is comptime-known and not -1, result is undefined
    //     * if rhs is -1 or runtime-known, compile error because there is a
    //        possible value (-min_int / -1)  for which division would be
    //        illegal behavior.
    //   * if lhs type is unsigned, undef is returned regardless of rhs.
    //
    // For floats:
    // If the rhs is zero:
    //  * comptime_float: compile error for division by zero.
    //  * other float type:
    //    * if the lhs is zero: QNaN
    //    * otherwise: +Inf or -Inf depending on lhs sign
    // If the rhs is undefined:
    //  * comptime_float: compile error because there is a possible
    //    value (zero) for which the division would be illegal behavior.
    //  * other float type: result is undefined
    // If the lhs is undefined, result is undefined.
    switch (scalar_tag) {
        .Int, .ComptimeInt, .ComptimeFloat => {
            if (maybe_lhs_val) |lhs_val| {
                if (!lhs_val.is_undef(mod)) {
                    if (try lhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                        const scalar_zero = switch (scalar_tag) {
                            .ComptimeFloat, .Float => try mod.float_value(resolved_type.scalar_type(mod), 0.0),
                            .ComptimeInt, .Int => try mod.int_value(resolved_type.scalar_type(mod), 0),
                            else => unreachable,
                        };
                        const zero_val = try sema.splat(resolved_type, scalar_zero);
                        return Air.interned_to_ref(zero_val.to_intern());
                    }
                }
            }
            if (maybe_rhs_val) |rhs_val| {
                if (rhs_val.is_undef(mod)) {
                    return sema.fail_with_use_of_undef(block, rhs_src);
                }
                if (!(try rhs_val.compare_all_with_zero_advanced(.neq, sema))) {
                    return sema.fail_with_divide_by_zero(block, rhs_src);
                }
                // TODO: if the RHS is one, return the LHS directly
            }
        },
        else => {},
    }

    const runtime_src = rs: {
        if (maybe_lhs_val) |lhs_val| {
            if (lhs_val.is_undef(mod)) {
                if (lhs_scalar_ty.is_signed_int(mod) and rhs_scalar_ty.is_signed_int(mod)) {
                    if (maybe_rhs_val) |rhs_val| {
                        if (try sema.compare_all(rhs_val, .neq, try mod.int_value(resolved_type, -1), resolved_type)) {
                            return mod.undef_ref(resolved_type);
                        }
                    }
                    return sema.fail_with_use_of_undef(block, rhs_src);
                }
                return mod.undef_ref(resolved_type);
            }

            if (maybe_rhs_val) |rhs_val| {
                if (is_int) {
                    var overflow_idx: ?usize = null;
                    const res = try lhs_val.int_div(rhs_val, resolved_type, &overflow_idx, sema.arena, mod);
                    if (overflow_idx) |vec_idx| {
                        return sema.fail_with_integer_overflow(block, src, resolved_type, res, vec_idx);
                    }
                    return Air.interned_to_ref(res.to_intern());
                } else {
                    return Air.interned_to_ref((try lhs_val.float_div(rhs_val, resolved_type, sema.arena, mod)).to_intern());
                }
            } else {
                break :rs rhs_src;
            }
        } else {
            break :rs lhs_src;
        }
    };

    try sema.require_runtime_block(block, src, runtime_src);

    if (block.want_safety()) {
        try sema.add_div_int_overflow_safety(block, src, resolved_type, lhs_scalar_ty, maybe_lhs_val, maybe_rhs_val, casted_lhs, casted_rhs, is_int);
        try sema.add_div_by_zero_safety(block, src, resolved_type, maybe_rhs_val, casted_rhs, is_int);
    }

    const air_tag = if (is_int) blk: {
        if (lhs_ty.is_signed_int(mod) or rhs_ty.is_signed_int(mod)) {
            return sema.fail(
                block,
                src,
                "division with '{}' and '{}': signed integers must use @div_trunc, @div_floor, or @div_exact",
                .{ lhs_ty.fmt(mod), rhs_ty.fmt(mod) },
            );
        }
        break :blk Air.Inst.Tag.div_trunc;
    } else switch (block.float_mode) {
        .optimized => Air.Inst.Tag.div_float_optimized,
        .strict => Air.Inst.Tag.div_float,
    };
    return block.add_bin_op(air_tag, casted_lhs, casted_rhs);
}

fn zir_div_exact(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src: LazySrcLoc = .{ .node_offset_bin_op = inst_data.src_node };
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const lhs = try sema.resolve_inst(extra.lhs);
    const rhs = try sema.resolve_inst(extra.rhs);
    const lhs_ty = sema.type_of(lhs);
    const rhs_ty = sema.type_of(rhs);
    const lhs_zig_ty_tag = try lhs_ty.zig_type_tag_or_poison(mod);
    const rhs_zig_ty_tag = try rhs_ty.zig_type_tag_or_poison(mod);
    try sema.check_vectorizable_binary_operands(block, src, lhs_ty, rhs_ty, lhs_src, rhs_src);
    try sema.check_invalid_ptr_arithmetic(block, src, lhs_ty);

    const instructions = &[_]Air.Inst.Ref{ lhs, rhs };
    const resolved_type = try sema.resolve_peer_types(block, src, instructions, .{
        .override = &[_]?LazySrcLoc{ lhs_src, rhs_src },
    });

    const casted_lhs = try sema.coerce(block, resolved_type, lhs, lhs_src);
    const casted_rhs = try sema.coerce(block, resolved_type, rhs, rhs_src);

    const lhs_scalar_ty = lhs_ty.scalar_type(mod);
    const scalar_tag = resolved_type.scalar_type(mod).zig_type_tag(mod);

    const is_int = scalar_tag == .Int or scalar_tag == .ComptimeInt;

    try sema.check_arithmetic_op(block, src, scalar_tag, lhs_zig_ty_tag, rhs_zig_ty_tag, .div_exact);

    const maybe_lhs_val = try sema.resolve_value_intable(casted_lhs);
    const maybe_rhs_val = try sema.resolve_value_intable(casted_rhs);

    const runtime_src = rs: {
        // For integers:
        // If the lhs is zero, then zero is returned regardless of rhs.
        // If the rhs is zero, compile error for division by zero.
        // If the rhs is undefined, compile error because there is a possible
        // value (zero) for which the division would be illegal behavior.
        // If the lhs is undefined, compile error because there is a possible
        // value for which the division would result in a remainder.
        // TODO: emit runtime safety for if there is a remainder
        // TODO: emit runtime safety for division by zero
        //
        // For floats:
        // If the rhs is zero, compile error for division by zero.
        // If the rhs is undefined, compile error because there is a possible
        // value (zero) for which the division would be illegal behavior.
        // If the lhs is undefined, compile error because there is a possible
        // value for which the division would result in a remainder.
        if (maybe_lhs_val) |lhs_val| {
            if (lhs_val.is_undef(mod)) {
                return sema.fail_with_use_of_undef(block, rhs_src);
            } else {
                if (try lhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                    const scalar_zero = switch (scalar_tag) {
                        .ComptimeFloat, .Float => try mod.float_value(resolved_type.scalar_type(mod), 0.0),
                        .ComptimeInt, .Int => try mod.int_value(resolved_type.scalar_type(mod), 0),
                        else => unreachable,
                    };
                    const zero_val = try sema.splat(resolved_type, scalar_zero);
                    return Air.interned_to_ref(zero_val.to_intern());
                }
            }
        }
        if (maybe_rhs_val) |rhs_val| {
            if (rhs_val.is_undef(mod)) {
                return sema.fail_with_use_of_undef(block, rhs_src);
            }
            if (!(try rhs_val.compare_all_with_zero_advanced(.neq, sema))) {
                return sema.fail_with_divide_by_zero(block, rhs_src);
            }
            // TODO: if the RHS is one, return the LHS directly
        }
        if (maybe_lhs_val) |lhs_val| {
            if (maybe_rhs_val) |rhs_val| {
                if (is_int) {
                    const modulus_val = try lhs_val.int_mod(rhs_val, resolved_type, sema.arena, mod);
                    if (!(modulus_val.compare_all_with_zero(.eq, mod))) {
                        return sema.fail(block, src, "exact division produced remainder", .{});
                    }
                    var overflow_idx: ?usize = null;
                    const res = try lhs_val.int_div(rhs_val, resolved_type, &overflow_idx, sema.arena, mod);
                    if (overflow_idx) |vec_idx| {
                        return sema.fail_with_integer_overflow(block, src, resolved_type, res, vec_idx);
                    }
                    return Air.interned_to_ref(res.to_intern());
                } else {
                    const modulus_val = try lhs_val.float_mod(rhs_val, resolved_type, sema.arena, mod);
                    if (!(modulus_val.compare_all_with_zero(.eq, mod))) {
                        return sema.fail(block, src, "exact division produced remainder", .{});
                    }
                    return Air.interned_to_ref((try lhs_val.float_div(rhs_val, resolved_type, sema.arena, mod)).to_intern());
                }
            } else break :rs rhs_src;
        } else break :rs lhs_src;
    };

    try sema.require_runtime_block(block, src, runtime_src);

    // Depending on whether safety is enabled, we will have a slightly different strategy
    // here. The `div_exact` AIR instruction causes undefined behavior if a remainder
    // is produced, so in the safety check case, it cannot be used. Instead we do a
    // div_trunc and check for remainder.

    if (block.want_safety()) {
        try sema.add_div_int_overflow_safety(block, src, resolved_type, lhs_scalar_ty, maybe_lhs_val, maybe_rhs_val, casted_lhs, casted_rhs, is_int);
        try sema.add_div_by_zero_safety(block, src, resolved_type, maybe_rhs_val, casted_rhs, is_int);

        const result = try block.add_bin_op(.div_trunc, casted_lhs, casted_rhs);
        const ok = if (!is_int) ok: {
            const floored = try block.add_un_op(.floor, result);

            if (resolved_type.zig_type_tag(mod) == .Vector) {
                const eql = try block.add_cmp_vector(result, floored, .eq);
                break :ok try block.add_inst(.{
                    .tag = switch (block.float_mode) {
                        .strict => .reduce,
                        .optimized => .reduce_optimized,
                    },
                    .data = .{ .reduce = .{
                        .operand = eql,
                        .operation = .And,
                    } },
                });
            } else {
                const is_in_range = try block.add_bin_op(switch (block.float_mode) {
                    .strict => .cmp_eq,
                    .optimized => .cmp_eq_optimized,
                }, result, floored);
                break :ok is_in_range;
            }
        } else ok: {
            const remainder = try block.add_bin_op(.rem, casted_lhs, casted_rhs);

            const scalar_zero = switch (scalar_tag) {
                .ComptimeFloat, .Float => try mod.float_value(resolved_type.scalar_type(mod), 0.0),
                .ComptimeInt, .Int => try mod.int_value(resolved_type.scalar_type(mod), 0),
                else => unreachable,
            };
            if (resolved_type.zig_type_tag(mod) == .Vector) {
                const zero_val = try sema.splat(resolved_type, scalar_zero);
                const zero = Air.interned_to_ref(zero_val.to_intern());
                const eql = try block.add_cmp_vector(remainder, zero, .eq);
                break :ok try block.add_inst(.{
                    .tag = .reduce,
                    .data = .{ .reduce = .{
                        .operand = eql,
                        .operation = .And,
                    } },
                });
            } else {
                const zero = Air.interned_to_ref(scalar_zero.to_intern());
                const is_in_range = try block.add_bin_op(.cmp_eq, remainder, zero);
                break :ok is_in_range;
            }
        };
        try sema.add_safety_check(block, src, ok, .exact_division_remainder);
        return result;
    }

    return block.add_bin_op(air_tag(block, is_int, .div_exact, .div_exact_optimized), casted_lhs, casted_rhs);
}

fn zir_div_floor(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src: LazySrcLoc = .{ .node_offset_bin_op = inst_data.src_node };
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const lhs = try sema.resolve_inst(extra.lhs);
    const rhs = try sema.resolve_inst(extra.rhs);
    const lhs_ty = sema.type_of(lhs);
    const rhs_ty = sema.type_of(rhs);
    const lhs_zig_ty_tag = try lhs_ty.zig_type_tag_or_poison(mod);
    const rhs_zig_ty_tag = try rhs_ty.zig_type_tag_or_poison(mod);
    try sema.check_vectorizable_binary_operands(block, src, lhs_ty, rhs_ty, lhs_src, rhs_src);
    try sema.check_invalid_ptr_arithmetic(block, src, lhs_ty);

    const instructions = &[_]Air.Inst.Ref{ lhs, rhs };
    const resolved_type = try sema.resolve_peer_types(block, src, instructions, .{
        .override = &[_]?LazySrcLoc{ lhs_src, rhs_src },
    });

    const casted_lhs = try sema.coerce(block, resolved_type, lhs, lhs_src);
    const casted_rhs = try sema.coerce(block, resolved_type, rhs, rhs_src);

    const lhs_scalar_ty = lhs_ty.scalar_type(mod);
    const rhs_scalar_ty = rhs_ty.scalar_type(mod);
    const scalar_tag = resolved_type.scalar_type(mod).zig_type_tag(mod);

    const is_int = scalar_tag == .Int or scalar_tag == .ComptimeInt;

    try sema.check_arithmetic_op(block, src, scalar_tag, lhs_zig_ty_tag, rhs_zig_ty_tag, .div_floor);

    const maybe_lhs_val = try sema.resolve_value_intable(casted_lhs);
    const maybe_rhs_val = try sema.resolve_value_intable(casted_rhs);

    const runtime_src = rs: {
        // For integers:
        // If the lhs is zero, then zero is returned regardless of rhs.
        // If the rhs is zero, compile error for division by zero.
        // If the rhs is undefined, compile error because there is a possible
        // value (zero) for which the division would be illegal behavior.
        // If the lhs is undefined:
        //   * if lhs type is signed:
        //     * if rhs is comptime-known and not -1, result is undefined
        //     * if rhs is -1 or runtime-known, compile error because there is a
        //        possible value (-min_int / -1)  for which division would be
        //        illegal behavior.
        //   * if lhs type is unsigned, undef is returned regardless of rhs.
        // TODO: emit runtime safety for division by zero
        //
        // For floats:
        // If the rhs is zero, compile error for division by zero.
        // If the rhs is undefined, compile error because there is a possible
        // value (zero) for which the division would be illegal behavior.
        // If the lhs is undefined, result is undefined.
        if (maybe_lhs_val) |lhs_val| {
            if (!lhs_val.is_undef(mod)) {
                if (try lhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                    const scalar_zero = switch (scalar_tag) {
                        .ComptimeFloat, .Float => try mod.float_value(resolved_type.scalar_type(mod), 0.0),
                        .ComptimeInt, .Int => try mod.int_value(resolved_type.scalar_type(mod), 0),
                        else => unreachable,
                    };
                    const zero_val = try sema.splat(resolved_type, scalar_zero);
                    return Air.interned_to_ref(zero_val.to_intern());
                }
            }
        }
        if (maybe_rhs_val) |rhs_val| {
            if (rhs_val.is_undef(mod)) {
                return sema.fail_with_use_of_undef(block, rhs_src);
            }
            if (!(try rhs_val.compare_all_with_zero_advanced(.neq, sema))) {
                return sema.fail_with_divide_by_zero(block, rhs_src);
            }
            // TODO: if the RHS is one, return the LHS directly
        }
        if (maybe_lhs_val) |lhs_val| {
            if (lhs_val.is_undef(mod)) {
                if (lhs_scalar_ty.is_signed_int(mod) and rhs_scalar_ty.is_signed_int(mod)) {
                    if (maybe_rhs_val) |rhs_val| {
                        if (try sema.compare_all(rhs_val, .neq, try mod.int_value(resolved_type, -1), resolved_type)) {
                            return mod.undef_ref(resolved_type);
                        }
                    }
                    return sema.fail_with_use_of_undef(block, rhs_src);
                }
                return mod.undef_ref(resolved_type);
            }

            if (maybe_rhs_val) |rhs_val| {
                if (is_int) {
                    return Air.interned_to_ref((try lhs_val.int_div_floor(rhs_val, resolved_type, sema.arena, mod)).to_intern());
                } else {
                    return Air.interned_to_ref((try lhs_val.float_div_floor(rhs_val, resolved_type, sema.arena, mod)).to_intern());
                }
            } else break :rs rhs_src;
        } else break :rs lhs_src;
    };

    try sema.require_runtime_block(block, src, runtime_src);

    if (block.want_safety()) {
        try sema.add_div_int_overflow_safety(block, src, resolved_type, lhs_scalar_ty, maybe_lhs_val, maybe_rhs_val, casted_lhs, casted_rhs, is_int);
        try sema.add_div_by_zero_safety(block, src, resolved_type, maybe_rhs_val, casted_rhs, is_int);
    }

    return block.add_bin_op(air_tag(block, is_int, .div_floor, .div_floor_optimized), casted_lhs, casted_rhs);
}

fn zir_div_trunc(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src: LazySrcLoc = .{ .node_offset_bin_op = inst_data.src_node };
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const lhs = try sema.resolve_inst(extra.lhs);
    const rhs = try sema.resolve_inst(extra.rhs);
    const lhs_ty = sema.type_of(lhs);
    const rhs_ty = sema.type_of(rhs);
    const lhs_zig_ty_tag = try lhs_ty.zig_type_tag_or_poison(mod);
    const rhs_zig_ty_tag = try rhs_ty.zig_type_tag_or_poison(mod);
    try sema.check_vectorizable_binary_operands(block, src, lhs_ty, rhs_ty, lhs_src, rhs_src);
    try sema.check_invalid_ptr_arithmetic(block, src, lhs_ty);

    const instructions = &[_]Air.Inst.Ref{ lhs, rhs };
    const resolved_type = try sema.resolve_peer_types(block, src, instructions, .{
        .override = &[_]?LazySrcLoc{ lhs_src, rhs_src },
    });

    const casted_lhs = try sema.coerce(block, resolved_type, lhs, lhs_src);
    const casted_rhs = try sema.coerce(block, resolved_type, rhs, rhs_src);

    const lhs_scalar_ty = lhs_ty.scalar_type(mod);
    const rhs_scalar_ty = rhs_ty.scalar_type(mod);
    const scalar_tag = resolved_type.scalar_type(mod).zig_type_tag(mod);

    const is_int = scalar_tag == .Int or scalar_tag == .ComptimeInt;

    try sema.check_arithmetic_op(block, src, scalar_tag, lhs_zig_ty_tag, rhs_zig_ty_tag, .div_trunc);

    const maybe_lhs_val = try sema.resolve_value_intable(casted_lhs);
    const maybe_rhs_val = try sema.resolve_value_intable(casted_rhs);

    const runtime_src = rs: {
        // For integers:
        // If the lhs is zero, then zero is returned regardless of rhs.
        // If the rhs is zero, compile error for division by zero.
        // If the rhs is undefined, compile error because there is a possible
        // value (zero) for which the division would be illegal behavior.
        // If the lhs is undefined:
        //   * if lhs type is signed:
        //     * if rhs is comptime-known and not -1, result is undefined
        //     * if rhs is -1 or runtime-known, compile error because there is a
        //        possible value (-min_int / -1)  for which division would be
        //        illegal behavior.
        //   * if lhs type is unsigned, undef is returned regardless of rhs.
        // TODO: emit runtime safety for division by zero
        //
        // For floats:
        // If the rhs is zero, compile error for division by zero.
        // If the rhs is undefined, compile error because there is a possible
        // value (zero) for which the division would be illegal behavior.
        // If the lhs is undefined, result is undefined.
        if (maybe_lhs_val) |lhs_val| {
            if (!lhs_val.is_undef(mod)) {
                if (try lhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                    const scalar_zero = switch (scalar_tag) {
                        .ComptimeFloat, .Float => try mod.float_value(resolved_type.scalar_type(mod), 0.0),
                        .ComptimeInt, .Int => try mod.int_value(resolved_type.scalar_type(mod), 0),
                        else => unreachable,
                    };
                    const zero_val = try sema.splat(resolved_type, scalar_zero);
                    return Air.interned_to_ref(zero_val.to_intern());
                }
            }
        }
        if (maybe_rhs_val) |rhs_val| {
            if (rhs_val.is_undef(mod)) {
                return sema.fail_with_use_of_undef(block, rhs_src);
            }
            if (!(try rhs_val.compare_all_with_zero_advanced(.neq, sema))) {
                return sema.fail_with_divide_by_zero(block, rhs_src);
            }
        }
        if (maybe_lhs_val) |lhs_val| {
            if (lhs_val.is_undef(mod)) {
                if (lhs_scalar_ty.is_signed_int(mod) and rhs_scalar_ty.is_signed_int(mod)) {
                    if (maybe_rhs_val) |rhs_val| {
                        if (try sema.compare_all(rhs_val, .neq, try mod.int_value(resolved_type, -1), resolved_type)) {
                            return mod.undef_ref(resolved_type);
                        }
                    }
                    return sema.fail_with_use_of_undef(block, rhs_src);
                }
                return mod.undef_ref(resolved_type);
            }

            if (maybe_rhs_val) |rhs_val| {
                if (is_int) {
                    var overflow_idx: ?usize = null;
                    const res = try lhs_val.int_div(rhs_val, resolved_type, &overflow_idx, sema.arena, mod);
                    if (overflow_idx) |vec_idx| {
                        return sema.fail_with_integer_overflow(block, src, resolved_type, res, vec_idx);
                    }
                    return Air.interned_to_ref(res.to_intern());
                } else {
                    return Air.interned_to_ref((try lhs_val.float_div_trunc(rhs_val, resolved_type, sema.arena, mod)).to_intern());
                }
            } else break :rs rhs_src;
        } else break :rs lhs_src;
    };

    try sema.require_runtime_block(block, src, runtime_src);

    if (block.want_safety()) {
        try sema.add_div_int_overflow_safety(block, src, resolved_type, lhs_scalar_ty, maybe_lhs_val, maybe_rhs_val, casted_lhs, casted_rhs, is_int);
        try sema.add_div_by_zero_safety(block, src, resolved_type, maybe_rhs_val, casted_rhs, is_int);
    }

    return block.add_bin_op(air_tag(block, is_int, .div_trunc, .div_trunc_optimized), casted_lhs, casted_rhs);
}

fn add_div_int_overflow_safety(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    resolved_type: Type,
    lhs_scalar_ty: Type,
    maybe_lhs_val: ?Value,
    maybe_rhs_val: ?Value,
    casted_lhs: Air.Inst.Ref,
    casted_rhs: Air.Inst.Ref,
    is_int: bool,
) CompileError!void {
    const mod = sema.mod;
    if (!is_int) return;

    // If the LHS is unsigned, it cannot cause overflow.
    if (!lhs_scalar_ty.is_signed_int(mod)) return;

    // If the LHS is widened to a larger integer type, no overflow is possible.
    if (lhs_scalar_ty.int_info(mod).bits < resolved_type.int_info(mod).bits) {
        return;
    }

    const min_int = try resolved_type.min_int(mod, resolved_type);
    const neg_one_scalar = try mod.int_value(lhs_scalar_ty, -1);
    const neg_one = try sema.splat(resolved_type, neg_one_scalar);

    // If the LHS is comptime-known to be not equal to the min int,
    // no overflow is possible.
    if (maybe_lhs_val) |lhs_val| {
        if (try lhs_val.compare_all(.neq, min_int, resolved_type, mod)) return;
    }

    // If the RHS is comptime-known to not be equal to -1, no overflow is possible.
    if (maybe_rhs_val) |rhs_val| {
        if (try rhs_val.compare_all(.neq, neg_one, resolved_type, mod)) return;
    }

    var ok: Air.Inst.Ref = .none;
    if (resolved_type.zig_type_tag(mod) == .Vector) {
        if (maybe_lhs_val == null) {
            const min_int_ref = Air.interned_to_ref(min_int.to_intern());
            ok = try block.add_cmp_vector(casted_lhs, min_int_ref, .neq);
        }
        if (maybe_rhs_val == null) {
            const neg_one_ref = Air.interned_to_ref(neg_one.to_intern());
            const rhs_ok = try block.add_cmp_vector(casted_rhs, neg_one_ref, .neq);
            if (ok == .none) {
                ok = rhs_ok;
            } else {
                ok = try block.add_bin_op(.bool_or, ok, rhs_ok);
            }
        }
        assert(ok != .none);
        ok = try block.add_inst(.{
            .tag = .reduce,
            .data = .{ .reduce = .{
                .operand = ok,
                .operation = .And,
            } },
        });
    } else {
        if (maybe_lhs_val == null) {
            const min_int_ref = Air.interned_to_ref(min_int.to_intern());
            ok = try block.add_bin_op(.cmp_neq, casted_lhs, min_int_ref);
        }
        if (maybe_rhs_val == null) {
            const neg_one_ref = Air.interned_to_ref(neg_one.to_intern());
            const rhs_ok = try block.add_bin_op(.cmp_neq, casted_rhs, neg_one_ref);
            if (ok == .none) {
                ok = rhs_ok;
            } else {
                ok = try block.add_bin_op(.bool_or, ok, rhs_ok);
            }
        }
        assert(ok != .none);
    }
    try sema.add_safety_check(block, src, ok, .integer_overflow);
}

fn add_div_by_zero_safety(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    resolved_type: Type,
    maybe_rhs_val: ?Value,
    casted_rhs: Air.Inst.Ref,
    is_int: bool,
) CompileError!void {
    // Strict IEEE floats have well-defined division by zero.
    if (!is_int and block.float_mode == .strict) return;

    // If rhs was comptime-known to be zero a compile error would have been
    // emitted above.
    if (maybe_rhs_val != null) return;

    const mod = sema.mod;
    const scalar_zero = if (is_int)
        try mod.int_value(resolved_type.scalar_type(mod), 0)
    else
        try mod.float_value(resolved_type.scalar_type(mod), 0.0);
    const ok = if (resolved_type.zig_type_tag(mod) == .Vector) ok: {
        const zero_val = try sema.splat(resolved_type, scalar_zero);
        const zero = Air.interned_to_ref(zero_val.to_intern());
        const ok = try block.add_cmp_vector(casted_rhs, zero, .neq);
        break :ok try block.add_inst(.{
            .tag = if (is_int) .reduce else .reduce_optimized,
            .data = .{ .reduce = .{
                .operand = ok,
                .operation = .And,
            } },
        });
    } else ok: {
        const zero = Air.interned_to_ref(scalar_zero.to_intern());
        break :ok try block.add_bin_op(if (is_int) .cmp_neq else .cmp_neq_optimized, casted_rhs, zero);
    };
    try sema.add_safety_check(block, src, ok, .divide_by_zero);
}

fn air_tag(block: *Block, is_int: bool, normal: Air.Inst.Tag, optimized: Air.Inst.Tag) Air.Inst.Tag {
    if (is_int) return normal;
    return switch (block.float_mode) {
        .strict => normal,
        .optimized => optimized,
    };
}

fn zir_mod_rem(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src: LazySrcLoc = .{ .node_offset_bin_op = inst_data.src_node };
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const lhs = try sema.resolve_inst(extra.lhs);
    const rhs = try sema.resolve_inst(extra.rhs);
    const lhs_ty = sema.type_of(lhs);
    const rhs_ty = sema.type_of(rhs);
    const lhs_zig_ty_tag = try lhs_ty.zig_type_tag_or_poison(mod);
    const rhs_zig_ty_tag = try rhs_ty.zig_type_tag_or_poison(mod);
    try sema.check_vectorizable_binary_operands(block, src, lhs_ty, rhs_ty, lhs_src, rhs_src);
    try sema.check_invalid_ptr_arithmetic(block, src, lhs_ty);

    const instructions = &[_]Air.Inst.Ref{ lhs, rhs };
    const resolved_type = try sema.resolve_peer_types(block, src, instructions, .{
        .override = &[_]?LazySrcLoc{ lhs_src, rhs_src },
    });

    const is_vector = resolved_type.zig_type_tag(mod) == .Vector;

    const casted_lhs = try sema.coerce(block, resolved_type, lhs, lhs_src);
    const casted_rhs = try sema.coerce(block, resolved_type, rhs, rhs_src);

    const lhs_scalar_ty = lhs_ty.scalar_type(mod);
    const rhs_scalar_ty = rhs_ty.scalar_type(mod);
    const scalar_tag = resolved_type.scalar_type(mod).zig_type_tag(mod);

    const is_int = scalar_tag == .Int or scalar_tag == .ComptimeInt;

    try sema.check_arithmetic_op(block, src, scalar_tag, lhs_zig_ty_tag, rhs_zig_ty_tag, .mod_rem);

    const maybe_lhs_val = try sema.resolve_value_intable(casted_lhs);
    const maybe_rhs_val = try sema.resolve_value_intable(casted_rhs);

    const runtime_src = rs: {
        // For integers:
        // Either operand being undef is a compile error because there exists
        // a possible value (TODO what is it?) that would invoke illegal behavior.
        // TODO: can lhs undef be handled better?
        //
        // For floats:
        // If the rhs is zero, compile error for division by zero.
        // If the rhs is undefined, compile error because there is a possible
        // value (zero) for which the division would be illegal behavior.
        // If the lhs is undefined, result is undefined.
        //
        // For either one: if the result would be different between @mod and @rem,
        // then emit a compile error saying you have to pick one.
        if (is_int) {
            if (maybe_lhs_val) |lhs_val| {
                if (lhs_val.is_undef(mod)) {
                    return sema.fail_with_use_of_undef(block, lhs_src);
                }
                if (try lhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                    const scalar_zero = switch (scalar_tag) {
                        .ComptimeFloat, .Float => try mod.float_value(resolved_type.scalar_type(mod), 0.0),
                        .ComptimeInt, .Int => try mod.int_value(resolved_type.scalar_type(mod), 0),
                        else => unreachable,
                    };
                    const zero_val = if (is_vector) Value.from_interned((try mod.intern(.{ .aggregate = .{
                        .ty = resolved_type.to_intern(),
                        .storage = .{ .repeated_elem = scalar_zero.to_intern() },
                    } }))) else scalar_zero;
                    return Air.interned_to_ref(zero_val.to_intern());
                }
            } else if (lhs_scalar_ty.is_signed_int(mod)) {
                return sema.fail_with_mod_rem_negative(block, lhs_src, lhs_ty, rhs_ty);
            }
            if (maybe_rhs_val) |rhs_val| {
                if (rhs_val.is_undef(mod)) {
                    return sema.fail_with_use_of_undef(block, rhs_src);
                }
                if (!(try rhs_val.compare_all_with_zero_advanced(.neq, sema))) {
                    return sema.fail_with_divide_by_zero(block, rhs_src);
                }
                if (!(try rhs_val.compare_all_with_zero_advanced(.gte, sema))) {
                    return sema.fail_with_mod_rem_negative(block, rhs_src, lhs_ty, rhs_ty);
                }
                if (maybe_lhs_val) |lhs_val| {
                    const rem_result = try sema.int_rem(resolved_type, lhs_val, rhs_val);
                    // If this answer could possibly be different by doing `int_mod`,
                    // we must emit a compile error. Otherwise, it's OK.
                    if (!(try lhs_val.compare_all_with_zero_advanced(.gte, sema)) and
                        !(try rem_result.compare_all_with_zero_advanced(.eq, sema)))
                    {
                        return sema.fail_with_mod_rem_negative(block, lhs_src, lhs_ty, rhs_ty);
                    }
                    return Air.interned_to_ref(rem_result.to_intern());
                }
                break :rs lhs_src;
            } else if (rhs_scalar_ty.is_signed_int(mod)) {
                return sema.fail_with_mod_rem_negative(block, rhs_src, lhs_ty, rhs_ty);
            } else {
                break :rs rhs_src;
            }
        }
        // float operands
        if (maybe_rhs_val) |rhs_val| {
            if (rhs_val.is_undef(mod)) {
                return sema.fail_with_use_of_undef(block, rhs_src);
            }
            if (!(try rhs_val.compare_all_with_zero_advanced(.neq, sema))) {
                return sema.fail_with_divide_by_zero(block, rhs_src);
            }
            if (!(try rhs_val.compare_all_with_zero_advanced(.gte, sema))) {
                return sema.fail_with_mod_rem_negative(block, rhs_src, lhs_ty, rhs_ty);
            }
            if (maybe_lhs_val) |lhs_val| {
                if (lhs_val.is_undef(mod) or !(try lhs_val.compare_all_with_zero_advanced(.gte, sema))) {
                    return sema.fail_with_mod_rem_negative(block, lhs_src, lhs_ty, rhs_ty);
                }
                return Air.interned_to_ref((try lhs_val.float_rem(rhs_val, resolved_type, sema.arena, mod)).to_intern());
            } else {
                return sema.fail_with_mod_rem_negative(block, lhs_src, lhs_ty, rhs_ty);
            }
        } else {
            return sema.fail_with_mod_rem_negative(block, rhs_src, lhs_ty, rhs_ty);
        }
    };

    try sema.require_runtime_block(block, src, runtime_src);

    if (block.want_safety()) {
        try sema.add_div_by_zero_safety(block, src, resolved_type, maybe_rhs_val, casted_rhs, is_int);
    }

    const air_tag = air_tag(block, is_int, .rem, .rem_optimized);
    return block.add_bin_op(air_tag, casted_lhs, casted_rhs);
}

fn int_rem(
    sema: *Sema,
    ty: Type,
    lhs: Value,
    rhs: Value,
) CompileError!Value {
    const mod = sema.mod;
    if (ty.zig_type_tag(mod) == .Vector) {
        const result_data = try sema.arena.alloc(InternPool.Index, ty.vector_len(mod));
        const scalar_ty = ty.scalar_type(mod);
        for (result_data, 0..) |*scalar, i| {
            const lhs_elem = try lhs.elem_value(mod, i);
            const rhs_elem = try rhs.elem_value(mod, i);
            scalar.* = (try sema.int_rem_scalar(lhs_elem, rhs_elem, scalar_ty)).to_intern();
        }
        return Value.from_interned((try mod.intern(.{ .aggregate = .{
            .ty = ty.to_intern(),
            .storage = .{ .elems = result_data },
        } })));
    }
    return sema.int_rem_scalar(lhs, rhs, ty);
}

fn int_rem_scalar(sema: *Sema, lhs: Value, rhs: Value, scalar_ty: Type) CompileError!Value {
    const mod = sema.mod;
    // TODO is this a performance issue? maybe we should try the operation without
    // resorting to BigInt first.
    var lhs_space: Value.BigIntSpace = undefined;
    var rhs_space: Value.BigIntSpace = undefined;
    const lhs_bigint = try lhs.to_big_int_advanced(&lhs_space, mod, sema);
    const rhs_bigint = try rhs.to_big_int_advanced(&rhs_space, mod, sema);
    const limbs_q = try sema.arena.alloc(
        math.big.Limb,
        lhs_bigint.limbs.len,
    );
    const limbs_r = try sema.arena.alloc(
        math.big.Limb,
        // TODO: consider reworking Sema to re-use Values rather than
        // always producing new Value objects.
        rhs_bigint.limbs.len,
    );
    const limbs_buffer = try sema.arena.alloc(
        math.big.Limb,
        math.big.int.calc_div_limbs_buffer_len(lhs_bigint.limbs.len, rhs_bigint.limbs.len),
    );
    var result_q = math.big.int.Mutable{ .limbs = limbs_q, .positive = undefined, .len = undefined };
    var result_r = math.big.int.Mutable{ .limbs = limbs_r, .positive = undefined, .len = undefined };
    result_q.div_trunc(&result_r, lhs_bigint, rhs_bigint, limbs_buffer);
    return mod.int_value_big(scalar_ty, result_r.to_const());
}

fn zir_mod(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src: LazySrcLoc = .{ .node_offset_bin_op = inst_data.src_node };
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const lhs = try sema.resolve_inst(extra.lhs);
    const rhs = try sema.resolve_inst(extra.rhs);
    const lhs_ty = sema.type_of(lhs);
    const rhs_ty = sema.type_of(rhs);
    const lhs_zig_ty_tag = try lhs_ty.zig_type_tag_or_poison(mod);
    const rhs_zig_ty_tag = try rhs_ty.zig_type_tag_or_poison(mod);
    try sema.check_vectorizable_binary_operands(block, src, lhs_ty, rhs_ty, lhs_src, rhs_src);
    try sema.check_invalid_ptr_arithmetic(block, src, lhs_ty);

    const instructions = &[_]Air.Inst.Ref{ lhs, rhs };
    const resolved_type = try sema.resolve_peer_types(block, src, instructions, .{
        .override = &[_]?LazySrcLoc{ lhs_src, rhs_src },
    });

    const casted_lhs = try sema.coerce(block, resolved_type, lhs, lhs_src);
    const casted_rhs = try sema.coerce(block, resolved_type, rhs, rhs_src);

    const scalar_tag = resolved_type.scalar_type(mod).zig_type_tag(mod);

    const is_int = scalar_tag == .Int or scalar_tag == .ComptimeInt;

    try sema.check_arithmetic_op(block, src, scalar_tag, lhs_zig_ty_tag, rhs_zig_ty_tag, .mod);

    const maybe_lhs_val = try sema.resolve_value_intable(casted_lhs);
    const maybe_rhs_val = try sema.resolve_value_intable(casted_rhs);

    const runtime_src = rs: {
        // For integers:
        // Either operand being undef is a compile error because there exists
        // a possible value (TODO what is it?) that would invoke illegal behavior.
        // TODO: can lhs zero be handled better?
        // TODO: can lhs undef be handled better?
        //
        // For floats:
        // If the rhs is zero, compile error for division by zero.
        // If the rhs is undefined, compile error because there is a possible
        // value (zero) for which the division would be illegal behavior.
        // If the lhs is undefined, result is undefined.
        if (is_int) {
            if (maybe_lhs_val) |lhs_val| {
                if (lhs_val.is_undef(mod)) {
                    return sema.fail_with_use_of_undef(block, lhs_src);
                }
            }
            if (maybe_rhs_val) |rhs_val| {
                if (rhs_val.is_undef(mod)) {
                    return sema.fail_with_use_of_undef(block, rhs_src);
                }
                if (!(try rhs_val.compare_all_with_zero_advanced(.neq, sema))) {
                    return sema.fail_with_divide_by_zero(block, rhs_src);
                }
                if (maybe_lhs_val) |lhs_val| {
                    return Air.interned_to_ref((try lhs_val.int_mod(rhs_val, resolved_type, sema.arena, mod)).to_intern());
                }
                break :rs lhs_src;
            } else {
                break :rs rhs_src;
            }
        }
        // float operands
        if (maybe_rhs_val) |rhs_val| {
            if (rhs_val.is_undef(mod)) {
                return sema.fail_with_use_of_undef(block, rhs_src);
            }
            if (!(try rhs_val.compare_all_with_zero_advanced(.neq, sema))) {
                return sema.fail_with_divide_by_zero(block, rhs_src);
            }
        }
        if (maybe_lhs_val) |lhs_val| {
            if (lhs_val.is_undef(mod)) {
                return mod.undef_ref(resolved_type);
            }
            if (maybe_rhs_val) |rhs_val| {
                return Air.interned_to_ref((try lhs_val.float_mod(rhs_val, resolved_type, sema.arena, mod)).to_intern());
            } else break :rs rhs_src;
        } else break :rs lhs_src;
    };

    try sema.require_runtime_block(block, src, runtime_src);

    if (block.want_safety()) {
        try sema.add_div_by_zero_safety(block, src, resolved_type, maybe_rhs_val, casted_rhs, is_int);
    }

    const air_tag = air_tag(block, is_int, .mod, .mod_optimized);
    return block.add_bin_op(air_tag, casted_lhs, casted_rhs);
}

fn zir_rem(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src: LazySrcLoc = .{ .node_offset_bin_op = inst_data.src_node };
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const lhs = try sema.resolve_inst(extra.lhs);
    const rhs = try sema.resolve_inst(extra.rhs);
    const lhs_ty = sema.type_of(lhs);
    const rhs_ty = sema.type_of(rhs);
    const lhs_zig_ty_tag = try lhs_ty.zig_type_tag_or_poison(mod);
    const rhs_zig_ty_tag = try rhs_ty.zig_type_tag_or_poison(mod);
    try sema.check_vectorizable_binary_operands(block, src, lhs_ty, rhs_ty, lhs_src, rhs_src);
    try sema.check_invalid_ptr_arithmetic(block, src, lhs_ty);

    const instructions = &[_]Air.Inst.Ref{ lhs, rhs };
    const resolved_type = try sema.resolve_peer_types(block, src, instructions, .{
        .override = &[_]?LazySrcLoc{ lhs_src, rhs_src },
    });

    const casted_lhs = try sema.coerce(block, resolved_type, lhs, lhs_src);
    const casted_rhs = try sema.coerce(block, resolved_type, rhs, rhs_src);

    const scalar_tag = resolved_type.scalar_type(mod).zig_type_tag(mod);

    const is_int = scalar_tag == .Int or scalar_tag == .ComptimeInt;

    try sema.check_arithmetic_op(block, src, scalar_tag, lhs_zig_ty_tag, rhs_zig_ty_tag, .rem);

    const maybe_lhs_val = try sema.resolve_value_intable(casted_lhs);
    const maybe_rhs_val = try sema.resolve_value_intable(casted_rhs);

    const runtime_src = rs: {
        // For integers:
        // Either operand being undef is a compile error because there exists
        // a possible value (TODO what is it?) that would invoke illegal behavior.
        // TODO: can lhs zero be handled better?
        // TODO: can lhs undef be handled better?
        //
        // For floats:
        // If the rhs is zero, compile error for division by zero.
        // If the rhs is undefined, compile error because there is a possible
        // value (zero) for which the division would be illegal behavior.
        // If the lhs is undefined, result is undefined.
        if (is_int) {
            if (maybe_lhs_val) |lhs_val| {
                if (lhs_val.is_undef(mod)) {
                    return sema.fail_with_use_of_undef(block, lhs_src);
                }
            }
            if (maybe_rhs_val) |rhs_val| {
                if (rhs_val.is_undef(mod)) {
                    return sema.fail_with_use_of_undef(block, rhs_src);
                }
                if (!(try rhs_val.compare_all_with_zero_advanced(.neq, sema))) {
                    return sema.fail_with_divide_by_zero(block, rhs_src);
                }
                if (maybe_lhs_val) |lhs_val| {
                    return Air.interned_to_ref((try sema.int_rem(resolved_type, lhs_val, rhs_val)).to_intern());
                }
                break :rs lhs_src;
            } else {
                break :rs rhs_src;
            }
        }
        // float operands
        if (maybe_rhs_val) |rhs_val| {
            if (rhs_val.is_undef(mod)) {
                return sema.fail_with_use_of_undef(block, rhs_src);
            }
            if (!(try rhs_val.compare_all_with_zero_advanced(.neq, sema))) {
                return sema.fail_with_divide_by_zero(block, rhs_src);
            }
        }
        if (maybe_lhs_val) |lhs_val| {
            if (lhs_val.is_undef(mod)) {
                return mod.undef_ref(resolved_type);
            }
            if (maybe_rhs_val) |rhs_val| {
                return Air.interned_to_ref((try lhs_val.float_rem(rhs_val, resolved_type, sema.arena, mod)).to_intern());
            } else break :rs rhs_src;
        } else break :rs lhs_src;
    };

    try sema.require_runtime_block(block, src, runtime_src);

    if (block.want_safety()) {
        try sema.add_div_by_zero_safety(block, src, resolved_type, maybe_rhs_val, casted_rhs, is_int);
    }

    const air_tag = air_tag(block, is_int, .rem, .rem_optimized);
    return block.add_bin_op(air_tag, casted_lhs, casted_rhs);
}

fn zir_overflow_arithmetic(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
    zir_tag: Zir.Inst.Extended,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const extra = sema.code.extra_data(Zir.Inst.BinNode, extended.operand).data;
    const src = LazySrcLoc.nodeOffset(extra.node);

    const lhs_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const rhs_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = extra.node };

    const uncasted_lhs = try sema.resolve_inst(extra.lhs);
    const uncasted_rhs = try sema.resolve_inst(extra.rhs);

    const lhs_ty = sema.type_of(uncasted_lhs);
    const rhs_ty = sema.type_of(uncasted_rhs);
    const mod = sema.mod;
    const ip = &mod.intern_pool;

    try sema.check_vectorizable_binary_operands(block, src, lhs_ty, rhs_ty, lhs_src, rhs_src);

    const instructions = &[_]Air.Inst.Ref{ uncasted_lhs, uncasted_rhs };
    const dest_ty = if (zir_tag == .shl_with_overflow)
        lhs_ty
    else
        try sema.resolve_peer_types(block, src, instructions, .{
            .override = &[_]?LazySrcLoc{ lhs_src, rhs_src },
        });

    const rhs_dest_ty = if (zir_tag == .shl_with_overflow)
        try sema.log2_int_type(block, lhs_ty, src)
    else
        dest_ty;

    const lhs = try sema.coerce(block, dest_ty, uncasted_lhs, lhs_src);
    const rhs = try sema.coerce(block, rhs_dest_ty, uncasted_rhs, rhs_src);

    if (dest_ty.scalar_type(mod).zig_type_tag(mod) != .Int) {
        return sema.fail(block, src, "expected vector of integers or integer tag type, found '{}'", .{dest_ty.fmt(mod)});
    }

    const maybe_lhs_val = try sema.resolve_value(lhs);
    const maybe_rhs_val = try sema.resolve_value(rhs);

    const tuple_ty = try sema.overflow_arithmetic_tuple_type(dest_ty);
    const overflow_ty = Type.from_interned(ip.index_to_key(tuple_ty.to_intern()).anon_struct_type.types.get(ip)[1]);

    var result: struct {
        inst: Air.Inst.Ref = .none,
        wrapped: Value = Value.@"unreachable",
        overflow_bit: Value,
    } = result: {
        const zero_bit = try mod.int_value(Type.u1, 0);
        switch (zir_tag) {
            .add_with_overflow => {
                // If either of the arguments is zero, `false` is returned and the other is stored
                // to the result, even if it is undefined..
                // Otherwise, if either of the argument is undefined, undefined is returned.
                if (maybe_lhs_val) |lhs_val| {
                    if (!lhs_val.is_undef(mod) and (try lhs_val.compare_all_with_zero_advanced(.eq, sema))) {
                        break :result .{ .overflow_bit = try sema.splat(overflow_ty, zero_bit), .inst = rhs };
                    }
                }
                if (maybe_rhs_val) |rhs_val| {
                    if (!rhs_val.is_undef(mod) and (try rhs_val.compare_all_with_zero_advanced(.eq, sema))) {
                        break :result .{ .overflow_bit = try sema.splat(overflow_ty, zero_bit), .inst = lhs };
                    }
                }
                if (maybe_lhs_val) |lhs_val| {
                    if (maybe_rhs_val) |rhs_val| {
                        if (lhs_val.is_undef(mod) or rhs_val.is_undef(mod)) {
                            break :result .{ .overflow_bit = Value.undef, .wrapped = Value.undef };
                        }

                        const result = try sema.int_add_with_overflow(lhs_val, rhs_val, dest_ty);
                        break :result .{ .overflow_bit = result.overflow_bit, .wrapped = result.wrapped_result };
                    }
                }
            },
            .sub_with_overflow => {
                // If the rhs is zero, then the result is lhs and no overflow occured.
                // Otherwise, if either result is undefined, both results are undefined.
                if (maybe_rhs_val) |rhs_val| {
                    if (rhs_val.is_undef(mod)) {
                        break :result .{ .overflow_bit = Value.undef, .wrapped = Value.undef };
                    } else if (try rhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                        break :result .{ .overflow_bit = try sema.splat(overflow_ty, zero_bit), .inst = lhs };
                    } else if (maybe_lhs_val) |lhs_val| {
                        if (lhs_val.is_undef(mod)) {
                            break :result .{ .overflow_bit = Value.undef, .wrapped = Value.undef };
                        }

                        const result = try sema.int_sub_with_overflow(lhs_val, rhs_val, dest_ty);
                        break :result .{ .overflow_bit = result.overflow_bit, .wrapped = result.wrapped_result };
                    }
                }
            },
            .mul_with_overflow => {
                // If either of the arguments is zero, the result is zero and no overflow occured.
                // If either of the arguments is one, the result is the other and no overflow occured.
                // Otherwise, if either of the arguments is undefined, both results are undefined.
                const scalar_one = try mod.int_value(dest_ty.scalar_type(mod), 1);
                if (maybe_lhs_val) |lhs_val| {
                    if (!lhs_val.is_undef(mod)) {
                        if (try lhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                            break :result .{ .overflow_bit = try sema.splat(overflow_ty, zero_bit), .inst = lhs };
                        } else if (try sema.compare_all(lhs_val, .eq, try sema.splat(dest_ty, scalar_one), dest_ty)) {
                            break :result .{ .overflow_bit = try sema.splat(overflow_ty, zero_bit), .inst = rhs };
                        }
                    }
                }

                if (maybe_rhs_val) |rhs_val| {
                    if (!rhs_val.is_undef(mod)) {
                        if (try rhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                            break :result .{ .overflow_bit = try sema.splat(overflow_ty, zero_bit), .inst = rhs };
                        } else if (try sema.compare_all(rhs_val, .eq, try sema.splat(dest_ty, scalar_one), dest_ty)) {
                            break :result .{ .overflow_bit = try sema.splat(overflow_ty, zero_bit), .inst = lhs };
                        }
                    }
                }

                if (maybe_lhs_val) |lhs_val| {
                    if (maybe_rhs_val) |rhs_val| {
                        if (lhs_val.is_undef(mod) or rhs_val.is_undef(mod)) {
                            break :result .{ .overflow_bit = Value.undef, .wrapped = Value.undef };
                        }

                        const result = try lhs_val.int_mul_with_overflow(rhs_val, dest_ty, sema.arena, mod);
                        break :result .{ .overflow_bit = result.overflow_bit, .wrapped = result.wrapped_result };
                    }
                }
            },
            .shl_with_overflow => {
                // If lhs is zero, the result is zero and no overflow occurred.
                // If rhs is zero, the result is lhs (even if undefined) and no overflow occurred.
                // Oterhwise if either of the arguments is undefined, both results are undefined.
                if (maybe_lhs_val) |lhs_val| {
                    if (!lhs_val.is_undef(mod) and (try lhs_val.compare_all_with_zero_advanced(.eq, sema))) {
                        break :result .{ .overflow_bit = try sema.splat(overflow_ty, zero_bit), .inst = lhs };
                    }
                }
                if (maybe_rhs_val) |rhs_val| {
                    if (!rhs_val.is_undef(mod) and (try rhs_val.compare_all_with_zero_advanced(.eq, sema))) {
                        break :result .{ .overflow_bit = try sema.splat(overflow_ty, zero_bit), .inst = lhs };
                    }
                }
                if (maybe_lhs_val) |lhs_val| {
                    if (maybe_rhs_val) |rhs_val| {
                        if (lhs_val.is_undef(mod) or rhs_val.is_undef(mod)) {
                            break :result .{ .overflow_bit = Value.undef, .wrapped = Value.undef };
                        }

                        const result = try lhs_val.shl_with_overflow(rhs_val, dest_ty, sema.arena, mod);
                        break :result .{ .overflow_bit = result.overflow_bit, .wrapped = result.wrapped_result };
                    }
                }
            },
            else => unreachable,
        }

        const air_tag: Air.Inst.Tag = switch (zir_tag) {
            .add_with_overflow => .add_with_overflow,
            .mul_with_overflow => .mul_with_overflow,
            .sub_with_overflow => .sub_with_overflow,
            .shl_with_overflow => .shl_with_overflow,
            else => unreachable,
        };

        const runtime_src = if (maybe_lhs_val == null) lhs_src else rhs_src;
        try sema.require_runtime_block(block, src, runtime_src);

        return block.add_inst(.{
            .tag = air_tag,
            .data = .{ .ty_pl = .{
                .ty = Air.interned_to_ref(tuple_ty.to_intern()),
                .payload = try block.sema.add_extra(Air.Bin{
                    .lhs = lhs,
                    .rhs = rhs,
                }),
            } },
        });
    };

    if (result.inst != .none) {
        if (try sema.resolve_value(result.inst)) |some| {
            result.wrapped = some;
            result.inst = .none;
        }
    }

    if (result.inst == .none) {
        return Air.interned_to_ref((try mod.intern(.{ .aggregate = .{
            .ty = tuple_ty.to_intern(),
            .storage = .{ .elems = &.{
                result.wrapped.to_intern(),
                result.overflow_bit.to_intern(),
            } },
        } })));
    }

    const element_refs = try sema.arena.alloc(Air.Inst.Ref, 2);
    element_refs[0] = result.inst;
    element_refs[1] = Air.interned_to_ref(result.overflow_bit.to_intern());
    return block.add_aggregate_init(tuple_ty, element_refs);
}

fn splat(sema: *Sema, ty: Type, val: Value) !Value {
    const mod = sema.mod;
    if (ty.zig_type_tag(mod) != .Vector) return val;
    const repeated = try mod.intern(.{ .aggregate = .{
        .ty = ty.to_intern(),
        .storage = .{ .repeated_elem = val.to_intern() },
    } });
    return Value.from_interned(repeated);
}

fn overflow_arithmetic_tuple_type(sema: *Sema, ty: Type) !Type {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const ov_ty = if (ty.zig_type_tag(mod) == .Vector) try mod.vector_type(.{
        .len = ty.vector_len(mod),
        .child = .u1_type,
    }) else Type.u1;

    const types = [2]InternPool.Index{ ty.to_intern(), ov_ty.to_intern() };
    const values = [2]InternPool.Index{ .none, .none };
    const tuple_ty = try ip.get_anon_struct_type(mod.gpa, .{
        .types = &types,
        .values = &values,
        .names = &.{},
    });
    return Type.from_interned(tuple_ty);
}

fn analyze_arithmetic(
    sema: *Sema,
    block: *Block,
    /// TODO performance investigation: make this comptime?
    zir_tag: Zir.Inst.Tag,
    lhs: Air.Inst.Ref,
    rhs: Air.Inst.Ref,
    src: LazySrcLoc,
    lhs_src: LazySrcLoc,
    rhs_src: LazySrcLoc,
    want_safety: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const lhs_ty = sema.type_of(lhs);
    const rhs_ty = sema.type_of(rhs);
    const lhs_zig_ty_tag = try lhs_ty.zig_type_tag_or_poison(mod);
    const rhs_zig_ty_tag = try rhs_ty.zig_type_tag_or_poison(mod);
    try sema.check_vectorizable_binary_operands(block, src, lhs_ty, rhs_ty, lhs_src, rhs_src);

    if (lhs_zig_ty_tag == .Pointer) switch (lhs_ty.ptr_size(mod)) {
        .One, .Slice => {},
        .Many, .C => {
            const air_tag: Air.Inst.Tag = switch (zir_tag) {
                .add => .ptr_add,
                .sub => .ptr_sub,
                else => return sema.fail(block, src, "invalid pointer arithmetic operator", .{}),
            };
            return sema.analyze_ptr_arithmetic(block, src, lhs, rhs, air_tag, lhs_src, rhs_src);
        },
    };

    const instructions = &[_]Air.Inst.Ref{ lhs, rhs };
    const resolved_type = try sema.resolve_peer_types(block, src, instructions, .{
        .override = &[_]?LazySrcLoc{ lhs_src, rhs_src },
    });

    const casted_lhs = try sema.coerce(block, resolved_type, lhs, lhs_src);
    const casted_rhs = try sema.coerce(block, resolved_type, rhs, rhs_src);

    const scalar_type = resolved_type.scalar_type(mod);
    const scalar_tag = scalar_type.zig_type_tag(mod);

    const is_int = scalar_tag == .Int or scalar_tag == .ComptimeInt;

    try sema.check_arithmetic_op(block, src, scalar_tag, lhs_zig_ty_tag, rhs_zig_ty_tag, zir_tag);

    const maybe_lhs_val = try sema.resolve_value_intable(casted_lhs);
    const maybe_rhs_val = try sema.resolve_value_intable(casted_rhs);
    const runtime_src: LazySrcLoc, const air_tag: Air.Inst.Tag, const air_tag_safe: Air.Inst.Tag = rs: {
        switch (zir_tag) {
            .add, .add_unsafe => {
                // For integers:int_add_sat
                // If either of the operands are zero, then the other operand is
                // returned, even if it is undefined.
                // If either of the operands are undefined, it's a compile error
                // because there is a possible value for which the addition would
                // overflow (max_int), causing illegal behavior.
                // For floats: either operand being undef makes the result undef.
                if (maybe_lhs_val) |lhs_val| {
                    if (!lhs_val.is_undef(mod) and (try lhs_val.compare_all_with_zero_advanced(.eq, sema))) {
                        return casted_rhs;
                    }
                }
                if (maybe_rhs_val) |rhs_val| {
                    if (rhs_val.is_undef(mod)) {
                        if (is_int) {
                            return sema.fail_with_use_of_undef(block, rhs_src);
                        } else {
                            return mod.undef_ref(resolved_type);
                        }
                    }
                    if (try rhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                        return casted_lhs;
                    }
                }
                const air_tag: Air.Inst.Tag = if (block.float_mode == .optimized) .add_optimized else .add;
                if (maybe_lhs_val) |lhs_val| {
                    if (lhs_val.is_undef(mod)) {
                        if (is_int) {
                            return sema.fail_with_use_of_undef(block, lhs_src);
                        } else {
                            return mod.undef_ref(resolved_type);
                        }
                    }
                    if (maybe_rhs_val) |rhs_val| {
                        if (is_int) {
                            var overflow_idx: ?usize = null;
                            const sum = try sema.int_add(lhs_val, rhs_val, resolved_type, &overflow_idx);
                            if (overflow_idx) |vec_idx| {
                                return sema.fail_with_integer_overflow(block, src, resolved_type, sum, vec_idx);
                            }
                            return Air.interned_to_ref(sum.to_intern());
                        } else {
                            return Air.interned_to_ref((try Value.float_add(lhs_val, rhs_val, resolved_type, sema.arena, mod)).to_intern());
                        }
                    } else break :rs .{ rhs_src, air_tag, .add_safe };
                } else break :rs .{ lhs_src, air_tag, .add_safe };
            },
            .addwrap => {
                // Integers only; floats are checked above.
                // If either of the operands are zero, the other operand is returned.
                // If either of the operands are undefined, the result is undefined.
                if (maybe_lhs_val) |lhs_val| {
                    if (!lhs_val.is_undef(mod) and (try lhs_val.compare_all_with_zero_advanced(.eq, sema))) {
                        return casted_rhs;
                    }
                }
                if (maybe_rhs_val) |rhs_val| {
                    if (rhs_val.is_undef(mod)) {
                        return mod.undef_ref(resolved_type);
                    }
                    if (try rhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                        return casted_lhs;
                    }
                    if (maybe_lhs_val) |lhs_val| {
                        return Air.interned_to_ref((try sema.number_add_wrap_scalar(lhs_val, rhs_val, resolved_type)).to_intern());
                    } else break :rs .{ lhs_src, .add_wrap, .add_wrap };
                } else break :rs .{ rhs_src, .add_wrap, .add_wrap };
            },
            .add_sat => {
                // Integers only; floats are checked above.
                // If either of the operands are zero, then the other operand is returned.
                // If either of the operands are undefined, the result is undefined.
                if (maybe_lhs_val) |lhs_val| {
                    if (!lhs_val.is_undef(mod) and (try lhs_val.compare_all_with_zero_advanced(.eq, sema))) {
                        return casted_rhs;
                    }
                }
                if (maybe_rhs_val) |rhs_val| {
                    if (rhs_val.is_undef(mod)) {
                        return mod.undef_ref(resolved_type);
                    }
                    if (try rhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                        return casted_lhs;
                    }
                    if (maybe_lhs_val) |lhs_val| {
                        if (lhs_val.is_undef(mod)) {
                            return mod.undef_ref(resolved_type);
                        }

                        const val = if (scalar_tag == .ComptimeInt)
                            try sema.int_add(lhs_val, rhs_val, resolved_type, undefined)
                        else
                            try lhs_val.int_add_sat(rhs_val, resolved_type, sema.arena, mod);

                        return Air.interned_to_ref(val.to_intern());
                    } else break :rs .{
                        lhs_src,
                        .add_sat,
                        .add_sat,
                    };
                } else break :rs .{
                    rhs_src,
                    .add_sat,
                    .add_sat,
                };
            },
            .sub => {
                // For integers:
                // If the rhs is zero, then the other operand is
                // returned, even if it is undefined.
                // If either of the operands are undefined, it's a compile error
                // because there is a possible value for which the subtraction would
                // overflow, causing illegal behavior.
                // For floats: either operand being undef makes the result undef.
                if (maybe_rhs_val) |rhs_val| {
                    if (rhs_val.is_undef(mod)) {
                        if (is_int) {
                            return sema.fail_with_use_of_undef(block, rhs_src);
                        } else {
                            return mod.undef_ref(resolved_type);
                        }
                    }
                    if (try rhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                        return casted_lhs;
                    }
                }
                const air_tag: Air.Inst.Tag = if (block.float_mode == .optimized) .sub_optimized else .sub;
                if (maybe_lhs_val) |lhs_val| {
                    if (lhs_val.is_undef(mod)) {
                        if (is_int) {
                            return sema.fail_with_use_of_undef(block, lhs_src);
                        } else {
                            return mod.undef_ref(resolved_type);
                        }
                    }
                    if (maybe_rhs_val) |rhs_val| {
                        if (is_int) {
                            var overflow_idx: ?usize = null;
                            const diff = try sema.int_sub(lhs_val, rhs_val, resolved_type, &overflow_idx);
                            if (overflow_idx) |vec_idx| {
                                return sema.fail_with_integer_overflow(block, src, resolved_type, diff, vec_idx);
                            }
                            return Air.interned_to_ref(diff.to_intern());
                        } else {
                            return Air.interned_to_ref((try Value.float_sub(lhs_val, rhs_val, resolved_type, sema.arena, mod)).to_intern());
                        }
                    } else break :rs .{ rhs_src, air_tag, .sub_safe };
                } else break :rs .{ lhs_src, air_tag, .sub_safe };
            },
            .subwrap => {
                // Integers only; floats are checked above.
                // If the RHS is zero, then the LHS is returned, even if it is undefined.
                // If either of the operands are undefined, the result is undefined.
                if (maybe_rhs_val) |rhs_val| {
                    if (rhs_val.is_undef(mod)) {
                        return mod.undef_ref(resolved_type);
                    }
                    if (try rhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                        return casted_lhs;
                    }
                }
                if (maybe_lhs_val) |lhs_val| {
                    if (lhs_val.is_undef(mod)) {
                        return mod.undef_ref(resolved_type);
                    }
                    if (maybe_rhs_val) |rhs_val| {
                        return Air.interned_to_ref((try sema.number_sub_wrap_scalar(lhs_val, rhs_val, resolved_type)).to_intern());
                    } else break :rs .{ rhs_src, .sub_wrap, .sub_wrap };
                } else break :rs .{ lhs_src, .sub_wrap, .sub_wrap };
            },
            .sub_sat => {
                // Integers only; floats are checked above.
                // If the RHS is zero, then the LHS is returned, even if it is undefined.
                // If either of the operands are undefined, the result is undefined.
                if (maybe_rhs_val) |rhs_val| {
                    if (rhs_val.is_undef(mod)) {
                        return mod.undef_ref(resolved_type);
                    }
                    if (try rhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                        return casted_lhs;
                    }
                }
                if (maybe_lhs_val) |lhs_val| {
                    if (lhs_val.is_undef(mod)) {
                        return mod.undef_ref(resolved_type);
                    }
                    if (maybe_rhs_val) |rhs_val| {
                        const val = if (scalar_tag == .ComptimeInt)
                            try sema.int_sub(lhs_val, rhs_val, resolved_type, undefined)
                        else
                            try lhs_val.int_sub_sat(rhs_val, resolved_type, sema.arena, mod);

                        return Air.interned_to_ref(val.to_intern());
                    } else break :rs .{ rhs_src, .sub_sat, .sub_sat };
                } else break :rs .{ lhs_src, .sub_sat, .sub_sat };
            },
            .mul => {
                // For integers:
                // If either of the operands are zero, the result is zero.
                // If either of the operands are one, the result is the other
                // operand, even if it is undefined.
                // If either of the operands are undefined, it's a compile error
                // because there is a possible value for which the addition would
                // overflow (max_int), causing illegal behavior.
                //
                // For floats:
                // If either of the operands are undefined, the result is undefined.
                // If either of the operands are inf, and the other operand is zero,
                // the result is nan.
                // If either of the operands are nan, the result is nan.
                const scalar_zero = switch (scalar_tag) {
                    .ComptimeFloat, .Float => try mod.float_value(scalar_type, 0.0),
                    .ComptimeInt, .Int => try mod.int_value(scalar_type, 0),
                    else => unreachable,
                };
                const scalar_one = switch (scalar_tag) {
                    .ComptimeFloat, .Float => try mod.float_value(scalar_type, 1.0),
                    .ComptimeInt, .Int => try mod.int_value(scalar_type, 1),
                    else => unreachable,
                };
                if (maybe_lhs_val) |lhs_val| {
                    if (!lhs_val.is_undef(mod)) {
                        if (lhs_val.is_nan(mod)) {
                            return Air.interned_to_ref(lhs_val.to_intern());
                        }
                        if (try lhs_val.compare_all_with_zero_advanced(.eq, sema)) lz: {
                            if (maybe_rhs_val) |rhs_val| {
                                if (rhs_val.is_nan(mod)) {
                                    return Air.interned_to_ref(rhs_val.to_intern());
                                }
                                if (rhs_val.is_inf(mod)) {
                                    return Air.interned_to_ref((try mod.float_value(resolved_type, std.math.nan(f128))).to_intern());
                                }
                            } else if (resolved_type.is_any_float()) {
                                break :lz;
                            }
                            const zero_val = try sema.splat(resolved_type, scalar_zero);
                            return Air.interned_to_ref(zero_val.to_intern());
                        }
                        if (try sema.compare_all(lhs_val, .eq, try sema.splat(resolved_type, scalar_one), resolved_type)) {
                            return casted_rhs;
                        }
                    }
                }
                const air_tag: Air.Inst.Tag = if (block.float_mode == .optimized) .mul_optimized else .mul;
                if (maybe_rhs_val) |rhs_val| {
                    if (rhs_val.is_undef(mod)) {
                        if (is_int) {
                            return sema.fail_with_use_of_undef(block, rhs_src);
                        } else {
                            return mod.undef_ref(resolved_type);
                        }
                    }
                    if (rhs_val.is_nan(mod)) {
                        return Air.interned_to_ref(rhs_val.to_intern());
                    }
                    if (try rhs_val.compare_all_with_zero_advanced(.eq, sema)) rz: {
                        if (maybe_lhs_val) |lhs_val| {
                            if (lhs_val.is_inf(mod)) {
                                return Air.interned_to_ref((try mod.float_value(resolved_type, std.math.nan(f128))).to_intern());
                            }
                        } else if (resolved_type.is_any_float()) {
                            break :rz;
                        }
                        const zero_val = try sema.splat(resolved_type, scalar_zero);
                        return Air.interned_to_ref(zero_val.to_intern());
                    }
                    if (try sema.compare_all(rhs_val, .eq, try sema.splat(resolved_type, scalar_one), resolved_type)) {
                        return casted_lhs;
                    }
                    if (maybe_lhs_val) |lhs_val| {
                        if (lhs_val.is_undef(mod)) {
                            if (is_int) {
                                return sema.fail_with_use_of_undef(block, lhs_src);
                            } else {
                                return mod.undef_ref(resolved_type);
                            }
                        }
                        if (is_int) {
                            var overflow_idx: ?usize = null;
                            const product = try lhs_val.int_mul(rhs_val, resolved_type, &overflow_idx, sema.arena, mod);
                            if (overflow_idx) |vec_idx| {
                                return sema.fail_with_integer_overflow(block, src, resolved_type, product, vec_idx);
                            }
                            return Air.interned_to_ref(product.to_intern());
                        } else {
                            return Air.interned_to_ref((try lhs_val.float_mul(rhs_val, resolved_type, sema.arena, mod)).to_intern());
                        }
                    } else break :rs .{ lhs_src, air_tag, .mul_safe };
                } else break :rs .{ rhs_src, air_tag, .mul_safe };
            },
            .mulwrap => {
                // Integers only; floats are handled above.
                // If either of the operands are zero, result is zero.
                // If either of the operands are one, result is the other operand.
                // If either of the operands are undefined, result is undefined.
                const scalar_zero = switch (scalar_tag) {
                    .ComptimeFloat, .Float => try mod.float_value(scalar_type, 0.0),
                    .ComptimeInt, .Int => try mod.int_value(scalar_type, 0),
                    else => unreachable,
                };
                const scalar_one = switch (scalar_tag) {
                    .ComptimeFloat, .Float => try mod.float_value(scalar_type, 1.0),
                    .ComptimeInt, .Int => try mod.int_value(scalar_type, 1),
                    else => unreachable,
                };
                if (maybe_lhs_val) |lhs_val| {
                    if (!lhs_val.is_undef(mod)) {
                        if (try lhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                            const zero_val = try sema.splat(resolved_type, scalar_zero);
                            return Air.interned_to_ref(zero_val.to_intern());
                        }
                        if (try sema.compare_all(lhs_val, .eq, try sema.splat(resolved_type, scalar_one), resolved_type)) {
                            return casted_rhs;
                        }
                    }
                }
                if (maybe_rhs_val) |rhs_val| {
                    if (rhs_val.is_undef(mod)) {
                        return mod.undef_ref(resolved_type);
                    }
                    if (try rhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                        const zero_val = try sema.splat(resolved_type, scalar_zero);
                        return Air.interned_to_ref(zero_val.to_intern());
                    }
                    if (try sema.compare_all(rhs_val, .eq, try sema.splat(resolved_type, scalar_one), resolved_type)) {
                        return casted_lhs;
                    }
                    if (maybe_lhs_val) |lhs_val| {
                        if (lhs_val.is_undef(mod)) {
                            return mod.undef_ref(resolved_type);
                        }
                        return Air.interned_to_ref((try lhs_val.number_mul_wrap(rhs_val, resolved_type, sema.arena, mod)).to_intern());
                    } else break :rs .{ lhs_src, .mul_wrap, .mul_wrap };
                } else break :rs .{ rhs_src, .mul_wrap, .mul_wrap };
            },
            .mul_sat => {
                // Integers only; floats are checked above.
                // If either of the operands are zero, result is zero.
                // If either of the operands are one, result is the other operand.
                // If either of the operands are undefined, result is undefined.
                const scalar_zero = switch (scalar_tag) {
                    .ComptimeFloat, .Float => try mod.float_value(scalar_type, 0.0),
                    .ComptimeInt, .Int => try mod.int_value(scalar_type, 0),
                    else => unreachable,
                };
                const scalar_one = switch (scalar_tag) {
                    .ComptimeFloat, .Float => try mod.float_value(scalar_type, 1.0),
                    .ComptimeInt, .Int => try mod.int_value(scalar_type, 1),
                    else => unreachable,
                };
                if (maybe_lhs_val) |lhs_val| {
                    if (!lhs_val.is_undef(mod)) {
                        if (try lhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                            const zero_val = try sema.splat(resolved_type, scalar_zero);
                            return Air.interned_to_ref(zero_val.to_intern());
                        }
                        if (try sema.compare_all(lhs_val, .eq, try sema.splat(resolved_type, scalar_one), resolved_type)) {
                            return casted_rhs;
                        }
                    }
                }
                if (maybe_rhs_val) |rhs_val| {
                    if (rhs_val.is_undef(mod)) {
                        return mod.undef_ref(resolved_type);
                    }
                    if (try rhs_val.compare_all_with_zero_advanced(.eq, sema)) {
                        const zero_val = try sema.splat(resolved_type, scalar_zero);
                        return Air.interned_to_ref(zero_val.to_intern());
                    }
                    if (try sema.compare_all(rhs_val, .eq, try sema.splat(resolved_type, scalar_one), resolved_type)) {
                        return casted_lhs;
                    }
                    if (maybe_lhs_val) |lhs_val| {
                        if (lhs_val.is_undef(mod)) {
                            return mod.undef_ref(resolved_type);
                        }

                        const val = if (scalar_tag == .ComptimeInt)
                            try lhs_val.int_mul(rhs_val, resolved_type, undefined, sema.arena, mod)
                        else
                            try lhs_val.int_mul_sat(rhs_val, resolved_type, sema.arena, mod);

                        return Air.interned_to_ref(val.to_intern());
                    } else break :rs .{ lhs_src, .mul_sat, .mul_sat };
                } else break :rs .{ rhs_src, .mul_sat, .mul_sat };
            },
            else => unreachable,
        }
    };

    try sema.require_runtime_block(block, src, runtime_src);

    if (block.want_safety() and want_safety and scalar_tag == .Int) {
        if (mod.backend_supports_feature(.safety_checked_instructions)) {
            if (air_tag != air_tag_safe) {
                _ = try sema.prepare_panic_id(block, .integer_overflow);
            }
            return block.add_bin_op(air_tag_safe, casted_lhs, casted_rhs);
        } else {
            const maybe_op_ov: ?Air.Inst.Tag = switch (air_tag) {
                .add => .add_with_overflow,
                .sub => .sub_with_overflow,
                .mul => .mul_with_overflow,
                else => null,
            };
            if (maybe_op_ov) |op_ov_tag| {
                const op_ov_tuple_ty = try sema.overflow_arithmetic_tuple_type(resolved_type);
                const op_ov = try block.add_inst(.{
                    .tag = op_ov_tag,
                    .data = .{ .ty_pl = .{
                        .ty = Air.interned_to_ref(op_ov_tuple_ty.to_intern()),
                        .payload = try sema.add_extra(Air.Bin{
                            .lhs = casted_lhs,
                            .rhs = casted_rhs,
                        }),
                    } },
                });
                const ov_bit = try sema.tuple_field_val_by_index(block, src, op_ov, 1, op_ov_tuple_ty);
                const any_ov_bit = if (resolved_type.zig_type_tag(mod) == .Vector)
                    try block.add_inst(.{
                        .tag = if (block.float_mode == .optimized) .reduce_optimized else .reduce,
                        .data = .{ .reduce = .{
                            .operand = ov_bit,
                            .operation = .Or,
                        } },
                    })
                else
                    ov_bit;
                const zero_ov = Air.interned_to_ref((try mod.int_value(Type.u1, 0)).to_intern());
                const no_ov = try block.add_bin_op(.cmp_eq, any_ov_bit, zero_ov);

                try sema.add_safety_check(block, src, no_ov, .integer_overflow);
                return sema.tuple_field_val_by_index(block, src, op_ov, 0, op_ov_tuple_ty);
            }
        }
    }
    return block.add_bin_op(air_tag, casted_lhs, casted_rhs);
}

fn analyze_ptr_arithmetic(
    sema: *Sema,
    block: *Block,
    op_src: LazySrcLoc,
    ptr: Air.Inst.Ref,
    uncasted_offset: Air.Inst.Ref,
    air_tag: Air.Inst.Tag,
    ptr_src: LazySrcLoc,
    offset_src: LazySrcLoc,
) CompileError!Air.Inst.Ref {
    // TODO if the operand is comptime-known to be negative, or is a negative int,
    // coerce to isize instead of usize.
    const offset = try sema.coerce(block, Type.usize, uncasted_offset, offset_src);
    const mod = sema.mod;
    const opt_ptr_val = try sema.resolve_value(ptr);
    const opt_off_val = try sema.resolve_defined_value(block, offset_src, offset);
    const ptr_ty = sema.type_of(ptr);
    const ptr_info = ptr_ty.ptr_info(mod);
    assert(ptr_info.flags.size == .Many or ptr_info.flags.size == .C);

    const new_ptr_ty = t: {
        // Calculate the new pointer alignment.
        // This code is duplicated in `elem_ptr_type`.
        if (ptr_info.flags.alignment == .none) {
            // ABI-aligned pointer. Any pointer arithmetic maintains the same ABI-alignedness.
            break :t ptr_ty;
        }
        // If the addend is not a comptime-known value we can still count on
        // it being a multiple of the type size.
        const elem_size = try sema.type_abi_size(Type.from_interned(ptr_info.child));
        const addend = if (opt_off_val) |off_val| a: {
            const off_int = try sema.usize_cast(block, offset_src, try off_val.to_unsigned_int_advanced(sema));
            break :a elem_size * off_int;
        } else elem_size;

        // The resulting pointer is aligned to the lcd between the offset (an
        // arbitrary number) and the alignment factor (always a power of two,
        // non zero).
        const new_align: Alignment = @enumFromInt(@min(
            @ctz(addend),
            @int_from_enum(ptr_info.flags.alignment),
        ));
        assert(new_align != .none);

        break :t try sema.ptr_type(.{
            .child = ptr_info.child,
            .sentinel = ptr_info.sentinel,
            .flags = .{
                .size = ptr_info.flags.size,
                .alignment = new_align,
                .is_const = ptr_info.flags.is_const,
                .is_volatile = ptr_info.flags.is_volatile,
                .is_allowzero = ptr_info.flags.is_allowzero,
                .address_space = ptr_info.flags.address_space,
            },
        });
    };

    const runtime_src = rs: {
        if (opt_ptr_val) |ptr_val| {
            if (opt_off_val) |offset_val| {
                if (ptr_val.is_undef(mod)) return mod.undef_ref(new_ptr_ty);

                const offset_int = try sema.usize_cast(block, offset_src, try offset_val.to_unsigned_int_advanced(sema));
                if (offset_int == 0) return ptr;
                if (air_tag == .ptr_sub) {
                    const elem_size = try sema.type_abi_size(Type.from_interned(ptr_info.child));
                    const new_ptr_val = try sema.ptr_subtract(block, op_src, ptr_val, offset_int * elem_size, new_ptr_ty);
                    return Air.interned_to_ref(new_ptr_val.to_intern());
                } else {
                    const new_ptr_val = try mod.get_coerced(try ptr_val.ptr_elem(offset_int, sema), new_ptr_ty);
                    return Air.interned_to_ref(new_ptr_val.to_intern());
                }
            } else break :rs offset_src;
        } else break :rs ptr_src;
    };

    try sema.require_runtime_block(block, op_src, runtime_src);
    return block.add_inst(.{
        .tag = air_tag,
        .data = .{ .ty_pl = .{
            .ty = Air.interned_to_ref(new_ptr_ty.to_intern()),
            .payload = try sema.add_extra(Air.Bin{
                .lhs = ptr,
                .rhs = offset,
            }),
        } },
    });
}

fn zir_load(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const ptr_src = src; // TODO better source location
    const ptr = try sema.resolve_inst(inst_data.operand);
    return sema.analyze_load(block, src, ptr, ptr_src);
}

fn zir_asm(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
    tmpl_is_expr: bool,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const extra = sema.code.extra_data(Zir.Inst.Asm, extended.operand);
    const src = LazySrcLoc.nodeOffset(extra.data.src_node);
    const ret_ty_src: LazySrcLoc = .{ .node_offset_asm_ret_ty = extra.data.src_node };
    const outputs_len: u5 = @truncate(extended.small);
    const inputs_len: u5 = @truncate(extended.small >> 5);
    const clobbers_len: u5 = @truncate(extended.small >> 10);
    const is_volatile = @as(u1, @truncate(extended.small >> 15)) != 0;
    const is_global_assembly = sema.func_index == .none;
    const zir_tags = sema.code.instructions.items(.tag);

    const asm_source: []const u8 = if (tmpl_is_expr) blk: {
        const tmpl: Zir.Inst.Ref = @enumFromInt(@int_from_enum(extra.data.asm_source));
        const s: []const u8 = try sema.resolve_const_string(block, src, tmpl, .{
            .needed_comptime_reason = "assembly code must be comptime-known",
        });
        break :blk s;
    } else sema.code.null_terminated_string(extra.data.asm_source);

    if (is_global_assembly) {
        if (outputs_len != 0) {
            return sema.fail(block, src, "module-level assembly does not support outputs", .{});
        }
        if (inputs_len != 0) {
            return sema.fail(block, src, "module-level assembly does not support inputs", .{});
        }
        if (clobbers_len != 0) {
            return sema.fail(block, src, "module-level assembly does not support clobbers", .{});
        }
        if (is_volatile) {
            return sema.fail(block, src, "volatile keyword is redundant on module-level assembly", .{});
        }
        try sema.mod.add_global_assembly(sema.owner_decl_index, asm_source);
        return .void_value;
    }

    if (block.is_comptime) {
        try sema.require_runtime_block(block, src, null);
    }

    var extra_i = extra.end;
    var output_type_bits = extra.data.output_type_bits;
    var needed_capacity: usize = @typeInfo(Air.Asm).Struct.fields.len + outputs_len + inputs_len;

    const ConstraintName = struct { c: []const u8, n: []const u8 };
    const out_args = try sema.arena.alloc(Air.Inst.Ref, outputs_len);
    const outputs = try sema.arena.alloc(ConstraintName, outputs_len);
    var expr_ty = Air.Inst.Ref.void_type;

    for (out_args, 0..) |*arg, out_i| {
        const output = sema.code.extra_data(Zir.Inst.Asm.Output, extra_i);
        extra_i = output.end;

        const is_type = @as(u1, @truncate(output_type_bits)) != 0;
        output_type_bits >>= 1;

        if (is_type) {
            // Indicate the output is the asm instruction return value.
            arg.* = .none;
            const out_ty = try sema.resolve_type(block, ret_ty_src, output.data.operand);
            try sema.queue_full_type_resolution(out_ty);
            expr_ty = Air.interned_to_ref(out_ty.to_intern());
        } else {
            arg.* = try sema.resolve_inst(output.data.operand);
        }

        const constraint = sema.code.null_terminated_string(output.data.constraint);
        const name = sema.code.null_terminated_string(output.data.name);
        needed_capacity += (constraint.len + name.len + (2 + 3)) / 4;

        if (output.data.operand.to_index()) |index| {
            if (zir_tags[@int_from_enum(index)] == .ref) {
                // TODO: better error location; it would be even nicer if there were notes that pointed at the output and the variable definition
                return sema.fail(block, src, "asm cannot output to const local '{s}'", .{name});
            }
        }

        outputs[out_i] = .{ .c = constraint, .n = name };
    }

    const args = try sema.arena.alloc(Air.Inst.Ref, inputs_len);
    const inputs = try sema.arena.alloc(ConstraintName, inputs_len);
    const mod = sema.mod;

    for (args, 0..) |*arg, arg_i| {
        const input = sema.code.extra_data(Zir.Inst.Asm.Input, extra_i);
        extra_i = input.end;

        const uncasted_arg = try sema.resolve_inst(input.data.operand);
        const uncasted_arg_ty = sema.type_of(uncasted_arg);
        switch (uncasted_arg_ty.zig_type_tag(mod)) {
            .ComptimeInt => arg.* = try sema.coerce(block, Type.usize, uncasted_arg, src),
            .ComptimeFloat => arg.* = try sema.coerce(block, Type.f64, uncasted_arg, src),
            else => {
                arg.* = uncasted_arg;
                try sema.queue_full_type_resolution(uncasted_arg_ty);
            },
        }

        const constraint = sema.code.null_terminated_string(input.data.constraint);
        const name = sema.code.null_terminated_string(input.data.name);
        needed_capacity += (constraint.len + name.len + (2 + 3)) / 4;
        inputs[arg_i] = .{ .c = constraint, .n = name };
    }

    const clobbers = try sema.arena.alloc([]const u8, clobbers_len);
    for (clobbers) |*name| {
        const name_index: Zir.NullTerminatedString = @enumFromInt(sema.code.extra[extra_i]);
        name.* = sema.code.null_terminated_string(name_index);
        extra_i += 1;

        needed_capacity += name.*.len / 4 + 1;
    }

    needed_capacity += (asm_source.len + 3) / 4;

    const gpa = sema.gpa;
    try sema.air_extra.ensure_unused_capacity(gpa, needed_capacity);
    const asm_air = try block.add_inst(.{
        .tag = .assembly,
        .data = .{ .ty_pl = .{
            .ty = expr_ty,
            .payload = sema.add_extra_assume_capacity(Air.Asm{
                .source_len = @int_cast(asm_source.len),
                .outputs_len = outputs_len,
                .inputs_len = @int_cast(args.len),
                .flags = (@as(u32, @int_from_bool(is_volatile)) << 31) | @as(u32, @int_cast(clobbers.len)),
            }),
        } },
    });
    sema.append_refs_assume_capacity(out_args);
    sema.append_refs_assume_capacity(args);
    for (outputs) |o| {
        const buffer = mem.slice_as_bytes(sema.air_extra.unused_capacity_slice());
        @memcpy(buffer[0..o.c.len], o.c);
        buffer[o.c.len] = 0;
        @memcpy(buffer[o.c.len + 1 ..][0..o.n.len], o.n);
        buffer[o.c.len + 1 + o.n.len] = 0;
        sema.air_extra.items.len += (o.c.len + o.n.len + (2 + 3)) / 4;
    }
    for (inputs) |input| {
        const buffer = mem.slice_as_bytes(sema.air_extra.unused_capacity_slice());
        @memcpy(buffer[0..input.c.len], input.c);
        buffer[input.c.len] = 0;
        @memcpy(buffer[input.c.len + 1 ..][0..input.n.len], input.n);
        buffer[input.c.len + 1 + input.n.len] = 0;
        sema.air_extra.items.len += (input.c.len + input.n.len + (2 + 3)) / 4;
    }
    for (clobbers) |clobber| {
        const buffer = mem.slice_as_bytes(sema.air_extra.unused_capacity_slice());
        @memcpy(buffer[0..clobber.len], clobber);
        buffer[clobber.len] = 0;
        sema.air_extra.items.len += clobber.len / 4 + 1;
    }
    {
        const buffer = mem.slice_as_bytes(sema.air_extra.unused_capacity_slice());
        @memcpy(buffer[0..asm_source.len], asm_source);
        sema.air_extra.items.len += (asm_source.len + 3) / 4;
    }
    return asm_air;
}

/// Only called for equality operators. See also `zir_cmp`.
fn zir_cmp_eq(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    op: std.math.CompareOperator,
    air_tag: Air.Inst.Tag,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const src: LazySrcLoc = inst_data.src();
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
    const lhs = try sema.resolve_inst(extra.lhs);
    const rhs = try sema.resolve_inst(extra.rhs);

    const lhs_ty = sema.type_of(lhs);
    const rhs_ty = sema.type_of(rhs);
    const lhs_ty_tag = lhs_ty.zig_type_tag(mod);
    const rhs_ty_tag = rhs_ty.zig_type_tag(mod);
    if (lhs_ty_tag == .Null and rhs_ty_tag == .Null) {
        // null == null, null != null
        return if (op == .eq) .bool_true else .bool_false;
    }

    // comparing null with optionals
    if (lhs_ty_tag == .Null and (rhs_ty_tag == .Optional or rhs_ty.is_cptr(mod))) {
        return sema.analyze_is_null(block, src, rhs, op == .neq);
    }
    if (rhs_ty_tag == .Null and (lhs_ty_tag == .Optional or lhs_ty.is_cptr(mod))) {
        return sema.analyze_is_null(block, src, lhs, op == .neq);
    }

    if (lhs_ty_tag == .Null or rhs_ty_tag == .Null) {
        const non_null_type = if (lhs_ty_tag == .Null) rhs_ty else lhs_ty;
        return sema.fail(block, src, "comparison of '{}' with null", .{non_null_type.fmt(mod)});
    }

    if (lhs_ty_tag == .Union and (rhs_ty_tag == .EnumLiteral or rhs_ty_tag == .Enum)) {
        return sema.analyze_cmp_union_tag(block, src, lhs, lhs_src, rhs, rhs_src, op);
    }
    if (rhs_ty_tag == .Union and (lhs_ty_tag == .EnumLiteral or lhs_ty_tag == .Enum)) {
        return sema.analyze_cmp_union_tag(block, src, rhs, rhs_src, lhs, lhs_src, op);
    }

    if (lhs_ty_tag == .ErrorSet and rhs_ty_tag == .ErrorSet) {
        const runtime_src: LazySrcLoc = src: {
            if (try sema.resolve_value(lhs)) |lval| {
                if (try sema.resolve_value(rhs)) |rval| {
                    if (lval.is_undef(mod) or rval.is_undef(mod)) {
                        return mod.undef_ref(Type.bool);
                    }
                    const lkey = mod.intern_pool.index_to_key(lval.to_intern());
                    const rkey = mod.intern_pool.index_to_key(rval.to_intern());
                    return if ((lkey.err.name == rkey.err.name) == (op == .eq))
                        .bool_true
                    else
                        .bool_false;
                } else {
                    break :src rhs_src;
                }
            } else {
                break :src lhs_src;
            }
        };
        try sema.require_runtime_block(block, src, runtime_src);
        return block.add_bin_op(air_tag, lhs, rhs);
    }
    if (lhs_ty_tag == .Type and rhs_ty_tag == .Type) {
        const lhs_as_type = try sema.analyze_as_type(block, lhs_src, lhs);
        const rhs_as_type = try sema.analyze_as_type(block, rhs_src, rhs);
        return if (lhs_as_type.eql(rhs_as_type, mod) == (op == .eq)) .bool_true else .bool_false;
    }
    return sema.analyze_cmp(block, src, lhs, rhs, op, lhs_src, rhs_src, true);
}

fn analyze_cmp_union_tag(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    un: Air.Inst.Ref,
    un_src: LazySrcLoc,
    tag: Air.Inst.Ref,
    tag_src: LazySrcLoc,
    op: std.math.CompareOperator,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const union_ty = sema.type_of(un);
    try sema.resolve_type_fields(union_ty);
    const union_tag_ty = union_ty.union_tag_type(mod) orelse {
        const msg = msg: {
            const msg = try sema.err_msg(block, un_src, "comparison of union and enum literal is only valid for tagged union types", .{});
            errdefer msg.destroy(sema.gpa);
            try mod.err_note_non_lazy(union_ty.decl_src_loc(mod), msg, "union '{}' is not a tagged union", .{union_ty.fmt(mod)});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    };
    // Coerce both the union and the tag to the union's tag type, and then execute the
    // enum comparison codepath.
    const coerced_tag = try sema.coerce(block, union_tag_ty, tag, tag_src);
    const coerced_union = try sema.coerce(block, union_tag_ty, un, un_src);

    if (try sema.resolve_value(coerced_tag)) |enum_val| {
        if (enum_val.is_undef(mod)) return mod.undef_ref(Type.bool);
        const field_ty = union_ty.union_field_type(enum_val, mod).?;
        if (field_ty.zig_type_tag(mod) == .NoReturn) {
            return .bool_false;
        }
    }

    return sema.cmp_self(block, src, coerced_union, coerced_tag, op, un_src, tag_src);
}

/// Only called for non-equality operators. See also `zir_cmp_eq`.
fn zir_cmp(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    op: std.math.CompareOperator,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const src: LazySrcLoc = inst_data.src();
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
    const lhs = try sema.resolve_inst(extra.lhs);
    const rhs = try sema.resolve_inst(extra.rhs);
    return sema.analyze_cmp(block, src, lhs, rhs, op, lhs_src, rhs_src, false);
}

fn analyze_cmp(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    lhs: Air.Inst.Ref,
    rhs: Air.Inst.Ref,
    op: std.math.CompareOperator,
    lhs_src: LazySrcLoc,
    rhs_src: LazySrcLoc,
    is_equality_cmp: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const lhs_ty = sema.type_of(lhs);
    const rhs_ty = sema.type_of(rhs);
    if (lhs_ty.zig_type_tag(mod) != .Optional and rhs_ty.zig_type_tag(mod) != .Optional) {
        try sema.check_vectorizable_binary_operands(block, src, lhs_ty, rhs_ty, lhs_src, rhs_src);
    }

    if (lhs_ty.zig_type_tag(mod) == .Vector and rhs_ty.zig_type_tag(mod) == .Vector) {
        return sema.cmp_vector(block, src, lhs, rhs, op, lhs_src, rhs_src);
    }
    if (lhs_ty.is_numeric(mod) and rhs_ty.is_numeric(mod)) {
        // This operation allows any combination of integer and float types, regardless of the
        // signed-ness, comptime-ness, and bit-width. So peer type resolution is incorrect for
        // numeric types.
        return sema.cmp_numeric(block, src, lhs, rhs, op, lhs_src, rhs_src);
    }
    if (is_equality_cmp and lhs_ty.zig_type_tag(mod) == .ErrorUnion and rhs_ty.zig_type_tag(mod) == .ErrorSet) {
        const casted_lhs = try sema.analyze_err_union_code(block, lhs_src, lhs);
        return sema.cmp_self(block, src, casted_lhs, rhs, op, lhs_src, rhs_src);
    }
    if (is_equality_cmp and lhs_ty.zig_type_tag(mod) == .ErrorSet and rhs_ty.zig_type_tag(mod) == .ErrorUnion) {
        const casted_rhs = try sema.analyze_err_union_code(block, rhs_src, rhs);
        return sema.cmp_self(block, src, lhs, casted_rhs, op, lhs_src, rhs_src);
    }
    const instructions = &[_]Air.Inst.Ref{ lhs, rhs };
    const resolved_type = try sema.resolve_peer_types(block, src, instructions, .{ .override = &[_]?LazySrcLoc{ lhs_src, rhs_src } });
    if (!resolved_type.is_self_comparable(mod, is_equality_cmp)) {
        return sema.fail(block, src, "operator {s} not allowed for type '{}'", .{
            compare_operator_name(op), resolved_type.fmt(mod),
        });
    }
    const casted_lhs = try sema.coerce(block, resolved_type, lhs, lhs_src);
    const casted_rhs = try sema.coerce(block, resolved_type, rhs, rhs_src);
    return sema.cmp_self(block, src, casted_lhs, casted_rhs, op, lhs_src, rhs_src);
}

fn compare_operator_name(comp: std.math.CompareOperator) []const u8 {
    return switch (comp) {
        .lt => "<",
        .lte => "<=",
        .eq => "==",
        .gte => ">=",
        .gt => ">",
        .neq => "!=",
    };
}

fn cmp_self(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    casted_lhs: Air.Inst.Ref,
    casted_rhs: Air.Inst.Ref,
    op: std.math.CompareOperator,
    lhs_src: LazySrcLoc,
    rhs_src: LazySrcLoc,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const resolved_type = sema.type_of(casted_lhs);
    const runtime_src: LazySrcLoc = src: {
        if (try sema.resolve_value(casted_lhs)) |lhs_val| {
            if (lhs_val.is_undef(mod)) return mod.undef_ref(Type.bool);
            if (try sema.resolve_value(casted_rhs)) |rhs_val| {
                if (rhs_val.is_undef(mod)) return mod.undef_ref(Type.bool);

                if (resolved_type.zig_type_tag(mod) == .Vector) {
                    const cmp_val = try sema.compare_vector(lhs_val, op, rhs_val, resolved_type);
                    return Air.interned_to_ref(cmp_val.to_intern());
                }

                return if (try sema.compare_all(lhs_val, op, rhs_val, resolved_type))
                    .bool_true
                else
                    .bool_false;
            } else {
                if (resolved_type.zig_type_tag(mod) == .Bool) {
                    // We can lower bool eq/neq more efficiently.
                    return sema.runtime_bool_cmp(block, src, op, casted_rhs, lhs_val.to_bool(), rhs_src);
                }
                break :src rhs_src;
            }
        } else {
            // For bools, we still check the other operand, because we can lower
            // bool eq/neq more efficiently.
            if (resolved_type.zig_type_tag(mod) == .Bool) {
                if (try sema.resolve_value(casted_rhs)) |rhs_val| {
                    if (rhs_val.is_undef(mod)) return mod.undef_ref(Type.bool);
                    return sema.runtime_bool_cmp(block, src, op, casted_lhs, rhs_val.to_bool(), lhs_src);
                }
            }
            break :src lhs_src;
        }
    };
    try sema.require_runtime_block(block, src, runtime_src);
    if (resolved_type.zig_type_tag(mod) == .Vector) {
        return block.add_cmp_vector(casted_lhs, casted_rhs, op);
    }
    const tag = Air.Inst.Tag.from_cmp_op(op, block.float_mode == .optimized);
    return block.add_bin_op(tag, casted_lhs, casted_rhs);
}

/// cmp_eq (x, false) => not(x)
/// cmp_eq (x, true ) => x
/// cmp_neq(x, false) => x
/// cmp_neq(x, true ) => not(x)
fn runtime_bool_cmp(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    op: std.math.CompareOperator,
    lhs: Air.Inst.Ref,
    rhs: bool,
    runtime_src: LazySrcLoc,
) CompileError!Air.Inst.Ref {
    if ((op == .neq) == rhs) {
        try sema.require_runtime_block(block, src, runtime_src);
        return block.add_ty_op(.not, Type.bool, lhs);
    } else {
        return lhs;
    }
}

fn zir_size_of(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const ty = try sema.resolve_type(block, operand_src, inst_data.operand);
    switch (ty.zig_type_tag(mod)) {
        .Fn,
        .NoReturn,
        .Undefined,
        .Null,
        .Opaque,
        => return sema.fail(block, operand_src, "no size available for type '{}'", .{ty.fmt(mod)}),

        .Type,
        .EnumLiteral,
        .ComptimeFloat,
        .ComptimeInt,
        .Void,
        => return mod.int_ref(Type.comptime_int, 0),

        .Bool,
        .Int,
        .Float,
        .Pointer,
        .Array,
        .Struct,
        .Optional,
        .ErrorUnion,
        .ErrorSet,
        .Enum,
        .Union,
        .Vector,
        .Frame,
        .AnyFrame,
        => {},
    }
    const val = try ty.lazy_abi_size(mod);
    if (val.is_lazy_size(mod)) {
        try sema.queue_full_type_resolution(ty);
    }
    return Air.interned_to_ref(val.to_intern());
}

fn zir_bit_size_of(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const operand_ty = try sema.resolve_type(block, operand_src, inst_data.operand);
    switch (operand_ty.zig_type_tag(mod)) {
        .Fn,
        .NoReturn,
        .Undefined,
        .Null,
        .Opaque,
        => return sema.fail(block, operand_src, "no size available for type '{}'", .{operand_ty.fmt(mod)}),

        .Type,
        .EnumLiteral,
        .ComptimeFloat,
        .ComptimeInt,
        .Void,
        => return mod.int_ref(Type.comptime_int, 0),

        .Bool,
        .Int,
        .Float,
        .Pointer,
        .Array,
        .Struct,
        .Optional,
        .ErrorUnion,
        .ErrorSet,
        .Enum,
        .Union,
        .Vector,
        .Frame,
        .AnyFrame,
        => {},
    }
    const bit_size = try operand_ty.bit_size_advanced(mod, sema);
    return mod.int_ref(Type.comptime_int, bit_size);
}

fn zir_this(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const this_decl_index = mod.namespace_ptr(block.namespace).decl_index;
    const src = LazySrcLoc.nodeOffset(@bit_cast(extended.operand));
    return sema.analyze_decl_val(block, src, this_decl_index);
}

fn zir_closure_get(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const captures = mod.namespace_ptr(block.namespace).get_type(mod).get_captures(mod);

    const src_node: i32 = @bit_cast(extended.operand);
    const src = LazySrcLoc.nodeOffset(src_node);

    const capture_ty = switch (captures.get(ip)[extended.small].unwrap()) {
        .@"comptime" => |index| return Air.interned_to_ref(index),
        .runtime => |index| index,
        .decl_val => |decl_index| return sema.analyze_decl_val(block, src, decl_index),
        .decl_ref => |decl_index| return sema.analyze_decl_ref(decl_index),
    };

    // The comptime case is handled already above. Runtime case below.

    if (!block.is_typeof and sema.func_index == .none) {
        const msg = msg: {
            const name = name: {
                const file = sema.owner_decl.get_file_scope(mod);
                const tree = file.get_tree(sema.gpa) catch |err| {
                    // In this case we emit a warning + a less precise source location.
                    log.warn("unable to load {s}: {s}", .{
                        file.sub_file_path, @errorName(err),
                    });
                    break :name null;
                };
                const node = sema.owner_decl.relative_to_node_index(src_node);
                const token = tree.nodes.items(.main_token)[node];
                break :name tree.token_slice(token);
            };

            const msg = if (name) |some|
                try sema.err_msg(block, src, "'{s}' not accessible outside function scope", .{some})
            else
                try sema.err_msg(block, src, "variable not accessible outside function scope", .{});
            errdefer msg.destroy(sema.gpa);

            // TODO add "declared here" note
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    if (!block.is_typeof and !block.is_comptime and sema.func_index != .none) {
        const msg = msg: {
            const name = name: {
                const file = sema.owner_decl.get_file_scope(mod);
                const tree = file.get_tree(sema.gpa) catch |err| {
                    // In this case we emit a warning + a less precise source location.
                    log.warn("unable to load {s}: {s}", .{
                        file.sub_file_path, @errorName(err),
                    });
                    break :name null;
                };
                const node = sema.owner_decl.relative_to_node_index(src_node);
                const token = tree.nodes.items(.main_token)[node];
                break :name tree.token_slice(token);
            };

            const msg = if (name) |some|
                try sema.err_msg(block, src, "'{s}' not accessible from inner function", .{some})
            else
                try sema.err_msg(block, src, "variable not accessible from inner function", .{});
            errdefer msg.destroy(sema.gpa);

            try sema.err_note(block, LazySrcLoc.nodeOffset(0), msg, "crossed function definition here", .{});

            // TODO add "declared here" note
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    assert(block.is_typeof);
    // We need a dummy runtime instruction with the correct type.
    return block.add_ty(.alloc, Type.from_interned(capture_ty));
}

fn zir_ret_addr(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    _ = extended;
    if (block.is_comptime) {
        // TODO: we could give a meaningful lazy value here. #14938
        return sema.mod.int_ref(Type.usize, 0);
    } else {
        return block.add_no_op(.ret_addr);
    }
}

fn zir_frame_address(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    const src = LazySrcLoc.nodeOffset(@bit_cast(extended.operand));
    try sema.require_runtime_block(block, src, null);
    return try block.add_no_op(.frame_addr);
}

fn zir_builtin_src(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    _ = block;
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const extra = sema.code.extra_data(Zir.Inst.Src, extended.operand).data;
    const fn_owner_decl = mod.func_owner_decl_ptr(sema.func_index);
    const ip = &mod.intern_pool;
    const gpa = sema.gpa;

    const func_name_val = v: {
        const func_name_len = fn_owner_decl.name.length(ip);
        const array_ty = try ip.get(gpa, .{ .array_type = .{
            .len = func_name_len,
            .sentinel = .zero_u8,
            .child = .u8_type,
        } });
        break :v try ip.get(gpa, .{ .slice = .{
            .ty = .slice_const_u8_sentinel_0_type,
            .ptr = try ip.get(gpa, .{ .ptr = .{
                .ty = .manyptr_const_u8_sentinel_0_type,
                .base_addr = .{ .anon_decl = .{
                    .orig_ty = .slice_const_u8_sentinel_0_type,
                    .val = try ip.get(gpa, .{ .aggregate = .{
                        .ty = array_ty,
                        .storage = .{ .bytes = fn_owner_decl.name.to_string() },
                    } }),
                } },
                .byte_offset = 0,
            } }),
            .len = (try mod.int_value(Type.usize, func_name_len)).to_intern(),
        } });
    };

    const file_name_val = v: {
        // The compiler must not call realpath anywhere.
        const file_name = try fn_owner_decl.get_file_scope(mod).full_path(sema.arena);
        const array_ty = try ip.get(gpa, .{ .array_type = .{
            .len = file_name.len,
            .sentinel = .zero_u8,
            .child = .u8_type,
        } });
        break :v try ip.get(gpa, .{ .slice = .{
            .ty = .slice_const_u8_sentinel_0_type,
            .ptr = try ip.get(gpa, .{ .ptr = .{
                .ty = .manyptr_const_u8_sentinel_0_type,
                .base_addr = .{ .anon_decl = .{
                    .orig_ty = .slice_const_u8_sentinel_0_type,
                    .val = try ip.get(gpa, .{ .aggregate = .{
                        .ty = array_ty,
                        .storage = .{
                            .bytes = try ip.get_or_put_string(gpa, file_name, .maybe_embedded_nulls),
                        },
                    } }),
                } },
                .byte_offset = 0,
            } }),
            .len = (try mod.int_value(Type.usize, file_name.len)).to_intern(),
        } });
    };

    const src_loc_ty = try sema.get_builtin_type("SourceLocation");
    const fields = .{
        // file: [:0]const u8,
        file_name_val,
        // fn_name: [:0]const u8,
        func_name_val,
        // line: u32,
        (try mod.int_value(Type.u32, extra.line + 1)).to_intern(),
        // column: u32,
        (try mod.int_value(Type.u32, extra.column + 1)).to_intern(),
    };
    return Air.interned_to_ref((try mod.intern(.{ .aggregate = .{
        .ty = src_loc_ty.to_intern(),
        .storage = .{ .elems = &fields },
    } })));
}

fn zir_type_info(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const ty = try sema.resolve_type(block, src, inst_data.operand);
    const type_info_ty = try sema.get_builtin_type("Type");
    const type_info_tag_ty = type_info_ty.union_tag_type(mod).?;

    if (ty.type_decl_inst(mod)) |type_decl_inst| {
        try sema.declare_dependency(.{ .namespace = type_decl_inst });
    }

    switch (ty.zig_type_tag(mod)) {
        .Type,
        .Void,
        .Bool,
        .NoReturn,
        .ComptimeFloat,
        .ComptimeInt,
        .Undefined,
        .Null,
        .EnumLiteral,
        => |type_info_tag| return Air.interned_to_ref((try mod.intern(.{ .un = .{
            .ty = type_info_ty.to_intern(),
            .tag = (try mod.enum_value_field_index(type_info_tag_ty, @int_from_enum(type_info_tag))).to_intern(),
            .val = .void_value,
        } }))),
        .Fn => {
            const fn_info_decl_index = (try sema.namespace_lookup(
                block,
                src,
                type_info_ty.get_namespace_index(mod),
                try ip.get_or_put_string(gpa, "Fn", .no_embedded_nulls),
            )).?;
            try sema.ensure_decl_analyzed(fn_info_decl_index);
            const fn_info_decl = mod.decl_ptr(fn_info_decl_index);
            const fn_info_ty = fn_info_decl.val.to_type();

            const param_info_decl_index = (try sema.namespace_lookup(
                block,
                src,
                fn_info_ty.get_namespace_index(mod),
                try ip.get_or_put_string(gpa, "Param", .no_embedded_nulls),
            )).?;
            try sema.ensure_decl_analyzed(param_info_decl_index);
            const param_info_decl = mod.decl_ptr(param_info_decl_index);
            const param_info_ty = param_info_decl.val.to_type();

            const func_ty_info = mod.type_to_func(ty).?;
            const param_vals = try sema.arena.alloc(InternPool.Index, func_ty_info.param_types.len);
            for (param_vals, 0..) |*param_val, i| {
                const param_ty = func_ty_info.param_types.get(ip)[i];
                const is_generic = param_ty == .generic_poison_type;
                const param_ty_val = try ip.get(gpa, .{ .opt = .{
                    .ty = try ip.get(gpa, .{ .opt_type = .type_type }),
                    .val = if (is_generic) .none else param_ty,
                } });

                const is_noalias = blk: {
                    const index = std.math.cast(u5, i) orelse break :blk false;
                    break :blk @as(u1, @truncate(func_ty_info.noalias_bits >> index)) != 0;
                };

                const param_fields = .{
                    // is_generic: bool,
                    Value.make_bool(is_generic).to_intern(),
                    // is_noalias: bool,
                    Value.make_bool(is_noalias).to_intern(),
                    // type: ?type,
                    param_ty_val,
                };
                param_val.* = try mod.intern(.{ .aggregate = .{
                    .ty = param_info_ty.to_intern(),
                    .storage = .{ .elems = &param_fields },
                } });
            }

            const args_val = v: {
                const new_decl_ty = try mod.array_type(.{
                    .len = param_vals.len,
                    .child = param_info_ty.to_intern(),
                });
                const new_decl_val = try mod.intern(.{ .aggregate = .{
                    .ty = new_decl_ty.to_intern(),
                    .storage = .{ .elems = param_vals },
                } });
                const slice_ty = (try sema.ptr_type(.{
                    .child = param_info_ty.to_intern(),
                    .flags = .{
                        .size = .Slice,
                        .is_const = true,
                    },
                })).to_intern();
                const manyptr_ty = Type.from_interned(slice_ty).slice_ptr_field_type(mod).to_intern();
                break :v try mod.intern(.{ .slice = .{
                    .ty = slice_ty,
                    .ptr = try mod.intern(.{ .ptr = .{
                        .ty = manyptr_ty,
                        .base_addr = .{ .anon_decl = .{
                            .orig_ty = manyptr_ty,
                            .val = new_decl_val,
                        } },
                        .byte_offset = 0,
                    } }),
                    .len = (try mod.int_value(Type.usize, param_vals.len)).to_intern(),
                } });
            };

            const ret_ty_opt = try mod.intern(.{ .opt = .{
                .ty = try ip.get(gpa, .{ .opt_type = .type_type }),
                .val = if (func_ty_info.return_type == .generic_poison_type)
                    .none
                else
                    func_ty_info.return_type,
            } });

            const callconv_ty = try sema.get_builtin_type("CallingConvention");

            const field_values = .{
                // calling_convention: CallingConvention,
                (try mod.enum_value_field_index(callconv_ty, @int_from_enum(func_ty_info.cc))).to_intern(),
                // is_generic: bool,
                Value.make_bool(func_ty_info.is_generic).to_intern(),
                // is_var_args: bool,
                Value.make_bool(func_ty_info.is_var_args).to_intern(),
                // return_type: ?type,
                ret_ty_opt,
                // args: []const Fn.Param,
                args_val,
            };
            return Air.interned_to_ref((try mod.intern(.{ .un = .{
                .ty = type_info_ty.to_intern(),
                .tag = (try mod.enum_value_field_index(type_info_tag_ty, @int_from_enum(std.builtin.TypeId.Fn))).to_intern(),
                .val = try mod.intern(.{ .aggregate = .{
                    .ty = fn_info_ty.to_intern(),
                    .storage = .{ .elems = &field_values },
                } }),
            } })));
        },
        .Int => {
            const int_info_decl_index = (try sema.namespace_lookup(
                block,
                src,
                type_info_ty.get_namespace_index(mod),
                try ip.get_or_put_string(gpa, "Int", .no_embedded_nulls),
            )).?;
            try sema.ensure_decl_analyzed(int_info_decl_index);
            const int_info_decl = mod.decl_ptr(int_info_decl_index);
            const int_info_ty = int_info_decl.val.to_type();

            const signedness_ty = try sema.get_builtin_type("Signedness");
            const info = ty.int_info(mod);
            const field_values = .{
                // signedness: Signedness,
                (try mod.enum_value_field_index(signedness_ty, @int_from_enum(info.signedness))).to_intern(),
                // bits: u16,
                (try mod.int_value(Type.u16, info.bits)).to_intern(),
            };
            return Air.interned_to_ref((try mod.intern(.{ .un = .{
                .ty = type_info_ty.to_intern(),
                .tag = (try mod.enum_value_field_index(type_info_tag_ty, @int_from_enum(std.builtin.TypeId.Int))).to_intern(),
                .val = try mod.intern(.{ .aggregate = .{
                    .ty = int_info_ty.to_intern(),
                    .storage = .{ .elems = &field_values },
                } }),
            } })));
        },
        .Float => {
            const float_info_decl_index = (try sema.namespace_lookup(
                block,
                src,
                type_info_ty.get_namespace_index(mod),
                try ip.get_or_put_string(gpa, "Float", .no_embedded_nulls),
            )).?;
            try sema.ensure_decl_analyzed(float_info_decl_index);
            const float_info_decl = mod.decl_ptr(float_info_decl_index);
            const float_info_ty = float_info_decl.val.to_type();

            const field_vals = .{
                // bits: u16,
                (try mod.int_value(Type.u16, ty.bit_size(mod))).to_intern(),
            };
            return Air.interned_to_ref((try mod.intern(.{ .un = .{
                .ty = type_info_ty.to_intern(),
                .tag = (try mod.enum_value_field_index(type_info_tag_ty, @int_from_enum(std.builtin.TypeId.Float))).to_intern(),
                .val = try mod.intern(.{ .aggregate = .{
                    .ty = float_info_ty.to_intern(),
                    .storage = .{ .elems = &field_vals },
                } }),
            } })));
        },
        .Pointer => {
            const info = ty.ptr_info(mod);
            const alignment = if (info.flags.alignment.to_byte_units()) |alignment|
                try mod.int_value(Type.comptime_int, alignment)
            else
                try Type.from_interned(info.child).lazy_abi_alignment(mod);

            const addrspace_ty = try sema.get_builtin_type("AddressSpace");
            const pointer_ty = t: {
                const decl_index = (try sema.namespace_lookup(
                    block,
                    src,
                    (try sema.get_builtin_type("Type")).get_namespace_index(mod),
                    try ip.get_or_put_string(gpa, "Pointer", .no_embedded_nulls),
                )).?;
                try sema.ensure_decl_analyzed(decl_index);
                const decl = mod.decl_ptr(decl_index);
                break :t decl.val.to_type();
            };
            const ptr_size_ty = t: {
                const decl_index = (try sema.namespace_lookup(
                    block,
                    src,
                    pointer_ty.get_namespace_index(mod),
                    try ip.get_or_put_string(gpa, "Size", .no_embedded_nulls),
                )).?;
                try sema.ensure_decl_analyzed(decl_index);
                const decl = mod.decl_ptr(decl_index);
                break :t decl.val.to_type();
            };

            const field_values = .{
                // size: Size,
                (try mod.enum_value_field_index(ptr_size_ty, @int_from_enum(info.flags.size))).to_intern(),
                // is_const: bool,
                Value.make_bool(info.flags.is_const).to_intern(),
                // is_volatile: bool,
                Value.make_bool(info.flags.is_volatile).to_intern(),
                // alignment: comptime_int,
                alignment.to_intern(),
                // address_space: AddressSpace
                (try mod.enum_value_field_index(addrspace_ty, @int_from_enum(info.flags.address_space))).to_intern(),
                // child: type,
                info.child,
                // is_allowzero: bool,
                Value.make_bool(info.flags.is_allowzero).to_intern(),
                // sentinel: ?*const anyopaque,
                (try sema.opt_ref_value(switch (info.sentinel) {
                    .none => null,
                    else => Value.from_interned(info.sentinel),
                })).to_intern(),
            };
            return Air.interned_to_ref((try mod.intern(.{ .un = .{
                .ty = type_info_ty.to_intern(),
                .tag = (try mod.enum_value_field_index(type_info_tag_ty, @int_from_enum(std.builtin.TypeId.Pointer))).to_intern(),
                .val = try mod.intern(.{ .aggregate = .{
                    .ty = pointer_ty.to_intern(),
                    .storage = .{ .elems = &field_values },
                } }),
            } })));
        },
        .Array => {
            const array_field_ty = t: {
                const array_field_ty_decl_index = (try sema.namespace_lookup(
                    block,
                    src,
                    type_info_ty.get_namespace_index(mod),
                    try ip.get_or_put_string(gpa, "Array", .no_embedded_nulls),
                )).?;
                try sema.ensure_decl_analyzed(array_field_ty_decl_index);
                const array_field_ty_decl = mod.decl_ptr(array_field_ty_decl_index);
                break :t array_field_ty_decl.val.to_type();
            };

            const info = ty.array_info(mod);
            const field_values = .{
                // len: comptime_int,
                (try mod.int_value(Type.comptime_int, info.len)).to_intern(),
                // child: type,
                info.elem_type.to_intern(),
                // sentinel: ?*const anyopaque,
                (try sema.opt_ref_value(info.sentinel)).to_intern(),
            };
            return Air.interned_to_ref((try mod.intern(.{ .un = .{
                .ty = type_info_ty.to_intern(),
                .tag = (try mod.enum_value_field_index(type_info_tag_ty, @int_from_enum(std.builtin.TypeId.Array))).to_intern(),
                .val = try mod.intern(.{ .aggregate = .{
                    .ty = array_field_ty.to_intern(),
                    .storage = .{ .elems = &field_values },
                } }),
            } })));
        },
        .Vector => {
            const vector_field_ty = t: {
                const vector_field_ty_decl_index = (try sema.namespace_lookup(
                    block,
                    src,
                    type_info_ty.get_namespace_index(mod),
                    try ip.get_or_put_string(gpa, "Vector", .no_embedded_nulls),
                )).?;
                try sema.ensure_decl_analyzed(vector_field_ty_decl_index);
                const vector_field_ty_decl = mod.decl_ptr(vector_field_ty_decl_index);
                break :t vector_field_ty_decl.val.to_type();
            };

            const info = ty.array_info(mod);
            const field_values = .{
                // len: comptime_int,
                (try mod.int_value(Type.comptime_int, info.len)).to_intern(),
                // child: type,
                info.elem_type.to_intern(),
            };
            return Air.interned_to_ref((try mod.intern(.{ .un = .{
                .ty = type_info_ty.to_intern(),
                .tag = (try mod.enum_value_field_index(type_info_tag_ty, @int_from_enum(std.builtin.TypeId.Vector))).to_intern(),
                .val = try mod.intern(.{ .aggregate = .{
                    .ty = vector_field_ty.to_intern(),
                    .storage = .{ .elems = &field_values },
                } }),
            } })));
        },
        .Optional => {
            const optional_field_ty = t: {
                const optional_field_ty_decl_index = (try sema.namespace_lookup(
                    block,
                    src,
                    type_info_ty.get_namespace_index(mod),
                    try ip.get_or_put_string(gpa, "Optional", .no_embedded_nulls),
                )).?;
                try sema.ensure_decl_analyzed(optional_field_ty_decl_index);
                const optional_field_ty_decl = mod.decl_ptr(optional_field_ty_decl_index);
                break :t optional_field_ty_decl.val.to_type();
            };

            const field_values = .{
                // child: type,
                ty.optional_child(mod).to_intern(),
            };
            return Air.interned_to_ref((try mod.intern(.{ .un = .{
                .ty = type_info_ty.to_intern(),
                .tag = (try mod.enum_value_field_index(type_info_tag_ty, @int_from_enum(std.builtin.TypeId.Optional))).to_intern(),
                .val = try mod.intern(.{ .aggregate = .{
                    .ty = optional_field_ty.to_intern(),
                    .storage = .{ .elems = &field_values },
                } }),
            } })));
        },
        .ErrorSet => {
            // Get the Error type
            const error_field_ty = t: {
                const set_field_ty_decl_index = (try sema.namespace_lookup(
                    block,
                    src,
                    type_info_ty.get_namespace_index(mod),
                    try ip.get_or_put_string(gpa, "Error", .no_embedded_nulls),
                )).?;
                try sema.ensure_decl_analyzed(set_field_ty_decl_index);
                const set_field_ty_decl = mod.decl_ptr(set_field_ty_decl_index);
                break :t set_field_ty_decl.val.to_type();
            };

            try sema.queue_full_type_resolution(error_field_ty);

            // Build our list of Error values
            // Optional value is only null if anyerror
            // Value can be zero-length slice otherwise
            const error_field_vals = switch (try sema.resolve_inferred_error_set_ty(block, src, ty.to_intern())) {
                .anyerror_type => null,
                else => |err_set_ty_index| blk: {
                    const names = ip.index_to_key(err_set_ty_index).error_set_type.names;
                    const vals = try sema.arena.alloc(InternPool.Index, names.len);
                    for (vals, 0..) |*field_val, error_index| {
                        const error_name = names.get(ip)[error_index];
                        const error_name_len = error_name.length(ip);
                        const error_name_val = v: {
                            const new_decl_ty = try mod.array_type(.{
                                .len = error_name_len,
                                .sentinel = .zero_u8,
                                .child = .u8_type,
                            });
                            const new_decl_val = try mod.intern(.{ .aggregate = .{
                                .ty = new_decl_ty.to_intern(),
                                .storage = .{ .bytes = error_name.to_string() },
                            } });
                            break :v try mod.intern(.{ .slice = .{
                                .ty = .slice_const_u8_sentinel_0_type,
                                .ptr = try mod.intern(.{ .ptr = .{
                                    .ty = .manyptr_const_u8_sentinel_0_type,
                                    .base_addr = .{ .anon_decl = .{
                                        .val = new_decl_val,
                                        .orig_ty = .slice_const_u8_sentinel_0_type,
                                    } },
                                    .byte_offset = 0,
                                } }),
                                .len = (try mod.int_value(Type.usize, error_name_len)).to_intern(),
                            } });
                        };

                        const error_field_fields = .{
                            // name: [:0]const u8,
                            error_name_val,
                        };
                        field_val.* = try mod.intern(.{ .aggregate = .{
                            .ty = error_field_ty.to_intern(),
                            .storage = .{ .elems = &error_field_fields },
                        } });
                    }

                    break :blk vals;
                },
            };

            // Build our ?[]const Error value
            const slice_errors_ty = try sema.ptr_type(.{
                .child = error_field_ty.to_intern(),
                .flags = .{
                    .size = .Slice,
                    .is_const = true,
                },
            });
            const opt_slice_errors_ty = try mod.optional_type(slice_errors_ty.to_intern());
            const errors_payload_val: InternPool.Index = if (error_field_vals) |vals| v: {
                const array_errors_ty = try mod.array_type(.{
                    .len = vals.len,
                    .child = error_field_ty.to_intern(),
                });
                const new_decl_val = try mod.intern(.{ .aggregate = .{
                    .ty = array_errors_ty.to_intern(),
                    .storage = .{ .elems = vals },
                } });
                const manyptr_errors_ty = slice_errors_ty.slice_ptr_field_type(mod).to_intern();
                break :v try mod.intern(.{ .slice = .{
                    .ty = slice_errors_ty.to_intern(),
                    .ptr = try mod.intern(.{ .ptr = .{
                        .ty = manyptr_errors_ty,
                        .base_addr = .{ .anon_decl = .{
                            .orig_ty = manyptr_errors_ty,
                            .val = new_decl_val,
                        } },
                        .byte_offset = 0,
                    } }),
                    .len = (try mod.int_value(Type.usize, vals.len)).to_intern(),
                } });
            } else .none;
            const errors_val = try mod.intern(.{ .opt = .{
                .ty = opt_slice_errors_ty.to_intern(),
                .val = errors_payload_val,
            } });

            // Construct Type{ .ErrorSet = errors_val }
            return Air.interned_to_ref((try mod.intern(.{ .un = .{
                .ty = type_info_ty.to_intern(),
                .tag = (try mod.enum_value_field_index(type_info_tag_ty, @int_from_enum(std.builtin.TypeId.ErrorSet))).to_intern(),
                .val = errors_val,
            } })));
        },
        .ErrorUnion => {
            const error_union_field_ty = t: {
                const error_union_field_ty_decl_index = (try sema.namespace_lookup(
                    block,
                    src,
                    type_info_ty.get_namespace_index(mod),
                    try ip.get_or_put_string(gpa, "ErrorUnion", .no_embedded_nulls),
                )).?;
                try sema.ensure_decl_analyzed(error_union_field_ty_decl_index);
                const error_union_field_ty_decl = mod.decl_ptr(error_union_field_ty_decl_index);
                break :t error_union_field_ty_decl.val.to_type();
            };

            const field_values = .{
                // error_set: type,
                ty.error_union_set(mod).to_intern(),
                // payload: type,
                ty.error_union_payload(mod).to_intern(),
            };
            return Air.interned_to_ref((try mod.intern(.{ .un = .{
                .ty = type_info_ty.to_intern(),
                .tag = (try mod.enum_value_field_index(type_info_tag_ty, @int_from_enum(std.builtin.TypeId.ErrorUnion))).to_intern(),
                .val = try mod.intern(.{ .aggregate = .{
                    .ty = error_union_field_ty.to_intern(),
                    .storage = .{ .elems = &field_values },
                } }),
            } })));
        },
        .Enum => {
            const is_exhaustive = Value.make_bool(ip.load_enum_type(ty.to_intern()).tag_mode != .nonexhaustive);

            const enum_field_ty = t: {
                const enum_field_ty_decl_index = (try sema.namespace_lookup(
                    block,
                    src,
                    type_info_ty.get_namespace_index(mod),
                    try ip.get_or_put_string(gpa, "EnumField", .no_embedded_nulls),
                )).?;
                try sema.ensure_decl_analyzed(enum_field_ty_decl_index);
                const enum_field_ty_decl = mod.decl_ptr(enum_field_ty_decl_index);
                break :t enum_field_ty_decl.val.to_type();
            };

            const enum_field_vals = try sema.arena.alloc(InternPool.Index, ip.load_enum_type(ty.to_intern()).names.len);
            for (enum_field_vals, 0..) |*field_val, tag_index| {
                const enum_type = ip.load_enum_type(ty.to_intern());
                const value_val = if (enum_type.values.len > 0)
                    try mod.intern_pool.get_coerced_ints(
                        mod.gpa,
                        mod.intern_pool.index_to_key(enum_type.values.get(ip)[tag_index]).int,
                        .comptime_int_type,
                    )
                else
                    (try mod.int_value(Type.comptime_int, tag_index)).to_intern();

                // TODO: write something like get_coerced_ints to avoid needing to dupe
                const name_val = v: {
                    const tag_name = enum_type.names.get(ip)[tag_index];
                    const tag_name_len = tag_name.length(ip);
                    const new_decl_ty = try mod.array_type(.{
                        .len = tag_name_len,
                        .sentinel = .zero_u8,
                        .child = .u8_type,
                    });
                    const new_decl_val = try mod.intern(.{ .aggregate = .{
                        .ty = new_decl_ty.to_intern(),
                        .storage = .{ .bytes = tag_name.to_string() },
                    } });
                    break :v try mod.intern(.{ .slice = .{
                        .ty = .slice_const_u8_sentinel_0_type,
                        .ptr = try mod.intern(.{ .ptr = .{
                            .ty = .manyptr_const_u8_sentinel_0_type,
                            .base_addr = .{ .anon_decl = .{
                                .val = new_decl_val,
                                .orig_ty = .slice_const_u8_sentinel_0_type,
                            } },
                            .byte_offset = 0,
                        } }),
                        .len = (try mod.int_value(Type.usize, tag_name_len)).to_intern(),
                    } });
                };

                const enum_field_fields = .{
                    // name: [:0]const u8,
                    name_val,
                    // value: comptime_int,
                    value_val,
                };
                field_val.* = try mod.intern(.{ .aggregate = .{
                    .ty = enum_field_ty.to_intern(),
                    .storage = .{ .elems = &enum_field_fields },
                } });
            }

            const fields_val = v: {
                const fields_array_ty = try mod.array_type(.{
                    .len = enum_field_vals.len,
                    .child = enum_field_ty.to_intern(),
                });
                const new_decl_val = try mod.intern(.{ .aggregate = .{
                    .ty = fields_array_ty.to_intern(),
                    .storage = .{ .elems = enum_field_vals },
                } });
                const slice_ty = (try sema.ptr_type(.{
                    .child = enum_field_ty.to_intern(),
                    .flags = .{
                        .size = .Slice,
                        .is_const = true,
                    },
                })).to_intern();
                const manyptr_ty = Type.from_interned(slice_ty).slice_ptr_field_type(mod).to_intern();
                break :v try mod.intern(.{ .slice = .{
                    .ty = slice_ty,
                    .ptr = try mod.intern(.{ .ptr = .{
                        .ty = manyptr_ty,
                        .base_addr = .{ .anon_decl = .{
                            .val = new_decl_val,
                            .orig_ty = manyptr_ty,
                        } },
                        .byte_offset = 0,
                    } }),
                    .len = (try mod.int_value(Type.usize, enum_field_vals.len)).to_intern(),
                } });
            };

            const decls_val = try sema.type_info_decls(block, src, type_info_ty, ip.load_enum_type(ty.to_intern()).namespace);

            const type_enum_ty = t: {
                const type_enum_ty_decl_index = (try sema.namespace_lookup(
                    block,
                    src,
                    type_info_ty.get_namespace_index(mod),
                    try ip.get_or_put_string(gpa, "Enum", .no_embedded_nulls),
                )).?;
                try sema.ensure_decl_analyzed(type_enum_ty_decl_index);
                const type_enum_ty_decl = mod.decl_ptr(type_enum_ty_decl_index);
                break :t type_enum_ty_decl.val.to_type();
            };

            const field_values = .{
                // tag_type: type,
                ip.load_enum_type(ty.to_intern()).tag_ty,
                // fields: []const EnumField,
                fields_val,
                // decls: []const Declaration,
                decls_val,
                // is_exhaustive: bool,
                is_exhaustive.to_intern(),
            };
            return Air.interned_to_ref((try mod.intern(.{ .un = .{
                .ty = type_info_ty.to_intern(),
                .tag = (try mod.enum_value_field_index(type_info_tag_ty, @int_from_enum(std.builtin.TypeId.Enum))).to_intern(),
                .val = try mod.intern(.{ .aggregate = .{
                    .ty = type_enum_ty.to_intern(),
                    .storage = .{ .elems = &field_values },
                } }),
            } })));
        },
        .Union => {
            const type_union_ty = t: {
                const type_union_ty_decl_index = (try sema.namespace_lookup(
                    block,
                    src,
                    type_info_ty.get_namespace_index(mod),
                    try ip.get_or_put_string(gpa, "Union", .no_embedded_nulls),
                )).?;
                try sema.ensure_decl_analyzed(type_union_ty_decl_index);
                const type_union_ty_decl = mod.decl_ptr(type_union_ty_decl_index);
                break :t type_union_ty_decl.val.to_type();
            };

            const union_field_ty = t: {
                const union_field_ty_decl_index = (try sema.namespace_lookup(
                    block,
                    src,
                    type_info_ty.get_namespace_index(mod),
                    try ip.get_or_put_string(gpa, "UnionField", .no_embedded_nulls),
                )).?;
                try sema.ensure_decl_analyzed(union_field_ty_decl_index);
                const union_field_ty_decl = mod.decl_ptr(union_field_ty_decl_index);
                break :t union_field_ty_decl.val.to_type();
            };

            try sema.resolve_type_layout(ty); // Getting alignment requires type layout
            const union_obj = mod.type_to_union(ty).?;
            const tag_type = union_obj.load_tag_type(ip);
            const layout = union_obj.get_layout(ip);

            const union_field_vals = try gpa.alloc(InternPool.Index, tag_type.names.len);
            defer gpa.free(union_field_vals);

            for (union_field_vals, 0..) |*field_val, field_index| {
                const name_val = v: {
                    const field_name = tag_type.names.get(ip)[field_index];
                    const field_name_len = field_name.length(ip);
                    const new_decl_ty = try mod.array_type(.{
                        .len = field_name_len,
                        .sentinel = .zero_u8,
                        .child = .u8_type,
                    });
                    const new_decl_val = try mod.intern(.{ .aggregate = .{
                        .ty = new_decl_ty.to_intern(),
                        .storage = .{ .bytes = field_name.to_string() },
                    } });
                    break :v try mod.intern(.{ .slice = .{
                        .ty = .slice_const_u8_sentinel_0_type,
                        .ptr = try mod.intern(.{ .ptr = .{
                            .ty = .manyptr_const_u8_sentinel_0_type,
                            .base_addr = .{ .anon_decl = .{
                                .val = new_decl_val,
                                .orig_ty = .slice_const_u8_sentinel_0_type,
                            } },
                            .byte_offset = 0,
                        } }),
                        .len = (try mod.int_value(Type.usize, field_name_len)).to_intern(),
                    } });
                };

                const alignment = switch (layout) {
                    .auto, .@"extern" => try sema.union_field_alignment(union_obj, @int_cast(field_index)),
                    .@"packed" => .none,
                };

                const field_ty = union_obj.field_types.get(ip)[field_index];
                const union_field_fields = .{
                    // name: [:0]const u8,
                    name_val,
                    // type: type,
                    field_ty,
                    // alignment: comptime_int,
                    (try mod.int_value(Type.comptime_int, alignment.to_byte_units() orelse 0)).to_intern(),
                };
                field_val.* = try mod.intern(.{ .aggregate = .{
                    .ty = union_field_ty.to_intern(),
                    .storage = .{ .elems = &union_field_fields },
                } });
            }

            const fields_val = v: {
                const array_fields_ty = try mod.array_type(.{
                    .len = union_field_vals.len,
                    .child = union_field_ty.to_intern(),
                });
                const new_decl_val = try mod.intern(.{ .aggregate = .{
                    .ty = array_fields_ty.to_intern(),
                    .storage = .{ .elems = union_field_vals },
                } });
                const slice_ty = (try sema.ptr_type(.{
                    .child = union_field_ty.to_intern(),
                    .flags = .{
                        .size = .Slice,
                        .is_const = true,
                    },
                })).to_intern();
                const manyptr_ty = Type.from_interned(slice_ty).slice_ptr_field_type(mod).to_intern();
                break :v try mod.intern(.{ .slice = .{
                    .ty = slice_ty,
                    .ptr = try mod.intern(.{ .ptr = .{
                        .ty = manyptr_ty,
                        .base_addr = .{ .anon_decl = .{
                            .orig_ty = manyptr_ty,
                            .val = new_decl_val,
                        } },
                        .byte_offset = 0,
                    } }),
                    .len = (try mod.int_value(Type.usize, union_field_vals.len)).to_intern(),
                } });
            };

            const decls_val = try sema.type_info_decls(block, src, type_info_ty, ty.get_namespace_index(mod));

            const enum_tag_ty_val = try mod.intern(.{ .opt = .{
                .ty = (try mod.optional_type(.type_type)).to_intern(),
                .val = if (ty.union_tag_type(mod)) |tag_ty| tag_ty.to_intern() else .none,
            } });

            const container_layout_ty = t: {
                const decl_index = (try sema.namespace_lookup(
                    block,
                    src,
                    (try sema.get_builtin_type("Type")).get_namespace_index(mod),
                    try ip.get_or_put_string(gpa, "ContainerLayout", .no_embedded_nulls),
                )).?;
                try sema.ensure_decl_analyzed(decl_index);
                const decl = mod.decl_ptr(decl_index);
                break :t decl.val.to_type();
            };

            const field_values = .{
                // layout: ContainerLayout,
                (try mod.enum_value_field_index(container_layout_ty, @int_from_enum(layout))).to_intern(),

                // tag_type: ?type,
                enum_tag_ty_val,
                // fields: []const UnionField,
                fields_val,
                // decls: []const Declaration,
                decls_val,
            };
            return Air.interned_to_ref((try mod.intern(.{ .un = .{
                .ty = type_info_ty.to_intern(),
                .tag = (try mod.enum_value_field_index(type_info_tag_ty, @int_from_enum(std.builtin.TypeId.Union))).to_intern(),
                .val = try mod.intern(.{ .aggregate = .{
                    .ty = type_union_ty.to_intern(),
                    .storage = .{ .elems = &field_values },
                } }),
            } })));
        },
        .Struct => {
            const type_struct_ty = t: {
                const type_struct_ty_decl_index = (try sema.namespace_lookup(
                    block,
                    src,
                    type_info_ty.get_namespace_index(mod),
                    try ip.get_or_put_string(gpa, "Struct", .no_embedded_nulls),
                )).?;
                try sema.ensure_decl_analyzed(type_struct_ty_decl_index);
                const type_struct_ty_decl = mod.decl_ptr(type_struct_ty_decl_index);
                break :t type_struct_ty_decl.val.to_type();
            };

            const struct_field_ty = t: {
                const struct_field_ty_decl_index = (try sema.namespace_lookup(
                    block,
                    src,
                    type_info_ty.get_namespace_index(mod),
                    try ip.get_or_put_string(gpa, "StructField", .no_embedded_nulls),
                )).?;
                try sema.ensure_decl_analyzed(struct_field_ty_decl_index);
                const struct_field_ty_decl = mod.decl_ptr(struct_field_ty_decl_index);
                break :t struct_field_ty_decl.val.to_type();
            };

            try sema.resolve_type_layout(ty); // Getting alignment requires type layout

            var struct_field_vals: []InternPool.Index = &.{};
            defer gpa.free(struct_field_vals);
            fv: {
                const struct_type = switch (ip.index_to_key(ty.to_intern())) {
                    .anon_struct_type => |anon_struct_type| {
                        struct_field_vals = try gpa.alloc(InternPool.Index, anon_struct_type.types.len);
                        for (struct_field_vals, 0..) |*struct_field_val, field_index| {
                            const field_ty = anon_struct_type.types.get(ip)[field_index];
                            const field_val = anon_struct_type.values.get(ip)[field_index];
                            const name_val = v: {
                                const field_name = if (anon_struct_type.names.len != 0)
                                    anon_struct_type.names.get(ip)[field_index]
                                else
                                    try ip.get_or_put_string_fmt(gpa, "{d}", .{field_index}, .no_embedded_nulls);
                                const field_name_len = field_name.length(ip);
                                const new_decl_ty = try mod.array_type(.{
                                    .len = field_name_len,
                                    .sentinel = .zero_u8,
                                    .child = .u8_type,
                                });
                                const new_decl_val = try mod.intern(.{ .aggregate = .{
                                    .ty = new_decl_ty.to_intern(),
                                    .storage = .{ .bytes = field_name.to_string() },
                                } });
                                break :v try mod.intern(.{ .slice = .{
                                    .ty = .slice_const_u8_sentinel_0_type,
                                    .ptr = try mod.intern(.{ .ptr = .{
                                        .ty = .manyptr_const_u8_sentinel_0_type,
                                        .base_addr = .{ .anon_decl = .{
                                            .val = new_decl_val,
                                            .orig_ty = .slice_const_u8_sentinel_0_type,
                                        } },
                                        .byte_offset = 0,
                                    } }),
                                    .len = (try mod.int_value(Type.usize, field_name_len)).to_intern(),
                                } });
                            };

                            try sema.resolve_type_layout(Type.from_interned(field_ty));

                            const is_comptime = field_val != .none;
                            const opt_default_val = if (is_comptime) Value.from_interned(field_val) else null;
                            const default_val_ptr = try sema.opt_ref_value(opt_default_val);
                            const struct_field_fields = .{
                                // name: [:0]const u8,
                                name_val,
                                // type: type,
                                field_ty,
                                // default_value: ?*const anyopaque,
                                default_val_ptr.to_intern(),
                                // is_comptime: bool,
                                Value.make_bool(is_comptime).to_intern(),
                                // alignment: comptime_int,
                                (try mod.int_value(Type.comptime_int, Type.from_interned(field_ty).abi_alignment(mod).to_byte_units() orelse 0)).to_intern(),
                            };
                            struct_field_val.* = try mod.intern(.{ .aggregate = .{
                                .ty = struct_field_ty.to_intern(),
                                .storage = .{ .elems = &struct_field_fields },
                            } });
                        }
                        break :fv;
                    },
                    .struct_type => ip.load_struct_type(ty.to_intern()),
                    else => unreachable,
                };
                struct_field_vals = try gpa.alloc(InternPool.Index, struct_type.field_types.len);

                try sema.resolve_struct_field_inits(ty);

                for (struct_field_vals, 0..) |*field_val, field_index| {
                    const field_name = if (struct_type.field_name(ip, field_index).unwrap()) |field_name|
                        field_name
                    else
                        try ip.get_or_put_string_fmt(gpa, "{d}", .{field_index}, .no_embedded_nulls);
                    const field_name_len = field_name.length(ip);
                    const field_ty = Type.from_interned(struct_type.field_types.get(ip)[field_index]);
                    const field_init = struct_type.field_init(ip, field_index);
                    const field_is_comptime = struct_type.field_is_comptime(ip, field_index);
                    const name_val = v: {
                        const new_decl_ty = try mod.array_type(.{
                            .len = field_name_len,
                            .sentinel = .zero_u8,
                            .child = .u8_type,
                        });
                        const new_decl_val = try mod.intern(.{ .aggregate = .{
                            .ty = new_decl_ty.to_intern(),
                            .storage = .{ .bytes = field_name.to_string() },
                        } });
                        break :v try mod.intern(.{ .slice = .{
                            .ty = .slice_const_u8_sentinel_0_type,
                            .ptr = try mod.intern(.{ .ptr = .{
                                .ty = .manyptr_const_u8_sentinel_0_type,
                                .base_addr = .{ .anon_decl = .{
                                    .val = new_decl_val,
                                    .orig_ty = .slice_const_u8_sentinel_0_type,
                                } },
                                .byte_offset = 0,
                            } }),
                            .len = (try mod.int_value(Type.usize, field_name_len)).to_intern(),
                        } });
                    };

                    const opt_default_val = if (field_init == .none) null else Value.from_interned(field_init);
                    const default_val_ptr = try sema.opt_ref_value(opt_default_val);
                    const alignment = switch (struct_type.layout) {
                        .@"packed" => .none,
                        else => try sema.struct_field_alignment(
                            struct_type.field_align(ip, field_index),
                            field_ty,
                            struct_type.layout,
                        ),
                    };

                    const struct_field_fields = .{
                        // name: [:0]const u8,
                        name_val,
                        // type: type,
                        field_ty.to_intern(),
                        // default_value: ?*const anyopaque,
                        default_val_ptr.to_intern(),
                        // is_comptime: bool,
                        Value.make_bool(field_is_comptime).to_intern(),
                        // alignment: comptime_int,
                        (try mod.int_value(Type.comptime_int, alignment.to_byte_units() orelse 0)).to_intern(),
                    };
                    field_val.* = try mod.intern(.{ .aggregate = .{
                        .ty = struct_field_ty.to_intern(),
                        .storage = .{ .elems = &struct_field_fields },
                    } });
                }
            }

            const fields_val = v: {
                const array_fields_ty = try mod.array_type(.{
                    .len = struct_field_vals.len,
                    .child = struct_field_ty.to_intern(),
                });
                const new_decl_val = try mod.intern(.{ .aggregate = .{
                    .ty = array_fields_ty.to_intern(),
                    .storage = .{ .elems = struct_field_vals },
                } });
                const slice_ty = (try sema.ptr_type(.{
                    .child = struct_field_ty.to_intern(),
                    .flags = .{
                        .size = .Slice,
                        .is_const = true,
                    },
                })).to_intern();
                const manyptr_ty = Type.from_interned(slice_ty).slice_ptr_field_type(mod).to_intern();
                break :v try mod.intern(.{ .slice = .{
                    .ty = slice_ty,
                    .ptr = try mod.intern(.{ .ptr = .{
                        .ty = manyptr_ty,
                        .base_addr = .{ .anon_decl = .{
                            .orig_ty = manyptr_ty,
                            .val = new_decl_val,
                        } },
                        .byte_offset = 0,
                    } }),
                    .len = (try mod.int_value(Type.usize, struct_field_vals.len)).to_intern(),
                } });
            };

            const decls_val = try sema.type_info_decls(block, src, type_info_ty, ty.get_namespace_index(mod));

            const backing_integer_val = try mod.intern(.{ .opt = .{
                .ty = (try mod.optional_type(.type_type)).to_intern(),
                .val = if (mod.type_to_packed_struct(ty)) |packed_struct| val: {
                    assert(Type.from_interned(packed_struct.backing_int_type(ip).*).is_int(mod));
                    break :val packed_struct.backing_int_type(ip).*;
                } else .none,
            } });

            const container_layout_ty = t: {
                const decl_index = (try sema.namespace_lookup(
                    block,
                    src,
                    (try sema.get_builtin_type("Type")).get_namespace_index(mod),
                    try ip.get_or_put_string(gpa, "ContainerLayout", .no_embedded_nulls),
                )).?;
                try sema.ensure_decl_analyzed(decl_index);
                const decl = mod.decl_ptr(decl_index);
                break :t decl.val.to_type();
            };

            const layout = ty.container_layout(mod);

            const field_values = [_]InternPool.Index{
                // layout: ContainerLayout,
                (try mod.enum_value_field_index(container_layout_ty, @int_from_enum(layout))).to_intern(),
                // backing_integer: ?type,
                backing_integer_val,
                // fields: []const StructField,
                fields_val,
                // decls: []const Declaration,
                decls_val,
                // is_tuple: bool,
                Value.make_bool(ty.is_tuple(mod)).to_intern(),
            };
            return Air.interned_to_ref((try mod.intern(.{ .un = .{
                .ty = type_info_ty.to_intern(),
                .tag = (try mod.enum_value_field_index(type_info_tag_ty, @int_from_enum(std.builtin.TypeId.Struct))).to_intern(),
                .val = try mod.intern(.{ .aggregate = .{
                    .ty = type_struct_ty.to_intern(),
                    .storage = .{ .elems = &field_values },
                } }),
            } })));
        },
        .Opaque => {
            const type_opaque_ty = t: {
                const type_opaque_ty_decl_index = (try sema.namespace_lookup(
                    block,
                    src,
                    type_info_ty.get_namespace_index(mod),
                    try ip.get_or_put_string(gpa, "Opaque", .no_embedded_nulls),
                )).?;
                try sema.ensure_decl_analyzed(type_opaque_ty_decl_index);
                const type_opaque_ty_decl = mod.decl_ptr(type_opaque_ty_decl_index);
                break :t type_opaque_ty_decl.val.to_type();
            };

            try sema.resolve_type_fields(ty);
            const decls_val = try sema.type_info_decls(block, src, type_info_ty, ty.get_namespace_index(mod));

            const field_values = .{
                // decls: []const Declaration,
                decls_val,
            };
            return Air.interned_to_ref((try mod.intern(.{ .un = .{
                .ty = type_info_ty.to_intern(),
                .tag = (try mod.enum_value_field_index(type_info_tag_ty, @int_from_enum(std.builtin.TypeId.Opaque))).to_intern(),
                .val = try mod.intern(.{ .aggregate = .{
                    .ty = type_opaque_ty.to_intern(),
                    .storage = .{ .elems = &field_values },
                } }),
            } })));
        },
        .Frame => return sema.fail_with_use_of_async(block, src),
        .AnyFrame => return sema.fail_with_use_of_async(block, src),
    }
}

fn type_info_decls(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    type_info_ty: Type,
    opt_namespace: InternPool.OptionalNamespaceIndex,
) CompileError!InternPool.Index {
    const mod = sema.mod;
    const gpa = sema.gpa;

    const declaration_ty = t: {
        const declaration_ty_decl_index = (try sema.namespace_lookup(
            block,
            src,
            type_info_ty.get_namespace_index(mod),
            try mod.intern_pool.get_or_put_string(gpa, "Declaration", .no_embedded_nulls),
        )).?;
        try sema.ensure_decl_analyzed(declaration_ty_decl_index);
        const declaration_ty_decl = mod.decl_ptr(declaration_ty_decl_index);
        break :t declaration_ty_decl.val.to_type();
    };
    try sema.queue_full_type_resolution(declaration_ty);

    var decl_vals = std.ArrayList(InternPool.Index).init(gpa);
    defer decl_vals.deinit();

    var seen_namespaces = std.AutoHashMap(*Namespace, void).init(gpa);
    defer seen_namespaces.deinit();

    try sema.type_info_namespace_decls(block, opt_namespace, declaration_ty, &decl_vals, &seen_namespaces);

    const array_decl_ty = try mod.array_type(.{
        .len = decl_vals.items.len,
        .child = declaration_ty.to_intern(),
    });
    const new_decl_val = try mod.intern(.{ .aggregate = .{
        .ty = array_decl_ty.to_intern(),
        .storage = .{ .elems = decl_vals.items },
    } });
    const slice_ty = (try sema.ptr_type(.{
        .child = declaration_ty.to_intern(),
        .flags = .{
            .size = .Slice,
            .is_const = true,
        },
    })).to_intern();
    const manyptr_ty = Type.from_interned(slice_ty).slice_ptr_field_type(mod).to_intern();
    return try mod.intern(.{ .slice = .{
        .ty = slice_ty,
        .ptr = try mod.intern(.{ .ptr = .{
            .ty = manyptr_ty,
            .base_addr = .{ .anon_decl = .{
                .orig_ty = manyptr_ty,
                .val = new_decl_val,
            } },
            .byte_offset = 0,
        } }),
        .len = (try mod.int_value(Type.usize, decl_vals.items.len)).to_intern(),
    } });
}

fn type_info_namespace_decls(
    sema: *Sema,
    block: *Block,
    opt_namespace_index: InternPool.OptionalNamespaceIndex,
    declaration_ty: Type,
    decl_vals: *std.ArrayList(InternPool.Index),
    seen_namespaces: *std.AutoHashMap(*Namespace, void),
) !void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;

    const namespace_index = opt_namespace_index.unwrap() orelse return;
    const namespace = mod.namespace_ptr(namespace_index);

    const gop = try seen_namespaces.get_or_put(namespace);
    if (gop.found_existing) return;

    const decls = namespace.decls.keys();
    for (decls) |decl_index| {
        const decl = mod.decl_ptr(decl_index);
        if (!decl.is_pub) continue;
        if (decl.kind == .@"usingnamespace") {
            if (decl.analysis == .in_progress) continue;
            try sema.ensure_decl_analyzed(decl_index);
            try sema.type_info_namespace_decls(block, decl.val.to_type().get_namespace_index(mod), declaration_ty, decl_vals, seen_namespaces);
            continue;
        }
        if (decl.kind != .named) continue;
        const name_val = v: {
            const decl_name_len = decl.name.length(ip);
            const new_decl_ty = try mod.array_type(.{
                .len = decl_name_len,
                .sentinel = .zero_u8,
                .child = .u8_type,
            });
            const new_decl_val = try mod.intern(.{ .aggregate = .{
                .ty = new_decl_ty.to_intern(),
                .storage = .{ .bytes = decl.name.to_string() },
            } });
            break :v try mod.intern(.{ .slice = .{
                .ty = .slice_const_u8_sentinel_0_type,
                .ptr = try mod.intern(.{ .ptr = .{
                    .ty = .manyptr_const_u8_sentinel_0_type,
                    .base_addr = .{ .anon_decl = .{
                        .orig_ty = .slice_const_u8_sentinel_0_type,
                        .val = new_decl_val,
                    } },
                    .byte_offset = 0,
                } }),
                .len = (try mod.int_value(Type.usize, decl_name_len)).to_intern(),
            } });
        };

        const fields = .{
            //name: [:0]const u8,
            name_val,
        };
        try decl_vals.append(try mod.intern(.{ .aggregate = .{
            .ty = declaration_ty.to_intern(),
            .storage = .{ .elems = &fields },
        } }));
    }
}

fn zir_typeof(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    _ = block;
    const zir_datas = sema.code.instructions.items(.data);
    const inst_data = zir_datas[@int_from_enum(inst)].un_node;
    const operand = try sema.resolve_inst(inst_data.operand);
    const operand_ty = sema.type_of(operand);
    return Air.interned_to_ref(operand_ty.to_intern());
}

fn zir_typeof_builtin(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const pl_node = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Block, pl_node.payload_index);
    const body = sema.code.body_slice(extra.end, extra.data.body_len);

    var child_block: Block = .{
        .parent = block,
        .sema = sema,
        .src_decl = block.src_decl,
        .namespace = block.namespace,
        .instructions = .{},
        .inlining = block.inlining,
        .is_comptime = false,
        .is_typeof = true,
        .want_safety = false,
        .error_return_trace_index = block.error_return_trace_index,
    };
    defer child_block.instructions.deinit(sema.gpa);

    const operand = try sema.resolve_inline_body(&child_block, body, inst);
    const operand_ty = sema.type_of(operand);
    if (operand_ty.is_generic_poison()) return error.GenericPoison;
    return Air.interned_to_ref(operand_ty.to_intern());
}

fn zir_typeof_log2_int_type(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand = try sema.resolve_inst(inst_data.operand);
    const operand_ty = sema.type_of(operand);
    const res_ty = try sema.log2_int_type(block, operand_ty, src);
    return Air.interned_to_ref(res_ty.to_intern());
}

fn log2_int_type(sema: *Sema, block: *Block, operand: Type, src: LazySrcLoc) CompileError!Type {
    const mod = sema.mod;
    switch (operand.zig_type_tag(mod)) {
        .ComptimeInt => return Type.comptime_int,
        .Int => {
            const bits = operand.bit_size(mod);
            const count = if (bits == 0)
                0
            else blk: {
                var count: u16 = 0;
                var s = bits - 1;
                while (s != 0) : (s >>= 1) {
                    count += 1;
                }
                break :blk count;
            };
            return mod.int_type(.unsigned, count);
        },
        .Vector => {
            const elem_ty = operand.elem_type2(mod);
            const log2_elem_ty = try sema.log2_int_type(block, elem_ty, src);
            return mod.vector_type(.{
                .len = operand.vector_len(mod),
                .child = log2_elem_ty.to_intern(),
            });
        },
        else => {},
    }
    return sema.fail(
        block,
        src,
        "bit shifting operation expected integer type, found '{}'",
        .{operand.fmt(mod)},
    );
}

fn zir_typeof_peer(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
    inst: Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const extra = sema.code.extra_data(Zir.Inst.TypeOfPeer, extended.operand);
    const src = LazySrcLoc.nodeOffset(extra.data.src_node);
    const body = sema.code.body_slice(extra.data.body_index, extra.data.body_len);

    var child_block: Block = .{
        .parent = block,
        .sema = sema,
        .src_decl = block.src_decl,
        .namespace = block.namespace,
        .instructions = .{},
        .inlining = block.inlining,
        .is_comptime = false,
        .is_typeof = true,
        .runtime_cond = block.runtime_cond,
        .runtime_loop = block.runtime_loop,
        .runtime_index = block.runtime_index,
    };
    defer child_block.instructions.deinit(sema.gpa);
    // Ignore the result, we only care about the instructions in `args`.
    _ = try sema.analyze_inline_body(&child_block, body, inst);

    const args = sema.code.ref_slice(extra.end, extended.small);

    const inst_list = try sema.gpa.alloc(Air.Inst.Ref, args.len);
    defer sema.gpa.free(inst_list);

    for (args, 0..) |arg_ref, i| {
        inst_list[i] = try sema.resolve_inst(arg_ref);
    }

    const result_type = try sema.resolve_peer_types(block, src, inst_list, .{ .typeof_builtin_call_node_offset = extra.data.src_node });
    return Air.interned_to_ref(result_type.to_intern());
}

fn zir_bool_not(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_un_op = inst_data.src_node };
    const uncasted_operand = try sema.resolve_inst(inst_data.operand);

    const operand = try sema.coerce(block, Type.bool, uncasted_operand, operand_src);
    if (try sema.resolve_value(operand)) |val| {
        return if (val.is_undef(mod))
            mod.undef_ref(Type.bool)
        else if (val.to_bool()) .bool_false else .bool_true;
    }
    try sema.require_runtime_block(block, src, null);
    return block.add_ty_op(.not, Type.bool, operand);
}

fn zir_bool_br(
    sema: *Sema,
    parent_block: *Block,
    inst: Zir.Inst.Index,
    is_bool_or: bool,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const gpa = sema.gpa;

    const datas = sema.code.instructions.items(.data);
    const inst_data = datas[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.BoolBr, inst_data.payload_index);

    const uncoerced_lhs = try sema.resolve_inst(extra.data.lhs);
    const body = sema.code.body_slice(extra.end, extra.data.body_len);
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };

    const lhs = try sema.coerce(parent_block, Type.bool, uncoerced_lhs, lhs_src);

    if (try sema.resolve_defined_value(parent_block, lhs_src, lhs)) |lhs_val| {
        if (is_bool_or and lhs_val.to_bool()) {
            return .bool_true;
        } else if (!is_bool_or and !lhs_val.to_bool()) {
            return .bool_false;
        }
        // comptime-known left-hand side. No need for a block here; the result
        // is simply the rhs expression. Here we rely on there only being 1
        // break instruction (`break_inline`).
        const rhs_result = try sema.resolve_inline_body(parent_block, body, inst);
        if (sema.type_of(rhs_result).is_no_return(mod)) {
            return rhs_result;
        }
        return sema.coerce(parent_block, Type.bool, rhs_result, rhs_src);
    }

    const block_inst: Air.Inst.Index = @enumFromInt(sema.air_instructions.len);
    try sema.air_instructions.append(gpa, .{
        .tag = .block,
        .data = .{ .ty_pl = .{
            .ty = .bool_type,
            .payload = undefined,
        } },
    });

    var child_block = parent_block.make_sub_block();
    child_block.runtime_loop = null;
    child_block.runtime_cond = mod.decl_ptr(child_block.src_decl).to_src_loc(lhs_src, mod);
    child_block.runtime_index.increment();
    defer child_block.instructions.deinit(gpa);

    var then_block = child_block.make_sub_block();
    defer then_block.instructions.deinit(gpa);

    var else_block = child_block.make_sub_block();
    defer else_block.instructions.deinit(gpa);

    const lhs_block = if (is_bool_or) &then_block else &else_block;
    const rhs_block = if (is_bool_or) &else_block else &then_block;

    const lhs_result: Air.Inst.Ref = if (is_bool_or) .bool_true else .bool_false;
    _ = try lhs_block.add_br(block_inst, lhs_result);

    const rhs_result = try sema.resolve_inline_body(rhs_block, body, inst);
    const rhs_noret = sema.type_of(rhs_result).is_no_return(mod);
    const coerced_rhs_result = if (!rhs_noret) rhs: {
        const coerced_result = try sema.coerce(rhs_block, Type.bool, rhs_result, rhs_src);
        _ = try rhs_block.add_br(block_inst, coerced_result);
        break :rhs coerced_result;
    } else rhs_result;

    const result = sema.finish_cond_br(parent_block, &child_block, &then_block, &else_block, lhs, block_inst);
    if (!rhs_noret) {
        if (try sema.resolve_defined_value(rhs_block, rhs_src, coerced_rhs_result)) |rhs_val| {
            if (is_bool_or and rhs_val.to_bool()) {
                return .bool_true;
            } else if (!is_bool_or and !rhs_val.to_bool()) {
                return .bool_false;
            }
        }
    }

    return result;
}

fn finish_cond_br(
    sema: *Sema,
    parent_block: *Block,
    child_block: *Block,
    then_block: *Block,
    else_block: *Block,
    cond: Air.Inst.Ref,
    block_inst: Air.Inst.Index,
) !Air.Inst.Ref {
    const gpa = sema.gpa;

    try sema.air_extra.ensure_unused_capacity(gpa, @typeInfo(Air.CondBr).Struct.fields.len +
        then_block.instructions.items.len + else_block.instructions.items.len +
        @typeInfo(Air.Block).Struct.fields.len + child_block.instructions.items.len + 1);

    const cond_br_payload = sema.add_extra_assume_capacity(Air.CondBr{
        .then_body_len = @int_cast(then_block.instructions.items.len),
        .else_body_len = @int_cast(else_block.instructions.items.len),
    });
    sema.air_extra.append_slice_assume_capacity(@ptr_cast(then_block.instructions.items));
    sema.air_extra.append_slice_assume_capacity(@ptr_cast(else_block.instructions.items));

    _ = try child_block.add_inst(.{ .tag = .cond_br, .data = .{ .pl_op = .{
        .operand = cond,
        .payload = cond_br_payload,
    } } });

    sema.air_instructions.items(.data)[@int_from_enum(block_inst)].ty_pl.payload = sema.add_extra_assume_capacity(
        Air.Block{ .body_len = @int_cast(child_block.instructions.items.len) },
    );
    sema.air_extra.append_slice_assume_capacity(@ptr_cast(child_block.instructions.items));

    try parent_block.instructions.append(gpa, block_inst);
    return block_inst.to_ref();
}

fn check_nullable_type(sema: *Sema, block: *Block, src: LazySrcLoc, ty: Type) !void {
    const mod = sema.mod;
    switch (ty.zig_type_tag(mod)) {
        .Optional, .Null, .Undefined => return,
        .Pointer => if (ty.is_ptr_like_optional(mod)) return,
        else => {},
    }
    return sema.fail_with_expected_optional_type(block, src, ty);
}

fn zir_is_non_null(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand = try sema.resolve_inst(inst_data.operand);
    try sema.check_nullable_type(block, src, sema.type_of(operand));
    return sema.analyze_is_null(block, src, operand, true);
}

fn zir_is_non_null_ptr(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const ptr = try sema.resolve_inst(inst_data.operand);
    try sema.check_nullable_type(block, src, sema.type_of(ptr).elem_type2(mod));
    if ((try sema.resolve_value(ptr)) == null) {
        return block.add_un_op(.is_non_null_ptr, ptr);
    }
    const loaded = try sema.analyze_load(block, src, ptr, src);
    return sema.analyze_is_null(block, src, loaded, true);
}

fn check_error_type(sema: *Sema, block: *Block, src: LazySrcLoc, ty: Type) !void {
    const mod = sema.mod;
    switch (ty.zig_type_tag(mod)) {
        .ErrorSet, .ErrorUnion, .Undefined => return,
        else => return sema.fail(block, src, "expected error union type, found '{}'", .{
            ty.fmt(mod),
        }),
    }
}

fn zir_is_non_err(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand = try sema.resolve_inst(inst_data.operand);
    try sema.check_error_type(block, src, sema.type_of(operand));
    return sema.analyze_is_non_err(block, src, operand);
}

fn zir_is_non_err_ptr(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const ptr = try sema.resolve_inst(inst_data.operand);
    try sema.check_error_type(block, src, sema.type_of(ptr).elem_type2(mod));
    const loaded = try sema.analyze_load(block, src, ptr, src);
    return sema.analyze_is_non_err(block, src, loaded);
}

fn zir_ret_is_non_err(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand = try sema.resolve_inst(inst_data.operand);
    return sema.analyze_is_non_err(block, src, operand);
}

fn zir_condbr(
    sema: *Sema,
    parent_block: *Block,
    inst: Zir.Inst.Index,
) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const cond_src: LazySrcLoc = .{ .node_offset_if_cond = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.CondBr, inst_data.payload_index);

    const then_body = sema.code.body_slice(extra.end, extra.data.then_body_len);
    const else_body = sema.code.body_slice(extra.end + then_body.len, extra.data.else_body_len);

    const uncasted_cond = try sema.resolve_inst(extra.data.condition);
    const cond = try sema.coerce(parent_block, Type.bool, uncasted_cond, cond_src);

    if (try sema.resolve_defined_value(parent_block, cond_src, cond)) |cond_val| {
        const body = if (cond_val.to_bool()) then_body else else_body;

        try sema.maybe_error_unwrap_condbr(parent_block, body, extra.data.condition, cond_src);
        // We use `analyze_body_inner` since we want to propagate any comptime control flow to the caller.
        return sema.analyze_body_inner(parent_block, body);
    }

    const gpa = sema.gpa;

    // We'll re-use the sub block to save on memory bandwidth, and yank out the
    // instructions array in between using it for the then block and else block.
    var sub_block = parent_block.make_sub_block();
    sub_block.runtime_loop = null;
    sub_block.runtime_cond = mod.decl_ptr(parent_block.src_decl).to_src_loc(cond_src, mod);
    sub_block.runtime_index.increment();
    sub_block.need_debug_scope = null; // this body is emitted regardless
    defer sub_block.instructions.deinit(gpa);

    try sema.analyze_body_runtime_break(&sub_block, then_body);
    const true_instructions = try sub_block.instructions.to_owned_slice(gpa);
    defer gpa.free(true_instructions);

    const err_cond = blk: {
        const index = extra.data.condition.to_index() orelse break :blk null;
        if (sema.code.instructions.items(.tag)[@int_from_enum(index)] != .is_non_err) break :blk null;

        const err_inst_data = sema.code.instructions.items(.data)[@int_from_enum(index)].un_node;
        const err_operand = try sema.resolve_inst(err_inst_data.operand);
        const operand_ty = sema.type_of(err_operand);
        assert(operand_ty.zig_type_tag(mod) == .ErrorUnion);
        const result_ty = operand_ty.error_union_set(mod);
        break :blk try sub_block.add_ty_op(.unwrap_errunion_err, result_ty, err_operand);
    };

    if (err_cond != null and try sema.maybe_error_unwrap(&sub_block, else_body, err_cond.?, cond_src, false)) {
        // nothing to do
    } else {
        try sema.analyze_body_runtime_break(&sub_block, else_body);
    }
    try sema.air_extra.ensure_unused_capacity(gpa, @typeInfo(Air.CondBr).Struct.fields.len +
        true_instructions.len + sub_block.instructions.items.len);
    _ = try parent_block.add_inst(.{
        .tag = .cond_br,
        .data = .{ .pl_op = .{
            .operand = cond,
            .payload = sema.add_extra_assume_capacity(Air.CondBr{
                .then_body_len = @int_cast(true_instructions.len),
                .else_body_len = @int_cast(sub_block.instructions.items.len),
            }),
        } },
    });
    sema.air_extra.append_slice_assume_capacity(@ptr_cast(true_instructions));
    sema.air_extra.append_slice_assume_capacity(@ptr_cast(sub_block.instructions.items));
}

fn zir_try(sema: *Sema, parent_block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Try, inst_data.payload_index);
    const body = sema.code.body_slice(extra.end, extra.data.body_len);
    const err_union = try sema.resolve_inst(extra.data.operand);
    const err_union_ty = sema.type_of(err_union);
    const mod = sema.mod;
    if (err_union_ty.zig_type_tag(mod) != .ErrorUnion) {
        return sema.fail(parent_block, operand_src, "expected error union type, found '{}'", .{
            err_union_ty.fmt(mod),
        });
    }
    const is_non_err = try sema.analyze_is_non_err_comptime_only(parent_block, operand_src, err_union);
    if (is_non_err != .none) {
        const is_non_err_val = (try sema.resolve_defined_value(parent_block, operand_src, is_non_err)).?;
        if (is_non_err_val.to_bool()) {
            return sema.analyze_err_union_payload(parent_block, src, err_union_ty, err_union, operand_src, false);
        }
        // We can analyze the body directly in the parent block because we know there are
        // no breaks from the body possible, and that the body is noreturn.
        try sema.analyze_body_inner(parent_block, body);
        return .unreachable_value;
    }

    var sub_block = parent_block.make_sub_block();
    defer sub_block.instructions.deinit(sema.gpa);

    // This body is guaranteed to end with noreturn and has no breaks.
    try sema.analyze_body_inner(&sub_block, body);

    try sema.air_extra.ensure_unused_capacity(sema.gpa, @typeInfo(Air.Try).Struct.fields.len +
        sub_block.instructions.items.len);
    const try_inst = try parent_block.add_inst(.{
        .tag = .@"try",
        .data = .{ .pl_op = .{
            .operand = err_union,
            .payload = sema.add_extra_assume_capacity(Air.Try{
                .body_len = @int_cast(sub_block.instructions.items.len),
            }),
        } },
    });
    sema.air_extra.append_slice_assume_capacity(@ptr_cast(sub_block.instructions.items));
    return try_inst;
}

fn zir_try_ptr(sema: *Sema, parent_block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Try, inst_data.payload_index);
    const body = sema.code.body_slice(extra.end, extra.data.body_len);
    const operand = try sema.resolve_inst(extra.data.operand);
    const err_union = try sema.analyze_load(parent_block, src, operand, operand_src);
    const err_union_ty = sema.type_of(err_union);
    const mod = sema.mod;
    if (err_union_ty.zig_type_tag(mod) != .ErrorUnion) {
        return sema.fail(parent_block, operand_src, "expected error union type, found '{}'", .{
            err_union_ty.fmt(mod),
        });
    }
    const is_non_err = try sema.analyze_is_non_err_comptime_only(parent_block, operand_src, err_union);
    if (is_non_err != .none) {
        const is_non_err_val = (try sema.resolve_defined_value(parent_block, operand_src, is_non_err)).?;
        if (is_non_err_val.to_bool()) {
            return sema.analyze_err_union_payload_ptr(parent_block, src, operand, false, false);
        }
        // We can analyze the body directly in the parent block because we know there are
        // no breaks from the body possible, and that the body is noreturn.
        try sema.analyze_body_inner(parent_block, body);
        return .unreachable_value;
    }

    var sub_block = parent_block.make_sub_block();
    defer sub_block.instructions.deinit(sema.gpa);

    // This body is guaranteed to end with noreturn and has no breaks.
    try sema.analyze_body_inner(&sub_block, body);

    const operand_ty = sema.type_of(operand);
    const ptr_info = operand_ty.ptr_info(mod);
    const res_ty = try sema.ptr_type(.{
        .child = err_union_ty.error_union_payload(mod).to_intern(),
        .flags = .{
            .is_const = ptr_info.flags.is_const,
            .is_volatile = ptr_info.flags.is_volatile,
            .is_allowzero = ptr_info.flags.is_allowzero,
            .address_space = ptr_info.flags.address_space,
        },
    });
    const res_ty_ref = Air.interned_to_ref(res_ty.to_intern());
    try sema.air_extra.ensure_unused_capacity(sema.gpa, @typeInfo(Air.TryPtr).Struct.fields.len +
        sub_block.instructions.items.len);
    const try_inst = try parent_block.add_inst(.{
        .tag = .try_ptr,
        .data = .{ .ty_pl = .{
            .ty = res_ty_ref,
            .payload = sema.add_extra_assume_capacity(Air.TryPtr{
                .ptr = operand,
                .body_len = @int_cast(sub_block.instructions.items.len),
            }),
        } },
    });
    sema.air_extra.append_slice_assume_capacity(@ptr_cast(sub_block.instructions.items));
    return try_inst;
}

fn ensure_post_hoc(sema: *Sema, block: *Block, dest_block: Zir.Inst.Index) !*LabeledBlock {
    const gop = sema.inst_map.get_or_put_assume_capacity(dest_block);
    if (gop.found_existing) existing: {
        // This may be a *result* from an earlier iteration of an inline loop.
        // In this case, there will not be a post-hoc block entry, and we can
        // continue with the logic below.
        const new_block_inst = gop.value_ptr.*.to_index() orelse break :existing;
        return sema.post_hoc_blocks.get(new_block_inst) orelse break :existing;
    }

    try sema.post_hoc_blocks.ensure_unused_capacity(sema.gpa, 1);

    const new_block_inst: Air.Inst.Index = @enumFromInt(sema.air_instructions.len);
    gop.value_ptr.* = new_block_inst.to_ref();
    try sema.air_instructions.append(sema.gpa, .{
        .tag = .block,
        .data = undefined,
    });
    const labeled_block = try sema.gpa.create(LabeledBlock);
    labeled_block.* = .{
        .label = .{
            .zir_block = dest_block,
            .merges = .{
                .src_locs = .{},
                .results = .{},
                .br_list = .{},
                .block_inst = new_block_inst,
            },
        },
        .block = .{
            .parent = block,
            .sema = sema,
            .src_decl = block.src_decl,
            .namespace = block.namespace,
            .instructions = .{},
            .label = &labeled_block.label,
            .inlining = block.inlining,
            .is_comptime = block.is_comptime,
        },
    };
    sema.post_hoc_blocks.put_assume_capacity_no_clobber(new_block_inst, labeled_block);
    return labeled_block;
}

/// A `break` statement is inside a runtime condition, but trying to
/// break from an inline loop. In such case we must convert it to
/// a runtime break.
fn add_runtime_break(sema: *Sema, child_block: *Block, block_inst: Zir.Inst.Index, break_operand: Zir.Inst.Ref) !void {
    const labeled_block = try sema.ensure_post_hoc(child_block, block_inst);

    const operand = try sema.resolve_inst(break_operand);
    const br_ref = try child_block.add_br(labeled_block.label.merges.block_inst, operand);

    try labeled_block.label.merges.results.append(sema.gpa, operand);
    try labeled_block.label.merges.br_list.append(sema.gpa, br_ref.to_index().?);
    try labeled_block.label.merges.src_locs.append(sema.gpa, null);

    labeled_block.block.runtime_index.increment();
    if (labeled_block.block.runtime_cond == null and labeled_block.block.runtime_loop == null) {
        labeled_block.block.runtime_cond = child_block.runtime_cond orelse child_block.runtime_loop;
        labeled_block.block.runtime_loop = child_block.runtime_loop;
    }
}

fn zir_unreachable(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].@"unreachable";
    const src = inst_data.src();

    if (block.is_comptime) {
        return sema.fail(block, src, "reached unreachable code", .{});
    }
    // TODO Add compile error for @optimizeFor occurring too late in a scope.
    block.add_unreachable(src, true) catch |err| switch (err) {
        error.AnalysisFail => {
            const msg = sema.err orelse return err;
            if (!mem.eql(u8, msg.msg, "runtime safety check not allowed in naked function")) return err;
            try sema.err_note(block, src, msg, "the end of a naked function is implicitly unreachable", .{});
            return err;
        },
        else => |e| return e,
    };
}

fn zir_ret_err_value(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
) CompileError!void {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].str_tok;
    const err_name = try mod.intern_pool.get_or_put_string(
        sema.gpa,
        inst_data.get(sema.code),
        .no_embedded_nulls,
    );
    _ = try mod.get_error_value(err_name);
    const src = inst_data.src();
    // Return the error code from the function.
    const error_set_type = try mod.single_error_set_type(err_name);
    const result_inst = Air.interned_to_ref((try mod.intern(.{ .err = .{
        .ty = error_set_type.to_intern(),
        .name = err_name,
    } })));
    return sema.analyze_ret(block, result_inst, src, src);
}

fn zir_ret_implicit(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_tok;
    const r_brace_src = inst_data.src();
    if (block.inlining == null and sema.func_is_naked) {
        assert(!block.is_comptime);
        if (block.want_safety()) {
            // Calling a safety function from a naked function would not be legal.
            _ = try block.add_no_op(.trap);
        } else {
            try block.add_unreachable(r_brace_src, false);
        }
        return;
    }

    const operand = try sema.resolve_inst(inst_data.operand);
    const ret_ty_src: LazySrcLoc = .{ .node_offset_fn_type_ret_ty = 0 };
    const base_tag = sema.fn_ret_ty.base_zig_type_tag(mod);
    if (base_tag == .NoReturn) {
        const msg = msg: {
            const msg = try sema.err_msg(block, ret_ty_src, "function declared '{}' implicitly returns", .{
                sema.fn_ret_ty.fmt(mod),
            });
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, r_brace_src, msg, "control flow reaches end of body here", .{});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    } else if (base_tag != .Void) {
        const msg = msg: {
            const msg = try sema.err_msg(block, ret_ty_src, "function with non-void return type '{}' implicitly returns", .{
                sema.fn_ret_ty.fmt(mod),
            });
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, r_brace_src, msg, "control flow reaches end of body here", .{});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    return sema.analyze_ret(block, operand, r_brace_src, r_brace_src);
}

fn zir_ret_node(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const operand = try sema.resolve_inst(inst_data.operand);
    const src = inst_data.src();

    return sema.analyze_ret(block, operand, src, .{ .node_offset_return_operand = inst_data.src_node });
}

fn zir_ret_load(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const ret_ptr = try sema.resolve_inst(inst_data.operand);

    if (block.is_comptime or block.inlining != null or sema.func_is_naked) {
        const operand = try sema.analyze_load(block, src, ret_ptr, src);
        return sema.analyze_ret(block, operand, src, .{ .node_offset_return_operand = inst_data.src_node });
    }

    if (sema.want_error_return_tracing(sema.fn_ret_ty)) {
        const is_non_err = try sema.analyze_ptr_is_non_err(block, src, ret_ptr);
        return sema.ret_with_err_tracing(block, src, is_non_err, .ret_load, ret_ptr);
    }

    _ = try block.add_un_op(.ret_load, ret_ptr);
}

fn ret_with_err_tracing(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    is_non_err: Air.Inst.Ref,
    ret_tag: Air.Inst.Tag,
    operand: Air.Inst.Ref,
) CompileError!void {
    const mod = sema.mod;
    const need_check = switch (is_non_err) {
        .bool_true => {
            _ = try block.add_un_op(ret_tag, operand);
            return;
        },
        .bool_false => false,
        else => true,
    };
    const gpa = sema.gpa;
    const stack_trace_ty = try sema.get_builtin_type("StackTrace");
    try sema.resolve_type_fields(stack_trace_ty);
    const ptr_stack_trace_ty = try mod.single_mut_ptr_type(stack_trace_ty);
    const err_return_trace = try block.add_ty(.err_return_trace, ptr_stack_trace_ty);
    const return_err_fn = try sema.get_builtin("return_error");
    const args: [1]Air.Inst.Ref = .{err_return_trace};

    if (!need_check) {
        try sema.call_builtin(block, src, return_err_fn, .never_inline, &args, .@"error return");
        _ = try block.add_un_op(ret_tag, operand);
        return;
    }

    var then_block = block.make_sub_block();
    defer then_block.instructions.deinit(gpa);
    _ = try then_block.add_un_op(ret_tag, operand);

    var else_block = block.make_sub_block();
    defer else_block.instructions.deinit(gpa);
    try sema.call_builtin(&else_block, src, return_err_fn, .never_inline, &args, .@"error return");
    _ = try else_block.add_un_op(ret_tag, operand);

    try sema.air_extra.ensure_unused_capacity(gpa, @typeInfo(Air.CondBr).Struct.fields.len +
        then_block.instructions.items.len + else_block.instructions.items.len +
        @typeInfo(Air.Block).Struct.fields.len + 1);

    const cond_br_payload = sema.add_extra_assume_capacity(Air.CondBr{
        .then_body_len = @int_cast(then_block.instructions.items.len),
        .else_body_len = @int_cast(else_block.instructions.items.len),
    });
    sema.air_extra.append_slice_assume_capacity(@ptr_cast(then_block.instructions.items));
    sema.air_extra.append_slice_assume_capacity(@ptr_cast(else_block.instructions.items));

    _ = try block.add_inst(.{ .tag = .cond_br, .data = .{ .pl_op = .{
        .operand = is_non_err,
        .payload = cond_br_payload,
    } } });
}

fn want_error_return_tracing(sema: *Sema, fn_ret_ty: Type) bool {
    const mod = sema.mod;
    return fn_ret_ty.is_error(mod) and mod.comp.config.any_error_tracing;
}

fn zir_save_err_ret_index(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].save_err_ret_index;

    if (!block.owner_module().error_tracing) return;

    // This is only relevant at runtime.
    if (block.is_comptime or block.is_typeof) return;

    const save_index = inst_data.operand == .none or b: {
        const operand = try sema.resolve_inst(inst_data.operand);
        const operand_ty = sema.type_of(operand);
        break :b operand_ty.is_error(mod);
    };

    if (save_index)
        block.error_return_trace_index = try sema.analyze_save_err_ret_index(block);
}

fn zir_restore_err_ret_index(sema: *Sema, start_block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!void {
    const extra = sema.code.extra_data(Zir.Inst.RestoreErrRetIndex, extended.operand).data;
    return sema.restore_err_ret_index(start_block, extra.src(), extra.block, extra.operand);
}

/// If `operand` is non-error (or is `none`), restores the error return trace to
/// its state at the point `block` was reached (or, if `block` is `none`, the
/// point this function began execution).
fn restore_err_ret_index(sema: *Sema, start_block: *Block, src: LazySrcLoc, target_block: Zir.Inst.Ref, operand_zir: Zir.Inst.Ref) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;

    const saved_index = if (target_block.to_index_allow_none()) |zir_block| b: {
        var block = start_block;
        while (true) {
            if (block.label) |label| {
                if (label.zir_block == zir_block) {
                    const target_trace_index = if (block.parent) |parent_block| tgt: {
                        break :tgt parent_block.error_return_trace_index;
                    } else sema.error_return_trace_index_on_fn_entry;

                    if (start_block.error_return_trace_index != target_trace_index)
                        break :b target_trace_index;

                    return; // No need to restore
                }
            }
            block = block.parent.?;
        }
    } else b: {
        if (start_block.error_return_trace_index != sema.error_return_trace_index_on_fn_entry)
            break :b sema.error_return_trace_index_on_fn_entry;

        return; // No need to restore
    };

    const operand = try sema.resolve_inst_allow_none(operand_zir);

    if (start_block.is_comptime or start_block.is_typeof) {
        const is_non_error = if (operand != .none) blk: {
            const is_non_error_inst = try sema.analyze_is_non_err(start_block, src, operand);
            const cond_val = try sema.resolve_defined_value(start_block, src, is_non_error_inst);
            break :blk cond_val.?.to_bool();
        } else true; // no operand means pop unconditionally

        if (is_non_error) return;

        const saved_index_val = try sema.resolve_defined_value(start_block, src, saved_index);
        const saved_index_int = saved_index_val.?.to_unsigned_int(mod);
        assert(saved_index_int <= sema.comptime_err_ret_trace.items.len);
        sema.comptime_err_ret_trace.items.len = @int_cast(saved_index_int);
        return;
    }

    if (!mod.intern_pool.func_analysis(sema.owner_func_index).calls_or_awaits_errorable_fn) return;
    if (!start_block.owner_module().error_tracing) return;

    assert(saved_index != .none); // The .error_return_trace_index field was dropped somewhere

    return sema.pop_error_return_trace(start_block, src, operand, saved_index);
}

fn add_to_inferred_error_set(sema: *Sema, uncasted_operand: Air.Inst.Ref) !void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    assert(sema.fn_ret_ty.zig_type_tag(mod) == .ErrorUnion);
    const err_set_ty = sema.fn_ret_ty.error_union_set(mod).to_intern();
    switch (err_set_ty) {
        .adhoc_inferred_error_set_type => {
            const ies = sema.fn_ret_ty_ies.?;
            assert(ies.func == .none);
            try sema.add_to_inferred_error_set_ptr(ies, sema.type_of(uncasted_operand));
        },
        else => if (ip.is_inferred_error_set_type(err_set_ty)) {
            const ies = sema.fn_ret_ty_ies.?;
            assert(ies.func == sema.func_index);
            try sema.add_to_inferred_error_set_ptr(ies, sema.type_of(uncasted_operand));
        },
    }
}

fn add_to_inferred_error_set_ptr(sema: *Sema, ies: *InferredErrorSet, op_ty: Type) !void {
    const arena = sema.arena;
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    switch (op_ty.zig_type_tag(mod)) {
        .ErrorSet => try ies.add_error_set(op_ty, ip, arena),
        .ErrorUnion => try ies.add_error_set(op_ty.error_union_set(mod), ip, arena),
        else => {},
    }
}

fn analyze_ret(
    sema: *Sema,
    block: *Block,
    uncasted_operand: Air.Inst.Ref,
    src: LazySrcLoc,
    operand_src: LazySrcLoc,
) CompileError!void {
    // Special case for returning an error to an inferred error set; we need to
    // add the error tag to the inferred error set of the in-scope function, so
    // that the coercion below works correctly.
    const mod = sema.mod;
    if (sema.fn_ret_ty_ies != null and sema.fn_ret_ty.zig_type_tag(mod) == .ErrorUnion) {
        try sema.add_to_inferred_error_set(uncasted_operand);
    }
    const operand = sema.coerce_extra(block, sema.fn_ret_ty, uncasted_operand, operand_src, .{ .is_ret = true }) catch |err| switch (err) {
        error.NotCoercible => unreachable,
        else => |e| return e,
    };

    if (block.inlining) |inlining| {
        if (block.is_comptime) {
            const ret_val = try sema.resolve_const_value(block, operand_src, operand, .{
                .needed_comptime_reason = "value being returned at comptime must be comptime-known",
            });
            inlining.comptime_result = operand;

            if (sema.fn_ret_ty.is_error(mod) and ret_val.get_error_name(mod) != .none) {
                const src_decl = mod.decl_ptr(block.src_decl);
                const src_loc = src_decl.to_src_loc(src, mod);
                try sema.comptime_err_ret_trace.append(src_loc);
            }
            return error.ComptimeReturn;
        }
        // We are inlining a function call; rewrite the `ret` as a `break`.
        const br_inst = try block.add_br(inlining.merges.block_inst, operand);
        try inlining.merges.results.append(sema.gpa, operand);
        try inlining.merges.br_list.append(sema.gpa, br_inst.to_index().?);
        try inlining.merges.src_locs.append(sema.gpa, operand_src);
        return;
    } else if (block.is_comptime) {
        return sema.fail(block, src, "function called at runtime cannot return value at comptime", .{});
    } else if (sema.func_is_naked) {
        const msg = msg: {
            const msg = try sema.err_msg(block, src, "cannot return from naked function", .{});
            errdefer msg.destroy(sema.gpa);

            try sema.err_note(block, src, msg, "can only return using assembly", .{});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    try sema.resolve_type_layout(sema.fn_ret_ty);

    try sema.validate_runtime_value(block, operand_src, operand);

    const air_tag: Air.Inst.Tag = if (block.want_safety()) .ret_safe else .ret;
    if (sema.want_error_return_tracing(sema.fn_ret_ty)) {
        // Avoid adding a frame to the error return trace in case the value is comptime-known
        // to be not an error.
        const is_non_err = try sema.analyze_is_non_err(block, operand_src, operand);
        return sema.ret_with_err_tracing(block, src, is_non_err, air_tag, operand);
    }

    _ = try block.add_un_op(air_tag, operand);
}

fn float_op_allowed(tag: Zir.Inst.Tag) bool {
    // extend this swich as additional operators are implemented
    return switch (tag) {
        .add, .sub, .mul, .div, .div_exact, .div_trunc, .div_floor, .mod, .rem, .mod_rem => true,
        else => false,
    };
}

fn zir_ptr_type(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].ptr_type;
    const extra = sema.code.extra_data(Zir.Inst.PtrType, inst_data.payload_index);
    const elem_ty_src: LazySrcLoc = .{ .node_offset_ptr_elem = extra.data.src_node };
    const sentinel_src: LazySrcLoc = .{ .node_offset_ptr_sentinel = extra.data.src_node };
    const align_src: LazySrcLoc = .{ .node_offset_ptr_align = extra.data.src_node };
    const addrspace_src: LazySrcLoc = .{ .node_offset_ptr_addrspace = extra.data.src_node };
    const bitoffset_src: LazySrcLoc = .{ .node_offset_ptr_bitoffset = extra.data.src_node };
    const hostsize_src: LazySrcLoc = .{ .node_offset_ptr_hostsize = extra.data.src_node };

    const elem_ty = blk: {
        const air_inst = try sema.resolve_inst(extra.data.elem_type);
        const ty = sema.analyze_as_type(block, elem_ty_src, air_inst) catch |err| {
            if (err == error.AnalysisFail and sema.err != null and sema.type_of(air_inst).is_single_pointer(mod)) {
                try sema.err_note(block, elem_ty_src, sema.err.?, "use '.*' to dereference pointer", .{});
            }
            return err;
        };
        if (ty.is_generic_poison()) return error.GenericPoison;
        break :blk ty;
    };

    if (elem_ty.zig_type_tag(mod) == .NoReturn)
        return sema.fail(block, elem_ty_src, "pointer to noreturn not allowed", .{});

    const target = mod.get_target();

    var extra_i = extra.end;

    const sentinel = if (inst_data.flags.has_sentinel) blk: {
        const ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_i]);
        extra_i += 1;
        const coerced = try sema.coerce(block, elem_ty, try sema.resolve_inst(ref), sentinel_src);
        const val = try sema.resolve_const_defined_value(block, sentinel_src, coerced, .{
            .needed_comptime_reason = "pointer sentinel value must be comptime-known",
        });
        break :blk val.to_intern();
    } else .none;

    const abi_align: Alignment = if (inst_data.flags.has_align) blk: {
        const ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_i]);
        extra_i += 1;
        const coerced = try sema.coerce(block, Type.u32, try sema.resolve_inst(ref), align_src);
        const val = try sema.resolve_const_defined_value(block, align_src, coerced, .{
            .needed_comptime_reason = "pointer alignment must be comptime-known",
        });
        // Check if this happens to be the lazy alignment of our element type, in
        // which case we can make this 0 without resolving it.
        switch (mod.intern_pool.index_to_key(val.to_intern())) {
            .int => |int| switch (int.storage) {
                .lazy_align => |lazy_ty| if (lazy_ty == elem_ty.to_intern()) break :blk .none,
                else => {},
            },
            else => {},
        }
        const align_bytes = (try val.get_unsigned_int_advanced(mod, sema)).?;
        break :blk try sema.validate_align_allow_zero(block, align_src, align_bytes);
    } else .none;

    const address_space: std.builtin.AddressSpace = if (inst_data.flags.has_addrspace) blk: {
        const ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_i]);
        extra_i += 1;
        break :blk try sema.resolve_address_space(block, addrspace_src, ref, .pointer);
    } else if (elem_ty.zig_type_tag(mod) == .Fn and target.cpu.arch == .avr) .flash else .generic;

    const bit_offset: u16 = if (inst_data.flags.has_bit_range) blk: {
        const ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_i]);
        extra_i += 1;
        const bit_offset = try sema.resolve_int(block, bitoffset_src, ref, Type.u16, .{
            .needed_comptime_reason = "pointer bit-offset must be comptime-known",
        });
        break :blk @int_cast(bit_offset);
    } else 0;

    const host_size: u16 = if (inst_data.flags.has_bit_range) blk: {
        const ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_i]);
        extra_i += 1;
        const host_size = try sema.resolve_int(block, hostsize_src, ref, Type.u16, .{
            .needed_comptime_reason = "pointer host size must be comptime-known",
        });
        break :blk @int_cast(host_size);
    } else 0;

    if (host_size != 0) {
        if (bit_offset >= host_size * 8) {
            return sema.fail(block, bitoffset_src, "packed type '{}' at bit offset {} starts {} bits after the end of a {} byte host integer", .{
                elem_ty.fmt(mod), bit_offset, bit_offset - host_size * 8, host_size,
            });
        }
        const elem_bit_size = try elem_ty.bit_size_advanced(mod, sema);
        if (elem_bit_size > host_size * 8 - bit_offset) {
            return sema.fail(block, bitoffset_src, "packed type '{}' at bit offset {} ends {} bits after the end of a {} byte host integer", .{
                elem_ty.fmt(mod), bit_offset, elem_bit_size - (host_size * 8 - bit_offset), host_size,
            });
        }
    }

    if (elem_ty.zig_type_tag(mod) == .Fn) {
        if (inst_data.size != .One) {
            return sema.fail(block, elem_ty_src, "function pointers must be single pointers", .{});
        }
    } else if (inst_data.size == .Many and elem_ty.zig_type_tag(mod) == .Opaque) {
        return sema.fail(block, elem_ty_src, "unknown-length pointer to opaque not allowed", .{});
    } else if (inst_data.size == .C) {
        if (!try sema.validate_extern_type(elem_ty, .other)) {
            const msg = msg: {
                const msg = try sema.err_msg(block, elem_ty_src, "C pointers cannot point to non-C-ABI-compatible type '{}'", .{elem_ty.fmt(mod)});
                errdefer msg.destroy(sema.gpa);

                const src_decl = mod.decl_ptr(block.src_decl);
                try sema.explain_why_type_is_not_extern(msg, src_decl.to_src_loc(elem_ty_src, mod), elem_ty, .other);

                try sema.add_declared_here_note(msg, elem_ty);
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        }
        if (elem_ty.zig_type_tag(mod) == .Opaque) {
            return sema.fail(block, elem_ty_src, "C pointers cannot point to opaque types", .{});
        }
    }

    if (host_size != 0 and !try sema.validate_packed_type(elem_ty)) {
        return sema.fail_with_owned_error_msg(block, msg: {
            const msg = try sema.err_msg(block, elem_ty_src, "bit-pointer cannot refer to value of type '{}'", .{elem_ty.fmt(mod)});
            errdefer msg.destroy(sema.gpa);
            const src_decl = mod.decl_ptr(block.src_decl);
            try sema.explain_why_type_is_not_packed(msg, src_decl.to_src_loc(elem_ty_src, mod), elem_ty);
            break :msg msg;
        });
    }

    const ty = try sema.ptr_type(.{
        .child = elem_ty.to_intern(),
        .sentinel = sentinel,
        .flags = .{
            .alignment = abi_align,
            .address_space = address_space,
            .is_const = !inst_data.flags.is_mutable,
            .is_allowzero = inst_data.flags.is_allowzero,
            .is_volatile = inst_data.flags.is_volatile,
            .size = inst_data.size,
        },
        .packed_offset = .{
            .bit_offset = bit_offset,
            .host_size = host_size,
        },
    });
    return Air.interned_to_ref(ty.to_intern());
}

fn zir_struct_init_empty(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const ty_src: LazySrcLoc = .{ .node_offset_init_ty = inst_data.src_node };
    const obj_ty = try sema.resolve_type(block, ty_src, inst_data.operand);
    const mod = sema.mod;

    switch (obj_ty.zig_type_tag(mod)) {
        .Struct => return sema.struct_init_empty(block, obj_ty, src, src),
        .Array, .Vector => return sema.array_init_empty(block, src, obj_ty),
        .Void => return Air.interned_to_ref(Value.void.to_intern()),
        .Union => return sema.fail(block, src, "union initializer must initialize one field", .{}),
        else => return sema.fail_with_array_init_not_supported(block, src, obj_ty),
    }
}

fn zir_struct_init_empty_result(sema: *Sema, block: *Block, inst: Zir.Inst.Index, is_byref: bool) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const ty_operand = sema.resolve_type(block, src, inst_data.operand) catch |err| switch (err) {
        // Generic poison means this is an untyped anonymous empty struct init
        error.GenericPoison => return .empty_struct,
        else => |e| return e,
    };
    const init_ty = if (is_byref) ty: {
        const ptr_ty = ty_operand.opt_eu_base_type(mod);
        assert(ptr_ty.zig_type_tag(mod) == .Pointer); // validated by a previous instruction
        if (!ptr_ty.is_slice(mod)) {
            break :ty ptr_ty.child_type(mod);
        }
        // To make `&.{}` a `[:s]T`, the init should be a `[0:s]T`.
        break :ty try mod.array_type(.{
            .len = 0,
            .sentinel = if (ptr_ty.sentinel(mod)) |s| s.to_intern() else .none,
            .child = ptr_ty.child_type(mod).to_intern(),
        });
    } else ty_operand;
    const obj_ty = init_ty.opt_eu_base_type(mod);

    const empty_ref = switch (obj_ty.zig_type_tag(mod)) {
        .Struct => try sema.struct_init_empty(block, obj_ty, src, src),
        .Array, .Vector => try sema.array_init_empty(block, src, obj_ty),
        .Union => return sema.fail(block, src, "union initializer must initialize one field", .{}),
        else => return sema.fail_with_array_init_not_supported(block, src, obj_ty),
    };
    const init_ref = try sema.coerce(block, init_ty, empty_ref, src);

    if (is_byref) {
        const init_val = (try sema.resolve_value(init_ref)).?;
        return anon_decl_ref(sema, init_val.to_intern());
    } else {
        return init_ref;
    }
}

fn struct_init_empty(
    sema: *Sema,
    block: *Block,
    struct_ty: Type,
    dest_src: LazySrcLoc,
    init_src: LazySrcLoc,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const gpa = sema.gpa;
    // This logic must be synchronized with that in `zir_struct_init`.
    try sema.resolve_type_fields(struct_ty);

    // The init values to use for the struct instance.
    const field_inits = try gpa.alloc(Air.Inst.Ref, struct_ty.struct_field_count(mod));
    defer gpa.free(field_inits);
    @memset(field_inits, .none);

    return sema.finish_struct_init(block, init_src, dest_src, field_inits, struct_ty, struct_ty, false);
}

fn array_init_empty(sema: *Sema, block: *Block, src: LazySrcLoc, obj_ty: Type) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const arr_len = obj_ty.array_len(mod);
    if (arr_len != 0) {
        if (obj_ty.zig_type_tag(mod) == .Array) {
            return sema.fail(block, src, "expected {d} array elements; found 0", .{arr_len});
        } else {
            return sema.fail(block, src, "expected {d} vector elements; found 0", .{arr_len});
        }
    }
    return Air.interned_to_ref((try mod.intern(.{ .aggregate = .{
        .ty = obj_ty.to_intern(),
        .storage = .{ .elems = &.{} },
    } })));
}

fn zir_union_init(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const ty_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const field_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const init_src: LazySrcLoc = .{ .node_offset_builtin_call_arg2 = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.UnionInit, inst_data.payload_index).data;
    const union_ty = try sema.resolve_type(block, ty_src, extra.union_type);
    if (union_ty.zig_type_tag(sema.mod) != .Union) {
        return sema.fail(block, ty_src, "expected union type, found '{}'", .{union_ty.fmt(sema.mod)});
    }
    const field_name = try sema.resolve_const_string_intern(block, field_src, extra.field_name, .{
        .needed_comptime_reason = "name of field being initialized must be comptime-known",
    });
    const init = try sema.resolve_inst(extra.init);
    return sema.union_init(block, init, init_src, union_ty, ty_src, field_name, field_src);
}

fn union_init(
    sema: *Sema,
    block: *Block,
    uncasted_init: Air.Inst.Ref,
    init_src: LazySrcLoc,
    union_ty: Type,
    union_ty_src: LazySrcLoc,
    field_name: InternPool.NullTerminatedString,
    field_src: LazySrcLoc,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const field_index = try sema.union_field_index(block, union_ty, field_name, field_src);
    const field_ty = Type.from_interned(mod.type_to_union(union_ty).?.field_types.get(ip)[field_index]);
    const init = try sema.coerce(block, field_ty, uncasted_init, init_src);

    if (try sema.resolve_value(init)) |init_val| {
        const tag_ty = union_ty.union_tag_type_hypothetical(mod);
        const tag_val = try mod.enum_value_field_index(tag_ty, field_index);
        return Air.interned_to_ref((try mod.intern(.{ .un = .{
            .ty = union_ty.to_intern(),
            .tag = tag_val.to_intern(),
            .val = init_val.to_intern(),
        } })));
    }

    try sema.require_runtime_block(block, init_src, null);
    _ = union_ty_src;
    try sema.queue_full_type_resolution(union_ty);
    return block.add_union_init(union_ty, field_index, init);
}

fn zir_struct_init(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    is_ref: bool,
) CompileError!Air.Inst.Ref {
    const gpa = sema.gpa;
    const zir_datas = sema.code.instructions.items(.data);
    const inst_data = zir_datas[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.StructInit, inst_data.payload_index);
    const src = inst_data.src();

    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const first_item = sema.code.extra_data(Zir.Inst.StructInit.Item, extra.end).data;
    const first_field_type_data = zir_datas[@int_from_enum(first_item.field_type)].pl_node;
    const first_field_type_extra = sema.code.extra_data(Zir.Inst.FieldType, first_field_type_data.payload_index).data;
    const result_ty = sema.resolve_type(block, src, first_field_type_extra.container_type) catch |err| switch (err) {
        error.GenericPoison => {
            // The type wasn't actually known, so treat this as an anon struct init.
            return sema.struct_init_anon(block, src, .typed_init, extra.data, extra.end, is_ref);
        },
        else => |e| return e,
    };
    const resolved_ty = result_ty.opt_eu_base_type(mod);
    try sema.resolve_type_layout(resolved_ty);

    if (resolved_ty.zig_type_tag(mod) == .Struct) {
        // This logic must be synchronized with that in `zir_struct_init_empty`.

        // Maps field index to field_type index of where it was already initialized.
        // For making sure all fields are accounted for and no fields are duplicated.
        const found_fields = try gpa.alloc(Zir.Inst.Index, resolved_ty.struct_field_count(mod));
        defer gpa.free(found_fields);

        // The init values to use for the struct instance.
        const field_inits = try gpa.alloc(Air.Inst.Ref, resolved_ty.struct_field_count(mod));
        defer gpa.free(field_inits);
        @memset(field_inits, .none);

        var field_i: u32 = 0;
        var extra_index = extra.end;

        const is_packed = resolved_ty.container_layout(mod) == .@"packed";
        while (field_i < extra.data.fields_len) : (field_i += 1) {
            const item = sema.code.extra_data(Zir.Inst.StructInit.Item, extra_index);
            extra_index = item.end;

            const field_type_data = zir_datas[@int_from_enum(item.data.field_type)].pl_node;
            const field_src: LazySrcLoc = .{ .node_offset_initializer = field_type_data.src_node };
            const field_type_extra = sema.code.extra_data(Zir.Inst.FieldType, field_type_data.payload_index).data;
            const field_name = try ip.get_or_put_string(
                gpa,
                sema.code.null_terminated_string(field_type_extra.name_start),
                .no_embedded_nulls,
            );
            const field_index = if (resolved_ty.is_tuple(mod))
                try sema.tuple_field_index(block, resolved_ty, field_name, field_src)
            else
                try sema.struct_field_index(block, resolved_ty, field_name, field_src);
            assert(field_inits[field_index] == .none);
            found_fields[field_index] = item.data.field_type;
            const uncoerced_init = try sema.resolve_inst(item.data.init);
            const field_ty = resolved_ty.struct_field_type(field_index, mod);
            field_inits[field_index] = try sema.coerce(block, field_ty, uncoerced_init, field_src);
            if (!is_packed) {
                try sema.resolve_struct_field_inits(resolved_ty);
                if (try resolved_ty.struct_field_value_comptime(mod, field_index)) |default_value| {
                    const init_val = (try sema.resolve_value(field_inits[field_index])) orelse {
                        return sema.fail_with_needed_comptime(block, field_src, .{
                            .needed_comptime_reason = "value stored in comptime field must be comptime-known",
                        });
                    };

                    if (!init_val.eql(default_value, resolved_ty.struct_field_type(field_index, mod), mod)) {
                        return sema.fail_with_invalid_comptime_field_store(block, field_src, resolved_ty, field_index);
                    }
                }
            }
        }

        return sema.finish_struct_init(block, src, src, field_inits, resolved_ty, result_ty, is_ref);
    } else if (resolved_ty.zig_type_tag(mod) == .Union) {
        if (extra.data.fields_len != 1) {
            return sema.fail(block, src, "union initialization expects exactly one field", .{});
        }

        const item = sema.code.extra_data(Zir.Inst.StructInit.Item, extra.end);

        const field_type_data = zir_datas[@int_from_enum(item.data.field_type)].pl_node;
        const field_src: LazySrcLoc = .{ .node_offset_initializer = field_type_data.src_node };
        const field_type_extra = sema.code.extra_data(Zir.Inst.FieldType, field_type_data.payload_index).data;
        const field_name = try ip.get_or_put_string(
            gpa,
            sema.code.null_terminated_string(field_type_extra.name_start),
            .no_embedded_nulls,
        );
        const field_index = try sema.union_field_index(block, resolved_ty, field_name, field_src);
        const tag_ty = resolved_ty.union_tag_type_hypothetical(mod);
        const tag_val = try mod.enum_value_field_index(tag_ty, field_index);
        const field_ty = Type.from_interned(mod.type_to_union(resolved_ty).?.field_types.get(ip)[field_index]);

        if (field_ty.zig_type_tag(mod) == .NoReturn) {
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "cannot initialize 'noreturn' field of union", .{});
                errdefer msg.destroy(sema.gpa);

                try sema.add_field_err_note(resolved_ty, field_index, msg, "field '{}' declared here", .{
                    field_name.fmt(ip),
                });
                try sema.add_declared_here_note(msg, resolved_ty);
                break :msg msg;
            });
        }

        const uncoerced_init_inst = try sema.resolve_inst(item.data.init);
        const init_inst = try sema.coerce(block, field_ty, uncoerced_init_inst, field_src);

        if (try sema.resolve_value(init_inst)) |val| {
            const struct_val = Value.from_interned((try mod.intern(.{ .un = .{
                .ty = resolved_ty.to_intern(),
                .tag = tag_val.to_intern(),
                .val = val.to_intern(),
            } })));
            const final_val_inst = try sema.coerce(block, result_ty, Air.interned_to_ref(struct_val.to_intern()), src);
            const final_val = (try sema.resolve_value(final_val_inst)).?;
            return sema.add_constant_maybe_ref(final_val.to_intern(), is_ref);
        }

        if (try sema.type_requires_comptime(resolved_ty)) {
            return sema.fail_with_needed_comptime(block, field_src, .{
                .needed_comptime_reason = "initializer of comptime only union must be comptime-known",
            });
        }

        try sema.validate_runtime_value(block, field_src, init_inst);

        if (is_ref) {
            const target = mod.get_target();
            const alloc_ty = try sema.ptr_type(.{
                .child = result_ty.to_intern(),
                .flags = .{ .address_space = target_util.default_address_space(target, .local) },
            });
            const alloc = try block.add_ty(.alloc, alloc_ty);
            const base_ptr = try sema.opt_eu_base_ptr_init(block, alloc, src);
            const field_ptr = try sema.union_field_ptr(block, field_src, base_ptr, field_name, field_src, resolved_ty, true);
            try sema.store_ptr(block, src, field_ptr, init_inst);
            const new_tag = Air.interned_to_ref(tag_val.to_intern());
            _ = try block.add_bin_op(.set_union_tag, base_ptr, new_tag);
            return sema.make_ptr_const(block, alloc);
        }

        try sema.require_runtime_block(block, src, null);
        try sema.queue_full_type_resolution(resolved_ty);
        const union_val = try block.add_union_init(resolved_ty, field_index, init_inst);
        return sema.coerce(block, result_ty, union_val, src);
    }
    unreachable;
}

fn finish_struct_init(
    sema: *Sema,
    block: *Block,
    init_src: LazySrcLoc,
    dest_src: LazySrcLoc,
    field_inits: []Air.Inst.Ref,
    struct_ty: Type,
    result_ty: Type,
    is_ref: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;

    var root_msg: ?*Module.ErrorMsg = null;
    errdefer if (root_msg) |msg| msg.destroy(sema.gpa);

    switch (ip.index_to_key(struct_ty.to_intern())) {
        .anon_struct_type => |anon_struct| {
            // We can't get the slices, as the coercion may invalidate them.
            for (0..anon_struct.types.len) |i| {
                if (field_inits[i] != .none) {
                    // Coerce the init value to the field type.
                    const field_ty = Type.from_interned(anon_struct.types.get(ip)[i]);
                    field_inits[i] = sema.coerce(block, field_ty, field_inits[i], .unneeded) catch |err| switch (err) {
                        error.NeededSourceLocation => {
                            const decl = mod.decl_ptr(block.src_decl);
                            const field_src = mod.init_src(init_src.node_offset.x, decl, i);
                            _ = try sema.coerce(block, field_ty, field_inits[i], field_src);
                            unreachable;
                        },
                        else => |e| return e,
                    };
                    continue;
                }

                const default_val = anon_struct.values.get(ip)[i];

                if (default_val == .none) {
                    if (anon_struct.names.len == 0) {
                        const template = "missing tuple field with index {d}";
                        if (root_msg) |msg| {
                            try sema.err_note(block, init_src, msg, template, .{i});
                        } else {
                            root_msg = try sema.err_msg(block, init_src, template, .{i});
                        }
                    } else {
                        const field_name = anon_struct.names.get(ip)[i];
                        const template = "missing struct field: {}";
                        const args = .{field_name.fmt(ip)};
                        if (root_msg) |msg| {
                            try sema.err_note(block, init_src, msg, template, args);
                        } else {
                            root_msg = try sema.err_msg(block, init_src, template, args);
                        }
                    }
                } else {
                    field_inits[i] = Air.interned_to_ref(default_val);
                }
            }
        },
        .struct_type => {
            const struct_type = ip.load_struct_type(struct_ty.to_intern());
            for (0..struct_type.field_types.len) |i| {
                if (field_inits[i] != .none) {
                    // Coerce the init value to the field type.
                    const field_ty = Type.from_interned(struct_type.field_types.get(ip)[i]);
                    field_inits[i] = sema.coerce(block, field_ty, field_inits[i], init_src) catch |err| switch (err) {
                        error.NeededSourceLocation => {
                            const decl = mod.decl_ptr(block.src_decl);
                            const field_src = mod.init_src(init_src.node_offset.x, decl, i);
                            _ = try sema.coerce(block, field_ty, field_inits[i], field_src);
                            unreachable;
                        },
                        else => |e| return e,
                    };
                    continue;
                }

                try sema.resolve_struct_field_inits(struct_ty);

                const field_init = struct_type.field_init(ip, i);
                if (field_init == .none) {
                    if (!struct_type.is_tuple(ip)) {
                        const field_name = struct_type.field_names.get(ip)[i];
                        const template = "missing struct field: {}";
                        const args = .{field_name.fmt(ip)};
                        if (root_msg) |msg| {
                            try sema.err_note(block, init_src, msg, template, args);
                        } else {
                            root_msg = try sema.err_msg(block, init_src, template, args);
                        }
                    } else {
                        const template = "missing tuple field with index {d}";
                        if (root_msg) |msg| {
                            try sema.err_note(block, init_src, msg, template, .{i});
                        } else {
                            root_msg = try sema.err_msg(block, init_src, template, .{i});
                        }
                    }
                } else {
                    field_inits[i] = Air.interned_to_ref(field_init);
                }
            }
        },
        else => unreachable,
    }

    if (root_msg) |msg| {
        if (mod.type_to_struct(struct_ty)) |struct_type| {
            const decl = mod.decl_ptr(struct_type.decl.unwrap().?);
            const fqn = try decl.fully_qualified_name(mod);
            try mod.err_note_non_lazy(
                decl.src_loc(mod),
                msg,
                "struct '{}' declared here",
                .{fqn.fmt(ip)},
            );
        }
        root_msg = null;
        return sema.fail_with_owned_error_msg(block, msg);
    }

    // Find which field forces the expression to be runtime, if any.
    const opt_runtime_index = for (field_inits, 0..) |field_init, i| {
        if (!(try sema.is_comptime_known(field_init))) {
            break i;
        }
    } else null;

    const runtime_index = opt_runtime_index orelse {
        const elems = try sema.arena.alloc(InternPool.Index, field_inits.len);
        for (elems, field_inits) |*elem, field_init| {
            elem.* = (sema.resolve_value(field_init) catch unreachable).?.to_intern();
        }
        const struct_val = try mod.intern(.{ .aggregate = .{
            .ty = struct_ty.to_intern(),
            .storage = .{ .elems = elems },
        } });
        const final_val_inst = try sema.coerce(block, result_ty, Air.interned_to_ref(struct_val), init_src);
        const final_val = (try sema.resolve_value(final_val_inst)).?;
        return sema.add_constant_maybe_ref(final_val.to_intern(), is_ref);
    };

    if (try sema.type_requires_comptime(struct_ty)) {
        const decl = mod.decl_ptr(block.src_decl);
        const field_src = mod.init_src(init_src.node_offset.x, decl, runtime_index);
        return sema.fail_with_needed_comptime(block, field_src, .{
            .needed_comptime_reason = "initializer of comptime only struct must be comptime-known",
        });
    }

    for (field_inits) |field_init| {
        try sema.validate_runtime_value(block, dest_src, field_init);
    }

    if (is_ref) {
        try sema.resolve_struct_layout(struct_ty);
        const target = sema.mod.get_target();
        const alloc_ty = try sema.ptr_type(.{
            .child = result_ty.to_intern(),
            .flags = .{ .address_space = target_util.default_address_space(target, .local) },
        });
        const alloc = try block.add_ty(.alloc, alloc_ty);
        const base_ptr = try sema.opt_eu_base_ptr_init(block, alloc, init_src);
        for (field_inits, 0..) |field_init, i_usize| {
            const i: u32 = @int_cast(i_usize);
            const field_src = dest_src;
            const field_ptr = try sema.struct_field_ptr_by_index(block, dest_src, base_ptr, i, field_src, struct_ty, true);
            try sema.store_ptr(block, dest_src, field_ptr, field_init);
        }

        return sema.make_ptr_const(block, alloc);
    }

    sema.require_runtime_block(block, .unneeded, null) catch |err| switch (err) {
        error.NeededSourceLocation => {
            const decl = mod.decl_ptr(block.src_decl);
            const field_src = mod.init_src(dest_src.node_offset.x, decl, runtime_index);
            try sema.require_runtime_block(block, dest_src, field_src);
            unreachable;
        },
        else => |e| return e,
    };
    try sema.resolve_struct_field_inits(struct_ty);
    try sema.queue_full_type_resolution(struct_ty);
    const struct_val = try block.add_aggregate_init(struct_ty, field_inits);
    return sema.coerce(block, result_ty, struct_val, init_src);
}

fn zir_struct_init_anon(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const extra = sema.code.extra_data(Zir.Inst.StructInitAnon, inst_data.payload_index);
    return sema.struct_init_anon(block, src, .anon_init, extra.data, extra.end, false);
}

fn struct_init_anon(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    /// It is possible for a typed struct_init to be downgraded to an anonymous init due to a
    /// generic poison type. In this case, we need to know to interpret the extra data differently.
    comptime kind: enum { anon_init, typed_init },
    extra_data: switch (kind) {
        .anon_init => Zir.Inst.StructInitAnon,
        .typed_init => Zir.Inst.StructInit,
    },
    extra_end: usize,
    is_ref: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;
    const zir_datas = sema.code.instructions.items(.data);

    const types = try sema.arena.alloc(InternPool.Index, extra_data.fields_len);
    const values = try sema.arena.alloc(InternPool.Index, types.len);
    const names = try sema.arena.alloc(InternPool.NullTerminatedString, types.len);

    // Find which field forces the expression to be runtime, if any.
    const opt_runtime_index = rs: {
        var runtime_index: ?usize = null;
        var extra_index = extra_end;
        for (types, values, names, 0..) |*field_ty, *field_val, *field_name, i_usize| {
            const item = switch (kind) {
                .anon_init => sema.code.extra_data(Zir.Inst.StructInitAnon.Item, extra_index),
                .typed_init => sema.code.extra_data(Zir.Inst.StructInit.Item, extra_index),
            };
            extra_index = item.end;

            const name = switch (kind) {
                .anon_init => sema.code.null_terminated_string(item.data.field_name),
                .typed_init => name: {
                    // `item.data.field_type` references a `field_type` instruction
                    const field_type_data = zir_datas[@int_from_enum(item.data.field_type)].pl_node;
                    const field_type_extra = sema.code.extra_data(Zir.Inst.FieldType, field_type_data.payload_index);
                    break :name sema.code.null_terminated_string(field_type_extra.data.name_start);
                },
            };

            field_name.* = try mod.intern_pool.get_or_put_string(gpa, name, .no_embedded_nulls);

            const init = try sema.resolve_inst(item.data.init);
            field_ty.* = sema.type_of(init).to_intern();
            if (Type.from_interned(field_ty.*).zig_type_tag(mod) == .Opaque) {
                const msg = msg: {
                    const decl = mod.decl_ptr(block.src_decl);
                    const field_src = mod.init_src(src.node_offset.x, decl, @int_cast(i_usize));
                    const msg = try sema.err_msg(block, field_src, "opaque types have unknown size and therefore cannot be directly embedded in structs", .{});
                    errdefer msg.destroy(sema.gpa);

                    try sema.add_declared_here_note(msg, Type.from_interned(field_ty.*));
                    break :msg msg;
                };
                return sema.fail_with_owned_error_msg(block, msg);
            }
            if (try sema.resolve_value(init)) |init_val| {
                field_val.* = init_val.to_intern();
            } else {
                field_val.* = .none;
                runtime_index = @int_cast(i_usize);
            }
        }
        break :rs runtime_index;
    };

    const tuple_ty = try ip.get_anon_struct_type(gpa, .{
        .names = names,
        .types = types,
        .values = values,
    });

    const runtime_index = opt_runtime_index orelse {
        const tuple_val = try mod.intern(.{ .aggregate = .{
            .ty = tuple_ty,
            .storage = .{ .elems = values },
        } });
        return sema.add_constant_maybe_ref(tuple_val, is_ref);
    };

    sema.require_runtime_block(block, .unneeded, null) catch |err| switch (err) {
        error.NeededSourceLocation => {
            const decl = mod.decl_ptr(block.src_decl);
            const field_src = mod.init_src(src.node_offset.x, decl, runtime_index);
            try sema.require_runtime_block(block, src, field_src);
            unreachable;
        },
        else => |e| return e,
    };

    if (is_ref) {
        const target = mod.get_target();
        const alloc_ty = try sema.ptr_type(.{
            .child = tuple_ty,
            .flags = .{ .address_space = target_util.default_address_space(target, .local) },
        });
        const alloc = try block.add_ty(.alloc, alloc_ty);
        var extra_index = extra_end;
        for (types, 0..) |field_ty, i_usize| {
            const i: u32 = @int_cast(i_usize);
            const item = switch (kind) {
                .anon_init => sema.code.extra_data(Zir.Inst.StructInitAnon.Item, extra_index),
                .typed_init => sema.code.extra_data(Zir.Inst.StructInit.Item, extra_index),
            };
            extra_index = item.end;

            const field_ptr_ty = try sema.ptr_type(.{
                .child = field_ty,
                .flags = .{ .address_space = target_util.default_address_space(target, .local) },
            });
            if (values[i] == .none) {
                const init = try sema.resolve_inst(item.data.init);
                const field_ptr = try block.add_struct_field_ptr(alloc, i, field_ptr_ty);
                _ = try block.add_bin_op(.store, field_ptr, init);
            }
        }

        return sema.make_ptr_const(block, alloc);
    }

    const element_refs = try sema.arena.alloc(Air.Inst.Ref, types.len);
    var extra_index = extra_end;
    for (types, 0..) |_, i| {
        const item = switch (kind) {
            .anon_init => sema.code.extra_data(Zir.Inst.StructInitAnon.Item, extra_index),
            .typed_init => sema.code.extra_data(Zir.Inst.StructInit.Item, extra_index),
        };
        extra_index = item.end;
        element_refs[i] = try sema.resolve_inst(item.data.init);
    }

    return block.add_aggregate_init(Type.from_interned(tuple_ty), element_refs);
}

fn zir_array_init(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    is_ref: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();

    const extra = sema.code.extra_data(Zir.Inst.MultiOp, inst_data.payload_index);
    const args = sema.code.ref_slice(extra.end, extra.data.operands_len);
    assert(args.len >= 2); // array_ty + at least one element

    const result_ty = sema.resolve_type(block, src, args[0]) catch |err| switch (err) {
        error.GenericPoison => {
            // The type wasn't actually known, so treat this as an anon array init.
            return sema.array_init_anon(block, src, args[1..], is_ref);
        },
        else => |e| return e,
    };
    const array_ty = result_ty.opt_eu_base_type(mod);
    const is_tuple = array_ty.zig_type_tag(mod) == .Struct;
    const sentinel_val = array_ty.sentinel(mod);

    var root_msg: ?*Module.ErrorMsg = null;
    errdefer if (root_msg) |msg| msg.destroy(sema.gpa);

    const final_len = try sema.usize_cast(block, src, array_ty.array_len_including_sentinel(mod));
    const resolved_args = try gpa.alloc(Air.Inst.Ref, final_len);
    defer gpa.free(resolved_args);
    for (resolved_args, 0..) |*dest, i| {
        // Less inits than needed.
        if (i + 2 > args.len) if (is_tuple) {
            const default_val = array_ty.struct_field_default_value(i, mod).to_intern();
            if (default_val == .unreachable_value) {
                const template = "missing tuple field with index {d}";
                if (root_msg) |msg| {
                    try sema.err_note(block, src, msg, template, .{i});
                } else {
                    root_msg = try sema.err_msg(block, src, template, .{i});
                }
            } else {
                dest.* = Air.interned_to_ref(default_val);
            }
            continue;
        } else {
            dest.* = Air.interned_to_ref(sentinel_val.?.to_intern());
            break;
        };

        const arg = args[i + 1];
        const resolved_arg = try sema.resolve_inst(arg);
        const elem_ty = if (is_tuple)
            array_ty.struct_field_type(i, mod)
        else
            array_ty.elem_type2(mod);
        dest.* = sema.coerce(block, elem_ty, resolved_arg, .unneeded) catch |err| switch (err) {
            error.NeededSourceLocation => {
                const decl = mod.decl_ptr(block.src_decl);
                const elem_src = mod.init_src(src.node_offset.x, decl, i);
                _ = try sema.coerce(block, elem_ty, resolved_arg, elem_src);
                unreachable;
            },
            else => return err,
        };
        if (is_tuple) {
            if (array_ty.struct_field_is_comptime(i, mod))
                try sema.resolve_struct_field_inits(array_ty);
            if (try array_ty.struct_field_value_comptime(mod, i)) |field_val| {
                const init_val = try sema.resolve_value(dest.*) orelse {
                    const decl = mod.decl_ptr(block.src_decl);
                    const elem_src = mod.init_src(src.node_offset.x, decl, i);
                    return sema.fail_with_needed_comptime(block, elem_src, .{
                        .needed_comptime_reason = "value stored in comptime field must be comptime-known",
                    });
                };
                if (!field_val.eql(init_val, elem_ty, mod)) {
                    const decl = mod.decl_ptr(block.src_decl);
                    const elem_src = mod.init_src(src.node_offset.x, decl, i);
                    return sema.fail_with_invalid_comptime_field_store(block, elem_src, array_ty, i);
                }
            }
        }
    }

    if (root_msg) |msg| {
        try sema.add_declared_here_note(msg, array_ty);
        root_msg = null;
        return sema.fail_with_owned_error_msg(block, msg);
    }

    const opt_runtime_index: ?u32 = for (resolved_args, 0..) |arg, i| {
        const comptime_known = try sema.is_comptime_known(arg);
        if (!comptime_known) break @int_cast(i);
    } else null;

    const runtime_index = opt_runtime_index orelse {
        const elem_vals = try sema.arena.alloc(InternPool.Index, resolved_args.len);
        for (elem_vals, resolved_args) |*val, arg| {
            // We checked that all args are comptime above.
            val.* = (sema.resolve_value(arg) catch unreachable).?.to_intern();
        }
        const arr_val = try mod.intern(.{ .aggregate = .{
            .ty = array_ty.to_intern(),
            .storage = .{ .elems = elem_vals },
        } });
        const result_ref = try sema.coerce(block, result_ty, Air.interned_to_ref(arr_val), src);
        const result_val = (try sema.resolve_value(result_ref)).?;
        return sema.add_constant_maybe_ref(result_val.to_intern(), is_ref);
    };

    sema.require_runtime_block(block, .unneeded, null) catch |err| switch (err) {
        error.NeededSourceLocation => {
            const decl = mod.decl_ptr(block.src_decl);
            const elem_src = mod.init_src(src.node_offset.x, decl, runtime_index);
            try sema.require_runtime_block(block, src, elem_src);
            unreachable;
        },
        else => return err,
    };
    try sema.queue_full_type_resolution(array_ty);

    if (is_ref) {
        const target = mod.get_target();
        const alloc_ty = try sema.ptr_type(.{
            .child = result_ty.to_intern(),
            .flags = .{ .address_space = target_util.default_address_space(target, .local) },
        });
        const alloc = try block.add_ty(.alloc, alloc_ty);
        const base_ptr = try sema.opt_eu_base_ptr_init(block, alloc, src);

        if (is_tuple) {
            for (resolved_args, 0..) |arg, i| {
                const elem_ptr_ty = try sema.ptr_type(.{
                    .child = array_ty.struct_field_type(i, mod).to_intern(),
                    .flags = .{ .address_space = target_util.default_address_space(target, .local) },
                });
                const elem_ptr_ty_ref = Air.interned_to_ref(elem_ptr_ty.to_intern());

                const index = try mod.int_ref(Type.usize, i);
                const elem_ptr = try block.add_ptr_elem_ptr_type_ref(base_ptr, index, elem_ptr_ty_ref);
                _ = try block.add_bin_op(.store, elem_ptr, arg);
            }
            return sema.make_ptr_const(block, alloc);
        }

        const elem_ptr_ty = try sema.ptr_type(.{
            .child = array_ty.elem_type2(mod).to_intern(),
            .flags = .{ .address_space = target_util.default_address_space(target, .local) },
        });
        const elem_ptr_ty_ref = Air.interned_to_ref(elem_ptr_ty.to_intern());

        for (resolved_args, 0..) |arg, i| {
            const index = try mod.int_ref(Type.usize, i);
            const elem_ptr = try block.add_ptr_elem_ptr_type_ref(base_ptr, index, elem_ptr_ty_ref);
            _ = try block.add_bin_op(.store, elem_ptr, arg);
        }
        return sema.make_ptr_const(block, alloc);
    }

    const arr_ref = try block.add_aggregate_init(array_ty, resolved_args);
    return sema.coerce(block, result_ty, arr_ref, src);
}

fn zir_array_init_anon(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const extra = sema.code.extra_data(Zir.Inst.MultiOp, inst_data.payload_index);
    const operands = sema.code.ref_slice(extra.end, extra.data.operands_len);
    return sema.array_init_anon(block, src, operands, false);
}

fn array_init_anon(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    operands: []const Zir.Inst.Ref,
    is_ref: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;

    const types = try sema.arena.alloc(InternPool.Index, operands.len);
    const values = try sema.arena.alloc(InternPool.Index, operands.len);

    const opt_runtime_src = rs: {
        var runtime_src: ?LazySrcLoc = null;
        for (operands, 0..) |operand, i| {
            const operand_src = src; // TODO better source location
            const elem = try sema.resolve_inst(operand);
            types[i] = sema.type_of(elem).to_intern();
            if (Type.from_interned(types[i]).zig_type_tag(mod) == .Opaque) {
                const msg = msg: {
                    const msg = try sema.err_msg(block, operand_src, "opaque types have unknown size and therefore cannot be directly embedded in structs", .{});
                    errdefer msg.destroy(gpa);

                    try sema.add_declared_here_note(msg, Type.from_interned(types[i]));
                    break :msg msg;
                };
                return sema.fail_with_owned_error_msg(block, msg);
            }
            if (try sema.resolve_value(elem)) |val| {
                values[i] = val.to_intern();
            } else {
                values[i] = .none;
                runtime_src = operand_src;
            }
        }
        break :rs runtime_src;
    };

    const tuple_ty = try ip.get_anon_struct_type(gpa, .{
        .types = types,
        .values = values,
        .names = &.{},
    });

    const runtime_src = opt_runtime_src orelse {
        const tuple_val = try mod.intern(.{ .aggregate = .{
            .ty = tuple_ty,
            .storage = .{ .elems = values },
        } });
        return sema.add_constant_maybe_ref(tuple_val, is_ref);
    };

    try sema.require_runtime_block(block, src, runtime_src);

    if (is_ref) {
        const target = sema.mod.get_target();
        const alloc_ty = try sema.ptr_type(.{
            .child = tuple_ty,
            .flags = .{ .address_space = target_util.default_address_space(target, .local) },
        });
        const alloc = try block.add_ty(.alloc, alloc_ty);
        for (operands, 0..) |operand, i_usize| {
            const i: u32 = @int_cast(i_usize);
            const field_ptr_ty = try sema.ptr_type(.{
                .child = types[i],
                .flags = .{ .address_space = target_util.default_address_space(target, .local) },
            });
            if (values[i] == .none) {
                const field_ptr = try block.add_struct_field_ptr(alloc, i, field_ptr_ty);
                _ = try block.add_bin_op(.store, field_ptr, try sema.resolve_inst(operand));
            }
        }

        return sema.make_ptr_const(block, alloc);
    }

    const element_refs = try sema.arena.alloc(Air.Inst.Ref, operands.len);
    for (operands, 0..) |operand, i| {
        element_refs[i] = try sema.resolve_inst(operand);
    }

    return block.add_aggregate_init(Type.from_interned(tuple_ty), element_refs);
}

fn add_constant_maybe_ref(sema: *Sema, val: InternPool.Index, is_ref: bool) !Air.Inst.Ref {
    return if (is_ref) anon_decl_ref(sema, val) else Air.interned_to_ref(val);
}

fn zir_field_type_ref(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.FieldTypeRef, inst_data.payload_index).data;
    const ty_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const field_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const aggregate_ty = try sema.resolve_type(block, ty_src, extra.container_type);
    const field_name = try sema.resolve_const_string_intern(block, field_src, extra.field_name, .{
        .needed_comptime_reason = "field name must be comptime-known",
    });
    return sema.field_type(block, aggregate_ty, field_name, field_src, ty_src);
}

fn zir_struct_init_field_type(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.FieldType, inst_data.payload_index).data;
    const ty_src = inst_data.src();
    const field_name_src: LazySrcLoc = .{ .node_offset_field_name_init = inst_data.src_node };
    const wrapped_aggregate_ty = sema.resolve_type(block, ty_src, extra.container_type) catch |err| switch (err) {
        // Since this is a ZIR instruction that returns a type, encountering
        // generic poison should not result in a failed compilation, but the
        // generic poison type. This prevents unnecessary failures when
        // constructing types at compile-time.
        error.GenericPoison => return .generic_poison_type,
        else => |e| return e,
    };
    const aggregate_ty = wrapped_aggregate_ty.opt_eu_base_type(mod);
    const zir_field_name = sema.code.null_terminated_string(extra.name_start);
    const field_name = try ip.get_or_put_string(sema.gpa, zir_field_name, .no_embedded_nulls);
    return sema.field_type(block, aggregate_ty, field_name, field_name_src, ty_src);
}

fn field_type(
    sema: *Sema,
    block: *Block,
    aggregate_ty: Type,
    field_name: InternPool.NullTerminatedString,
    field_src: LazySrcLoc,
    ty_src: LazySrcLoc,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    var cur_ty = aggregate_ty;
    while (true) {
        try sema.resolve_type_fields(cur_ty);
        switch (cur_ty.zig_type_tag(mod)) {
            .Struct => switch (ip.index_to_key(cur_ty.to_intern())) {
                .anon_struct_type => |anon_struct| {
                    const field_index = if (anon_struct.names.len == 0)
                        try sema.tuple_field_index(block, cur_ty, field_name, field_src)
                    else
                        try sema.anon_struct_field_index(block, cur_ty, field_name, field_src);
                    return Air.interned_to_ref(anon_struct.types.get(ip)[field_index]);
                },
                .struct_type => {
                    const struct_type = ip.load_struct_type(cur_ty.to_intern());
                    const field_index = struct_type.name_index(ip, field_name) orelse
                        return sema.fail_with_bad_struct_field_access(block, struct_type, field_src, field_name);
                    const field_ty = struct_type.field_types.get(ip)[field_index];
                    return Air.interned_to_ref(field_ty);
                },
                else => unreachable,
            },
            .Union => {
                const union_obj = mod.type_to_union(cur_ty).?;
                const field_index = union_obj.load_tag_type(ip).name_index(ip, field_name) orelse
                    return sema.fail_with_bad_union_field_access(block, union_obj, field_src, field_name);
                const field_ty = union_obj.field_types.get(ip)[field_index];
                return Air.interned_to_ref(field_ty);
            },
            .Optional => {
                // Struct/array init through optional requires the child type to not be a pointer.
                // If the child of .optional is a pointer it'll error on the next loop.
                cur_ty = Type.from_interned(ip.index_to_key(cur_ty.to_intern()).opt_type);
                continue;
            },
            .ErrorUnion => {
                cur_ty = cur_ty.error_union_payload(mod);
                continue;
            },
            else => {},
        }
        return sema.fail(block, ty_src, "expected struct or union; found '{}'", .{
            cur_ty.fmt(sema.mod),
        });
    }
}

fn zir_error_return_trace(sema: *Sema, block: *Block) CompileError!Air.Inst.Ref {
    return sema.get_error_return_trace(block);
}

fn get_error_return_trace(sema: *Sema, block: *Block) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const stack_trace_ty = try sema.get_builtin_type("StackTrace");
    try sema.resolve_type_fields(stack_trace_ty);
    const ptr_stack_trace_ty = try mod.single_mut_ptr_type(stack_trace_ty);
    const opt_ptr_stack_trace_ty = try mod.optional_type(ptr_stack_trace_ty.to_intern());

    if (sema.owner_func_index != .none and
        ip.func_analysis(sema.owner_func_index).calls_or_awaits_errorable_fn and
        block.owner_module().error_tracing)
    {
        return block.add_ty(.err_return_trace, opt_ptr_stack_trace_ty);
    }
    return Air.interned_to_ref((try mod.intern(.{ .opt = .{
        .ty = opt_ptr_stack_trace_ty.to_intern(),
        .val = .none,
    } })));
}

fn zir_frame(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    const src = LazySrcLoc.nodeOffset(@bit_cast(extended.operand));
    return sema.fail_with_use_of_async(block, src);
}

fn zir_align_of(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const ty = try sema.resolve_type(block, operand_src, inst_data.operand);
    if (ty.is_no_return(mod)) {
        return sema.fail(block, operand_src, "no align available for type '{}'", .{ty.fmt(sema.mod)});
    }
    const val = try ty.lazy_abi_alignment(mod);
    if (val.is_lazy_align(mod)) {
        try sema.queue_full_type_resolution(ty);
    }
    return Air.interned_to_ref(val.to_intern());
}

fn zir_int_from_bool(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand = try sema.resolve_inst(inst_data.operand);
    const operand_ty = sema.type_of(operand);
    const is_vector = operand_ty.zig_type_tag(mod) == .Vector;
    const operand_scalar_ty = operand_ty.scalar_type(mod);
    if (operand_scalar_ty.to_intern() != .bool_type) {
        return sema.fail(block, src, "expected 'bool', found '{}'", .{operand_scalar_ty.zig_type_tag(mod)});
    }
    if (try sema.resolve_value(operand)) |val| {
        if (!is_vector) {
            if (val.is_undef(mod)) return mod.undef_ref(Type.u1);
            if (val.to_bool()) return Air.interned_to_ref((try mod.int_value(Type.u1, 1)).to_intern());
            return Air.interned_to_ref((try mod.int_value(Type.u1, 0)).to_intern());
        }
        const len = operand_ty.vector_len(mod);
        const dest_ty = try mod.vector_type(.{ .child = .u1_type, .len = len });
        if (val.is_undef(mod)) return mod.undef_ref(dest_ty);
        const new_elems = try sema.arena.alloc(InternPool.Index, len);
        for (new_elems, 0..) |*new_elem, i| {
            const old_elem = try val.elem_value(mod, i);
            const new_val = if (old_elem.is_undef(mod))
                try mod.undef_value(Type.u1)
            else if (old_elem.to_bool())
                try mod.int_value(Type.u1, 1)
            else
                try mod.int_value(Type.u1, 0);
            new_elem.* = new_val.to_intern();
        }
        return Air.interned_to_ref(try mod.intern(.{ .aggregate = .{
            .ty = dest_ty.to_intern(),
            .storage = .{ .elems = new_elems },
        } }));
    }
    if (!is_vector) {
        return block.add_un_op(.int_from_bool, operand);
    }
    const len = operand_ty.vector_len(mod);
    const dest_ty = try mod.vector_type(.{ .child = .u1_type, .len = len });
    const new_elems = try sema.arena.alloc(Air.Inst.Ref, len);
    for (new_elems, 0..) |*new_elem, i| {
        const idx_ref = try mod.int_ref(Type.usize, i);
        const old_elem = try block.add_bin_op(.array_elem_val, operand, idx_ref);
        new_elem.* = try block.add_un_op(.int_from_bool, old_elem);
    }
    return block.add_aggregate_init(dest_ty, new_elems);
}

fn zir_error_name(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const uncoerced_operand = try sema.resolve_inst(inst_data.operand);
    const operand = try sema.coerce(block, Type.anyerror, uncoerced_operand, operand_src);

    if (try sema.resolve_defined_value(block, operand_src, operand)) |val| {
        const err_name = sema.mod.intern_pool.index_to_key(val.to_intern()).err.name;
        return sema.add_null_terminated_str_lit(err_name);
    }

    // Similar to zir_tag_name, we have special AIR instruction for the error name in case an optimimzation pass
    // might be able to resolve the result at compile time.
    return block.add_un_op(.error_name, operand);
}

fn zir_abs(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const operand = try sema.resolve_inst(inst_data.operand);
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const operand_ty = sema.type_of(operand);
    const scalar_ty = operand_ty.scalar_type(mod);

    const result_ty = switch (scalar_ty.zig_type_tag(mod)) {
        .ComptimeFloat, .Float, .ComptimeInt => operand_ty,
        .Int => if (scalar_ty.is_signed_int(mod)) try operand_ty.to_unsigned(mod) else return operand,
        else => return sema.fail(
            block,
            operand_src,
            "expected integer, float, or vector of either integers or floats, found '{}'",
            .{operand_ty.fmt(mod)},
        ),
    };

    return (try sema.maybe_constant_unary_math(operand, result_ty, Value.abs)) orelse {
        try sema.require_runtime_block(block, operand_src, null);
        return block.add_ty_op(.abs, result_ty, operand);
    };
}

fn maybe_constant_unary_math(
    sema: *Sema,
    operand: Air.Inst.Ref,
    result_ty: Type,
    comptime eval: fn (Value, Type, Allocator, *Module) Allocator.Error!Value,
) CompileError!?Air.Inst.Ref {
    const mod = sema.mod;
    switch (result_ty.zig_type_tag(mod)) {
        .Vector => if (try sema.resolve_value(operand)) |val| {
            const scalar_ty = result_ty.scalar_type(mod);
            const vec_len = result_ty.vector_len(mod);
            if (val.is_undef(mod))
                return try mod.undef_ref(result_ty);

            const elems = try sema.arena.alloc(InternPool.Index, vec_len);
            for (elems, 0..) |*elem, i| {
                const elem_val = try val.elem_value(sema.mod, i);
                elem.* = (try eval(elem_val, scalar_ty, sema.arena, sema.mod)).to_intern();
            }
            return Air.interned_to_ref((try mod.intern(.{ .aggregate = .{
                .ty = result_ty.to_intern(),
                .storage = .{ .elems = elems },
            } })));
        },
        else => if (try sema.resolve_value(operand)) |operand_val| {
            if (operand_val.is_undef(mod))
                return try mod.undef_ref(result_ty);
            const result_val = try eval(operand_val, result_ty, sema.arena, sema.mod);
            return Air.interned_to_ref(result_val.to_intern());
        },
    }
    return null;
}

fn zir_unary_math(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    air_tag: Air.Inst.Tag,
    comptime eval: fn (Value, Type, Allocator, *Module) Allocator.Error!Value,
) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const operand = try sema.resolve_inst(inst_data.operand);
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const operand_ty = sema.type_of(operand);
    const scalar_ty = operand_ty.scalar_type(mod);

    switch (scalar_ty.zig_type_tag(mod)) {
        .ComptimeFloat, .Float => {},
        else => return sema.fail(
            block,
            operand_src,
            "expected vector of floats or float type, found '{}'",
            .{operand_ty.fmt(sema.mod)},
        ),
    }

    return (try sema.maybe_constant_unary_math(operand, operand_ty, eval)) orelse {
        try sema.require_runtime_block(block, operand_src, null);
        return block.add_un_op(air_tag, operand);
    };
}

fn zir_tag_name(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const src = inst_data.src();
    const operand = try sema.resolve_inst(inst_data.operand);
    const operand_ty = sema.type_of(operand);
    const mod = sema.mod;
    const ip = &mod.intern_pool;

    try sema.resolve_type_layout(operand_ty);
    const enum_ty = switch (operand_ty.zig_type_tag(mod)) {
        .EnumLiteral => {
            const val = try sema.resolve_const_defined_value(block, .unneeded, operand, undefined);
            const tag_name = ip.index_to_key(val.to_intern()).enum_literal;
            return sema.add_null_terminated_str_lit(tag_name);
        },
        .Enum => operand_ty,
        .Union => operand_ty.union_tag_type(mod) orelse
            return sema.fail(block, src, "union '{}' is untagged", .{operand_ty.fmt(sema.mod)}),
        else => return sema.fail(block, operand_src, "expected enum or union; found '{}'", .{
            operand_ty.fmt(mod),
        }),
    };
    if (enum_ty.enum_field_count(mod) == 0) {
        // TODO I don't think this is the correct way to handle this but
        // it prevents a crash.
        // https://github.com/ziglang/zig/issues/15909
        return sema.fail(block, operand_src, "cannot get @tag_name of empty enum '{}'", .{
            enum_ty.fmt(mod),
        });
    }
    const enum_decl_index = enum_ty.get_owner_decl(mod);
    const casted_operand = try sema.coerce(block, enum_ty, operand, operand_src);
    if (try sema.resolve_defined_value(block, operand_src, casted_operand)) |val| {
        const field_index = enum_ty.enum_tag_field_index(val, mod) orelse {
            const enum_decl = mod.decl_ptr(enum_decl_index);
            const msg = msg: {
                const msg = try sema.err_msg(block, src, "no field with value '{}' in enum '{}'", .{
                    val.fmt_value(sema.mod, sema), enum_decl.name.fmt(ip),
                });
                errdefer msg.destroy(sema.gpa);
                try mod.err_note_non_lazy(enum_decl.src_loc(mod), msg, "declared here", .{});
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        };
        // TODO: write something like get_coerced_ints to avoid needing to dupe
        const field_name = enum_ty.enum_field_name(field_index, mod);
        return sema.add_null_terminated_str_lit(field_name);
    }
    try sema.require_runtime_block(block, src, operand_src);
    if (block.want_safety() and sema.mod.backend_supports_feature(.is_named_enum_value)) {
        const ok = try block.add_un_op(.is_named_enum_value, casted_operand);
        try sema.add_safety_check(block, src, ok, .invalid_enum_value);
    }
    // In case the value is runtime-known, we have an AIR instruction for this instead
    // of trying to lower it in Sema because an optimization pass may result in the operand
    // being comptime-known, which would let us elide the `tag_name` AIR instruction.
    return block.add_un_op(.tag_name, casted_operand);
}

fn zir_reify(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
    inst: Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;
    const name_strategy: Zir.Inst.NameStrategy = @enumFromInt(extended.small);
    const extra = sema.code.extra_data(Zir.Inst.UnNode, extended.operand).data;
    const src = LazySrcLoc.nodeOffset(extra.node);
    const type_info_ty = try sema.get_builtin_type("Type");
    const uncasted_operand = try sema.resolve_inst(extra.operand);
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const type_info = try sema.coerce(block, type_info_ty, uncasted_operand, operand_src);
    const val = try sema.resolve_const_defined_value(block, operand_src, type_info, .{
        .needed_comptime_reason = "operand to @Type must be comptime-known",
    });
    const union_val = ip.index_to_key(val.to_intern()).un;
    if (try sema.any_undef(block, operand_src, Value.from_interned(union_val.val))) {
        return sema.fail_with_use_of_undef(block, operand_src);
    }
    const tag_index = type_info_ty.union_tag_field_index(Value.from_interned(union_val.tag), mod).?;
    switch (@as(std.builtin.TypeId, @enumFromInt(tag_index))) {
        .Type => return .type_type,
        .Void => return .void_type,
        .Bool => return .bool_type,
        .NoReturn => return .noreturn_type,
        .ComptimeFloat => return .comptime_float_type,
        .ComptimeInt => return .comptime_int_type,
        .Undefined => return .undefined_type,
        .Null => return .null_type,
        .AnyFrame => return sema.fail_with_use_of_async(block, src),
        .EnumLiteral => return .enum_literal_type,
        .Int => {
            const struct_type = ip.load_struct_type(ip.type_of(union_val.val));
            const signedness_val = try Value.from_interned(union_val.val).field_value(
                mod,
                struct_type.name_index(ip, try ip.get_or_put_string(gpa, "signedness", .no_embedded_nulls)).?,
            );
            const bits_val = try Value.from_interned(union_val.val).field_value(
                mod,
                struct_type.name_index(ip, try ip.get_or_put_string(gpa, "bits", .no_embedded_nulls)).?,
            );

            const signedness = mod.to_enum(std.builtin.Signedness, signedness_val);
            const bits: u16 = @int_cast(try bits_val.to_unsigned_int_advanced(sema));
            const ty = try mod.int_type(signedness, bits);
            return Air.interned_to_ref(ty.to_intern());
        },
        .Vector => {
            const struct_type = ip.load_struct_type(ip.type_of(union_val.val));
            const len_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "len", .no_embedded_nulls),
            ).?);
            const child_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "child", .no_embedded_nulls),
            ).?);

            const len: u32 = @int_cast(try len_val.to_unsigned_int_advanced(sema));
            const child_ty = child_val.to_type();

            try sema.check_vector_elem_type(block, src, child_ty);

            const ty = try mod.vector_type(.{
                .len = len,
                .child = child_ty.to_intern(),
            });
            return Air.interned_to_ref(ty.to_intern());
        },
        .Float => {
            const struct_type = ip.load_struct_type(ip.type_of(union_val.val));
            const bits_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "bits", .no_embedded_nulls),
            ).?);

            const bits: u16 = @int_cast(try bits_val.to_unsigned_int_advanced(sema));
            const ty = switch (bits) {
                16 => Type.f16,
                32 => Type.f32,
                64 => Type.f64,
                80 => Type.f80,
                128 => Type.f128,
                else => return sema.fail(block, src, "{}-bit float unsupported", .{bits}),
            };
            return Air.interned_to_ref(ty.to_intern());
        },
        .Pointer => {
            const struct_type = ip.load_struct_type(ip.type_of(union_val.val));
            const size_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "size", .no_embedded_nulls),
            ).?);
            const is_const_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "is_const", .no_embedded_nulls),
            ).?);
            const is_volatile_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "is_volatile", .no_embedded_nulls),
            ).?);
            const alignment_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "alignment", .no_embedded_nulls),
            ).?);
            const address_space_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "address_space", .no_embedded_nulls),
            ).?);
            const child_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "child", .no_embedded_nulls),
            ).?);
            const is_allowzero_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "is_allowzero", .no_embedded_nulls),
            ).?);
            const sentinel_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "sentinel", .no_embedded_nulls),
            ).?);

            if (!try sema.int_fits_in_type(alignment_val, Type.u32, null)) {
                return sema.fail(block, src, "alignment must fit in 'u32'", .{});
            }

            const alignment_val_int = (try alignment_val.get_unsigned_int_advanced(mod, sema)).?;
            if (alignment_val_int > 0 and !math.is_power_of_two(alignment_val_int)) {
                return sema.fail(block, src, "alignment value '{d}' is not a power of two or zero", .{alignment_val_int});
            }
            const abi_align = Alignment.from_byte_units(alignment_val_int);

            const elem_ty = child_val.to_type();
            if (abi_align != .none) {
                try sema.resolve_type_layout(elem_ty);
            }

            const ptr_size = mod.to_enum(std.builtin.Type.Pointer.Size, size_val);

            const actual_sentinel: InternPool.Index = s: {
                if (!sentinel_val.is_null(mod)) {
                    if (ptr_size == .One or ptr_size == .C) {
                        return sema.fail(block, src, "sentinels are only allowed on slices and unknown-length pointers", .{});
                    }
                    const sentinel_ptr_val = sentinel_val.optional_value(mod).?;
                    const ptr_ty = try mod.single_mut_ptr_type(elem_ty);
                    const sent_val = (try sema.pointer_deref(block, src, sentinel_ptr_val, ptr_ty)).?;
                    break :s sent_val.to_intern();
                }
                break :s .none;
            };

            if (elem_ty.zig_type_tag(mod) == .NoReturn) {
                return sema.fail(block, src, "pointer to noreturn not allowed", .{});
            } else if (elem_ty.zig_type_tag(mod) == .Fn) {
                if (ptr_size != .One) {
                    return sema.fail(block, src, "function pointers must be single pointers", .{});
                }
            } else if (ptr_size == .Many and elem_ty.zig_type_tag(mod) == .Opaque) {
                return sema.fail(block, src, "unknown-length pointer to opaque not allowed", .{});
            } else if (ptr_size == .C) {
                if (!try sema.validate_extern_type(elem_ty, .other)) {
                    const msg = msg: {
                        const msg = try sema.err_msg(block, src, "C pointers cannot point to non-C-ABI-compatible type '{}'", .{elem_ty.fmt(mod)});
                        errdefer msg.destroy(gpa);

                        const src_decl = mod.decl_ptr(block.src_decl);
                        try sema.explain_why_type_is_not_extern(msg, src_decl.to_src_loc(src, mod), elem_ty, .other);

                        try sema.add_declared_here_note(msg, elem_ty);
                        break :msg msg;
                    };
                    return sema.fail_with_owned_error_msg(block, msg);
                }
                if (elem_ty.zig_type_tag(mod) == .Opaque) {
                    return sema.fail(block, src, "C pointers cannot point to opaque types", .{});
                }
            }

            const ty = try sema.ptr_type(.{
                .child = elem_ty.to_intern(),
                .sentinel = actual_sentinel,
                .flags = .{
                    .size = ptr_size,
                    .is_const = is_const_val.to_bool(),
                    .is_volatile = is_volatile_val.to_bool(),
                    .alignment = abi_align,
                    .address_space = mod.to_enum(std.builtin.AddressSpace, address_space_val),
                    .is_allowzero = is_allowzero_val.to_bool(),
                },
            });
            return Air.interned_to_ref(ty.to_intern());
        },
        .Array => {
            const struct_type = ip.load_struct_type(ip.type_of(union_val.val));
            const len_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "len", .no_embedded_nulls),
            ).?);
            const child_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "child", .no_embedded_nulls),
            ).?);
            const sentinel_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "sentinel", .no_embedded_nulls),
            ).?);

            const len = try len_val.to_unsigned_int_advanced(sema);
            const child_ty = child_val.to_type();
            const sentinel = if (sentinel_val.optional_value(mod)) |p| blk: {
                const ptr_ty = try mod.single_mut_ptr_type(child_ty);
                break :blk (try sema.pointer_deref(block, src, p, ptr_ty)).?;
            } else null;

            const ty = try mod.array_type(.{
                .len = len,
                .sentinel = if (sentinel) |s| s.to_intern() else .none,
                .child = child_ty.to_intern(),
            });
            return Air.interned_to_ref(ty.to_intern());
        },
        .Optional => {
            const struct_type = ip.load_struct_type(ip.type_of(union_val.val));
            const child_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "child", .no_embedded_nulls),
            ).?);

            const child_ty = child_val.to_type();

            const ty = try mod.optional_type(child_ty.to_intern());
            return Air.interned_to_ref(ty.to_intern());
        },
        .ErrorUnion => {
            const struct_type = ip.load_struct_type(ip.type_of(union_val.val));
            const error_set_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "error_set", .no_embedded_nulls),
            ).?);
            const payload_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "payload", .no_embedded_nulls),
            ).?);

            const error_set_ty = error_set_val.to_type();
            const payload_ty = payload_val.to_type();

            if (error_set_ty.zig_type_tag(mod) != .ErrorSet) {
                return sema.fail(block, src, "Type.ErrorUnion.error_set must be an error set type", .{});
            }

            const ty = try mod.error_union_type(error_set_ty, payload_ty);
            return Air.interned_to_ref(ty.to_intern());
        },
        .ErrorSet => {
            const payload_val = Value.from_interned(union_val.val).optional_value(mod) orelse
                return Air.interned_to_ref(Type.anyerror.to_intern());

            const names_val = try sema.deref_slice_as_array(block, src, payload_val, .{
                .needed_comptime_reason = "error set contents must be comptime-known",
            });

            const len = try sema.usize_cast(block, src, names_val.type_of(mod).array_len(mod));
            var names: InferredErrorSet.NameMap = .{};
            try names.ensure_unused_capacity(sema.arena, len);
            for (0..len) |i| {
                const elem_val = try names_val.elem_value(mod, i);
                const elem_struct_type = ip.load_struct_type(ip.type_of(elem_val.to_intern()));
                const name_val = try elem_val.field_value(mod, elem_struct_type.name_index(
                    ip,
                    try ip.get_or_put_string(gpa, "name", .no_embedded_nulls),
                ).?);

                const name = try sema.slice_to_ip_string(block, src, name_val, .{
                    .needed_comptime_reason = "error set contents must be comptime-known",
                });
                _ = try mod.get_error_value(name);
                const gop = names.get_or_put_assume_capacity(name);
                if (gop.found_existing) {
                    return sema.fail(block, src, "duplicate error '{}'", .{
                        name.fmt(ip),
                    });
                }
            }

            const ty = try mod.error_set_from_unsorted_names(names.keys());
            return Air.interned_to_ref(ty.to_intern());
        },
        .Struct => {
            const struct_type = ip.load_struct_type(ip.type_of(union_val.val));
            const layout_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "layout", .no_embedded_nulls),
            ).?);
            const backing_integer_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "backing_integer", .no_embedded_nulls),
            ).?);
            const fields_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "fields", .no_embedded_nulls),
            ).?);
            const decls_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "decls", .no_embedded_nulls),
            ).?);
            const is_tuple_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "is_tuple", .no_embedded_nulls),
            ).?);

            const layout = mod.to_enum(std.builtin.Type.ContainerLayout, layout_val);

            // Decls
            if (try decls_val.slice_len(sema) > 0) {
                return sema.fail(block, src, "reified structs must have no decls", .{});
            }

            if (layout != .@"packed" and !backing_integer_val.is_null(mod)) {
                return sema.fail(block, src, "non-packed struct does not support backing integer type", .{});
            }

            const fields_arr = try sema.deref_slice_as_array(block, operand_src, fields_val, .{
                .needed_comptime_reason = "struct fields must be comptime-known",
            });

            return try sema.reify_struct(block, inst, src, layout, backing_integer_val, fields_arr, name_strategy, is_tuple_val.to_bool());
        },
        .Enum => {
            const struct_type = ip.load_struct_type(ip.type_of(union_val.val));
            const tag_type_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "tag_type", .no_embedded_nulls),
            ).?);
            const fields_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "fields", .no_embedded_nulls),
            ).?);
            const decls_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "decls", .no_embedded_nulls),
            ).?);
            const is_exhaustive_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "is_exhaustive", .no_embedded_nulls),
            ).?);

            if (try decls_val.slice_len(sema) > 0) {
                return sema.fail(block, src, "reified enums must have no decls", .{});
            }

            const fields_arr = try sema.deref_slice_as_array(block, operand_src, fields_val, .{
                .needed_comptime_reason = "enum fields must be comptime-known",
            });

            return sema.reify_enum(block, inst, src, tag_type_val.to_type(), is_exhaustive_val.to_bool(), fields_arr, name_strategy);
        },
        .Opaque => {
            const struct_type = ip.load_struct_type(ip.type_of(union_val.val));
            const decls_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "decls", .no_embedded_nulls),
            ).?);

            // Decls
            if (try decls_val.slice_len(sema) > 0) {
                return sema.fail(block, src, "reified opaque must have no decls", .{});
            }

            const wip_ty = switch (try ip.get_opaque_type(gpa, .{
                .has_namespace = false,
                .key = .{ .reified = .{
                    .zir_index = try ip.track_zir(gpa, block.get_file_scope(mod), inst),
                } },
            })) {
                .existing => |ty| return Air.interned_to_ref(ty),
                .wip => |wip| wip,
            };
            errdefer wip_ty.cancel(ip);

            const new_decl_index = try sema.create_anonymous_decl_type_named(
                block,
                src,
                Value.from_interned(wip_ty.index),
                name_strategy,
                "opaque",
                inst,
            );
            mod.decl_ptr(new_decl_index).owns_tv = true;
            errdefer mod.abort_anon_decl(new_decl_index);

            try mod.finalize_anon_decl(new_decl_index);

            return Air.interned_to_ref(wip_ty.finish(ip, new_decl_index, .none));
        },
        .Union => {
            const struct_type = ip.load_struct_type(ip.type_of(union_val.val));
            const layout_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "layout", .no_embedded_nulls),
            ).?);
            const tag_type_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "tag_type", .no_embedded_nulls),
            ).?);
            const fields_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "fields", .no_embedded_nulls),
            ).?);
            const decls_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "decls", .no_embedded_nulls),
            ).?);

            if (try decls_val.slice_len(sema) > 0) {
                return sema.fail(block, src, "reified unions must have no decls", .{});
            }
            const layout = mod.to_enum(std.builtin.Type.ContainerLayout, layout_val);

            const fields_arr = try sema.deref_slice_as_array(block, operand_src, fields_val, .{
                .needed_comptime_reason = "union fields must be comptime-known",
            });

            return sema.reify_union(block, inst, src, layout, tag_type_val, fields_arr, name_strategy);
        },
        .Fn => {
            const struct_type = ip.load_struct_type(ip.type_of(union_val.val));
            const calling_convention_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "calling_convention", .no_embedded_nulls),
            ).?);
            const is_generic_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "is_generic", .no_embedded_nulls),
            ).?);
            const is_var_args_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "is_var_args", .no_embedded_nulls),
            ).?);
            const return_type_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "return_type", .no_embedded_nulls),
            ).?);
            const params_slice_val = try Value.from_interned(union_val.val).field_value(mod, struct_type.name_index(
                ip,
                try ip.get_or_put_string(gpa, "params", .no_embedded_nulls),
            ).?);

            const is_generic = is_generic_val.to_bool();
            if (is_generic) {
                return sema.fail(block, src, "Type.Fn.is_generic must be false for @Type", .{});
            }

            const is_var_args = is_var_args_val.to_bool();
            const cc = mod.to_enum(std.builtin.CallingConvention, calling_convention_val);
            if (is_var_args) {
                try sema.check_call_conv_supports_var_args(block, src, cc);
            }

            const return_type = return_type_val.optional_value(mod) orelse
                return sema.fail(block, src, "Type.Fn.return_type must be non-null for @Type", .{});

            const params_val = try sema.deref_slice_as_array(block, operand_src, params_slice_val, .{
                .needed_comptime_reason = "function parameters must be comptime-known",
            });

            const args_len = try sema.usize_cast(block, src, params_val.type_of(mod).array_len(mod));
            const param_types = try sema.arena.alloc(InternPool.Index, args_len);

            var noalias_bits: u32 = 0;
            for (param_types, 0..) |*param_type, i| {
                const elem_val = try params_val.elem_value(mod, i);
                const elem_struct_type = ip.load_struct_type(ip.type_of(elem_val.to_intern()));
                const param_is_generic_val = try elem_val.field_value(mod, elem_struct_type.name_index(
                    ip,
                    try ip.get_or_put_string(gpa, "is_generic", .no_embedded_nulls),
                ).?);
                const param_is_noalias_val = try elem_val.field_value(mod, elem_struct_type.name_index(
                    ip,
                    try ip.get_or_put_string(gpa, "is_noalias", .no_embedded_nulls),
                ).?);
                const opt_param_type_val = try elem_val.field_value(mod, elem_struct_type.name_index(
                    ip,
                    try ip.get_or_put_string(gpa, "type", .no_embedded_nulls),
                ).?);

                if (param_is_generic_val.to_bool()) {
                    return sema.fail(block, src, "Type.Fn.Param.is_generic must be false for @Type", .{});
                }

                const param_type_val = opt_param_type_val.optional_value(mod) orelse
                    return sema.fail(block, src, "Type.Fn.Param.type must be non-null for @Type", .{});
                param_type.* = param_type_val.to_intern();

                if (param_is_noalias_val.to_bool()) {
                    if (!Type.from_interned(param_type.*).is_ptr_at_runtime(mod)) {
                        return sema.fail(block, src, "non-pointer parameter declared noalias", .{});
                    }
                    noalias_bits |= @as(u32, 1) << (std.math.cast(u5, i) orelse
                        return sema.fail(block, src, "this compiler implementation only supports 'noalias' on the first 32 parameters", .{}));
                }
            }

            const ty = try mod.func_type(.{
                .param_types = param_types,
                .noalias_bits = noalias_bits,
                .return_type = return_type.to_intern(),
                .cc = cc,
                .is_var_args = is_var_args,
            });
            return Air.interned_to_ref(ty.to_intern());
        },
        .Frame => return sema.fail_with_use_of_async(block, src),
    }
}

fn reify_enum(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    src: LazySrcLoc,
    tag_ty: Type,
    is_exhaustive: bool,
    fields_val: Value,
    name_strategy: Zir.Inst.NameStrategy,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;

    // This logic must stay in sync with the structure of `std.builtin.Type.Enum` - search for `field_value`.

    const fields_len: u32 = @int_cast(fields_val.type_of(mod).array_len(mod));

    // The validation work here is non-trivial, and it's possible the type already exists.
    // So in this first pass, let's just construct a hash to optimize for this case. If the
    // inputs turn out to be invalid, we can cancel the WIP type later.

    // For deduplication purposes, we must create a hash including all details of this type.
    // TODO: use a longer hash!
    var hasher = std.hash.Wyhash.init(0);
    std.hash.auto_hash(&hasher, tag_ty.to_intern());
    std.hash.auto_hash(&hasher, is_exhaustive);
    std.hash.auto_hash(&hasher, fields_len);

    for (0..fields_len) |field_idx| {
        const field_info = try fields_val.elem_value(mod, field_idx);

        const field_name_val = try field_info.field_value(mod, 0);
        const field_value_val = try sema.resolve_lazy_value(try field_info.field_value(mod, 1));

        const field_name = try sema.slice_to_ip_string(block, src, field_name_val, .{
            .needed_comptime_reason = "enum field name must be comptime-known",
        });

        std.hash.auto_hash(&hasher, .{
            field_name,
            field_value_val.to_intern(),
        });
    }

    const wip_ty = switch (try ip.get_enum_type(gpa, .{
        .has_namespace = false,
        .has_values = true,
        .tag_mode = if (is_exhaustive) .explicit else .nonexhaustive,
        .fields_len = fields_len,
        .key = .{ .reified = .{
            .zir_index = try ip.track_zir(gpa, block.get_file_scope(mod), inst),
            .type_hash = hasher.final(),
        } },
    })) {
        .wip => |wip| wip,
        .existing => |ty| return Air.interned_to_ref(ty),
    };
    errdefer wip_ty.cancel(ip);

    if (tag_ty.zig_type_tag(mod) != .Int) {
        return sema.fail(block, src, "Type.Enum.tag_type must be an integer type", .{});
    }

    const new_decl_index = try sema.create_anonymous_decl_type_named(
        block,
        src,
        Value.from_interned(wip_ty.index),
        name_strategy,
        "enum",
        inst,
    );
    mod.decl_ptr(new_decl_index).owns_tv = true;
    errdefer mod.abort_anon_decl(new_decl_index);

    wip_ty.prepare(ip, new_decl_index, .none);
    wip_ty.set_tag_ty(ip, tag_ty.to_intern());

    for (0..fields_len) |field_idx| {
        const field_info = try fields_val.elem_value(mod, field_idx);

        const field_name_val = try field_info.field_value(mod, 0);
        const field_value_val = try sema.resolve_lazy_value(try field_info.field_value(mod, 1));

        // Don't pass a reason; first loop acts as an assertion that this is valid.
        const field_name = try sema.slice_to_ip_string(block, src, field_name_val, undefined);

        if (!try sema.int_fits_in_type(field_value_val, tag_ty, null)) {
            // TODO: better source location
            return sema.fail(block, src, "field '{}' with enumeration value '{}' is too large for backing int type '{}'", .{
                field_name.fmt(ip),
                field_value_val.fmt_value(mod, sema),
                tag_ty.fmt(mod),
            });
        }

        const coerced_field_val = try mod.get_coerced(field_value_val, tag_ty);
        if (wip_ty.next_field(ip, field_name, coerced_field_val.to_intern())) |conflict| {
            return sema.fail_with_owned_error_msg(block, switch (conflict.kind) {
                .name => msg: {
                    const msg = try sema.err_msg(block, src, "duplicate enum field '{}'", .{field_name.fmt(ip)});
                    errdefer msg.destroy(gpa);
                    _ = conflict.prev_field_idx; // TODO: this note is incorrect
                    try sema.err_note(block, src, msg, "other field here", .{});
                    break :msg msg;
                },
                .value => msg: {
                    const msg = try sema.err_msg(block, src, "enum tag value {} already taken", .{field_value_val.fmt_value(mod, sema)});
                    errdefer msg.destroy(gpa);
                    _ = conflict.prev_field_idx; // TODO: this note is incorrect
                    try sema.err_note(block, src, msg, "other enum tag value here", .{});
                    break :msg msg;
                },
            });
        }
    }

    if (!is_exhaustive and fields_len > 1 and std.math.log2_int(u64, fields_len) == tag_ty.bit_size(mod)) {
        return sema.fail(block, src, "non-exhaustive enum specified every value", .{});
    }

    try mod.finalize_anon_decl(new_decl_index);
    return Air.interned_to_ref(wip_ty.index);
}

fn reify_union(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    src: LazySrcLoc,
    layout: std.builtin.Type.ContainerLayout,
    opt_tag_type_val: Value,
    fields_val: Value,
    name_strategy: Zir.Inst.NameStrategy,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;

    // This logic must stay in sync with the structure of `std.builtin.Type.Union` - search for `field_value`.

    const fields_len: u32 = @int_cast(fields_val.type_of(mod).array_len(mod));

    // The validation work here is non-trivial, and it's possible the type already exists.
    // So in this first pass, let's just construct a hash to optimize for this case. If the
    // inputs turn out to be invalid, we can cancel the WIP type later.

    // For deduplication purposes, we must create a hash including all details of this type.
    // TODO: use a longer hash!
    var hasher = std.hash.Wyhash.init(0);
    std.hash.auto_hash(&hasher, layout);
    std.hash.auto_hash(&hasher, opt_tag_type_val.to_intern());
    std.hash.auto_hash(&hasher, fields_len);

    var any_aligns = false;

    for (0..fields_len) |field_idx| {
        const field_info = try fields_val.elem_value(mod, field_idx);

        const field_name_val = try field_info.field_value(mod, 0);
        const field_type_val = try field_info.field_value(mod, 1);
        const field_align_val = try sema.resolve_lazy_value(try field_info.field_value(mod, 2));

        const field_name = try sema.slice_to_ip_string(block, src, field_name_val, .{
            .needed_comptime_reason = "union field name must be comptime-known",
        });

        std.hash.auto_hash(&hasher, .{
            field_name,
            field_type_val.to_intern(),
            field_align_val.to_intern(),
        });

        if (field_align_val.to_unsigned_int(mod) != 0) {
            any_aligns = true;
        }
    }

    const wip_ty = switch (try ip.get_union_type(gpa, .{
        .flags = .{
            .layout = layout,
            .status = .none,
            .runtime_tag = if (opt_tag_type_val.optional_value(mod) != null)
                .tagged
            else if (layout != .auto)
                .none
            else switch (block.want_safety()) {
                true => .safety,
                false => .none,
            },
            .any_aligned_fields = any_aligns,
            .requires_comptime = .unknown,
            .assumed_runtime_bits = false,
            .assumed_pointer_aligned = false,
            .alignment = .none,
        },
        .has_namespace = false,
        .fields_len = fields_len,
        .enum_tag_ty = .none, // set later because not yet validated
        .field_types = &.{}, // set later
        .field_aligns = &.{}, // set later
        .key = .{ .reified = .{
            .zir_index = try ip.track_zir(gpa, block.get_file_scope(mod), inst),
            .type_hash = hasher.final(),
        } },
    })) {
        .wip => |wip| wip,
        .existing => |ty| return Air.interned_to_ref(ty),
    };
    errdefer wip_ty.cancel(ip);

    const new_decl_index = try sema.create_anonymous_decl_type_named(
        block,
        src,
        Value.from_interned(wip_ty.index),
        name_strategy,
        "union",
        inst,
    );
    mod.decl_ptr(new_decl_index).owns_tv = true;
    errdefer mod.abort_anon_decl(new_decl_index);

    const field_types = try sema.arena.alloc(InternPool.Index, fields_len);
    const field_aligns = if (any_aligns) try sema.arena.alloc(InternPool.Alignment, fields_len) else undefined;

    const enum_tag_ty, const has_explicit_tag = if (opt_tag_type_val.optional_value(mod)) |tag_type_val| tag_ty: {
        switch (ip.index_to_key(tag_type_val.to_intern())) {
            .enum_type => {},
            else => return sema.fail(block, src, "Type.Union.tag_type must be an enum type", .{}),
        }
        const enum_tag_ty = tag_type_val.to_type();

        // We simply track which fields of the tag type have been seen.
        const tag_ty_fields_len = enum_tag_ty.enum_field_count(mod);
        var seen_tags = try std.DynamicBitSetUnmanaged.init_empty(sema.arena, tag_ty_fields_len);

        for (field_types, 0..) |*field_ty, field_idx| {
            const field_info = try fields_val.elem_value(mod, field_idx);

            const field_name_val = try field_info.field_value(mod, 0);
            const field_type_val = try field_info.field_value(mod, 1);

            // Don't pass a reason; first loop acts as an assertion that this is valid.
            const field_name = try sema.slice_to_ip_string(block, src, field_name_val, undefined);

            const enum_index = enum_tag_ty.enum_field_index(field_name, mod) orelse {
                // TODO: better source location
                return sema.fail(block, src, "no field named '{}' in enum '{}'", .{
                    field_name.fmt(ip), enum_tag_ty.fmt(mod),
                });
            };
            if (seen_tags.is_set(enum_index)) {
                // TODO: better source location
                return sema.fail(block, src, "duplicate union field {}", .{field_name.fmt(ip)});
            }
            seen_tags.set(enum_index);

            field_ty.* = field_type_val.to_intern();
            if (any_aligns) {
                const byte_align = try (try field_info.field_value(mod, 2)).to_unsigned_int_advanced(sema);
                if (byte_align > 0 and !math.is_power_of_two(byte_align)) {
                    // TODO: better source location
                    return sema.fail(block, src, "alignment value '{d}' is not a power of two or zero", .{byte_align});
                }
                field_aligns[field_idx] = Alignment.from_byte_units(byte_align);
            }
        }

        if (tag_ty_fields_len > fields_len) return sema.fail_with_owned_error_msg(block, msg: {
            const msg = try sema.err_msg(block, src, "enum fields missing in union", .{});
            errdefer msg.destroy(gpa);
            var it = seen_tags.iterator(.{ .kind = .unset });
            while (it.next()) |enum_index| {
                const field_name = enum_tag_ty.enum_field_name(enum_index, mod);
                try sema.add_field_err_note(enum_tag_ty, enum_index, msg, "field '{}' missing, declared here", .{
                    field_name.fmt(ip),
                });
            }
            try sema.add_declared_here_note(msg, enum_tag_ty);
            break :msg msg;
        });

        break :tag_ty .{ enum_tag_ty.to_intern(), true };
    } else tag_ty: {
        // We must track field names and set up the tag type ourselves.
        var field_names: std.AutoArrayHashMapUnmanaged(InternPool.NullTerminatedString, void) = .{};
        try field_names.ensure_total_capacity(sema.arena, fields_len);

        for (field_types, 0..) |*field_ty, field_idx| {
            const field_info = try fields_val.elem_value(mod, field_idx);

            const field_name_val = try field_info.field_value(mod, 0);
            const field_type_val = try field_info.field_value(mod, 1);

            // Don't pass a reason; first loop acts as an assertion that this is valid.
            const field_name = try sema.slice_to_ip_string(block, src, field_name_val, undefined);
            const gop = field_names.get_or_put_assume_capacity(field_name);
            if (gop.found_existing) {
                // TODO: better source location
                return sema.fail(block, src, "duplicate union field {}", .{field_name.fmt(ip)});
            }

            field_ty.* = field_type_val.to_intern();
            if (any_aligns) {
                const byte_align = try (try field_info.field_value(mod, 2)).to_unsigned_int_advanced(sema);
                if (byte_align > 0 and !math.is_power_of_two(byte_align)) {
                    // TODO: better source location
                    return sema.fail(block, src, "alignment value '{d}' is not a power of two or zero", .{byte_align});
                }
                field_aligns[field_idx] = Alignment.from_byte_units(byte_align);
            }
        }

        const enum_tag_ty = try sema.generate_union_tag_type_simple(block, field_names.keys(), mod.decl_ptr(new_decl_index));
        break :tag_ty .{ enum_tag_ty, false };
    };
    errdefer if (!has_explicit_tag) ip.remove(enum_tag_ty); // remove generated tag type on error

    for (field_types) |field_ty_ip| {
        const field_ty = Type.from_interned(field_ty_ip);
        if (field_ty.zig_type_tag(mod) == .Opaque) {
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "opaque types have unknown size and therefore cannot be directly embedded in unions", .{});
                errdefer msg.destroy(gpa);

                try sema.add_declared_here_note(msg, field_ty);
                break :msg msg;
            });
        }
        if (layout == .@"extern" and !try sema.validate_extern_type(field_ty, .union_field)) {
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "extern unions cannot contain fields of type '{}'", .{field_ty.fmt(mod)});
                errdefer msg.destroy(gpa);

                const src_decl = mod.decl_ptr(block.src_decl);
                try sema.explain_why_type_is_not_extern(msg, src_decl.to_src_loc(src, mod), field_ty, .union_field);

                try sema.add_declared_here_note(msg, field_ty);
                break :msg msg;
            });
        } else if (layout == .@"packed" and !try sema.validate_packed_type(field_ty)) {
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "packed unions cannot contain fields of type '{}'", .{field_ty.fmt(mod)});
                errdefer msg.destroy(gpa);

                const src_decl = mod.decl_ptr(block.src_decl);
                try sema.explain_why_type_is_not_packed(msg, src_decl.to_src_loc(src, mod), field_ty);

                try sema.add_declared_here_note(msg, field_ty);
                break :msg msg;
            });
        }
    }

    const loaded_union = ip.load_union_type(wip_ty.index);
    loaded_union.set_field_types(ip, field_types);
    if (any_aligns) {
        loaded_union.set_field_aligns(ip, field_aligns);
    }
    loaded_union.tag_type_ptr(ip).* = enum_tag_ty;
    loaded_union.flags_ptr(ip).status = .have_field_types;

    try mod.finalize_anon_decl(new_decl_index);
    return Air.interned_to_ref(wip_ty.finish(ip, new_decl_index, .none));
}

fn reify_struct(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    src: LazySrcLoc,
    layout: std.builtin.Type.ContainerLayout,
    opt_backing_int_val: Value,
    fields_val: Value,
    name_strategy: Zir.Inst.NameStrategy,
    is_tuple: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;

    // This logic must stay in sync with the structure of `std.builtin.Type.Struct` - search for `field_value`.

    const fields_len: u32 = @int_cast(fields_val.type_of(mod).array_len(mod));

    // The validation work here is non-trivial, and it's possible the type already exists.
    // So in this first pass, let's just construct a hash to optimize for this case. If the
    // inputs turn out to be invalid, we can cancel the WIP type later.

    // For deduplication purposes, we must create a hash including all details of this type.
    // TODO: use a longer hash!
    var hasher = std.hash.Wyhash.init(0);
    std.hash.auto_hash(&hasher, layout);
    std.hash.auto_hash(&hasher, opt_backing_int_val.to_intern());
    std.hash.auto_hash(&hasher, is_tuple);
    std.hash.auto_hash(&hasher, fields_len);

    var any_comptime_fields = false;
    var any_default_inits = false;
    var any_aligned_fields = false;

    for (0..fields_len) |field_idx| {
        const field_info = try fields_val.elem_value(mod, field_idx);

        const field_name_val = try field_info.field_value(mod, 0);
        const field_type_val = try field_info.field_value(mod, 1);
        const field_default_value_val = try field_info.field_value(mod, 2);
        const field_is_comptime_val = try field_info.field_value(mod, 3);
        const field_alignment_val = try sema.resolve_lazy_value(try field_info.field_value(mod, 4));

        const field_name = try sema.slice_to_ip_string(block, src, field_name_val, .{
            .needed_comptime_reason = "struct field name must be comptime-known",
        });
        const field_is_comptime = field_is_comptime_val.to_bool();
        const field_default_value: InternPool.Index = if (field_default_value_val.optional_value(mod)) |ptr_val| d: {
            const ptr_ty = try mod.single_const_ptr_type(field_type_val.to_type());
            // We need to do this deref here, so we won't check for this error case later on.
            const val = try sema.pointer_deref(block, src, ptr_val, ptr_ty) orelse return sema.fail_with_needed_comptime(
                block,
                src,
                .{ .needed_comptime_reason = "struct field default value must be comptime-known" },
            );
            // Resolve the value so that lazy values do not create distinct types.
            break :d (try sema.resolve_lazy_value(val)).to_intern();
        } else .none;

        std.hash.auto_hash(&hasher, .{
            field_name,
            field_type_val.to_intern(),
            field_default_value,
            field_is_comptime,
            field_alignment_val.to_intern(),
        });

        if (field_is_comptime) any_comptime_fields = true;
        if (field_default_value != .none) any_default_inits = true;
        switch (try field_alignment_val.order_against_zero_advanced(mod, sema)) {
            .eq => {},
            .gt => any_aligned_fields = true,
            .lt => unreachable,
        }
    }

    const wip_ty = switch (try ip.get_struct_type(gpa, .{
        .layout = layout,
        .fields_len = fields_len,
        .known_non_opv = false,
        .requires_comptime = .unknown,
        .is_tuple = is_tuple,
        .any_comptime_fields = any_comptime_fields,
        .any_default_inits = any_default_inits,
        .any_aligned_fields = any_aligned_fields,
        .inits_resolved = true,
        .has_namespace = false,
        .key = .{ .reified = .{
            .zir_index = try ip.track_zir(gpa, block.get_file_scope(mod), inst),
            .type_hash = hasher.final(),
        } },
    })) {
        .wip => |wip| wip,
        .existing => |ty| return Air.interned_to_ref(ty),
    };
    errdefer wip_ty.cancel(ip);

    if (is_tuple) switch (layout) {
        .@"extern" => return sema.fail(block, src, "extern tuples are not supported", .{}),
        .@"packed" => return sema.fail(block, src, "packed tuples are not supported", .{}),
        .auto => {},
    };

    const new_decl_index = try sema.create_anonymous_decl_type_named(
        block,
        src,
        Value.from_interned(wip_ty.index),
        name_strategy,
        "struct",
        inst,
    );
    mod.decl_ptr(new_decl_index).owns_tv = true;
    errdefer mod.abort_anon_decl(new_decl_index);

    const struct_type = ip.load_struct_type(wip_ty.index);

    for (0..fields_len) |field_idx| {
        const field_info = try fields_val.elem_value(mod, field_idx);

        const field_name_val = try field_info.field_value(mod, 0);
        const field_type_val = try field_info.field_value(mod, 1);
        const field_default_value_val = try field_info.field_value(mod, 2);
        const field_is_comptime_val = try field_info.field_value(mod, 3);
        const field_alignment_val = try field_info.field_value(mod, 4);

        const field_ty = field_type_val.to_type();
        // Don't pass a reason; first loop acts as an assertion that this is valid.
        const field_name = try sema.slice_to_ip_string(block, src, field_name_val, undefined);
        if (is_tuple) {
            const field_name_index = field_name.to_unsigned(ip) orelse return sema.fail(
                block,
                src,
                "tuple cannot have non-numeric field '{}'",
                .{field_name.fmt(ip)},
            );
            if (field_name_index != field_idx) {
                return sema.fail(
                    block,
                    src,
                    "tuple field name '{}' does not match field index {}",
                    .{ field_name_index, field_idx },
                );
            }
        } else if (struct_type.add_field_name(ip, field_name)) |prev_index| {
            _ = prev_index; // TODO: better source location
            return sema.fail(block, src, "duplicate struct field name {}", .{field_name.fmt(ip)});
        }

        if (any_aligned_fields) {
            if (!try sema.int_fits_in_type(field_alignment_val, Type.u32, null)) {
                return sema.fail(block, src, "alignment must fit in 'u32'", .{});
            }

            const byte_align = try field_alignment_val.to_unsigned_int_advanced(sema);
            if (byte_align == 0) {
                if (layout != .@"packed") {
                    struct_type.field_aligns.get(ip)[field_idx] = .none;
                }
            } else {
                if (layout == .@"packed") return sema.fail(block, src, "alignment in a packed struct field must be set to 0", .{});
                if (!math.is_power_of_two(byte_align)) return sema.fail(block, src, "alignment value '{d}' is not a power of two or zero", .{byte_align});
                struct_type.field_aligns.get(ip)[field_idx] = Alignment.from_nonzero_byte_units(byte_align);
            }
        }

        const field_is_comptime = field_is_comptime_val.to_bool();
        if (field_is_comptime) {
            assert(any_comptime_fields);
            switch (layout) {
                .@"extern" => return sema.fail(block, src, "extern struct fields cannot be marked comptime", .{}),
                .@"packed" => return sema.fail(block, src, "packed struct fields cannot be marked comptime", .{}),
                .auto => struct_type.set_field_comptime(ip, field_idx),
            }
        }

        const field_default: InternPool.Index = d: {
            if (!any_default_inits) break :d .none;
            const ptr_val = field_default_value_val.optional_value(mod) orelse break :d .none;
            const ptr_ty = try mod.single_const_ptr_type(field_ty);
            // Asserted comptime-dereferencable above.
            const val = (try sema.pointer_deref(block, src, ptr_val, ptr_ty)).?;
            // We already resolved this for deduplication, so we may as well do it now.
            break :d (try sema.resolve_lazy_value(val)).to_intern();
        };

        if (field_is_comptime and field_default == .none) {
            return sema.fail(block, src, "comptime field without default initialization value", .{});
        }

        struct_type.field_types.get(ip)[field_idx] = field_type_val.to_intern();
        if (field_default != .none) {
            struct_type.field_inits.get(ip)[field_idx] = field_default;
        }

        if (field_ty.zig_type_tag(mod) == .Opaque) {
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "opaque types have unknown size and therefore cannot be directly embedded in structs", .{});
                errdefer msg.destroy(gpa);

                try sema.add_declared_here_note(msg, field_ty);
                break :msg msg;
            });
        }
        if (field_ty.zig_type_tag(mod) == .NoReturn) {
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "struct fields cannot be 'noreturn'", .{});
                errdefer msg.destroy(gpa);

                try sema.add_declared_here_note(msg, field_ty);
                break :msg msg;
            });
        }
        if (layout == .@"extern" and !try sema.validate_extern_type(field_ty, .struct_field)) {
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "extern structs cannot contain fields of type '{}'", .{field_ty.fmt(sema.mod)});
                errdefer msg.destroy(gpa);

                const src_decl = sema.mod.decl_ptr(block.src_decl);
                try sema.explain_why_type_is_not_extern(msg, src_decl.to_src_loc(src, mod), field_ty, .struct_field);

                try sema.add_declared_here_note(msg, field_ty);
                break :msg msg;
            });
        } else if (layout == .@"packed" and !try sema.validate_packed_type(field_ty)) {
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "packed structs cannot contain fields of type '{}'", .{field_ty.fmt(sema.mod)});
                errdefer msg.destroy(gpa);

                const src_decl = sema.mod.decl_ptr(block.src_decl);
                try sema.explain_why_type_is_not_packed(msg, src_decl.to_src_loc(src, mod), field_ty);

                try sema.add_declared_here_note(msg, field_ty);
                break :msg msg;
            });
        }
    }

    if (layout == .@"packed") {
        var fields_bit_sum: u64 = 0;
        for (0..struct_type.field_types.len) |field_idx| {
            const field_ty = Type.from_interned(struct_type.field_types.get(ip)[field_idx]);
            sema.resolve_type_layout(field_ty) catch |err| switch (err) {
                error.AnalysisFail => {
                    const msg = sema.err orelse return err;
                    try sema.err_note(block, src, msg, "while checking a field of this struct", .{});
                    return err;
                },
                else => return err,
            };
            fields_bit_sum += field_ty.bit_size(mod);
        }

        if (opt_backing_int_val.optional_value(mod)) |backing_int_val| {
            const backing_int_ty = backing_int_val.to_type();
            try sema.check_backing_int_type(block, src, backing_int_ty, fields_bit_sum);
            struct_type.backing_int_type(ip).* = backing_int_ty.to_intern();
        } else {
            const backing_int_ty = try mod.int_type(.unsigned, @int_cast(fields_bit_sum));
            struct_type.backing_int_type(ip).* = backing_int_ty.to_intern();
        }
    }

    try mod.finalize_anon_decl(new_decl_index);
    return Air.interned_to_ref(wip_ty.finish(ip, new_decl_index, .none));
}

fn resolve_va_list_ref(sema: *Sema, block: *Block, src: LazySrcLoc, zir_ref: Zir.Inst.Ref) CompileError!Air.Inst.Ref {
    const va_list_ty = try sema.get_builtin_type("VaList");
    const va_list_ptr = try sema.mod.single_mut_ptr_type(va_list_ty);

    const inst = try sema.resolve_inst(zir_ref);
    return sema.coerce(block, va_list_ptr, inst, src);
}

fn zir_cva_arg(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const extra = sema.code.extra_data(Zir.Inst.BinNode, extended.operand).data;
    const src = LazySrcLoc.nodeOffset(extra.node);
    const va_list_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const ty_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = extra.node };

    const va_list_ref = try sema.resolve_va_list_ref(block, va_list_src, extra.lhs);
    const arg_ty = try sema.resolve_type(block, ty_src, extra.rhs);

    if (!try sema.validate_extern_type(arg_ty, .param_ty)) {
        const msg = msg: {
            const msg = try sema.err_msg(block, ty_src, "cannot get '{}' from variadic argument", .{arg_ty.fmt(sema.mod)});
            errdefer msg.destroy(sema.gpa);

            const src_decl = sema.mod.decl_ptr(block.src_decl);
            try sema.explain_why_type_is_not_extern(msg, src_decl.to_src_loc(ty_src, mod), arg_ty, .param_ty);

            try sema.add_declared_here_note(msg, arg_ty);
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    try sema.require_runtime_block(block, src, null);
    return block.add_ty_op(.c_va_arg, arg_ty, va_list_ref);
}

fn zir_cva_copy(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!Air.Inst.Ref {
    const extra = sema.code.extra_data(Zir.Inst.UnNode, extended.operand).data;
    const src = LazySrcLoc.nodeOffset(extra.node);
    const va_list_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };

    const va_list_ref = try sema.resolve_va_list_ref(block, va_list_src, extra.operand);
    const va_list_ty = try sema.get_builtin_type("VaList");

    try sema.require_runtime_block(block, src, null);
    return block.add_ty_op(.c_va_copy, va_list_ty, va_list_ref);
}

fn zir_cva_end(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!Air.Inst.Ref {
    const extra = sema.code.extra_data(Zir.Inst.UnNode, extended.operand).data;
    const src = LazySrcLoc.nodeOffset(extra.node);
    const va_list_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };

    const va_list_ref = try sema.resolve_va_list_ref(block, va_list_src, extra.operand);

    try sema.require_runtime_block(block, src, null);
    return block.add_un_op(.c_va_end, va_list_ref);
}

fn zir_cva_start(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!Air.Inst.Ref {
    const src = LazySrcLoc.nodeOffset(@bit_cast(extended.operand));

    const va_list_ty = try sema.get_builtin_type("VaList");
    try sema.require_runtime_block(block, src, null);
    return block.add_inst(.{
        .tag = .c_va_start,
        .data = .{ .ty = va_list_ty },
    });
}

fn zir_type_name(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;

    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const ty_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const ty = try sema.resolve_type(block, ty_src, inst_data.operand);

    const type_name = try ip.get_or_put_string_fmt(sema.gpa, "{}", .{ty.fmt(mod)}, .no_embedded_nulls);
    return sema.add_null_terminated_str_lit(type_name);
}

fn zir_frame_type(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    return sema.fail_with_use_of_async(block, src);
}

fn zir_frame_size(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    return sema.fail_with_use_of_async(block, src);
}

fn zir_int_from_float(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const dest_ty = try sema.resolve_dest_type(block, src, extra.lhs, .remove_eu_opt, "@int_from_float");
    const operand = try sema.resolve_inst(extra.rhs);
    const operand_ty = sema.type_of(operand);

    try sema.check_vectorizable_binary_operands(block, operand_src, dest_ty, operand_ty, src, operand_src);
    const is_vector = dest_ty.zig_type_tag(mod) == .Vector;

    const dest_scalar_ty = dest_ty.scalar_type(mod);
    const operand_scalar_ty = operand_ty.scalar_type(mod);

    _ = try sema.check_int_type(block, src, dest_scalar_ty);
    try sema.check_float_type(block, operand_src, operand_scalar_ty);

    if (try sema.resolve_value(operand)) |operand_val| {
        const result_val = try sema.int_from_float(block, operand_src, operand_val, operand_ty, dest_ty, .truncate);
        return Air.interned_to_ref(result_val.to_intern());
    } else if (dest_scalar_ty.zig_type_tag(mod) == .ComptimeInt) {
        return sema.fail_with_needed_comptime(block, operand_src, .{
            .needed_comptime_reason = "value being casted to 'comptime_int' must be comptime-known",
        });
    }

    try sema.require_runtime_block(block, inst_data.src(), operand_src);
    if (dest_scalar_ty.int_info(mod).bits == 0) {
        if (!is_vector) {
            if (block.want_safety()) {
                const ok = try block.add_bin_op(if (block.float_mode == .optimized) .cmp_eq_optimized else .cmp_eq, operand, Air.interned_to_ref((try mod.float_value(operand_ty, 0.0)).to_intern()));
                try sema.add_safety_check(block, src, ok, .integer_part_out_of_bounds);
            }
            return Air.interned_to_ref((try mod.int_value(dest_ty, 0)).to_intern());
        }
        if (block.want_safety()) {
            const len = dest_ty.vector_len(mod);
            for (0..len) |i| {
                const idx_ref = try mod.int_ref(Type.usize, i);
                const elem_ref = try block.add_bin_op(.array_elem_val, operand, idx_ref);
                const ok = try block.add_bin_op(if (block.float_mode == .optimized) .cmp_eq_optimized else .cmp_eq, elem_ref, Air.interned_to_ref((try mod.float_value(operand_scalar_ty, 0.0)).to_intern()));
                try sema.add_safety_check(block, src, ok, .integer_part_out_of_bounds);
            }
        }
        return Air.interned_to_ref(try mod.intern(.{ .aggregate = .{
            .ty = dest_ty.to_intern(),
            .storage = .{ .repeated_elem = (try mod.int_value(dest_scalar_ty, 0)).to_intern() },
        } }));
    }
    if (!is_vector) {
        const result = try block.add_ty_op(if (block.float_mode == .optimized) .int_from_float_optimized else .int_from_float, dest_ty, operand);
        if (block.want_safety()) {
            const back = try block.add_ty_op(.float_from_int, operand_ty, result);
            const diff = try block.add_bin_op(.sub, operand, back);
            const ok_pos = try block.add_bin_op(if (block.float_mode == .optimized) .cmp_lt_optimized else .cmp_lt, diff, Air.interned_to_ref((try mod.float_value(operand_ty, 1.0)).to_intern()));
            const ok_neg = try block.add_bin_op(if (block.float_mode == .optimized) .cmp_gt_optimized else .cmp_gt, diff, Air.interned_to_ref((try mod.float_value(operand_ty, -1.0)).to_intern()));
            const ok = try block.add_bin_op(.bool_and, ok_pos, ok_neg);
            try sema.add_safety_check(block, src, ok, .integer_part_out_of_bounds);
        }
        return result;
    }
    const len = dest_ty.vector_len(mod);
    const new_elems = try sema.arena.alloc(Air.Inst.Ref, len);
    for (new_elems, 0..) |*new_elem, i| {
        const idx_ref = try mod.int_ref(Type.usize, i);
        const old_elem = try block.add_bin_op(.array_elem_val, operand, idx_ref);
        const result = try block.add_ty_op(if (block.float_mode == .optimized) .int_from_float_optimized else .int_from_float, dest_scalar_ty, old_elem);
        if (block.want_safety()) {
            const back = try block.add_ty_op(.float_from_int, operand_scalar_ty, result);
            const diff = try block.add_bin_op(.sub, old_elem, back);
            const ok_pos = try block.add_bin_op(if (block.float_mode == .optimized) .cmp_lt_optimized else .cmp_lt, diff, Air.interned_to_ref((try mod.float_value(operand_scalar_ty, 1.0)).to_intern()));
            const ok_neg = try block.add_bin_op(if (block.float_mode == .optimized) .cmp_gt_optimized else .cmp_gt, diff, Air.interned_to_ref((try mod.float_value(operand_scalar_ty, -1.0)).to_intern()));
            const ok = try block.add_bin_op(.bool_and, ok_pos, ok_neg);
            try sema.add_safety_check(block, src, ok, .integer_part_out_of_bounds);
        }
        new_elem.* = result;
    }
    return block.add_aggregate_init(dest_ty, new_elems);
}

fn zir_float_from_int(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const dest_ty = try sema.resolve_dest_type(block, src, extra.lhs, .remove_eu_opt, "@float_from_int");
    const operand = try sema.resolve_inst(extra.rhs);
    const operand_ty = sema.type_of(operand);

    try sema.check_vectorizable_binary_operands(block, operand_src, dest_ty, operand_ty, src, operand_src);
    const is_vector = dest_ty.zig_type_tag(mod) == .Vector;

    const dest_scalar_ty = dest_ty.scalar_type(mod);
    const operand_scalar_ty = operand_ty.scalar_type(mod);

    try sema.check_float_type(block, src, dest_scalar_ty);
    _ = try sema.check_int_type(block, operand_src, operand_scalar_ty);

    if (try sema.resolve_value(operand)) |operand_val| {
        const result_val = try operand_val.float_from_int_advanced(sema.arena, operand_ty, dest_ty, mod, sema);
        return Air.interned_to_ref(result_val.to_intern());
    } else if (dest_scalar_ty.zig_type_tag(mod) == .ComptimeFloat) {
        return sema.fail_with_needed_comptime(block, operand_src, .{
            .needed_comptime_reason = "value being casted to 'comptime_float' must be comptime-known",
        });
    }

    try sema.require_runtime_block(block, src, operand_src);
    if (!is_vector) {
        return block.add_ty_op(.float_from_int, dest_ty, operand);
    }
    const len = operand_ty.vector_len(mod);
    const new_elems = try sema.arena.alloc(Air.Inst.Ref, len);
    for (new_elems, 0..) |*new_elem, i| {
        const idx_ref = try mod.int_ref(Type.usize, i);
        const old_elem = try block.add_bin_op(.array_elem_val, operand, idx_ref);
        new_elem.* = try block.add_ty_op(.float_from_int, dest_scalar_ty, old_elem);
    }
    return block.add_aggregate_init(dest_ty, new_elems);
}

fn zir_ptr_from_int(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();

    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;

    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const operand_res = try sema.resolve_inst(extra.rhs);

    const uncoerced_operand_ty = sema.type_of(operand_res);
    const dest_ty = try sema.resolve_dest_type(block, src, extra.lhs, .remove_eu, "@ptrFromInt");
    try sema.check_vectorizable_binary_operands(block, operand_src, dest_ty, uncoerced_operand_ty, src, operand_src);

    const is_vector = dest_ty.zig_type_tag(mod) == .Vector;
    const operand_ty = if (is_vector) operand_ty: {
        const len = dest_ty.vector_len(mod);
        break :operand_ty try mod.vector_type(.{ .child = .usize_type, .len = len });
    } else Type.usize;

    const operand_coerced = try sema.coerce(block, operand_ty, operand_res, operand_src);

    const ptr_ty = dest_ty.scalar_type(mod);
    try sema.check_ptr_type(block, src, ptr_ty, true);

    const elem_ty = ptr_ty.elem_type2(mod);
    const ptr_align = try ptr_ty.ptr_alignment_advanced(mod, sema);

    if (ptr_ty.is_slice(mod)) {
        const msg = msg: {
            const msg = try sema.err_msg(block, src, "integer cannot be converted to slice type '{}'", .{ptr_ty.fmt(sema.mod)});
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, src, msg, "slice length cannot be inferred from address", .{});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    if (try sema.resolve_defined_value(block, operand_src, operand_coerced)) |val| {
        if (!is_vector) {
            const ptr_val = try sema.ptr_from_int_val(block, operand_src, val, ptr_ty, ptr_align);
            return Air.interned_to_ref(ptr_val.to_intern());
        }
        const len = dest_ty.vector_len(mod);
        const new_elems = try sema.arena.alloc(InternPool.Index, len);
        for (new_elems, 0..) |*new_elem, i| {
            const elem = try val.elem_value(mod, i);
            const ptr_val = try sema.ptr_from_int_val(block, operand_src, elem, ptr_ty, ptr_align);
            new_elem.* = ptr_val.to_intern();
        }
        return Air.interned_to_ref(try mod.intern(.{ .aggregate = .{
            .ty = dest_ty.to_intern(),
            .storage = .{ .elems = new_elems },
        } }));
    }
    if (try sema.type_requires_comptime(ptr_ty)) {
        return sema.fail_with_owned_error_msg(block, msg: {
            const msg = try sema.err_msg(block, src, "pointer to comptime-only type '{}' must be comptime-known, but operand is runtime-known", .{ptr_ty.fmt(mod)});
            errdefer msg.destroy(sema.gpa);

            const src_decl = mod.decl_ptr(block.src_decl);
            try sema.explain_why_type_is_comptime(msg, src_decl.to_src_loc(src, mod), ptr_ty);
            break :msg msg;
        });
    }
    try sema.require_runtime_block(block, src, operand_src);
    if (!is_vector) {
        if (block.want_safety() and (try sema.type_has_runtime_bits(elem_ty) or elem_ty.zig_type_tag(mod) == .Fn)) {
            if (!ptr_ty.is_allowzero_ptr(mod)) {
                const is_non_zero = try block.add_bin_op(.cmp_neq, operand_coerced, .zero_usize);
                try sema.add_safety_check(block, src, is_non_zero, .cast_to_null);
            }
            if (ptr_align.compare(.gt, .@"1")) {
                const align_bytes_minus_1 = ptr_align.to_byte_units().? - 1;
                const align_minus_1 = Air.interned_to_ref((try mod.int_value(Type.usize, align_bytes_minus_1)).to_intern());
                const remainder = try block.add_bin_op(.bit_and, operand_coerced, align_minus_1);
                const is_aligned = try block.add_bin_op(.cmp_eq, remainder, .zero_usize);
                try sema.add_safety_check(block, src, is_aligned, .incorrect_alignment);
            }
        }
        return block.add_bit_cast(dest_ty, operand_coerced);
    }

    const len = dest_ty.vector_len(mod);
    if (block.want_safety() and (try sema.type_has_runtime_bits(elem_ty) or elem_ty.zig_type_tag(mod) == .Fn)) {
        for (0..len) |i| {
            const idx_ref = try mod.int_ref(Type.usize, i);
            const elem_coerced = try block.add_bin_op(.array_elem_val, operand_coerced, idx_ref);
            if (!ptr_ty.is_allowzero_ptr(mod)) {
                const is_non_zero = try block.add_bin_op(.cmp_neq, elem_coerced, .zero_usize);
                try sema.add_safety_check(block, src, is_non_zero, .cast_to_null);
            }
            if (ptr_align.compare(.gt, .@"1")) {
                const align_bytes_minus_1 = ptr_align.to_byte_units().? - 1;
                const align_minus_1 = Air.interned_to_ref((try mod.int_value(Type.usize, align_bytes_minus_1)).to_intern());
                const remainder = try block.add_bin_op(.bit_and, elem_coerced, align_minus_1);
                const is_aligned = try block.add_bin_op(.cmp_eq, remainder, .zero_usize);
                try sema.add_safety_check(block, src, is_aligned, .incorrect_alignment);
            }
        }
    }

    const new_elems = try sema.arena.alloc(Air.Inst.Ref, len);
    for (new_elems, 0..) |*new_elem, i| {
        const idx_ref = try mod.int_ref(Type.usize, i);
        const old_elem = try block.add_bin_op(.array_elem_val, operand_coerced, idx_ref);
        new_elem.* = try block.add_bit_cast(ptr_ty, old_elem);
    }
    return block.add_aggregate_init(dest_ty, new_elems);
}

fn ptr_from_int_val(
    sema: *Sema,
    block: *Block,
    operand_src: LazySrcLoc,
    operand_val: Value,
    ptr_ty: Type,
    ptr_align: Alignment,
) !Value {
    const zcu = sema.mod;
    if (operand_val.is_undef(zcu)) {
        if (ptr_ty.is_allowzero_ptr(zcu) and ptr_align == .@"1") {
            return zcu.undef_value(ptr_ty);
        }
        return sema.fail_with_use_of_undef(block, operand_src);
    }
    const addr = try operand_val.to_unsigned_int_advanced(sema);
    if (!ptr_ty.is_allowzero_ptr(zcu) and addr == 0)
        return sema.fail(block, operand_src, "pointer type '{}' does not allow address zero", .{ptr_ty.fmt(zcu)});
    if (addr != 0 and ptr_align != .none and !ptr_align.check(addr))
        return sema.fail(block, operand_src, "pointer type '{}' requires aligned address", .{ptr_ty.fmt(zcu)});

    return switch (ptr_ty.zig_type_tag(zcu)) {
        .Optional => Value.from_interned((try zcu.intern(.{ .opt = .{
            .ty = ptr_ty.to_intern(),
            .val = if (addr == 0) .none else (try zcu.ptr_int_value(ptr_ty.child_type(zcu), addr)).to_intern(),
        } }))),
        .Pointer => try zcu.ptr_int_value(ptr_ty, addr),
        else => unreachable,
    };
}

fn zir_error_cast(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const extra = sema.code.extra_data(Zir.Inst.BinNode, extended.operand).data;
    const src = LazySrcLoc.nodeOffset(extra.node);
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const base_dest_ty = try sema.resolve_dest_type(block, src, extra.lhs, .remove_opt, "@errorCast");
    const operand = try sema.resolve_inst(extra.rhs);
    const base_operand_ty = sema.type_of(operand);
    const dest_tag = base_dest_ty.zig_type_tag(mod);
    const operand_tag = base_operand_ty.zig_type_tag(mod);

    if (dest_tag != .ErrorSet and dest_tag != .ErrorUnion) {
        return sema.fail(block, src, "expected error set or error union type, found '{s}'", .{@tag_name(dest_tag)});
    }
    if (operand_tag != .ErrorSet and operand_tag != .ErrorUnion) {
        return sema.fail(block, src, "expected error set or error union type, found '{s}'", .{@tag_name(operand_tag)});
    }
    if (dest_tag == .ErrorSet and operand_tag == .ErrorUnion) {
        return sema.fail(block, src, "cannot cast an error union type to error set", .{});
    }
    if (dest_tag == .ErrorUnion and operand_tag == .ErrorUnion and
        base_dest_ty.error_union_payload(mod).to_intern() != base_operand_ty.error_union_payload(mod).to_intern())
    {
        return sema.fail_with_owned_error_msg(block, msg: {
            const msg = try sema.err_msg(block, src, "payload types of error unions must match", .{});
            errdefer msg.destroy(sema.gpa);
            const dest_ty = base_dest_ty.error_union_payload(mod);
            const operand_ty = base_operand_ty.error_union_payload(mod);
            try sema.err_note(block, src, msg, "destination payload is '{}'", .{dest_ty.fmt(mod)});
            try sema.err_note(block, src, msg, "operand payload is '{}'", .{operand_ty.fmt(mod)});
            try add_declared_here_note(sema, msg, dest_ty);
            try add_declared_here_note(sema, msg, operand_ty);
            break :msg msg;
        });
    }
    const dest_ty = if (dest_tag == .ErrorUnion) base_dest_ty.error_union_set(mod) else base_dest_ty;
    const operand_ty = if (operand_tag == .ErrorUnion) base_operand_ty.error_union_set(mod) else base_operand_ty;

    // operand must be defined since it can be an invalid error value
    const maybe_operand_val = try sema.resolve_defined_value(block, operand_src, operand);

    const disjoint = disjoint: {
        // Try avoiding resolving inferred error sets if we can
        if (!dest_ty.is_any_error(mod) and dest_ty.error_set_is_empty(mod)) break :disjoint true;
        if (!operand_ty.is_any_error(mod) and operand_ty.error_set_is_empty(mod)) break :disjoint true;
        if (dest_ty.is_any_error(mod)) break :disjoint false;
        if (operand_ty.is_any_error(mod)) break :disjoint false;
        const dest_err_names = dest_ty.error_set_names(mod);
        for (0..dest_err_names.len) |dest_err_index| {
            if (Type.error_set_has_field_ip(ip, operand_ty.to_intern(), dest_err_names.get(ip)[dest_err_index]))
                break :disjoint false;
        }

        if (!ip.is_inferred_error_set_type(dest_ty.to_intern()) and
            !ip.is_inferred_error_set_type(operand_ty.to_intern()))
        {
            break :disjoint true;
        }

        _ = try sema.resolve_inferred_error_set_ty(block, src, dest_ty.to_intern());
        _ = try sema.resolve_inferred_error_set_ty(block, operand_src, operand_ty.to_intern());
        for (0..dest_err_names.len) |dest_err_index| {
            if (Type.error_set_has_field_ip(ip, operand_ty.to_intern(), dest_err_names.get(ip)[dest_err_index]))
                break :disjoint false;
        }

        break :disjoint true;
    };
    if (disjoint and dest_tag != .ErrorUnion) {
        return sema.fail(block, src, "error sets '{}' and '{}' have no common errors", .{
            operand_ty.fmt(sema.mod), dest_ty.fmt(sema.mod),
        });
    }

    if (maybe_operand_val) |val| {
        if (!dest_ty.is_any_error(mod)) check: {
            const operand_val = mod.intern_pool.index_to_key(val.to_intern());
            var error_name: InternPool.NullTerminatedString = undefined;
            if (operand_tag == .ErrorUnion) {
                if (operand_val.error_union.val != .err_name) break :check;
                error_name = operand_val.error_union.val.err_name;
            } else {
                error_name = operand_val.err.name;
            }
            if (!Type.error_set_has_field_ip(ip, dest_ty.to_intern(), error_name)) {
                return sema.fail(block, src, "'error.{}' not a member of error set '{}'", .{
                    error_name.fmt(ip), dest_ty.fmt(sema.mod),
                });
            }
        }

        return Air.interned_to_ref((try mod.get_coerced(val, base_dest_ty)).to_intern());
    }

    try sema.require_runtime_block(block, src, operand_src);
    const err_int_ty = try mod.error_int_type();
    if (block.want_safety() and !dest_ty.is_any_error(mod) and
        dest_ty.to_intern() != .adhoc_inferred_error_set_type and
        sema.mod.backend_supports_feature(.error_set_has_value))
    {
        if (dest_tag == .ErrorUnion) {
            const err_code = try sema.analyze_err_union_code(block, operand_src, operand);
            const err_int = try block.add_bit_cast(err_int_ty, err_code);
            const zero_err = try mod.int_ref(try mod.error_int_type(), 0);

            const is_zero = try block.add_bin_op(.cmp_eq, err_int, zero_err);
            if (disjoint) {
                // Error must be zero.
                try sema.add_safety_check(block, src, is_zero, .invalid_error_code);
            } else {
                // Error must be in destination set or zero.
                const has_value = try block.add_ty_op(.error_set_has_value, dest_ty, err_code);
                const ok = try block.add_bin_op(.bool_or, has_value, is_zero);
                try sema.add_safety_check(block, src, ok, .invalid_error_code);
            }
        } else {
            const err_int_inst = try block.add_bit_cast(err_int_ty, operand);
            const ok = try block.add_ty_op(.error_set_has_value, dest_ty, err_int_inst);
            try sema.add_safety_check(block, src, ok, .invalid_error_code);
        }
    }
    return block.add_bit_cast(base_dest_ty, operand);
}

fn zir_ptr_cast_full(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!Air.Inst.Ref {
    const FlagsInt = @typeInfo(Zir.Inst.FullPtrCastFlags).Struct.backing_integer.?;
    const flags: Zir.Inst.FullPtrCastFlags = @bit_cast(@as(FlagsInt, @truncate(extended.small)));
    const extra = sema.code.extra_data(Zir.Inst.BinNode, extended.operand).data;
    const src = LazySrcLoc.nodeOffset(extra.node);
    const operand_src: LazySrcLoc = .{ .node_offset_ptrcast_operand = extra.node };
    const operand = try sema.resolve_inst(extra.rhs);
    const dest_ty = try sema.resolve_dest_type(block, src, extra.lhs, .remove_eu, flags.need_result_type_builtin_name());
    return sema.ptr_cast_full(
        block,
        flags,
        src,
        operand,
        operand_src,
        dest_ty,
        flags.need_result_type_builtin_name(),
    );
}

fn zir_ptr_cast(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const dest_ty = try sema.resolve_dest_type(block, src, extra.lhs, .remove_eu, "@ptr_cast");
    const operand = try sema.resolve_inst(extra.rhs);

    return sema.ptr_cast_full(
        block,
        .{ .ptr_cast = true },
        src,
        operand,
        operand_src,
        dest_ty,
        "@ptr_cast",
    );
}

fn ptr_cast_full(
    sema: *Sema,
    block: *Block,
    flags: Zir.Inst.FullPtrCastFlags,
    src: LazySrcLoc,
    operand: Air.Inst.Ref,
    operand_src: LazySrcLoc,
    dest_ty: Type,
    operation: []const u8,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const operand_ty = sema.type_of(operand);

    try sema.check_ptr_type(block, src, dest_ty, true);
    try sema.check_ptr_operand(block, operand_src, operand_ty);

    const src_info = operand_ty.ptr_info(mod);
    const dest_info = dest_ty.ptr_info(mod);

    try sema.resolve_type_layout(Type.from_interned(src_info.child));
    try sema.resolve_type_layout(Type.from_interned(dest_info.child));

    const src_slice_like = src_info.flags.size == .Slice or
        (src_info.flags.size == .One and Type.from_interned(src_info.child).zig_type_tag(mod) == .Array);

    const dest_slice_like = dest_info.flags.size == .Slice or
        (dest_info.flags.size == .One and Type.from_interned(dest_info.child).zig_type_tag(mod) == .Array);

    if (dest_info.flags.size == .Slice and !src_slice_like) {
        return sema.fail(block, src, "illegal pointer cast to slice", .{});
    }

    if (dest_info.flags.size == .Slice) {
        const src_elem_size = switch (src_info.flags.size) {
            .Slice => Type.from_interned(src_info.child).abi_size(mod),
            // pointer to array
            .One => Type.from_interned(src_info.child).child_type(mod).abi_size(mod),
            else => unreachable,
        };
        const dest_elem_size = Type.from_interned(dest_info.child).abi_size(mod);
        if (src_elem_size != dest_elem_size) {
            return sema.fail(block, src, "TODO: implement {s} between slices changing the length", .{operation});
        }
    }

    // The checking logic in this function must stay in sync with Sema.coerce_in_memory_allowed_ptrs

    if (!flags.ptr_cast) {
        check_size: {
            if (src_info.flags.size == dest_info.flags.size) break :check_size;
            if (src_slice_like and dest_slice_like) break :check_size;
            if (src_info.flags.size == .C) break :check_size;
            if (dest_info.flags.size == .C) break :check_size;
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "cannot implicitly convert {s} pointer to {s} pointer", .{
                    pointer_size_string(src_info.flags.size),
                    pointer_size_string(dest_info.flags.size),
                });
                errdefer msg.destroy(sema.gpa);
                if (dest_info.flags.size == .Many and
                    (src_info.flags.size == .Slice or
                    (src_info.flags.size == .One and Type.from_interned(src_info.child).zig_type_tag(mod) == .Array)))
                {
                    try sema.err_note(block, src, msg, "use 'ptr' field to convert slice to many pointer", .{});
                } else {
                    try sema.err_note(block, src, msg, "use @ptr_cast to change pointer size", .{});
                }
                break :msg msg;
            });
        }

        check_child: {
            const src_child = if (dest_info.flags.size == .Slice and src_info.flags.size == .One) blk: {
                // *[n]T -> []T
                break :blk Type.from_interned(src_info.child).child_type(mod);
            } else Type.from_interned(src_info.child);

            const dest_child = Type.from_interned(dest_info.child);

            const imc_res = try sema.coerce_in_memory_allowed(
                block,
                dest_child,
                src_child,
                !dest_info.flags.is_const,
                mod.get_target(),
                src,
                operand_src,
            );
            if (imc_res == .ok) break :check_child;
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "pointer element type '{}' cannot coerce into element type '{}'", .{
                    src_child.fmt(mod),
                    dest_child.fmt(mod),
                });
                errdefer msg.destroy(sema.gpa);
                try imc_res.report(sema, block, src, msg);
                try sema.err_note(block, src, msg, "use @ptr_cast to cast pointer element type", .{});
                break :msg msg;
            });
        }

        check_sent: {
            if (dest_info.sentinel == .none) break :check_sent;
            if (src_info.flags.size == .C) break :check_sent;
            if (src_info.sentinel != .none) {
                const coerced_sent = try mod.intern_pool.get_coerced(sema.gpa, src_info.sentinel, dest_info.child);
                if (dest_info.sentinel == coerced_sent) break :check_sent;
            }
            if (src_slice_like and src_info.flags.size == .One and dest_info.flags.size == .Slice) {
                // [*]nT -> []T
                const arr_ty = Type.from_interned(src_info.child);
                if (arr_ty.sentinel(mod)) |src_sentinel| {
                    const coerced_sent = try mod.intern_pool.get_coerced(sema.gpa, src_sentinel.to_intern(), dest_info.child);
                    if (dest_info.sentinel == coerced_sent) break :check_sent;
                }
            }
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = if (src_info.sentinel == .none) blk: {
                    break :blk try sema.err_msg(block, src, "destination pointer requires '{}' sentinel", .{
                        Value.from_interned(dest_info.sentinel).fmt_value(mod, sema),
                    });
                } else blk: {
                    break :blk try sema.err_msg(block, src, "pointer sentinel '{}' cannot coerce into pointer sentinel '{}'", .{
                        Value.from_interned(src_info.sentinel).fmt_value(mod, sema),
                        Value.from_interned(dest_info.sentinel).fmt_value(mod, sema),
                    });
                };
                errdefer msg.destroy(sema.gpa);
                try sema.err_note(block, src, msg, "use @ptr_cast to cast pointer sentinel", .{});
                break :msg msg;
            });
        }

        if (src_info.packed_offset.host_size != dest_info.packed_offset.host_size) {
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "pointer host size '{}' cannot coerce into pointer host size '{}'", .{
                    src_info.packed_offset.host_size,
                    dest_info.packed_offset.host_size,
                });
                errdefer msg.destroy(sema.gpa);
                try sema.err_note(block, src, msg, "use @ptr_cast to cast pointer host size", .{});
                break :msg msg;
            });
        }

        if (src_info.packed_offset.bit_offset != dest_info.packed_offset.bit_offset) {
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "pointer bit offset '{}' cannot coerce into pointer bit offset '{}'", .{
                    src_info.packed_offset.bit_offset,
                    dest_info.packed_offset.bit_offset,
                });
                errdefer msg.destroy(sema.gpa);
                try sema.err_note(block, src, msg, "use @ptr_cast to cast pointer bit offset", .{});
                break :msg msg;
            });
        }

        check_allowzero: {
            const src_allows_zero = operand_ty.ptr_allows_zero(mod);
            const dest_allows_zero = dest_ty.ptr_allows_zero(mod);
            if (!src_allows_zero) break :check_allowzero;
            if (dest_allows_zero) break :check_allowzero;

            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "'{}' could have null values which are illegal in type '{}'", .{
                    operand_ty.fmt(mod),
                    dest_ty.fmt(mod),
                });
                errdefer msg.destroy(sema.gpa);
                try sema.err_note(block, src, msg, "use @ptr_cast to assert the pointer is not null", .{});
                break :msg msg;
            });
        }

        // TODO: vector index?
    }

    const src_align = if (src_info.flags.alignment != .none)
        src_info.flags.alignment
    else
        Type.from_interned(src_info.child).abi_alignment(mod);

    const dest_align = if (dest_info.flags.alignment != .none)
        dest_info.flags.alignment
    else
        Type.from_interned(dest_info.child).abi_alignment(mod);

    if (!flags.align_cast) {
        if (dest_align.compare(.gt, src_align)) {
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "{s} increases pointer alignment", .{operation});
                errdefer msg.destroy(sema.gpa);
                try sema.err_note(block, operand_src, msg, "'{}' has alignment '{d}'", .{
                    operand_ty.fmt(mod), src_align.to_byte_units() orelse 0,
                });
                try sema.err_note(block, src, msg, "'{}' has alignment '{d}'", .{
                    dest_ty.fmt(mod), dest_align.to_byte_units() orelse 0,
                });
                try sema.err_note(block, src, msg, "use @align_cast to assert pointer alignment", .{});
                break :msg msg;
            });
        }
    }

    if (!flags.addrspace_cast) {
        if (src_info.flags.address_space != dest_info.flags.address_space) {
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "{s} changes pointer address space", .{operation});
                errdefer msg.destroy(sema.gpa);
                try sema.err_note(block, operand_src, msg, "'{}' has address space '{s}'", .{
                    operand_ty.fmt(mod), @tag_name(src_info.flags.address_space),
                });
                try sema.err_note(block, src, msg, "'{}' has address space '{s}'", .{
                    dest_ty.fmt(mod), @tag_name(dest_info.flags.address_space),
                });
                try sema.err_note(block, src, msg, "use @addrSpaceCast to cast pointer address space", .{});
                break :msg msg;
            });
        }
    } else {
        // Some address space casts are always disallowed
        if (!target_util.addr_space_cast_is_valid(mod.get_target(), src_info.flags.address_space, dest_info.flags.address_space)) {
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "invalid address space cast", .{});
                errdefer msg.destroy(sema.gpa);
                try sema.err_note(block, operand_src, msg, "address space '{s}' is not compatible with address space '{s}'", .{
                    @tag_name(src_info.flags.address_space),
                    @tag_name(dest_info.flags.address_space),
                });
                break :msg msg;
            });
        }
    }

    if (!flags.const_cast) {
        if (src_info.flags.is_const and !dest_info.flags.is_const) {
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "{s} discards const qualifier", .{operation});
                errdefer msg.destroy(sema.gpa);
                try sema.err_note(block, src, msg, "use @constCast to discard const qualifier", .{});
                break :msg msg;
            });
        }
    }

    if (!flags.volatile_cast) {
        if (src_info.flags.is_volatile and !dest_info.flags.is_volatile) {
            return sema.fail_with_owned_error_msg(block, msg: {
                const msg = try sema.err_msg(block, src, "{s} discards volatile qualifier", .{operation});
                errdefer msg.destroy(sema.gpa);
                try sema.err_note(block, src, msg, "use @volatileCast to discard volatile qualifier", .{});
                break :msg msg;
            });
        }
    }

    const ptr = if (src_info.flags.size == .Slice and dest_info.flags.size != .Slice) ptr: {
        if (operand_ty.zig_type_tag(mod) == .Optional) {
            break :ptr try sema.analyze_optional_slice_ptr(block, operand_src, operand, operand_ty);
        } else {
            break :ptr try sema.analyze_slice_ptr(block, operand_src, operand, operand_ty);
        }
    } else operand;

    const dest_ptr_ty = if (dest_info.flags.size == .Slice and src_info.flags.size != .Slice) blk: {
        // Only convert to a many-pointer at first
        var info = dest_info;
        info.flags.size = .Many;
        const ty = try sema.ptr_type(info);
        if (dest_ty.zig_type_tag(mod) == .Optional) {
            break :blk try mod.optional_type(ty.to_intern());
        } else {
            break :blk ty;
        }
    } else dest_ty;

    // Cannot do @addrSpaceCast at comptime
    if (!flags.addrspace_cast) {
        if (try sema.resolve_value(ptr)) |ptr_val| {
            if (!dest_ty.ptr_allows_zero(mod) and ptr_val.is_undef(mod)) {
                return sema.fail_with_use_of_undef(block, operand_src);
            }
            if (!dest_ty.ptr_allows_zero(mod) and ptr_val.is_null(mod)) {
                return sema.fail(block, operand_src, "null pointer casted to type '{}'", .{dest_ty.fmt(mod)});
            }
            if (dest_align.compare(.gt, src_align)) {
                if (try ptr_val.get_unsigned_int_advanced(mod, null)) |addr| {
                    if (!dest_align.check(addr)) {
                        return sema.fail(block, operand_src, "pointer address 0x{X} is not aligned to {d} bytes", .{
                            addr,
                            dest_align.to_byte_units().?,
                        });
                    }
                }
            }
            if (dest_info.flags.size == .Slice and src_info.flags.size != .Slice) {
                if (ptr_val.is_undef(mod)) return mod.undef_ref(dest_ty);
                const arr_len = try mod.int_value(Type.usize, Type.from_interned(src_info.child).array_len(mod));
                const ptr_val_key = mod.intern_pool.index_to_key(ptr_val.to_intern()).ptr;
                return Air.interned_to_ref((try mod.intern(.{ .slice = .{
                    .ty = dest_ty.to_intern(),
                    .ptr = try mod.intern(.{ .ptr = .{
                        .ty = dest_ty.slice_ptr_field_type(mod).to_intern(),
                        .base_addr = ptr_val_key.base_addr,
                        .byte_offset = ptr_val_key.byte_offset,
                    } }),
                    .len = arr_len.to_intern(),
                } })));
            } else {
                assert(dest_ptr_ty.eql(dest_ty, mod));
                return Air.interned_to_ref((try mod.get_coerced(ptr_val, dest_ty)).to_intern());
            }
        }
    }

    try sema.require_runtime_block(block, src, null);
    try sema.validate_runtime_value(block, operand_src, ptr);

    if (block.want_safety() and operand_ty.ptr_allows_zero(mod) and !dest_ty.ptr_allows_zero(mod) and
        (try sema.type_has_runtime_bits(Type.from_interned(dest_info.child)) or Type.from_interned(dest_info.child).zig_type_tag(mod) == .Fn))
    {
        const ptr_int = try block.add_un_op(.int_from_ptr, ptr);
        const is_non_zero = try block.add_bin_op(.cmp_neq, ptr_int, .zero_usize);
        const ok = if (src_info.flags.size == .Slice and dest_info.flags.size == .Slice) ok: {
            const len = try sema.analyze_slice_len(block, operand_src, ptr);
            const len_zero = try block.add_bin_op(.cmp_eq, len, .zero_usize);
            break :ok try block.add_bin_op(.bool_or, len_zero, is_non_zero);
        } else is_non_zero;
        try sema.add_safety_check(block, src, ok, .cast_to_null);
    }

    if (block.want_safety() and
        dest_align.compare(.gt, src_align) and
        try sema.type_has_runtime_bits(Type.from_interned(dest_info.child)))
    {
        const align_bytes_minus_1 = dest_align.to_byte_units().? - 1;
        const align_minus_1 = Air.interned_to_ref((try mod.int_value(Type.usize, align_bytes_minus_1)).to_intern());
        const ptr_int = try block.add_un_op(.int_from_ptr, ptr);
        const remainder = try block.add_bin_op(.bit_and, ptr_int, align_minus_1);
        const is_aligned = try block.add_bin_op(.cmp_eq, remainder, .zero_usize);
        const ok = if (src_info.flags.size == .Slice and dest_info.flags.size == .Slice) ok: {
            const len = try sema.analyze_slice_len(block, operand_src, ptr);
            const len_zero = try block.add_bin_op(.cmp_eq, len, .zero_usize);
            break :ok try block.add_bin_op(.bool_or, len_zero, is_aligned);
        } else is_aligned;
        try sema.add_safety_check(block, src, ok, .incorrect_alignment);
    }

    // If we're going from an array pointer to a slice, this will only be the pointer part!
    const result_ptr = if (flags.addrspace_cast) ptr: {
        // We can't change address spaces with a bitcast, so this requires two instructions
        var intermediate_info = src_info;
        intermediate_info.flags.address_space = dest_info.flags.address_space;
        const intermediate_ptr_ty = try sema.ptr_type(intermediate_info);
        const intermediate_ty = if (dest_ptr_ty.zig_type_tag(mod) == .Optional) blk: {
            break :blk try mod.optional_type(intermediate_ptr_ty.to_intern());
        } else intermediate_ptr_ty;
        const intermediate = try block.add_inst(.{
            .tag = .addrspace_cast,
            .data = .{ .ty_op = .{
                .ty = Air.interned_to_ref(intermediate_ty.to_intern()),
                .operand = ptr,
            } },
        });
        if (intermediate_ty.eql(dest_ptr_ty, mod)) {
            // We only changed the address space, so no need for a bitcast
            break :ptr intermediate;
        }
        break :ptr try block.add_bit_cast(dest_ptr_ty, intermediate);
    } else ptr: {
        break :ptr try block.add_bit_cast(dest_ptr_ty, ptr);
    };

    if (dest_info.flags.size == .Slice and src_info.flags.size != .Slice) {
        // We have to construct a slice using the operand's child's array length
        // Note that we know from the check at the start of the function that operand_ty is slice-like
        const arr_len = Air.interned_to_ref((try mod.int_value(Type.usize, Type.from_interned(src_info.child).array_len(mod))).to_intern());
        return block.add_inst(.{
            .tag = .slice,
            .data = .{ .ty_pl = .{
                .ty = Air.interned_to_ref(dest_ty.to_intern()),
                .payload = try sema.add_extra(Air.Bin{
                    .lhs = result_ptr,
                    .rhs = arr_len,
                }),
            } },
        });
    } else {
        assert(dest_ptr_ty.eql(dest_ty, mod));
        try sema.check_known_alloc_ptr(block, operand, result_ptr);
        return result_ptr;
    }
}

fn zir_ptr_cast_no_dest(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const FlagsInt = @typeInfo(Zir.Inst.FullPtrCastFlags).Struct.backing_integer.?;
    const flags: Zir.Inst.FullPtrCastFlags = @bit_cast(@as(FlagsInt, @truncate(extended.small)));
    const extra = sema.code.extra_data(Zir.Inst.UnNode, extended.operand).data;
    const src = LazySrcLoc.nodeOffset(extra.node);
    const operand_src: LazySrcLoc = .{ .node_offset_ptrcast_operand = extra.node };
    const operand = try sema.resolve_inst(extra.operand);
    const operand_ty = sema.type_of(operand);
    try sema.check_ptr_operand(block, operand_src, operand_ty);

    var ptr_info = operand_ty.ptr_info(mod);
    if (flags.const_cast) ptr_info.flags.is_const = false;
    if (flags.volatile_cast) ptr_info.flags.is_volatile = false;

    const dest_ty = blk: {
        const dest_ty = try sema.ptr_type(ptr_info);
        if (operand_ty.zig_type_tag(mod) == .Optional) {
            break :blk try mod.optional_type(dest_ty.to_intern());
        }
        break :blk dest_ty;
    };

    if (try sema.resolve_value(operand)) |operand_val| {
        return Air.interned_to_ref((try mod.get_coerced(operand_val, dest_ty)).to_intern());
    }

    try sema.require_runtime_block(block, src, null);
    const new_ptr = try block.add_bit_cast(dest_ty, operand);
    try sema.check_known_alloc_ptr(block, operand, new_ptr);
    return new_ptr;
}

fn zir_truncate(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const dest_ty = try sema.resolve_dest_type(block, src, extra.lhs, .remove_eu_opt, "@truncate");
    const dest_scalar_ty = try sema.check_int_or_vector_allow_comptime(block, dest_ty, src);
    const operand = try sema.resolve_inst(extra.rhs);
    const operand_ty = sema.type_of(operand);
    const operand_scalar_ty = try sema.check_int_or_vector_allow_comptime(block, operand_ty, operand_src);

    const operand_is_vector = operand_ty.zig_type_tag(mod) == .Vector;
    const dest_is_vector = dest_ty.zig_type_tag(mod) == .Vector;
    if (operand_is_vector != dest_is_vector) {
        return sema.fail(block, operand_src, "expected type '{}', found '{}'", .{ dest_ty.fmt(mod), operand_ty.fmt(mod) });
    }

    if (dest_scalar_ty.zig_type_tag(mod) == .ComptimeInt) {
        return sema.coerce(block, dest_ty, operand, operand_src);
    }

    const dest_info = dest_scalar_ty.int_info(mod);

    if (try sema.type_has_one_possible_value(dest_ty)) |val| {
        return Air.interned_to_ref(val.to_intern());
    }

    if (operand_scalar_ty.zig_type_tag(mod) != .ComptimeInt) {
        const operand_info = operand_ty.int_info(mod);
        if (try sema.type_has_one_possible_value(operand_ty)) |val| {
            return Air.interned_to_ref(val.to_intern());
        }

        if (operand_info.signedness != dest_info.signedness) {
            return sema.fail(block, operand_src, "expected {s} integer type, found '{}'", .{
                @tag_name(dest_info.signedness), operand_ty.fmt(mod),
            });
        }
        if (operand_info.bits < dest_info.bits) {
            const msg = msg: {
                const msg = try sema.err_msg(
                    block,
                    src,
                    "destination type '{}' has more bits than source type '{}'",
                    .{ dest_ty.fmt(mod), operand_ty.fmt(mod) },
                );
                errdefer msg.destroy(sema.gpa);
                try sema.err_note(block, src, msg, "destination type has {d} bits", .{
                    dest_info.bits,
                });
                try sema.err_note(block, operand_src, msg, "operand type has {d} bits", .{
                    operand_info.bits,
                });
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        }
    }

    if (try sema.resolve_value_intable(operand)) |val| {
        if (val.is_undef(mod)) return mod.undef_ref(dest_ty);
        if (!dest_is_vector) {
            return Air.interned_to_ref((try mod.get_coerced(
                try val.int_trunc(operand_ty, sema.arena, dest_info.signedness, dest_info.bits, mod),
                dest_ty,
            )).to_intern());
        }
        const elems = try sema.arena.alloc(InternPool.Index, operand_ty.vector_len(mod));
        for (elems, 0..) |*elem, i| {
            const elem_val = try val.elem_value(mod, i);
            const uncoerced_elem = try elem_val.int_trunc(operand_scalar_ty, sema.arena, dest_info.signedness, dest_info.bits, mod);
            elem.* = (try mod.get_coerced(uncoerced_elem, dest_scalar_ty)).to_intern();
        }
        return Air.interned_to_ref((try mod.intern(.{ .aggregate = .{
            .ty = dest_ty.to_intern(),
            .storage = .{ .elems = elems },
        } })));
    }

    try sema.require_runtime_block(block, src, operand_src);
    return block.add_ty_op(.trunc, dest_ty, operand);
}

fn zir_bit_count(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    air_tag: Air.Inst.Tag,
    comptime comptimeOp: fn (val: Value, ty: Type, mod: *Module) u64,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const operand = try sema.resolve_inst(inst_data.operand);
    const operand_ty = sema.type_of(operand);
    _ = try sema.check_int_or_vector(block, operand, operand_src);
    const bits = operand_ty.int_info(mod).bits;

    if (try sema.type_has_one_possible_value(operand_ty)) |val| {
        return Air.interned_to_ref(val.to_intern());
    }

    const result_scalar_ty = try mod.smallest_unsigned_int(bits);
    switch (operand_ty.zig_type_tag(mod)) {
        .Vector => {
            const vec_len = operand_ty.vector_len(mod);
            const result_ty = try mod.vector_type(.{
                .len = vec_len,
                .child = result_scalar_ty.to_intern(),
            });
            if (try sema.resolve_value(operand)) |val| {
                if (val.is_undef(mod)) return mod.undef_ref(result_ty);

                const elems = try sema.arena.alloc(InternPool.Index, vec_len);
                const scalar_ty = operand_ty.scalar_type(mod);
                for (elems, 0..) |*elem, i| {
                    const elem_val = try val.elem_value(mod, i);
                    const count = comptimeOp(elem_val, scalar_ty, mod);
                    elem.* = (try mod.int_value(result_scalar_ty, count)).to_intern();
                }
                return Air.interned_to_ref((try mod.intern(.{ .aggregate = .{
                    .ty = result_ty.to_intern(),
                    .storage = .{ .elems = elems },
                } })));
            } else {
                try sema.require_runtime_block(block, src, operand_src);
                return block.add_ty_op(air_tag, result_ty, operand);
            }
        },
        .Int => {
            if (try sema.resolve_value_resolve_lazy(operand)) |val| {
                if (val.is_undef(mod)) return mod.undef_ref(result_scalar_ty);
                return mod.int_ref(result_scalar_ty, comptimeOp(val, operand_ty, mod));
            } else {
                try sema.require_runtime_block(block, src, operand_src);
                return block.add_ty_op(air_tag, result_scalar_ty, operand);
            }
        },
        else => unreachable,
    }
}

fn zir_byte_swap(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const operand = try sema.resolve_inst(inst_data.operand);
    const operand_ty = sema.type_of(operand);
    const scalar_ty = try sema.check_int_or_vector(block, operand, operand_src);
    const bits = scalar_ty.int_info(mod).bits;
    if (bits % 8 != 0) {
        return sema.fail(
            block,
            operand_src,
            "@byte_swap requires the number of bits to be evenly divisible by 8, but {} has {} bits",
            .{ scalar_ty.fmt(mod), bits },
        );
    }

    if (try sema.type_has_one_possible_value(operand_ty)) |val| {
        return Air.interned_to_ref(val.to_intern());
    }

    switch (operand_ty.zig_type_tag(mod)) {
        .Int => {
            const runtime_src = if (try sema.resolve_value(operand)) |val| {
                if (val.is_undef(mod)) return mod.undef_ref(operand_ty);
                const result_val = try val.byte_swap(operand_ty, mod, sema.arena);
                return Air.interned_to_ref(result_val.to_intern());
            } else operand_src;

            try sema.require_runtime_block(block, src, runtime_src);
            return block.add_ty_op(.byte_swap, operand_ty, operand);
        },
        .Vector => {
            const runtime_src = if (try sema.resolve_value(operand)) |val| {
                if (val.is_undef(mod))
                    return mod.undef_ref(operand_ty);

                const vec_len = operand_ty.vector_len(mod);
                const elems = try sema.arena.alloc(InternPool.Index, vec_len);
                for (elems, 0..) |*elem, i| {
                    const elem_val = try val.elem_value(mod, i);
                    elem.* = (try elem_val.byte_swap(scalar_ty, mod, sema.arena)).to_intern();
                }
                return Air.interned_to_ref((try mod.intern(.{ .aggregate = .{
                    .ty = operand_ty.to_intern(),
                    .storage = .{ .elems = elems },
                } })));
            } else operand_src;

            try sema.require_runtime_block(block, src, runtime_src);
            return block.add_ty_op(.byte_swap, operand_ty, operand);
        },
        else => unreachable,
    }
}

fn zir_bit_reverse(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const operand = try sema.resolve_inst(inst_data.operand);
    const operand_ty = sema.type_of(operand);
    const scalar_ty = try sema.check_int_or_vector(block, operand, operand_src);

    if (try sema.type_has_one_possible_value(operand_ty)) |val| {
        return Air.interned_to_ref(val.to_intern());
    }

    const mod = sema.mod;
    switch (operand_ty.zig_type_tag(mod)) {
        .Int => {
            const runtime_src = if (try sema.resolve_value(operand)) |val| {
                if (val.is_undef(mod)) return mod.undef_ref(operand_ty);
                const result_val = try val.bit_reverse(operand_ty, mod, sema.arena);
                return Air.interned_to_ref(result_val.to_intern());
            } else operand_src;

            try sema.require_runtime_block(block, src, runtime_src);
            return block.add_ty_op(.bit_reverse, operand_ty, operand);
        },
        .Vector => {
            const runtime_src = if (try sema.resolve_value(operand)) |val| {
                if (val.is_undef(mod))
                    return mod.undef_ref(operand_ty);

                const vec_len = operand_ty.vector_len(mod);
                const elems = try sema.arena.alloc(InternPool.Index, vec_len);
                for (elems, 0..) |*elem, i| {
                    const elem_val = try val.elem_value(mod, i);
                    elem.* = (try elem_val.bit_reverse(scalar_ty, mod, sema.arena)).to_intern();
                }
                return Air.interned_to_ref((try mod.intern(.{ .aggregate = .{
                    .ty = operand_ty.to_intern(),
                    .storage = .{ .elems = elems },
                } })));
            } else operand_src;

            try sema.require_runtime_block(block, src, runtime_src);
            return block.add_ty_op(.bit_reverse, operand_ty, operand);
        },
        else => unreachable,
    }
}

fn zir_bit_offset_of(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const offset = try sema.bit_offset_of(block, inst);
    return sema.mod.int_ref(Type.comptime_int, offset);
}

fn zir_offset_of(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const offset = try sema.bit_offset_of(block, inst);
    // TODO reminder to make this a compile error for packed structs
    return sema.mod.int_ref(Type.comptime_int, offset / 8);
}

fn bit_offset_of(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!u64 {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const src: LazySrcLoc = .{ .node_offset_bin_op = inst_data.src_node };
    const lhs_src: LazySrcLoc = .{ .node_offset_bin_lhs = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_bin_rhs = inst_data.src_node };
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;

    const ty = try sema.resolve_type(block, lhs_src, extra.lhs);
    const field_name = try sema.resolve_const_string_intern(block, rhs_src, extra.rhs, .{
        .needed_comptime_reason = "name of field must be comptime-known",
    });

    const mod = sema.mod;
    const ip = &mod.intern_pool;
    try sema.resolve_type_layout(ty);
    switch (ty.zig_type_tag(mod)) {
        .Struct => {},
        else => return sema.fail(block, lhs_src, "expected struct type, found '{}'", .{ty.fmt(mod)}),
    }

    const field_index = if (ty.is_tuple(mod)) blk: {
        if (field_name.eql_slice("len", ip)) {
            return sema.fail(block, src, "no offset available for 'len' field of tuple", .{});
        }
        break :blk try sema.tuple_field_index(block, ty, field_name, rhs_src);
    } else try sema.struct_field_index(block, ty, field_name, rhs_src);

    if (ty.struct_field_is_comptime(field_index, mod)) {
        return sema.fail(block, src, "no offset available for comptime field", .{});
    }

    switch (ty.container_layout(mod)) {
        .@"packed" => {
            var bit_sum: u64 = 0;
            const struct_type = ip.load_struct_type(ty.to_intern());
            for (0..struct_type.field_types.len) |i| {
                if (i == field_index) {
                    return bit_sum;
                }
                const field_ty = Type.from_interned(struct_type.field_types.get(ip)[i]);
                bit_sum += field_ty.bit_size(mod);
            } else unreachable;
        },
        else => return ty.struct_field_offset(field_index, mod) * 8,
    }
}

fn check_namespace_type(sema: *Sema, block: *Block, src: LazySrcLoc, ty: Type) CompileError!void {
    const mod = sema.mod;
    switch (ty.zig_type_tag(mod)) {
        .Struct, .Enum, .Union, .Opaque => return,
        else => return sema.fail(block, src, "expected struct, enum, union, or opaque; found '{}'", .{ty.fmt(mod)}),
    }
}

/// Returns `true` if the type was a comptime_int.
fn check_int_type(sema: *Sema, block: *Block, src: LazySrcLoc, ty: Type) CompileError!bool {
    const mod = sema.mod;
    switch (try ty.zig_type_tag_or_poison(mod)) {
        .ComptimeInt => return true,
        .Int => return false,
        else => return sema.fail(block, src, "expected integer type, found '{}'", .{ty.fmt(mod)}),
    }
}

fn check_invalid_ptr_arithmetic(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    ty: Type,
) CompileError!void {
    const mod = sema.mod;
    switch (try ty.zig_type_tag_or_poison(mod)) {
        .Pointer => switch (ty.ptr_size(mod)) {
            .One, .Slice => return,
            .Many, .C => return sema.fail(
                block,
                src,
                "invalid pointer arithmetic operator",
                .{},
            ),
        },
        else => return,
    }
}

fn check_arithmetic_op(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    scalar_tag: std.builtin.TypeId,
    lhs_zig_ty_tag: std.builtin.TypeId,
    rhs_zig_ty_tag: std.builtin.TypeId,
    zir_tag: Zir.Inst.Tag,
) CompileError!void {
    const is_int = scalar_tag == .Int or scalar_tag == .ComptimeInt;
    const is_float = scalar_tag == .Float or scalar_tag == .ComptimeFloat;

    if (!is_int and !(is_float and float_op_allowed(zir_tag))) {
        return sema.fail(block, src, "invalid operands to binary expression: '{s}' and '{s}'", .{
            @tag_name(lhs_zig_ty_tag), @tag_name(rhs_zig_ty_tag),
        });
    }
}

fn check_ptr_operand(
    sema: *Sema,
    block: *Block,
    ty_src: LazySrcLoc,
    ty: Type,
) CompileError!void {
    const mod = sema.mod;
    switch (ty.zig_type_tag(mod)) {
        .Pointer => return,
        .Fn => {
            const msg = msg: {
                const msg = try sema.err_msg(
                    block,
                    ty_src,
                    "expected pointer, found '{}'",
                    .{ty.fmt(mod)},
                );
                errdefer msg.destroy(sema.gpa);

                try sema.err_note(block, ty_src, msg, "use '&' to obtain a function pointer", .{});

                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        },
        .Optional => if (ty.child_type(mod).zig_type_tag(mod) == .Pointer) return,
        else => {},
    }
    return sema.fail(block, ty_src, "expected pointer type, found '{}'", .{ty.fmt(mod)});
}

fn check_ptr_type(
    sema: *Sema,
    block: *Block,
    ty_src: LazySrcLoc,
    ty: Type,
    allow_slice: bool,
) CompileError!void {
    const mod = sema.mod;
    switch (ty.zig_type_tag(mod)) {
        .Pointer => if (allow_slice or !ty.is_slice(mod)) return,
        .Fn => {
            const msg = msg: {
                const msg = try sema.err_msg(
                    block,
                    ty_src,
                    "expected pointer type, found '{}'",
                    .{ty.fmt(mod)},
                );
                errdefer msg.destroy(sema.gpa);

                try sema.err_note(block, ty_src, msg, "use '*const ' to make a function pointer type", .{});

                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        },
        .Optional => if (ty.child_type(mod).zig_type_tag(mod) == .Pointer) return,
        else => {},
    }
    return sema.fail(block, ty_src, "expected pointer type, found '{}'", .{ty.fmt(mod)});
}

fn check_vector_elem_type(
    sema: *Sema,
    block: *Block,
    ty_src: LazySrcLoc,
    ty: Type,
) CompileError!void {
    const mod = sema.mod;
    switch (ty.zig_type_tag(mod)) {
        .Int, .Float, .Bool => return,
        .Optional, .Pointer => if (ty.is_ptr_at_runtime(mod)) return,
        else => {},
    }
    return sema.fail(block, ty_src, "expected integer, float, bool, or pointer for the vector element type; found '{}'", .{ty.fmt(mod)});
}

fn check_float_type(
    sema: *Sema,
    block: *Block,
    ty_src: LazySrcLoc,
    ty: Type,
) CompileError!void {
    const mod = sema.mod;
    switch (ty.zig_type_tag(mod)) {
        .ComptimeInt, .ComptimeFloat, .Float => {},
        else => return sema.fail(block, ty_src, "expected float type, found '{}'", .{ty.fmt(mod)}),
    }
}

fn check_numeric_type(
    sema: *Sema,
    block: *Block,
    ty_src: LazySrcLoc,
    ty: Type,
) CompileError!void {
    const mod = sema.mod;
    switch (ty.zig_type_tag(mod)) {
        .ComptimeFloat, .Float, .ComptimeInt, .Int => {},
        .Vector => switch (ty.child_type(mod).zig_type_tag(mod)) {
            .ComptimeFloat, .Float, .ComptimeInt, .Int => {},
            else => |t| return sema.fail(block, ty_src, "expected number, found '{}'", .{t}),
        },
        else => return sema.fail(block, ty_src, "expected number, found '{}'", .{ty.fmt(mod)}),
    }
}

/// Returns the casted pointer.
fn check_atomic_ptr_operand(
    sema: *Sema,
    block: *Block,
    elem_ty: Type,
    elem_ty_src: LazySrcLoc,
    ptr: Air.Inst.Ref,
    ptr_src: LazySrcLoc,
    ptr_const: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    var diag: Module.AtomicPtrAlignmentDiagnostics = .{};
    const alignment = mod.atomic_ptr_alignment(elem_ty, &diag) catch |err| switch (err) {
        error.OutOfMemory => return error.OutOfMemory,
        error.FloatTooBig => return sema.fail(
            block,
            elem_ty_src,
            "expected {d}-bit float type or smaller; found {d}-bit float type",
            .{ diag.max_bits, diag.bits },
        ),
        error.IntTooBig => return sema.fail(
            block,
            elem_ty_src,
            "expected {d}-bit integer type or smaller; found {d}-bit integer type",
            .{ diag.max_bits, diag.bits },
        ),
        error.BadType => return sema.fail(
            block,
            elem_ty_src,
            "expected bool, integer, float, enum, or pointer type; found '{}'",
            .{elem_ty.fmt(mod)},
        ),
    };

    var wanted_ptr_data: InternPool.Key.PtrType = .{
        .child = elem_ty.to_intern(),
        .flags = .{
            .alignment = alignment,
            .is_const = ptr_const,
        },
    };

    const ptr_ty = sema.type_of(ptr);
    const ptr_data = switch (try ptr_ty.zig_type_tag_or_poison(mod)) {
        .Pointer => ptr_ty.ptr_info(mod),
        else => {
            const wanted_ptr_ty = try sema.ptr_type(wanted_ptr_data);
            _ = try sema.coerce(block, wanted_ptr_ty, ptr, ptr_src);
            unreachable;
        },
    };

    wanted_ptr_data.flags.address_space = ptr_data.flags.address_space;
    wanted_ptr_data.flags.is_allowzero = ptr_data.flags.is_allowzero;
    wanted_ptr_data.flags.is_volatile = ptr_data.flags.is_volatile;

    const wanted_ptr_ty = try sema.ptr_type(wanted_ptr_data);
    const casted_ptr = try sema.coerce(block, wanted_ptr_ty, ptr, ptr_src);

    return casted_ptr;
}

fn check_ptr_is_not_comptime_mutable(
    sema: *Sema,
    block: *Block,
    ptr_val: Value,
    ptr_src: LazySrcLoc,
    operand_src: LazySrcLoc,
) CompileError!void {
    _ = operand_src;
    if (sema.is_comptime_mutable_ptr(ptr_val)) {
        return sema.fail(block, ptr_src, "cannot store runtime value in compile time variable", .{});
    }
}

fn check_int_or_vector(
    sema: *Sema,
    block: *Block,
    operand: Air.Inst.Ref,
    operand_src: LazySrcLoc,
) CompileError!Type {
    const mod = sema.mod;
    const operand_ty = sema.type_of(operand);
    switch (try operand_ty.zig_type_tag_or_poison(mod)) {
        .Int => return operand_ty,
        .Vector => {
            const elem_ty = operand_ty.child_type(mod);
            switch (try elem_ty.zig_type_tag_or_poison(mod)) {
                .Int => return elem_ty,
                else => return sema.fail(block, operand_src, "expected vector of integers; found vector of '{}'", .{
                    elem_ty.fmt(mod),
                }),
            }
        },
        else => return sema.fail(block, operand_src, "expected integer or vector, found '{}'", .{
            operand_ty.fmt(mod),
        }),
    }
}

fn check_int_or_vector_allow_comptime(
    sema: *Sema,
    block: *Block,
    operand_ty: Type,
    operand_src: LazySrcLoc,
) CompileError!Type {
    const mod = sema.mod;
    switch (try operand_ty.zig_type_tag_or_poison(mod)) {
        .Int, .ComptimeInt => return operand_ty,
        .Vector => {
            const elem_ty = operand_ty.child_type(mod);
            switch (try elem_ty.zig_type_tag_or_poison(mod)) {
                .Int, .ComptimeInt => return elem_ty,
                else => return sema.fail(block, operand_src, "expected vector of integers; found vector of '{}'", .{
                    elem_ty.fmt(mod),
                }),
            }
        },
        else => return sema.fail(block, operand_src, "expected integer or vector, found '{}'", .{
            operand_ty.fmt(mod),
        }),
    }
}

const SimdBinOp = struct {
    len: ?usize,
    /// Coerced to `result_ty`.
    lhs: Air.Inst.Ref,
    /// Coerced to `result_ty`.
    rhs: Air.Inst.Ref,
    lhs_val: ?Value,
    rhs_val: ?Value,
    /// Only different than `scalar_ty` when it is a vector operation.
    result_ty: Type,
    scalar_ty: Type,
};

fn check_simd_bin_op(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    uncasted_lhs: Air.Inst.Ref,
    uncasted_rhs: Air.Inst.Ref,
    lhs_src: LazySrcLoc,
    rhs_src: LazySrcLoc,
) CompileError!SimdBinOp {
    const mod = sema.mod;
    const lhs_ty = sema.type_of(uncasted_lhs);
    const rhs_ty = sema.type_of(uncasted_rhs);

    try sema.check_vectorizable_binary_operands(block, src, lhs_ty, rhs_ty, lhs_src, rhs_src);
    const vec_len: ?usize = if (lhs_ty.zig_type_tag(mod) == .Vector) lhs_ty.vector_len(mod) else null;
    const result_ty = try sema.resolve_peer_types(block, src, &.{ uncasted_lhs, uncasted_rhs }, .{
        .override = &[_]?LazySrcLoc{ lhs_src, rhs_src },
    });
    const lhs = try sema.coerce(block, result_ty, uncasted_lhs, lhs_src);
    const rhs = try sema.coerce(block, result_ty, uncasted_rhs, rhs_src);

    return SimdBinOp{
        .len = vec_len,
        .lhs = lhs,
        .rhs = rhs,
        .lhs_val = try sema.resolve_value(lhs),
        .rhs_val = try sema.resolve_value(rhs),
        .result_ty = result_ty,
        .scalar_ty = result_ty.scalar_type(mod),
    };
}

fn check_vectorizable_binary_operands(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    lhs_ty: Type,
    rhs_ty: Type,
    lhs_src: LazySrcLoc,
    rhs_src: LazySrcLoc,
) CompileError!void {
    const mod = sema.mod;
    const lhs_zig_ty_tag = try lhs_ty.zig_type_tag_or_poison(mod);
    const rhs_zig_ty_tag = try rhs_ty.zig_type_tag_or_poison(mod);
    if (lhs_zig_ty_tag != .Vector and rhs_zig_ty_tag != .Vector) return;

    const lhs_is_vector = switch (lhs_zig_ty_tag) {
        .Vector, .Array => true,
        else => false,
    };
    const rhs_is_vector = switch (rhs_zig_ty_tag) {
        .Vector, .Array => true,
        else => false,
    };

    if (lhs_is_vector and rhs_is_vector) {
        const lhs_len = lhs_ty.array_len(mod);
        const rhs_len = rhs_ty.array_len(mod);
        if (lhs_len != rhs_len) {
            const msg = msg: {
                const msg = try sema.err_msg(block, src, "vector length mismatch", .{});
                errdefer msg.destroy(sema.gpa);
                try sema.err_note(block, lhs_src, msg, "length {d} here", .{lhs_len});
                try sema.err_note(block, rhs_src, msg, "length {d} here", .{rhs_len});
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        }
    } else {
        const msg = msg: {
            const msg = try sema.err_msg(block, src, "mixed scalar and vector operands: '{}' and '{}'", .{
                lhs_ty.fmt(mod), rhs_ty.fmt(mod),
            });
            errdefer msg.destroy(sema.gpa);
            if (lhs_is_vector) {
                try sema.err_note(block, lhs_src, msg, "vector here", .{});
                try sema.err_note(block, rhs_src, msg, "scalar here", .{});
            } else {
                try sema.err_note(block, lhs_src, msg, "scalar here", .{});
                try sema.err_note(block, rhs_src, msg, "vector here", .{});
            }
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }
}

fn maybe_options_src(sema: *Sema, block: *Block, base_src: LazySrcLoc, wanted: []const u8) LazySrcLoc {
    if (base_src == .unneeded) return .unneeded;
    const mod = sema.mod;
    return mod.options_src(mod.decl_ptr(block.src_decl), base_src, wanted);
}

fn resolve_export_options(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    zir_ref: Zir.Inst.Ref,
) CompileError!Module.Export.Options {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;
    const export_options_ty = try sema.get_builtin_type("ExportOptions");
    const air_ref = try sema.resolve_inst(zir_ref);
    const options = try sema.coerce(block, export_options_ty, air_ref, src);

    const name_src = sema.maybe_options_src(block, src, "name");
    const linkage_src = sema.maybe_options_src(block, src, "linkage");
    const section_src = sema.maybe_options_src(block, src, "section");
    const visibility_src = sema.maybe_options_src(block, src, "visibility");

    const name_operand = try sema.field_val(block, src, options, try ip.get_or_put_string(gpa, "name", .no_embedded_nulls), name_src);
    const name = try sema.to_const_string(block, name_src, name_operand, .{
        .needed_comptime_reason = "name of exported value must be comptime-known",
    });

    const linkage_operand = try sema.field_val(block, src, options, try ip.get_or_put_string(gpa, "linkage", .no_embedded_nulls), linkage_src);
    const linkage_val = try sema.resolve_const_defined_value(block, linkage_src, linkage_operand, .{
        .needed_comptime_reason = "linkage of exported value must be comptime-known",
    });
    const linkage = mod.to_enum(std.builtin.GlobalLinkage, linkage_val);

    const section_operand = try sema.field_val(block, src, options, try ip.get_or_put_string(gpa, "section", .no_embedded_nulls), section_src);
    const section_opt_val = try sema.resolve_const_defined_value(block, section_src, section_operand, .{
        .needed_comptime_reason = "linksection of exported value must be comptime-known",
    });
    const section = if (section_opt_val.optional_value(mod)) |section_val|
        try sema.to_const_string(block, section_src, Air.interned_to_ref(section_val.to_intern()), .{
            .needed_comptime_reason = "linksection of exported value must be comptime-known",
        })
    else
        null;

    const visibility_operand = try sema.field_val(block, src, options, try ip.get_or_put_string(gpa, "visibility", .no_embedded_nulls), visibility_src);
    const visibility_val = try sema.resolve_const_defined_value(block, visibility_src, visibility_operand, .{
        .needed_comptime_reason = "visibility of exported value must be comptime-known",
    });
    const visibility = mod.to_enum(std.builtin.SymbolVisibility, visibility_val);

    if (name.len < 1) {
        return sema.fail(block, name_src, "exported symbol name cannot be empty", .{});
    }

    if (visibility != .default and linkage == .internal) {
        return sema.fail(block, visibility_src, "symbol '{s}' exported with internal linkage has non-default visibility {s}", .{
            name, @tag_name(visibility),
        });
    }

    return .{
        .name = try ip.get_or_put_string(gpa, name, .no_embedded_nulls),
        .linkage = linkage,
        .section = try ip.get_or_put_string_opt(gpa, section, .no_embedded_nulls),
        .visibility = visibility,
    };
}

fn resolve_builtin_enum(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    zir_ref: Zir.Inst.Ref,
    comptime name: []const u8,
    reason: NeededComptimeReason,
) CompileError!@field(std.builtin, name) {
    const mod = sema.mod;
    const ty = try sema.get_builtin_type(name);
    const air_ref = try sema.resolve_inst(zir_ref);
    const coerced = try sema.coerce(block, ty, air_ref, src);
    const val = try sema.resolve_const_defined_value(block, src, coerced, reason);
    return mod.to_enum(@field(std.builtin, name), val);
}

fn resolve_atomic_order(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    zir_ref: Zir.Inst.Ref,
    reason: NeededComptimeReason,
) CompileError!std.builtin.AtomicOrder {
    return sema.resolve_builtin_enum(block, src, zir_ref, "AtomicOrder", reason);
}

fn resolve_atomic_rmw_op(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    zir_ref: Zir.Inst.Ref,
) CompileError!std.builtin.AtomicRmwOp {
    return sema.resolve_builtin_enum(block, src, zir_ref, "AtomicRmwOp", .{
        .needed_comptime_reason = "@atomicRmW operation must be comptime-known",
    });
}

fn zir_cmpxchg(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const extra = sema.code.extra_data(Zir.Inst.Cmpxchg, extended.operand).data;
    const air_tag: Air.Inst.Tag = switch (extended.small) {
        0 => .cmpxchg_weak,
        1 => .cmpxchg_strong,
        else => unreachable,
    };
    const src = LazySrcLoc.nodeOffset(extra.node);
    // zig fmt: off
    const elem_ty_src      : LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const ptr_src          : LazySrcLoc = .{ .node_offset_builtin_call_arg1 = extra.node };
    const expected_src     : LazySrcLoc = .{ .node_offset_builtin_call_arg2 = extra.node };
    const new_value_src    : LazySrcLoc = .{ .node_offset_builtin_call_arg3 = extra.node };
    const success_order_src: LazySrcLoc = .{ .node_offset_builtin_call_arg4 = extra.node };
    const failure_order_src: LazySrcLoc = .{ .node_offset_builtin_call_arg5 = extra.node };
    // zig fmt: on
    const expected_value = try sema.resolve_inst(extra.expected_value);
    const elem_ty = sema.type_of(expected_value);
    if (elem_ty.zig_type_tag(mod) == .Float) {
        return sema.fail(
            block,
            elem_ty_src,
            "expected bool, integer, enum, or pointer type; found '{}'",
            .{elem_ty.fmt(mod)},
        );
    }
    const uncasted_ptr = try sema.resolve_inst(extra.ptr);
    const ptr = try sema.check_atomic_ptr_operand(block, elem_ty, elem_ty_src, uncasted_ptr, ptr_src, false);
    const new_value = try sema.coerce(block, elem_ty, try sema.resolve_inst(extra.new_value), new_value_src);
    const success_order = try sema.resolve_atomic_order(block, success_order_src, extra.success_order, .{
        .needed_comptime_reason = "atomic order of cmpxchg success must be comptime-known",
    });
    const failure_order = try sema.resolve_atomic_order(block, failure_order_src, extra.failure_order, .{
        .needed_comptime_reason = "atomic order of cmpxchg failure must be comptime-known",
    });

    if (@int_from_enum(success_order) < @int_from_enum(std.builtin.AtomicOrder.monotonic)) {
        return sema.fail(block, success_order_src, "success atomic ordering must be monotonic or stricter", .{});
    }
    if (@int_from_enum(failure_order) < @int_from_enum(std.builtin.AtomicOrder.monotonic)) {
        return sema.fail(block, failure_order_src, "failure atomic ordering must be monotonic or stricter", .{});
    }
    if (@int_from_enum(failure_order) > @int_from_enum(success_order)) {
        return sema.fail(block, failure_order_src, "failure atomic ordering must be no stricter than success", .{});
    }
    if (failure_order == .release or failure_order == .acq_rel) {
        return sema.fail(block, failure_order_src, "failure atomic ordering must not be release or acq_rel", .{});
    }

    const result_ty = try mod.optional_type(elem_ty.to_intern());

    // special case zero bit types
    if ((try sema.type_has_one_possible_value(elem_ty)) != null) {
        return Air.interned_to_ref((try mod.intern(.{ .opt = .{
            .ty = result_ty.to_intern(),
            .val = .none,
        } })));
    }

    const runtime_src = if (try sema.resolve_defined_value(block, ptr_src, ptr)) |ptr_val| rs: {
        if (try sema.resolve_value(expected_value)) |expected_val| {
            if (try sema.resolve_value(new_value)) |new_val| {
                if (expected_val.is_undef(mod) or new_val.is_undef(mod)) {
                    // TODO: this should probably cause the memory stored at the pointer
                    // to become undef as well
                    return mod.undef_ref(result_ty);
                }
                const ptr_ty = sema.type_of(ptr);
                const stored_val = (try sema.pointer_deref(block, ptr_src, ptr_val, ptr_ty)) orelse break :rs ptr_src;
                const result_val = try mod.intern(.{ .opt = .{
                    .ty = result_ty.to_intern(),
                    .val = if (stored_val.eql(expected_val, elem_ty, mod)) blk: {
                        try sema.store_ptr(block, src, ptr, new_value);
                        break :blk .none;
                    } else stored_val.to_intern(),
                } });
                return Air.interned_to_ref(result_val);
            } else break :rs new_value_src;
        } else break :rs expected_src;
    } else ptr_src;

    const flags: u32 = @as(u32, @int_from_enum(success_order)) |
        (@as(u32, @int_from_enum(failure_order)) << 3);

    try sema.require_runtime_block(block, src, runtime_src);
    return block.add_inst(.{
        .tag = air_tag,
        .data = .{ .ty_pl = .{
            .ty = Air.interned_to_ref(result_ty.to_intern()),
            .payload = try sema.add_extra(Air.Cmpxchg{
                .ptr = ptr,
                .expected_value = expected_value,
                .new_value = new_value,
                .flags = flags,
            }),
        } },
    });
}

fn zir_splat(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const src = inst_data.src();
    const scalar_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const dest_ty = try sema.resolve_dest_type(block, src, extra.lhs, .remove_eu_opt, "@splat");

    if (!dest_ty.is_vector(mod)) return sema.fail(block, src, "expected vector type, found '{}'", .{dest_ty.fmt(mod)});

    if (!dest_ty.has_runtime_bits(mod)) {
        const empty_aggregate = try mod.intern(.{ .aggregate = .{
            .ty = dest_ty.to_intern(),
            .storage = .{ .elems = &[_]InternPool.Index{} },
        } });
        return Air.interned_to_ref(empty_aggregate);
    }

    const operand = try sema.resolve_inst(extra.rhs);
    const scalar_ty = dest_ty.child_type(mod);
    const scalar = try sema.coerce(block, scalar_ty, operand, scalar_src);
    if (try sema.resolve_value(scalar)) |scalar_val| {
        if (scalar_val.is_undef(mod)) return mod.undef_ref(dest_ty);
        return Air.interned_to_ref((try sema.splat(dest_ty, scalar_val)).to_intern());
    }

    try sema.require_runtime_block(block, inst_data.src(), scalar_src);
    return block.add_ty_op(.splat, dest_ty, scalar);
}

fn zir_reduce(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const op_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const operand_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const operation = try sema.resolve_builtin_enum(block, op_src, extra.lhs, "ReduceOp", .{
        .needed_comptime_reason = "@reduce operation must be comptime-known",
    });
    const operand = try sema.resolve_inst(extra.rhs);
    const operand_ty = sema.type_of(operand);
    const mod = sema.mod;

    if (operand_ty.zig_type_tag(mod) != .Vector) {
        return sema.fail(block, operand_src, "expected vector, found '{}'", .{operand_ty.fmt(mod)});
    }

    const scalar_ty = operand_ty.child_type(mod);

    // Type-check depending on operation.
    switch (operation) {
        .And, .Or, .Xor => switch (scalar_ty.zig_type_tag(mod)) {
            .Int, .Bool => {},
            else => return sema.fail(block, operand_src, "@reduce operation '{s}' requires integer or boolean operand; found '{}'", .{
                @tag_name(operation), operand_ty.fmt(mod),
            }),
        },
        .Min, .Max, .Add, .Mul => switch (scalar_ty.zig_type_tag(mod)) {
            .Int, .Float => {},
            else => return sema.fail(block, operand_src, "@reduce operation '{s}' requires integer or float operand; found '{}'", .{
                @tag_name(operation), operand_ty.fmt(mod),
            }),
        },
    }

    const vec_len = operand_ty.vector_len(mod);
    if (vec_len == 0) {
        // TODO re-evaluate if we should introduce a "neutral value" for some operations,
        // e.g. zero for add and one for mul.
        return sema.fail(block, operand_src, "@reduce operation requires a vector with nonzero length", .{});
    }

    if (try sema.resolve_value(operand)) |operand_val| {
        if (operand_val.is_undef(mod)) return mod.undef_ref(scalar_ty);

        var accum: Value = try operand_val.elem_value(mod, 0);
        var i: u32 = 1;
        while (i < vec_len) : (i += 1) {
            const elem_val = try operand_val.elem_value(mod, i);
            switch (operation) {
                .And => accum = try accum.bitwise_and(elem_val, scalar_ty, sema.arena, mod),
                .Or => accum = try accum.bitwise_or(elem_val, scalar_ty, sema.arena, mod),
                .Xor => accum = try accum.bitwise_xor(elem_val, scalar_ty, sema.arena, mod),
                .Min => accum = accum.number_min(elem_val, mod),
                .Max => accum = accum.number_max(elem_val, mod),
                .Add => accum = try sema.number_add_wrap_scalar(accum, elem_val, scalar_ty),
                .Mul => accum = try accum.number_mul_wrap(elem_val, scalar_ty, sema.arena, mod),
            }
        }
        return Air.interned_to_ref(accum.to_intern());
    }

    try sema.require_runtime_block(block, inst_data.src(), operand_src);
    return block.add_inst(.{
        .tag = if (block.float_mode == .optimized) .reduce_optimized else .reduce,
        .data = .{ .reduce = .{
            .operand = operand,
            .operation = operation,
        } },
    });
}

fn zir_shuffle(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Shuffle, inst_data.payload_index).data;
    const elem_ty_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const mask_src: LazySrcLoc = .{ .node_offset_builtin_call_arg3 = inst_data.src_node };

    const elem_ty = try sema.resolve_type(block, elem_ty_src, extra.elem_type);
    try sema.check_vector_elem_type(block, elem_ty_src, elem_ty);
    const a = try sema.resolve_inst(extra.a);
    const b = try sema.resolve_inst(extra.b);
    var mask = try sema.resolve_inst(extra.mask);
    var mask_ty = sema.type_of(mask);

    const mask_len = switch (sema.type_of(mask).zig_type_tag(mod)) {
        .Array, .Vector => sema.type_of(mask).array_len(mod),
        else => return sema.fail(block, mask_src, "expected vector or array, found '{}'", .{sema.type_of(mask).fmt(sema.mod)}),
    };
    mask_ty = try mod.vector_type(.{
        .len = @int_cast(mask_len),
        .child = .i32_type,
    });
    mask = try sema.coerce(block, mask_ty, mask, mask_src);
    const mask_val = try sema.resolve_const_value(block, mask_src, mask, .{
        .needed_comptime_reason = "shuffle mask must be comptime-known",
    });
    return sema.analyze_shuffle(block, inst_data.src_node, elem_ty, a, b, mask_val, @int_cast(mask_len));
}

fn analyze_shuffle(
    sema: *Sema,
    block: *Block,
    src_node: i32,
    elem_ty: Type,
    a_arg: Air.Inst.Ref,
    b_arg: Air.Inst.Ref,
    mask: Value,
    mask_len: u32,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const a_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = src_node };
    const b_src: LazySrcLoc = .{ .node_offset_builtin_call_arg2 = src_node };
    const mask_src: LazySrcLoc = .{ .node_offset_builtin_call_arg3 = src_node };
    var a = a_arg;
    var b = b_arg;

    const res_ty = try mod.vector_type(.{
        .len = mask_len,
        .child = elem_ty.to_intern(),
    });

    const maybe_a_len = switch (sema.type_of(a).zig_type_tag(mod)) {
        .Array, .Vector => sema.type_of(a).array_len(mod),
        .Undefined => null,
        else => return sema.fail(block, a_src, "expected vector or array with element type '{}', found '{}'", .{
            elem_ty.fmt(sema.mod),
            sema.type_of(a).fmt(sema.mod),
        }),
    };
    const maybe_b_len = switch (sema.type_of(b).zig_type_tag(mod)) {
        .Array, .Vector => sema.type_of(b).array_len(mod),
        .Undefined => null,
        else => return sema.fail(block, b_src, "expected vector or array with element type '{}', found '{}'", .{
            elem_ty.fmt(sema.mod),
            sema.type_of(b).fmt(sema.mod),
        }),
    };
    if (maybe_a_len == null and maybe_b_len == null) {
        return mod.undef_ref(res_ty);
    }
    const a_len: u32 = @int_cast(maybe_a_len orelse maybe_b_len.?);
    const b_len: u32 = @int_cast(maybe_b_len orelse a_len);

    const a_ty = try mod.vector_type(.{
        .len = a_len,
        .child = elem_ty.to_intern(),
    });
    const b_ty = try mod.vector_type(.{
        .len = b_len,
        .child = elem_ty.to_intern(),
    });

    if (maybe_a_len == null) a = try mod.undef_ref(a_ty) else a = try sema.coerce(block, a_ty, a, a_src);
    if (maybe_b_len == null) b = try mod.undef_ref(b_ty) else b = try sema.coerce(block, b_ty, b, b_src);

    const operand_info = [2]std.meta.Tuple(&.{ u64, LazySrcLoc, Type }){
        .{ a_len, a_src, a_ty },
        .{ b_len, b_src, b_ty },
    };

    for (0..@int_cast(mask_len)) |i| {
        const elem = try mask.elem_value(sema.mod, i);
        if (elem.is_undef(mod)) continue;
        const elem_resolved = try sema.resolve_lazy_value(elem);
        const int = elem_resolved.to_signed_int(mod);
        var unsigned: u32 = undefined;
        var chosen: u32 = undefined;
        if (int >= 0) {
            unsigned = @int_cast(int);
            chosen = 0;
        } else {
            unsigned = @int_cast(~int);
            chosen = 1;
        }
        if (unsigned >= operand_info[chosen][0]) {
            const msg = msg: {
                const msg = try sema.err_msg(block, mask_src, "mask index '{d}' has out-of-bounds selection", .{i});
                errdefer msg.destroy(sema.gpa);

                try sema.err_note(block, operand_info[chosen][1], msg, "selected index '{d}' out of bounds of '{}'", .{
                    unsigned,
                    operand_info[chosen][2].fmt(sema.mod),
                });

                if (chosen == 0) {
                    try sema.err_note(block, b_src, msg, "selections from the second vector are specified with negative numbers", .{});
                }

                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        }
    }

    if (try sema.resolve_value(a)) |a_val| {
        if (try sema.resolve_value(b)) |b_val| {
            const values = try sema.arena.alloc(InternPool.Index, mask_len);
            for (values, 0..) |*value, i| {
                const mask_elem_val = try mask.elem_value(sema.mod, i);
                if (mask_elem_val.is_undef(mod)) {
                    value.* = try mod.intern(.{ .undef = elem_ty.to_intern() });
                    continue;
                }
                const int = mask_elem_val.to_signed_int(mod);
                const unsigned: u32 = @int_cast(if (int >= 0) int else ~int);
                values[i] = (try (if (int >= 0) a_val else b_val).elem_value(mod, unsigned)).to_intern();
            }
            return Air.interned_to_ref((try mod.intern(.{ .aggregate = .{
                .ty = res_ty.to_intern(),
                .storage = .{ .elems = values },
            } })));
        }
    }

    // All static analysis passed, and not comptime.
    // For runtime codegen, vectors a and b must be the same length. Here we
    // recursively @shuffle the smaller vector to append undefined elements
    // to it up to the length of the longer vector. This recursion terminates
    // in 1 call because these calls to analyze_shuffle guarantee a_len == b_len.
    if (a_len != b_len) {
        const min_len = @min(a_len, b_len);
        const max_src = if (a_len > b_len) a_src else b_src;
        const max_len = try sema.usize_cast(block, max_src, @max(a_len, b_len));

        const expand_mask_values = try sema.arena.alloc(InternPool.Index, max_len);
        for (@int_cast(0)..@int_cast(min_len)) |i| {
            expand_mask_values[i] = (try mod.int_value(Type.comptime_int, i)).to_intern();
        }
        for (@int_cast(min_len)..@int_cast(max_len)) |i| {
            expand_mask_values[i] = (try mod.int_value(Type.comptime_int, -1)).to_intern();
        }
        const expand_mask = try mod.intern(.{ .aggregate = .{
            .ty = (try mod.vector_type(.{ .len = @int_cast(max_len), .child = .comptime_int_type })).to_intern(),
            .storage = .{ .elems = expand_mask_values },
        } });

        if (a_len < b_len) {
            const undef = try mod.undef_ref(a_ty);
            a = try sema.analyze_shuffle(block, src_node, elem_ty, a, undef, Value.from_interned(expand_mask), @int_cast(max_len));
        } else {
            const undef = try mod.undef_ref(b_ty);
            b = try sema.analyze_shuffle(block, src_node, elem_ty, b, undef, Value.from_interned(expand_mask), @int_cast(max_len));
        }
    }

    return block.add_inst(.{
        .tag = .shuffle,
        .data = .{ .ty_pl = .{
            .ty = Air.interned_to_ref(res_ty.to_intern()),
            .payload = try block.sema.add_extra(Air.Shuffle{
                .a = a,
                .b = b,
                .mask = mask.to_intern(),
                .mask_len = mask_len,
            }),
        } },
    });
}

fn zir_select(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const extra = sema.code.extra_data(Zir.Inst.Select, extended.operand).data;

    const src = LazySrcLoc.nodeOffset(extra.node);
    const elem_ty_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const pred_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = extra.node };
    const a_src: LazySrcLoc = .{ .node_offset_builtin_call_arg2 = extra.node };
    const b_src: LazySrcLoc = .{ .node_offset_builtin_call_arg3 = extra.node };

    const elem_ty = try sema.resolve_type(block, elem_ty_src, extra.elem_type);
    try sema.check_vector_elem_type(block, elem_ty_src, elem_ty);
    const pred_uncoerced = try sema.resolve_inst(extra.pred);
    const pred_ty = sema.type_of(pred_uncoerced);

    const vec_len_u64 = switch (try pred_ty.zig_type_tag_or_poison(mod)) {
        .Vector, .Array => pred_ty.array_len(mod),
        else => return sema.fail(block, pred_src, "expected vector or array, found '{}'", .{pred_ty.fmt(mod)}),
    };
    const vec_len: u32 = @int_cast(try sema.usize_cast(block, pred_src, vec_len_u64));

    const bool_vec_ty = try mod.vector_type(.{
        .len = vec_len,
        .child = .bool_type,
    });
    const pred = try sema.coerce(block, bool_vec_ty, pred_uncoerced, pred_src);

    const vec_ty = try mod.vector_type(.{
        .len = vec_len,
        .child = elem_ty.to_intern(),
    });
    const a = try sema.coerce(block, vec_ty, try sema.resolve_inst(extra.a), a_src);
    const b = try sema.coerce(block, vec_ty, try sema.resolve_inst(extra.b), b_src);

    const maybe_pred = try sema.resolve_value(pred);
    const maybe_a = try sema.resolve_value(a);
    const maybe_b = try sema.resolve_value(b);

    const runtime_src = if (maybe_pred) |pred_val| rs: {
        if (pred_val.is_undef(mod)) return mod.undef_ref(vec_ty);

        if (maybe_a) |a_val| {
            if (a_val.is_undef(mod)) return mod.undef_ref(vec_ty);

            if (maybe_b) |b_val| {
                if (b_val.is_undef(mod)) return mod.undef_ref(vec_ty);

                const elems = try sema.gpa.alloc(InternPool.Index, vec_len);
                defer sema.gpa.free(elems);
                for (elems, 0..) |*elem, i| {
                    const pred_elem_val = try pred_val.elem_value(mod, i);
                    const should_choose_a = pred_elem_val.to_bool();
                    elem.* = (try (if (should_choose_a) a_val else b_val).elem_value(mod, i)).to_intern();
                }

                return Air.interned_to_ref((try mod.intern(.{ .aggregate = .{
                    .ty = vec_ty.to_intern(),
                    .storage = .{ .elems = elems },
                } })));
            } else {
                break :rs b_src;
            }
        } else {
            if (maybe_b) |b_val| {
                if (b_val.is_undef(mod)) return mod.undef_ref(vec_ty);
            }
            break :rs a_src;
        }
    } else rs: {
        if (maybe_a) |a_val| {
            if (a_val.is_undef(mod)) return mod.undef_ref(vec_ty);
        }
        if (maybe_b) |b_val| {
            if (b_val.is_undef(mod)) return mod.undef_ref(vec_ty);
        }
        break :rs pred_src;
    };

    try sema.require_runtime_block(block, src, runtime_src);
    return block.add_inst(.{
        .tag = .select,
        .data = .{ .pl_op = .{
            .operand = pred,
            .payload = try block.sema.add_extra(Air.Bin{
                .lhs = a,
                .rhs = b,
            }),
        } },
    });
}

fn zir_atomic_load(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.AtomicLoad, inst_data.payload_index).data;
    // zig fmt: off
    const elem_ty_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const ptr_src    : LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const order_src  : LazySrcLoc = .{ .node_offset_builtin_call_arg2 = inst_data.src_node };
    // zig fmt: on
    const elem_ty = try sema.resolve_type(block, elem_ty_src, extra.elem_type);
    const uncasted_ptr = try sema.resolve_inst(extra.ptr);
    const ptr = try sema.check_atomic_ptr_operand(block, elem_ty, elem_ty_src, uncasted_ptr, ptr_src, true);
    const order = try sema.resolve_atomic_order(block, order_src, extra.ordering, .{
        .needed_comptime_reason = "atomic order of @atomicLoad must be comptime-known",
    });

    switch (order) {
        .release, .acq_rel => {
            return sema.fail(
                block,
                order_src,
                "@atomicLoad atomic ordering must not be release or acq_rel",
                .{},
            );
        },
        else => {},
    }

    if (try sema.type_has_one_possible_value(elem_ty)) |val| {
        return Air.interned_to_ref(val.to_intern());
    }

    if (try sema.resolve_defined_value(block, ptr_src, ptr)) |ptr_val| {
        if (try sema.pointer_deref(block, ptr_src, ptr_val, sema.type_of(ptr))) |elem_val| {
            return Air.interned_to_ref(elem_val.to_intern());
        }
    }

    try sema.require_runtime_block(block, inst_data.src(), ptr_src);
    return block.add_inst(.{
        .tag = .atomic_load,
        .data = .{ .atomic_load = .{
            .ptr = ptr,
            .order = order,
        } },
    });
}

fn zir_atomic_rmw(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.AtomicRmw, inst_data.payload_index).data;
    const src = inst_data.src();
    // zig fmt: off
    const elem_ty_src   : LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const ptr_src       : LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const op_src        : LazySrcLoc = .{ .node_offset_builtin_call_arg2 = inst_data.src_node };
    const operand_src   : LazySrcLoc = .{ .node_offset_builtin_call_arg3 = inst_data.src_node };
    const order_src     : LazySrcLoc = .{ .node_offset_builtin_call_arg4 = inst_data.src_node };
    // zig fmt: on
    const operand = try sema.resolve_inst(extra.operand);
    const elem_ty = sema.type_of(operand);
    const uncasted_ptr = try sema.resolve_inst(extra.ptr);
    const ptr = try sema.check_atomic_ptr_operand(block, elem_ty, elem_ty_src, uncasted_ptr, ptr_src, false);
    const op = try sema.resolve_atomic_rmw_op(block, op_src, extra.operation);

    switch (elem_ty.zig_type_tag(mod)) {
        .Enum => if (op != .Xchg) {
            return sema.fail(block, op_src, "@atomicRmw with enum only allowed with .Xchg", .{});
        },
        .Bool => if (op != .Xchg) {
            return sema.fail(block, op_src, "@atomicRmw with bool only allowed with .Xchg", .{});
        },
        .Float => switch (op) {
            .Xchg, .Add, .Sub, .Max, .Min => {},
            else => return sema.fail(block, op_src, "@atomicRmw with float only allowed with .Xchg, .Add, .Sub, .Max, and .Min", .{}),
        },
        else => {},
    }
    const order = try sema.resolve_atomic_order(block, order_src, extra.ordering, .{
        .needed_comptime_reason = "atomic order of @atomicRmW must be comptime-known",
    });

    if (order == .unordered) {
        return sema.fail(block, order_src, "@atomicRmw atomic ordering must not be unordered", .{});
    }

    // special case zero bit types
    if (try sema.type_has_one_possible_value(elem_ty)) |val| {
        return Air.interned_to_ref(val.to_intern());
    }

    const runtime_src = if (try sema.resolve_defined_value(block, ptr_src, ptr)) |ptr_val| rs: {
        const maybe_operand_val = try sema.resolve_value(operand);
        const operand_val = maybe_operand_val orelse {
            try sema.check_ptr_is_not_comptime_mutable(block, ptr_val, ptr_src, operand_src);
            break :rs operand_src;
        };
        if (sema.is_comptime_mutable_ptr(ptr_val)) {
            const ptr_ty = sema.type_of(ptr);
            const stored_val = (try sema.pointer_deref(block, ptr_src, ptr_val, ptr_ty)) orelse break :rs ptr_src;
            const new_val = switch (op) {
                // zig fmt: off
                .Xchg => operand_val,
                .Add  => try sema.number_add_wrap_scalar(stored_val, operand_val, elem_ty),
                .Sub  => try sema.number_sub_wrap_scalar(stored_val, operand_val, elem_ty),
                .And  => try                   stored_val.bitwise_and   (operand_val, elem_ty, sema.arena, mod),
                .Nand => try                   stored_val.bitwise_nand  (operand_val, elem_ty, sema.arena, mod),
                .Or   => try                   stored_val.bitwise_or    (operand_val, elem_ty, sema.arena, mod),
                .Xor  => try                   stored_val.bitwise_xor   (operand_val, elem_ty, sema.arena, mod),
                .Max  =>                       stored_val.number_max    (operand_val,                      mod),
                .Min  =>                       stored_val.number_min    (operand_val,                      mod),
                // zig fmt: on
            };
            try sema.store_ptr_val(block, src, ptr_val, new_val, elem_ty);
            return Air.interned_to_ref(stored_val.to_intern());
        } else break :rs ptr_src;
    } else ptr_src;

    const flags: u32 = @as(u32, @int_from_enum(order)) | (@as(u32, @int_from_enum(op)) << 3);

    try sema.require_runtime_block(block, src, runtime_src);
    return block.add_inst(.{
        .tag = .atomic_rmw,
        .data = .{ .pl_op = .{
            .operand = ptr,
            .payload = try sema.add_extra(Air.AtomicRmw{
                .operand = operand,
                .flags = flags,
            }),
        } },
    });
}

fn zir_atomic_store(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.AtomicStore, inst_data.payload_index).data;
    const src = inst_data.src();
    // zig fmt: off
    const elem_ty_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const ptr_src       : LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const operand_src   : LazySrcLoc = .{ .node_offset_builtin_call_arg2 = inst_data.src_node };
    const order_src     : LazySrcLoc = .{ .node_offset_builtin_call_arg3 = inst_data.src_node };
    // zig fmt: on
    const operand = try sema.resolve_inst(extra.operand);
    const elem_ty = sema.type_of(operand);
    const uncasted_ptr = try sema.resolve_inst(extra.ptr);
    const ptr = try sema.check_atomic_ptr_operand(block, elem_ty, elem_ty_src, uncasted_ptr, ptr_src, false);
    const order = try sema.resolve_atomic_order(block, order_src, extra.ordering, .{
        .needed_comptime_reason = "atomic order of @atomicStore must be comptime-known",
    });

    const air_tag: Air.Inst.Tag = switch (order) {
        .acquire, .acq_rel => {
            return sema.fail(
                block,
                order_src,
                "@atomicStore atomic ordering must not be acquire or acq_rel",
                .{},
            );
        },
        .unordered => .atomic_store_unordered,
        .monotonic => .atomic_store_monotonic,
        .release => .atomic_store_release,
        .seq_cst => .atomic_store_seq_cst,
    };

    return sema.store_ptr2(block, src, ptr, ptr_src, operand, operand_src, air_tag);
}

fn zir_mul_add(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.MulAdd, inst_data.payload_index).data;
    const src = inst_data.src();

    const mulend1_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const mulend2_src: LazySrcLoc = .{ .node_offset_builtin_call_arg2 = inst_data.src_node };
    const addend_src: LazySrcLoc = .{ .node_offset_builtin_call_arg3 = inst_data.src_node };

    const addend = try sema.resolve_inst(extra.addend);
    const ty = sema.type_of(addend);
    const mulend1 = try sema.coerce(block, ty, try sema.resolve_inst(extra.mulend1), mulend1_src);
    const mulend2 = try sema.coerce(block, ty, try sema.resolve_inst(extra.mulend2), mulend2_src);

    const maybe_mulend1 = try sema.resolve_value(mulend1);
    const maybe_mulend2 = try sema.resolve_value(mulend2);
    const maybe_addend = try sema.resolve_value(addend);
    const mod = sema.mod;

    switch (ty.scalar_type(mod).zig_type_tag(mod)) {
        .ComptimeFloat, .Float => {},
        else => return sema.fail(block, src, "expected vector of floats or float type, found '{}'", .{ty.fmt(sema.mod)}),
    }

    const runtime_src = if (maybe_mulend1) |mulend1_val| rs: {
        if (maybe_mulend2) |mulend2_val| {
            if (mulend2_val.is_undef(mod)) return mod.undef_ref(ty);

            if (maybe_addend) |addend_val| {
                if (addend_val.is_undef(mod)) return mod.undef_ref(ty);
                const result_val = try Value.mul_add(ty, mulend1_val, mulend2_val, addend_val, sema.arena, sema.mod);
                return Air.interned_to_ref(result_val.to_intern());
            } else {
                break :rs addend_src;
            }
        } else {
            if (maybe_addend) |addend_val| {
                if (addend_val.is_undef(mod)) return mod.undef_ref(ty);
            }
            break :rs mulend2_src;
        }
    } else rs: {
        if (maybe_mulend2) |mulend2_val| {
            if (mulend2_val.is_undef(mod)) return mod.undef_ref(ty);
        }
        if (maybe_addend) |addend_val| {
            if (addend_val.is_undef(mod)) return mod.undef_ref(ty);
        }
        break :rs mulend1_src;
    };

    try sema.require_runtime_block(block, src, runtime_src);
    return block.add_inst(.{
        .tag = .mul_add,
        .data = .{ .pl_op = .{
            .operand = addend,
            .payload = try sema.add_extra(Air.Bin{
                .lhs = mulend1,
                .rhs = mulend2,
            }),
        } },
    });
}

fn zir_builtin_call(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const modifier_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const func_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const args_src: LazySrcLoc = .{ .node_offset_builtin_call_arg2 = inst_data.src_node };
    const call_src = inst_data.src();

    const extra = sema.code.extra_data(Zir.Inst.BuiltinCall, inst_data.payload_index).data;
    const func = try sema.resolve_inst(extra.callee);

    const modifier_ty = try sema.get_builtin_type("CallModifier");
    const air_ref = try sema.resolve_inst(extra.modifier);
    const modifier_ref = try sema.coerce(block, modifier_ty, air_ref, modifier_src);
    const modifier_val = try sema.resolve_const_defined_value(block, modifier_src, modifier_ref, .{
        .needed_comptime_reason = "call modifier must be comptime-known",
    });
    var modifier = mod.to_enum(std.builtin.CallModifier, modifier_val);
    switch (modifier) {
        // These can be upgraded to comptime or nosuspend calls.
        .auto, .never_tail, .no_async => {
            if (block.is_comptime) {
                if (modifier == .never_tail) {
                    return sema.fail(block, modifier_src, "unable to perform 'never_tail' call at compile-time", .{});
                }
                modifier = .compile_time;
            } else if (extra.flags.is_nosuspend) {
                modifier = .no_async;
            }
        },
        // These can be upgraded to comptime. nosuspend bit can be safely ignored.
        .always_inline, .compile_time => {
            _ = (try sema.resolve_defined_value(block, func_src, func)) orelse {
                return sema.fail(block, func_src, "modifier '{s}' requires a comptime-known function", .{@tag_name(modifier)});
            };

            if (block.is_comptime) {
                modifier = .compile_time;
            }
        },
        .always_tail => {
            if (block.is_comptime) {
                modifier = .compile_time;
            }
        },
        .async_kw => {
            if (extra.flags.is_nosuspend) {
                return sema.fail(block, modifier_src, "modifier 'async_kw' cannot be used inside nosuspend block", .{});
            }
            if (block.is_comptime) {
                return sema.fail(block, modifier_src, "modifier 'async_kw' cannot be used in combination with comptime function call", .{});
            }
        },
        .never_inline => {
            if (block.is_comptime) {
                return sema.fail(block, modifier_src, "unable to perform 'never_inline' call at compile-time", .{});
            }
        },
    }

    const args = try sema.resolve_inst(extra.args);

    const args_ty = sema.type_of(args);
    if (!args_ty.is_tuple(mod) and args_ty.to_intern() != .empty_struct_type) {
        return sema.fail(block, args_src, "expected a tuple, found '{}'", .{args_ty.fmt(sema.mod)});
    }

    const resolved_args: []Air.Inst.Ref = try sema.arena.alloc(Air.Inst.Ref, args_ty.struct_field_count(mod));
    for (resolved_args, 0..) |*resolved, i| {
        resolved.* = try sema.tuple_field_val_by_index(block, args_src, args, @int_cast(i), args_ty);
    }

    const callee_ty = sema.type_of(func);
    const func_ty = try sema.check_call_argument_count(block, func, func_src, callee_ty, resolved_args.len, false);
    const ensure_result_used = extra.flags.ensure_result_used;
    return sema.analyze_call(
        block,
        func,
        func_ty,
        func_src,
        call_src,
        modifier,
        ensure_result_used,
        .{ .call_builtin = .{
            .call_node_offset = inst_data.src_node,
            .args = resolved_args,
        } },
        null,
        .@"@call",
    );
}

fn zir_field_parent_ptr(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!Air.Inst.Ref {
    const zcu = sema.mod;
    const ip = &zcu.intern_pool;

    const extra = sema.code.extra_data(Zir.Inst.FieldParentPtr, extended.operand).data;
    const FlagsInt = @typeInfo(Zir.Inst.FullPtrCastFlags).Struct.backing_integer.?;
    const flags: Zir.Inst.FullPtrCastFlags = @bit_cast(@as(FlagsInt, @truncate(extended.small)));
    assert(!flags.ptr_cast);
    const inst_src = extra.src();
    const field_name_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.src_node };
    const field_ptr_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = extra.src_node };

    const parent_ptr_ty = try sema.resolve_dest_type(block, inst_src, extra.parent_ptr_type, .remove_eu, "@fieldParentPtr");
    try sema.check_ptr_type(block, inst_src, parent_ptr_ty, true);
    const parent_ptr_info = parent_ptr_ty.ptr_info(zcu);
    if (parent_ptr_info.flags.size != .One) {
        return sema.fail(block, inst_src, "expected single pointer type, found '{}'", .{parent_ptr_ty.fmt(zcu)});
    }
    const parent_ty = Type.from_interned(parent_ptr_info.child);
    switch (parent_ty.zig_type_tag(zcu)) {
        .Struct, .Union => {},
        else => return sema.fail(block, inst_src, "expected pointer to struct or union type, found '{}'", .{parent_ptr_ty.fmt(zcu)}),
    }
    try sema.resolve_type_layout(parent_ty);

    const field_name = try sema.resolve_const_string_intern(block, field_name_src, extra.field_name, .{
        .needed_comptime_reason = "field name must be comptime-known",
    });
    const field_index = switch (parent_ty.zig_type_tag(zcu)) {
        .Struct => blk: {
            if (parent_ty.is_tuple(zcu)) {
                if (field_name.eql_slice("len", ip)) {
                    return sema.fail(block, inst_src, "cannot get @fieldParentPtr of 'len' field of tuple", .{});
                }
                break :blk try sema.tuple_field_index(block, parent_ty, field_name, field_name_src);
            } else {
                break :blk try sema.struct_field_index(block, parent_ty, field_name, field_name_src);
            }
        },
        .Union => try sema.union_field_index(block, parent_ty, field_name, field_name_src),
        else => unreachable,
    };
    if (parent_ty.zig_type_tag(zcu) == .Struct and parent_ty.struct_field_is_comptime(field_index, zcu)) {
        return sema.fail(block, field_name_src, "cannot get @fieldParentPtr of a comptime field", .{});
    }

    const field_ptr = try sema.resolve_inst(extra.field_ptr);
    const field_ptr_ty = sema.type_of(field_ptr);
    try sema.check_ptr_operand(block, field_ptr_src, field_ptr_ty);
    const field_ptr_info = field_ptr_ty.ptr_info(zcu);

    var actual_parent_ptr_info: InternPool.Key.PtrType = .{
        .child = parent_ty.to_intern(),
        .flags = .{
            .alignment = try parent_ptr_ty.ptr_alignment_advanced(zcu, sema),
            .is_const = field_ptr_info.flags.is_const,
            .is_volatile = field_ptr_info.flags.is_volatile,
            .is_allowzero = field_ptr_info.flags.is_allowzero,
            .address_space = field_ptr_info.flags.address_space,
        },
        .packed_offset = parent_ptr_info.packed_offset,
    };
    const field_ty = parent_ty.struct_field_type(field_index, zcu);
    var actual_field_ptr_info: InternPool.Key.PtrType = .{
        .child = field_ty.to_intern(),
        .flags = .{
            .alignment = try field_ptr_ty.ptr_alignment_advanced(zcu, sema),
            .is_const = field_ptr_info.flags.is_const,
            .is_volatile = field_ptr_info.flags.is_volatile,
            .is_allowzero = field_ptr_info.flags.is_allowzero,
            .address_space = field_ptr_info.flags.address_space,
        },
        .packed_offset = field_ptr_info.packed_offset,
    };
    switch (parent_ty.container_layout(zcu)) {
        .auto => {
            actual_parent_ptr_info.flags.alignment = actual_field_ptr_info.flags.alignment.min_strict(
                if (zcu.type_to_struct(parent_ty)) |struct_obj| try sema.struct_field_alignment(
                    struct_obj.field_align(ip, field_index),
                    field_ty,
                    struct_obj.layout,
                ) else if (zcu.type_to_union(parent_ty)) |union_obj|
                    try sema.union_field_alignment(union_obj, field_index)
                else
                    actual_field_ptr_info.flags.alignment,
            );

            actual_parent_ptr_info.packed_offset = .{ .bit_offset = 0, .host_size = 0 };
            actual_field_ptr_info.packed_offset = .{ .bit_offset = 0, .host_size = 0 };
        },
        .@"extern" => {
            const field_offset = parent_ty.struct_field_offset(field_index, zcu);
            actual_parent_ptr_info.flags.alignment = actual_field_ptr_info.flags.alignment.min_strict(if (field_offset > 0)
                Alignment.from_log2_units(@ctz(field_offset))
            else
                actual_field_ptr_info.flags.alignment);

            actual_parent_ptr_info.packed_offset = .{ .bit_offset = 0, .host_size = 0 };
            actual_field_ptr_info.packed_offset = .{ .bit_offset = 0, .host_size = 0 };
        },
        .@"packed" => {
            const byte_offset = std.math.div_exact(u32, @abs(@as(i32, actual_parent_ptr_info.packed_offset.bit_offset) +
                (if (zcu.type_to_struct(parent_ty)) |struct_obj| zcu.struct_packed_field_bit_offset(struct_obj, field_index) else 0) -
                actual_field_ptr_info.packed_offset.bit_offset), 8) catch
                return sema.fail(block, inst_src, "pointer bit-offset mismatch", .{});
            actual_parent_ptr_info.flags.alignment = actual_field_ptr_info.flags.alignment.min_strict(if (byte_offset > 0)
                Alignment.from_log2_units(@ctz(byte_offset))
            else
                actual_field_ptr_info.flags.alignment);
        },
    }

    const actual_field_ptr_ty = try sema.ptr_type(actual_field_ptr_info);
    const casted_field_ptr = try sema.coerce(block, actual_field_ptr_ty, field_ptr, field_ptr_src);
    const actual_parent_ptr_ty = try sema.ptr_type(actual_parent_ptr_info);

    const result = if (try sema.resolve_defined_value(block, field_ptr_src, casted_field_ptr)) |field_ptr_val| result: {
        switch (parent_ty.zig_type_tag(zcu)) {
            .Struct => switch (parent_ty.container_layout(zcu)) {
                .auto => {},
                .@"extern" => {
                    const byte_offset = parent_ty.struct_field_offset(field_index, zcu);
                    const parent_ptr_val = try sema.ptr_subtract(block, field_ptr_src, field_ptr_val, byte_offset, actual_parent_ptr_ty);
                    break :result Air.interned_to_ref(parent_ptr_val.to_intern());
                },
                .@"packed" => {
                    // Logic lifted from type computation above - I'm just assuming it's correct.
                    // `catch unreachable` since error case handled above.
                    const byte_offset = std.math.div_exact(u32, @abs(@as(i32, actual_parent_ptr_info.packed_offset.bit_offset) +
                        zcu.struct_packed_field_bit_offset(zcu.type_to_struct(parent_ty).?, field_index) -
                        actual_field_ptr_info.packed_offset.bit_offset), 8) catch unreachable;
                    const parent_ptr_val = try sema.ptr_subtract(block, field_ptr_src, field_ptr_val, byte_offset, actual_parent_ptr_ty);
                    break :result Air.interned_to_ref(parent_ptr_val.to_intern());
                },
            },
            .Union => switch (parent_ty.container_layout(zcu)) {
                .auto => {},
                .@"extern", .@"packed" => {
                    // For an extern or packed union, just coerce the pointer.
                    const parent_ptr_val = try zcu.get_coerced(field_ptr_val, actual_parent_ptr_ty);
                    break :result Air.interned_to_ref(parent_ptr_val.to_intern());
                },
            },
            else => unreachable,
        }

        const opt_field: ?InternPool.Key.Ptr.BaseAddr.BaseIndex = opt_field: {
            const ptr = switch (ip.index_to_key(field_ptr_val.to_intern())) {
                .ptr => |ptr| ptr,
                else => break :opt_field null,
            };
            if (ptr.byte_offset != 0) break :opt_field null;
            break :opt_field switch (ptr.base_addr) {
                .field => |field| field,
                else => null,
            };
        };

        const field = opt_field orelse {
            return sema.fail(block, field_ptr_src, "pointer value not based on parent struct", .{});
        };

        if (Value.from_interned(field.base).type_of(zcu).child_type(zcu).to_intern() != parent_ty.to_intern()) {
            return sema.fail(block, field_ptr_src, "pointer value not based on parent struct", .{});
        }

        if (field.index != field_index) {
            return sema.fail(block, inst_src, "field '{}' has index '{d}' but pointer value is index '{d}' of struct '{}'", .{
                field_name.fmt(ip), field_index, field.index, parent_ty.fmt(zcu),
            });
        }
        break :result try sema.coerce(block, actual_parent_ptr_ty, Air.interned_to_ref(field.base), inst_src);
    } else result: {
        try sema.require_runtime_block(block, inst_src, field_ptr_src);
        try sema.queue_full_type_resolution(parent_ty);
        break :result try block.add_inst(.{
            .tag = .field_parent_ptr,
            .data = .{ .ty_pl = .{
                .ty = Air.interned_to_ref(actual_parent_ptr_ty.to_intern()),
                .payload = try block.sema.add_extra(Air.FieldParentPtr{
                    .field_ptr = casted_field_ptr,
                    .field_index = @int_cast(field_index),
                }),
            } },
        });
    };
    return sema.ptr_cast_full(block, flags, inst_src, result, inst_src, parent_ptr_ty, "@fieldParentPtr");
}

fn ptr_subtract(sema: *Sema, block: *Block, src: LazySrcLoc, ptr_val: Value, byte_subtract: u64, new_ty: Type) !Value {
    const zcu = sema.mod;
    if (byte_subtract == 0) return zcu.get_coerced(ptr_val, new_ty);
    var ptr = switch (zcu.intern_pool.index_to_key(ptr_val.to_intern())) {
        .undef => return sema.fail_with_use_of_undef(block, src),
        .ptr => |ptr| ptr,
        else => unreachable,
    };
    if (ptr.byte_offset < byte_subtract) {
        return sema.fail_with_owned_error_msg(block, msg: {
            const msg = try sema.err_msg(block, src, "pointer computation here causes undefined behavior", .{});
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, src, msg, "resulting pointer exceeds bounds of containing value which may trigger overflow", .{});
            break :msg msg;
        });
    }
    ptr.byte_offset -= byte_subtract;
    ptr.ty = new_ty.to_intern();
    return Value.from_interned(try zcu.intern(.{ .ptr = ptr }));
}

fn zir_min_max(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
    comptime air_tag: Air.Inst.Tag,
) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const src = inst_data.src();
    const lhs_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const rhs_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const lhs = try sema.resolve_inst(extra.lhs);
    const rhs = try sema.resolve_inst(extra.rhs);
    try sema.check_numeric_type(block, lhs_src, sema.type_of(lhs));
    try sema.check_numeric_type(block, rhs_src, sema.type_of(rhs));
    return sema.analyze_min_max(block, src, air_tag, &.{ lhs, rhs }, &.{ lhs_src, rhs_src });
}

fn zir_min_max_multi(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
    comptime air_tag: Air.Inst.Tag,
) CompileError!Air.Inst.Ref {
    const extra = sema.code.extra_data(Zir.Inst.NodeMultiOp, extended.operand);
    const src_node = extra.data.src_node;
    const src = LazySrcLoc.nodeOffset(src_node);
    const operands = sema.code.ref_slice(extra.end, extended.small);

    const air_refs = try sema.arena.alloc(Air.Inst.Ref, operands.len);
    const operand_srcs = try sema.arena.alloc(LazySrcLoc, operands.len);

    for (operands, air_refs, operand_srcs, 0..) |zir_ref, *air_ref, *op_src, i| {
        op_src.* = switch (i) {
            0 => .{ .node_offset_builtin_call_arg0 = src_node },
            1 => .{ .node_offset_builtin_call_arg1 = src_node },
            2 => .{ .node_offset_builtin_call_arg2 = src_node },
            3 => .{ .node_offset_builtin_call_arg3 = src_node },
            4 => .{ .node_offset_builtin_call_arg4 = src_node },
            5 => .{ .node_offset_builtin_call_arg5 = src_node },
            else => src, // TODO: better source location
        };
        air_ref.* = try sema.resolve_inst(zir_ref);
        try sema.check_numeric_type(block, op_src.*, sema.type_of(air_ref.*));
    }

    return sema.analyze_min_max(block, src, air_tag, air_refs, operand_srcs);
}

fn analyze_min_max(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    comptime air_tag: Air.Inst.Tag,
    operands: []const Air.Inst.Ref,
    operand_srcs: []const LazySrcLoc,
) CompileError!Air.Inst.Ref {
    assert(operands.len == operand_srcs.len);
    assert(operands.len > 0);
    const mod = sema.mod;

    if (operands.len == 1) return operands[0];

    const opFunc = switch (air_tag) {
        .min => Value.number_min,
        .max => Value.number_max,
        else => @compile_error("unreachable"),
    };

    // The set of runtime-known operands. Set up in the loop below.
    var runtime_known = try std.DynamicBitSet.init_full(sema.arena, operands.len);
    // The current minmax value - initially this will always be comptime-known, then we'll add
    // runtime values into the mix later.
    var cur_minmax: ?Air.Inst.Ref = null;
    var cur_minmax_src: LazySrcLoc = undefined; // defined if cur_minmax not null
    // The current known scalar bounds of the value.
    var bounds_status: enum {
        unknown, // We've only seen undef comptime_ints so far, so do not know the bounds.
        defined, // We've seen only integers, so the bounds are defined.
        non_integral, // There are floats in the mix, so the bounds aren't defined.
    } = .unknown;
    var cur_min_scalar: Value = undefined;
    var cur_max_scalar: Value = undefined;

    // First, find all comptime-known arguments, and get their min/max

    for (operands, operand_srcs, 0..) |operand, operand_src, operand_idx| {
        // Resolve the value now to avoid redundant calls to `check_simd_bin_op` - we'll have to call
        // it in the runtime path anyway since the result type may have been refined
        const unresolved_uncoerced_val = try sema.resolve_value(operand) orelse continue;
        const uncoerced_val = try sema.resolve_lazy_value(unresolved_uncoerced_val);

        runtime_known.unset(operand_idx);

        switch (bounds_status) {
            .unknown, .defined => refine_bounds: {
                const ty = sema.type_of(operand);
                if (!ty.scalar_type(mod).is_int(mod) and !ty.scalar_type(mod).eql(Type.comptime_int, mod)) {
                    bounds_status = .non_integral;
                    break :refine_bounds;
                }
                const scalar_bounds: ?[2]Value = bounds: {
                    if (!ty.is_vector(mod)) break :bounds try uncoerced_val.int_value_bounds(mod);
                    var cur_bounds: [2]Value = try Value.int_value_bounds(try uncoerced_val.elem_value(mod, 0), mod) orelse break :bounds null;
                    const len = try sema.usize_cast(block, src, ty.vector_len(mod));
                    for (1..len) |i| {
                        const elem = try uncoerced_val.elem_value(mod, i);
                        const elem_bounds = try elem.int_value_bounds(mod) orelse break :bounds null;
                        cur_bounds = .{
                            Value.number_min(elem_bounds[0], cur_bounds[0], mod),
                            Value.number_max(elem_bounds[1], cur_bounds[1], mod),
                        };
                    }
                    break :bounds cur_bounds;
                };
                if (scalar_bounds) |bounds| {
                    if (bounds_status == .unknown) {
                        cur_min_scalar = bounds[0];
                        cur_max_scalar = bounds[1];
                        bounds_status = .defined;
                    } else {
                        cur_min_scalar = opFunc(cur_min_scalar, bounds[0], mod);
                        cur_max_scalar = opFunc(cur_max_scalar, bounds[1], mod);
                    }
                }
            },
            .non_integral => {},
        }

        const cur = cur_minmax orelse {
            cur_minmax = operand;
            cur_minmax_src = operand_src;
            continue;
        };

        const simd_op = try sema.check_simd_bin_op(block, src, cur, operand, cur_minmax_src, operand_src);
        const cur_val = try sema.resolve_lazy_value(simd_op.lhs_val.?); // cur_minmax is comptime-known
        const operand_val = try sema.resolve_lazy_value(simd_op.rhs_val.?); // we checked the operand was resolvable above

        const vec_len = simd_op.len orelse {
            const result_val = opFunc(cur_val, operand_val, mod);
            cur_minmax = Air.interned_to_ref(result_val.to_intern());
            continue;
        };
        const elems = try sema.arena.alloc(InternPool.Index, vec_len);
        for (elems, 0..) |*elem, i| {
            const lhs_elem_val = try cur_val.elem_value(mod, i);
            const rhs_elem_val = try operand_val.elem_value(mod, i);
            const uncoerced_elem = opFunc(lhs_elem_val, rhs_elem_val, mod);
            elem.* = (try mod.get_coerced(uncoerced_elem, simd_op.scalar_ty)).to_intern();
        }
        cur_minmax = Air.interned_to_ref((try mod.intern(.{ .aggregate = .{
            .ty = simd_op.result_ty.to_intern(),
            .storage = .{ .elems = elems },
        } })));
    }

    const opt_runtime_idx = runtime_known.find_first_set();

    if (cur_minmax) |ct_minmax_ref| refine: {
        // Refine the comptime-known result type based on the bounds. This isn't strictly necessary
        // in the runtime case, since we'll refine the type again later, but keeping things as small
        // as possible will allow us to emit more optimal AIR (if all the runtime operands have
        // smaller types than the non-refined comptime type).

        const val = (try sema.resolve_value(ct_minmax_ref)).?;
        const orig_ty = sema.type_of(ct_minmax_ref);

        if (opt_runtime_idx == null and orig_ty.scalar_type(mod).eql(Type.comptime_int, mod)) {
            // If all arguments were `comptime_int`, and there are no runtime args, we'll preserve that type
            break :refine;
        }

        // We can't refine float types
        if (orig_ty.scalar_type(mod).is_any_float()) break :refine;

        assert(bounds_status == .defined); // there was a non-comptime-int integral comptime-known arg

        const refined_scalar_ty = try mod.int_fitting_range(cur_min_scalar, cur_max_scalar);
        const refined_ty = if (orig_ty.is_vector(mod)) try mod.vector_type(.{
            .len = orig_ty.vector_len(mod),
            .child = refined_scalar_ty.to_intern(),
        }) else refined_scalar_ty;

        // Apply the refined type to the current value
        if (std.debug.runtime_safety) {
            assert(try sema.int_fits_in_type(val, refined_ty, null));
        }
        cur_minmax = try sema.coerce_in_memory(val, refined_ty);
    }

    const runtime_idx = opt_runtime_idx orelse return cur_minmax.?;
    const runtime_src = operand_srcs[runtime_idx];
    try sema.require_runtime_block(block, src, runtime_src);

    // Now, iterate over runtime operands, emitting a min/max instruction for each. We'll refine the
    // type again at the end, based on the comptime-known bound.

    // If the comptime-known part is undef we can avoid emitting actual instructions later
    const known_undef = if (cur_minmax) |operand| blk: {
        const val = (try sema.resolve_value(operand)).?;
        break :blk val.is_undef(mod);
    } else false;

    if (cur_minmax == null) {
        // No comptime operands - use the first operand as the starting value
        assert(bounds_status == .unknown);
        assert(runtime_idx == 0);
        cur_minmax = operands[0];
        cur_minmax_src = runtime_src;
        runtime_known.unset(0); // don't look at this operand in the loop below
        const scalar_ty = sema.type_of(cur_minmax.?).scalar_type(mod);
        if (scalar_ty.is_int(mod)) {
            cur_min_scalar = try scalar_ty.min_int(mod, scalar_ty);
            cur_max_scalar = try scalar_ty.max_int(mod, scalar_ty);
            bounds_status = .defined;
        } else {
            bounds_status = .non_integral;
        }
    }

    var it = runtime_known.iterator(.{});
    while (it.next()) |idx| {
        const lhs = cur_minmax.?;
        const lhs_src = cur_minmax_src;
        const rhs = operands[idx];
        const rhs_src = operand_srcs[idx];
        const simd_op = try sema.check_simd_bin_op(block, src, lhs, rhs, lhs_src, rhs_src);
        if (known_undef) {
            cur_minmax = try mod.undef_ref(simd_op.result_ty);
        } else {
            cur_minmax = try block.add_bin_op(air_tag, simd_op.lhs, simd_op.rhs);
        }
        // Compute the bounds of this type
        switch (bounds_status) {
            .unknown, .defined => refine_bounds: {
                const scalar_ty = sema.type_of(rhs).scalar_type(mod);
                if (scalar_ty.is_any_float()) {
                    bounds_status = .non_integral;
                    break :refine_bounds;
                }
                const scalar_min = try scalar_ty.min_int(mod, scalar_ty);
                const scalar_max = try scalar_ty.max_int(mod, scalar_ty);
                if (bounds_status == .unknown) {
                    cur_min_scalar = scalar_min;
                    cur_max_scalar = scalar_max;
                    bounds_status = .defined;
                } else {
                    cur_min_scalar = opFunc(cur_min_scalar, scalar_min, mod);
                    cur_max_scalar = opFunc(cur_max_scalar, scalar_max, mod);
                }
            },
            .non_integral => {},
        }
    }

    // Finally, refine the type based on the known bounds.
    const unrefined_ty = sema.type_of(cur_minmax.?);
    if (unrefined_ty.scalar_type(mod).is_any_float()) {
        // We can't refine floats, so we're done.
        return cur_minmax.?;
    }
    assert(bounds_status == .defined); // there were integral runtime operands
    const refined_scalar_ty = try mod.int_fitting_range(cur_min_scalar, cur_max_scalar);
    const refined_ty = if (unrefined_ty.is_vector(mod)) try mod.vector_type(.{
        .len = unrefined_ty.vector_len(mod),
        .child = refined_scalar_ty.to_intern(),
    }) else refined_scalar_ty;

    if (!refined_ty.eql(unrefined_ty, mod)) {
        // We've reduced the type - cast the result down
        return block.add_ty_op(.intcast, refined_ty, cur_minmax.?);
    }

    return cur_minmax.?;
}

fn upgrade_to_array_ptr(sema: *Sema, block: *Block, ptr: Air.Inst.Ref, len: u64) !Air.Inst.Ref {
    const mod = sema.mod;
    const ptr_ty = sema.type_of(ptr);
    const info = ptr_ty.ptr_info(mod);
    if (info.flags.size == .One) {
        // Already an array pointer.
        return ptr;
    }
    const new_ty = try sema.ptr_type(.{
        .child = (try mod.array_type(.{
            .len = len,
            .sentinel = info.sentinel,
            .child = info.child,
        })).to_intern(),
        .flags = .{
            .alignment = info.flags.alignment,
            .is_const = info.flags.is_const,
            .is_volatile = info.flags.is_volatile,
            .is_allowzero = info.flags.is_allowzero,
            .address_space = info.flags.address_space,
        },
    });
    const non_slice_ptr = if (info.flags.size == .Slice)
        try block.add_ty_op(.slice_ptr, ptr_ty.slice_ptr_field_type(mod), ptr)
    else
        ptr;
    return block.add_bit_cast(new_ty, non_slice_ptr);
}

fn zir_memcpy(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const src = inst_data.src();
    const dest_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const src_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const dest_ptr = try sema.resolve_inst(extra.lhs);
    const src_ptr = try sema.resolve_inst(extra.rhs);
    const dest_ty = sema.type_of(dest_ptr);
    const src_ty = sema.type_of(src_ptr);
    const dest_len = try indexable_ptr_len_or_none(sema, block, dest_src, dest_ptr);
    const src_len = try indexable_ptr_len_or_none(sema, block, src_src, src_ptr);
    const target = sema.mod.get_target();
    const mod = sema.mod;

    if (dest_ty.is_const_ptr(mod)) {
        return sema.fail(block, dest_src, "cannot memcpy to constant pointer", .{});
    }

    if (dest_len == .none and src_len == .none) {
        const msg = msg: {
            const msg = try sema.err_msg(block, src, "unknown @memcpy length", .{});
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, dest_src, msg, "destination type '{}' provides no length", .{
                dest_ty.fmt(sema.mod),
            });
            try sema.err_note(block, src_src, msg, "source type '{}' provides no length", .{
                src_ty.fmt(sema.mod),
            });
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    var len_val: ?Value = null;

    if (dest_len != .none and src_len != .none) check: {
        // If we can check at compile-time, no need for runtime safety.
        if (try sema.resolve_defined_value(block, dest_src, dest_len)) |dest_len_val| {
            len_val = dest_len_val;
            if (try sema.resolve_defined_value(block, src_src, src_len)) |src_len_val| {
                if (!(try sema.values_equal(dest_len_val, src_len_val, Type.usize))) {
                    const msg = msg: {
                        const msg = try sema.err_msg(block, src, "non-matching @memcpy lengths", .{});
                        errdefer msg.destroy(sema.gpa);
                        try sema.err_note(block, dest_src, msg, "length {} here", .{
                            dest_len_val.fmt_value(sema.mod, sema),
                        });
                        try sema.err_note(block, src_src, msg, "length {} here", .{
                            src_len_val.fmt_value(sema.mod, sema),
                        });
                        break :msg msg;
                    };
                    return sema.fail_with_owned_error_msg(block, msg);
                }
                break :check;
            }
        } else if (try sema.resolve_defined_value(block, src_src, src_len)) |src_len_val| {
            len_val = src_len_val;
        }

        if (block.want_safety()) {
            const ok = try block.add_bin_op(.cmp_eq, dest_len, src_len);
            try sema.add_safety_check(block, src, ok, .memcpy_len_mismatch);
        }
    } else if (dest_len != .none) {
        if (try sema.resolve_defined_value(block, dest_src, dest_len)) |dest_len_val| {
            len_val = dest_len_val;
        }
    } else if (src_len != .none) {
        if (try sema.resolve_defined_value(block, src_src, src_len)) |src_len_val| {
            len_val = src_len_val;
        }
    }

    const runtime_src = if (try sema.resolve_defined_value(block, dest_src, dest_ptr)) |dest_ptr_val| rs: {
        if (!sema.is_comptime_mutable_ptr(dest_ptr_val)) break :rs dest_src;
        if (try sema.resolve_defined_value(block, src_src, src_ptr)) |_| {
            const len_u64 = (try len_val.?.get_unsigned_int_advanced(mod, sema)).?;
            const len = try sema.usize_cast(block, dest_src, len_u64);
            for (0..len) |i| {
                const elem_index = try mod.int_ref(Type.usize, i);
                const dest_elem_ptr = try sema.elem_ptr_one_layer_only(
                    block,
                    src,
                    dest_ptr,
                    elem_index,
                    src,
                    true, // init
                    false, // oob_safety
                );
                const src_elem_ptr = try sema.elem_ptr_one_layer_only(
                    block,
                    src,
                    src_ptr,
                    elem_index,
                    src,
                    false, // init
                    false, // oob_safety
                );
                const uncoerced_elem = try sema.analyze_load(block, src, src_elem_ptr, src_src);
                try sema.store_ptr2(
                    block,
                    src,
                    dest_elem_ptr,
                    dest_src,
                    uncoerced_elem,
                    src_src,
                    .store,
                );
            }
            return;
        } else break :rs src_src;
    } else dest_src;

    // If in-memory coercion is not allowed, explode this memcpy call into a
    // for loop that copies element-wise.
    // Likewise if this is an iterable rather than a pointer, do the same
    // lowering. The AIR instruction requires pointers with element types of
    // equal ABI size.

    if (dest_ty.zig_type_tag(mod) != .Pointer or src_ty.zig_type_tag(mod) != .Pointer) {
        return sema.fail(block, src, "TODO: lower @memcpy to a for loop because the source or destination iterable is a tuple", .{});
    }

    const dest_elem_ty = dest_ty.elem_type2(mod);
    const src_elem_ty = src_ty.elem_type2(mod);
    if (.ok != try sema.coerce_in_memory_allowed(block, dest_elem_ty, src_elem_ty, true, target, dest_src, src_src)) {
        return sema.fail(block, src, "TODO: lower @memcpy to a for loop because the element types have different ABI sizes", .{});
    }

    // If the length is comptime-known, then upgrade src and destination types
    // into pointer-to-array. At this point we know they are both pointers
    // already.
    var new_dest_ptr = dest_ptr;
    var new_src_ptr = src_ptr;
    if (len_val) |val| {
        const len = try val.to_unsigned_int_advanced(sema);
        if (len == 0) {
            // This AIR instruction guarantees length > 0 if it is comptime-known.
            return;
        }
        new_dest_ptr = try upgrade_to_array_ptr(sema, block, dest_ptr, len);
        new_src_ptr = try upgrade_to_array_ptr(sema, block, src_ptr, len);
    }

    if (dest_len != .none) {
        // Change the src from slice to a many pointer, to avoid multiple ptr
        // slice extractions in AIR instructions.
        const new_src_ptr_ty = sema.type_of(new_src_ptr);
        if (new_src_ptr_ty.is_slice(mod)) {
            new_src_ptr = try sema.analyze_slice_ptr(block, src_src, new_src_ptr, new_src_ptr_ty);
        }
    } else if (dest_len == .none and len_val == null) {
        // Change the dest to a slice, since its type must have the length.
        const dest_ptr_ptr = try sema.analyze_ref(block, dest_src, new_dest_ptr);
        new_dest_ptr = try sema.analyze_slice(block, dest_src, dest_ptr_ptr, .zero, src_len, .none, .unneeded, dest_src, dest_src, dest_src, false);
        const new_src_ptr_ty = sema.type_of(new_src_ptr);
        if (new_src_ptr_ty.is_slice(mod)) {
            new_src_ptr = try sema.analyze_slice_ptr(block, src_src, new_src_ptr, new_src_ptr_ty);
        }
    }

    try sema.require_runtime_block(block, src, runtime_src);

    // Aliasing safety check.
    if (block.want_safety()) {
        const len = if (len_val) |v|
            Air.interned_to_ref(v.to_intern())
        else if (dest_len != .none)
            dest_len
        else
            src_len;

        // Extract raw pointer from dest slice. The AIR instructions could support them, but
        // it would cause redundant machine code instructions.
        const new_dest_ptr_ty = sema.type_of(new_dest_ptr);
        const raw_dest_ptr = if (new_dest_ptr_ty.is_slice(mod))
            try sema.analyze_slice_ptr(block, dest_src, new_dest_ptr, new_dest_ptr_ty)
        else if (new_dest_ptr_ty.ptr_size(mod) == .One) ptr: {
            var dest_manyptr_ty_key = mod.intern_pool.index_to_key(new_dest_ptr_ty.to_intern()).ptr_type;
            assert(dest_manyptr_ty_key.flags.size == .One);
            dest_manyptr_ty_key.child = dest_elem_ty.to_intern();
            dest_manyptr_ty_key.flags.size = .Many;
            break :ptr try sema.coerce_compatible_ptrs(block, try sema.ptr_type(dest_manyptr_ty_key), new_dest_ptr, dest_src);
        } else new_dest_ptr;

        const new_src_ptr_ty = sema.type_of(new_src_ptr);
        const raw_src_ptr = if (new_src_ptr_ty.is_slice(mod))
            try sema.analyze_slice_ptr(block, src_src, new_src_ptr, new_src_ptr_ty)
        else if (new_src_ptr_ty.ptr_size(mod) == .One) ptr: {
            var src_manyptr_ty_key = mod.intern_pool.index_to_key(new_src_ptr_ty.to_intern()).ptr_type;
            assert(src_manyptr_ty_key.flags.size == .One);
            src_manyptr_ty_key.child = src_elem_ty.to_intern();
            src_manyptr_ty_key.flags.size = .Many;
            break :ptr try sema.coerce_compatible_ptrs(block, try sema.ptr_type(src_manyptr_ty_key), new_src_ptr, src_src);
        } else new_src_ptr;

        // ok1: dest >= src + len
        // ok2: src >= dest + len
        const src_plus_len = try sema.analyze_ptr_arithmetic(block, src, raw_src_ptr, len, .ptr_add, src_src, src);
        const dest_plus_len = try sema.analyze_ptr_arithmetic(block, src, raw_dest_ptr, len, .ptr_add, dest_src, src);
        const ok1 = try block.add_bin_op(.cmp_gte, raw_dest_ptr, src_plus_len);
        const ok2 = try block.add_bin_op(.cmp_gte, new_src_ptr, dest_plus_len);
        const ok = try block.add_bin_op(.bool_or, ok1, ok2);
        try sema.add_safety_check(block, src, ok, .memcpy_alias);
    }

    _ = try block.add_inst(.{
        .tag = .memcpy,
        .data = .{ .bin_op = .{
            .lhs = new_dest_ptr,
            .rhs = new_src_ptr,
        } },
    });
}

fn zir_memset(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!void {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.Bin, inst_data.payload_index).data;
    const src = inst_data.src();
    const dest_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = inst_data.src_node };
    const value_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = inst_data.src_node };
    const dest_ptr = try sema.resolve_inst(extra.lhs);
    const uncoerced_elem = try sema.resolve_inst(extra.rhs);
    const dest_ptr_ty = sema.type_of(dest_ptr);
    try check_mem_operand(sema, block, dest_src, dest_ptr_ty);

    if (dest_ptr_ty.is_const_ptr(mod)) {
        return sema.fail(block, dest_src, "cannot memset constant pointer", .{});
    }

    const dest_elem_ty: Type = dest_elem_ty: {
        const ptr_info = dest_ptr_ty.ptr_info(mod);
        switch (ptr_info.flags.size) {
            .Slice => break :dest_elem_ty Type.from_interned(ptr_info.child),
            .One => {
                if (Type.from_interned(ptr_info.child).zig_type_tag(mod) == .Array) {
                    break :dest_elem_ty Type.from_interned(ptr_info.child).child_type(mod);
                }
            },
            .Many, .C => {},
        }
        return sema.fail_with_owned_error_msg(block, msg: {
            const msg = try sema.err_msg(block, src, "unknown @memset length", .{});
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, dest_src, msg, "destination type '{}' provides no length", .{
                dest_ptr_ty.fmt(mod),
            });
            break :msg msg;
        });
    };

    const elem = try sema.coerce(block, dest_elem_ty, uncoerced_elem, value_src);

    const runtime_src = rs: {
        const ptr_val = try sema.resolve_defined_value(block, dest_src, dest_ptr) orelse break :rs dest_src;
        const len_air_ref = try sema.field_val(block, src, dest_ptr, try ip.get_or_put_string(gpa, "len", .no_embedded_nulls), dest_src);
        const len_val = (try sema.resolve_defined_value(block, dest_src, len_air_ref)) orelse break :rs dest_src;
        const len_u64 = (try len_val.get_unsigned_int_advanced(mod, sema)).?;
        const len = try sema.usize_cast(block, dest_src, len_u64);
        if (len == 0) {
            // This AIR instruction guarantees length > 0 if it is comptime-known.
            return;
        }

        if (!sema.is_comptime_mutable_ptr(ptr_val)) break :rs dest_src;
        const elem_val = try sema.resolve_value(elem) orelse break :rs value_src;
        const array_ty = try mod.array_type(.{
            .child = dest_elem_ty.to_intern(),
            .len = len_u64,
        });
        const array_val = Value.from_interned((try mod.intern(.{ .aggregate = .{
            .ty = array_ty.to_intern(),
            .storage = .{ .repeated_elem = elem_val.to_intern() },
        } })));
        const array_ptr_ty = ty: {
            var info = dest_ptr_ty.ptr_info(mod);
            info.flags.size = .One;
            info.child = array_ty.to_intern();
            break :ty try mod.ptr_type(info);
        };
        const raw_ptr_val = if (dest_ptr_ty.is_slice(mod)) ptr_val.slice_ptr(mod) else ptr_val;
        const array_ptr_val = try mod.get_coerced(raw_ptr_val, array_ptr_ty);
        return sema.store_ptr_val(block, src, array_ptr_val, array_val, array_ty);
    };

    try sema.require_runtime_block(block, src, runtime_src);
    _ = try block.add_inst(.{
        .tag = if (block.want_safety()) .memset_safe else .memset,
        .data = .{ .bin_op = .{
            .lhs = dest_ptr,
            .rhs = elem,
        } },
    });
}

fn zir_builtin_async_call(sema: *Sema, block: *Block, extended: Zir.Inst.Extended.InstData) CompileError!Air.Inst.Ref {
    const extra = sema.code.extra_data(Zir.Inst.UnNode, extended.operand).data;
    const src = LazySrcLoc.nodeOffset(extra.node);
    return sema.fail_with_use_of_async(block, src);
}

fn zir_resume(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();
    return sema.fail_with_use_of_async(block, src);
}

fn zir_await(
    sema: *Sema,
    block: *Block,
    inst: Zir.Inst.Index,
) CompileError!Air.Inst.Ref {
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].un_node;
    const src = inst_data.src();

    return sema.fail_with_use_of_async(block, src);
}

fn zir_await_nosuspend(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    const extra = sema.code.extra_data(Zir.Inst.UnNode, extended.operand).data;
    const src = LazySrcLoc.nodeOffset(extra.node);

    return sema.fail_with_use_of_async(block, src);
}

fn zir_var_extended(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const extra = sema.code.extra_data(Zir.Inst.ExtendedVar, extended.operand);
    const ty_src: LazySrcLoc = .{ .node_offset_var_decl_ty = 0 };
    const init_src: LazySrcLoc = .{ .node_offset_var_decl_init = 0 };
    const small: Zir.Inst.ExtendedVar.Small = @bit_cast(extended.small);

    var extra_index: usize = extra.end;

    const lib_name = if (small.has_lib_name) lib_name: {
        const lib_name_index: Zir.NullTerminatedString = @enumFromInt(sema.code.extra[extra_index]);
        const lib_name = sema.code.null_terminated_string(lib_name_index);
        extra_index += 1;
        try sema.handle_extern_lib_name(block, ty_src, lib_name);
        break :lib_name lib_name;
    } else null;

    // ZIR supports encoding this information but it is not used; the information
    // is encoded via the Decl entry.
    assert(!small.has_align);

    const uncasted_init: Air.Inst.Ref = if (small.has_init) blk: {
        const init_ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
        extra_index += 1;
        break :blk try sema.resolve_inst(init_ref);
    } else .none;

    const have_ty = extra.data.var_type != .none;
    const var_ty = if (have_ty)
        try sema.resolve_type(block, ty_src, extra.data.var_type)
    else
        sema.type_of(uncasted_init);

    const init_val = if (uncasted_init != .none) blk: {
        const init = if (have_ty)
            try sema.coerce(block, var_ty, uncasted_init, init_src)
        else
            uncasted_init;

        break :blk ((try sema.resolve_value(init)) orelse {
            return sema.fail_with_needed_comptime(block, init_src, .{
                .needed_comptime_reason = "container level variable initializers must be comptime-known",
            });
        }).to_intern();
    } else .none;

    try sema.validate_var_type(block, ty_src, var_ty, small.is_extern);

    return Air.interned_to_ref((try mod.intern(.{ .variable = .{
        .ty = var_ty.to_intern(),
        .init = init_val,
        .decl = sema.owner_decl_index,
        .lib_name = try mod.intern_pool.get_or_put_string_opt(sema.gpa, lib_name, .no_embedded_nulls),
        .is_extern = small.is_extern,
        .is_const = small.is_const,
        .is_threadlocal = small.is_threadlocal,
        .is_weak_linkage = false,
    } })));
}

fn zir_func_fancy(sema: *Sema, block: *Block, inst: Zir.Inst.Index) CompileError!Air.Inst.Ref {
    const tracy = trace(@src());
    defer tracy.end();

    const mod = sema.mod;
    const inst_data = sema.code.instructions.items(.data)[@int_from_enum(inst)].pl_node;
    const extra = sema.code.extra_data(Zir.Inst.FuncFancy, inst_data.payload_index);
    const target = mod.get_target();

    const align_src: LazySrcLoc = .{ .node_offset_fn_type_align = inst_data.src_node };
    const addrspace_src: LazySrcLoc = .{ .node_offset_fn_type_addrspace = inst_data.src_node };
    const section_src: LazySrcLoc = .{ .node_offset_fn_type_section = inst_data.src_node };
    const cc_src: LazySrcLoc = .{ .node_offset_fn_type_cc = inst_data.src_node };
    const ret_src: LazySrcLoc = .{ .node_offset_fn_type_ret_ty = inst_data.src_node };
    const has_body = extra.data.body_len != 0;

    var extra_index: usize = extra.end;

    const lib_name: ?[]const u8 = if (extra.data.bits.has_lib_name) blk: {
        const lib_name_index: Zir.NullTerminatedString = @enumFromInt(sema.code.extra[extra_index]);
        const lib_name = sema.code.null_terminated_string(lib_name_index);
        extra_index += 1;
        break :blk lib_name;
    } else null;

    if (has_body and
        (extra.data.bits.has_align_body or extra.data.bits.has_align_ref) and
        !target_util.supports_function_alignment(target))
    {
        return sema.fail(block, align_src, "target does not support function alignment", .{});
    }

    const @"align": ?Alignment = if (extra.data.bits.has_align_body) blk: {
        const body_len = sema.code.extra[extra_index];
        extra_index += 1;
        const body = sema.code.body_slice(extra_index, body_len);
        extra_index += body.len;

        const val = try sema.resolve_generic_body(block, align_src, body, inst, Type.u29, .{
            .needed_comptime_reason = "alignment must be comptime-known",
        });
        if (val.is_generic_poison()) {
            break :blk null;
        }
        const alignment = try sema.validate_align_allow_zero(block, align_src, try val.to_unsigned_int_advanced(sema));
        const default = target_util.default_function_alignment(target);
        break :blk if (alignment == default) .none else alignment;
    } else if (extra.data.bits.has_align_ref) blk: {
        const align_ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
        extra_index += 1;
        const uncoerced_align = sema.resolve_inst(align_ref) catch |err| switch (err) {
            error.GenericPoison => break :blk null,
            else => |e| return e,
        };
        const coerced_align = sema.coerce(block, Type.u29, uncoerced_align, align_src) catch |err| switch (err) {
            error.GenericPoison => break :blk null,
            else => |e| return e,
        };
        const align_val = sema.resolve_const_defined_value(block, align_src, coerced_align, .{
            .needed_comptime_reason = "alignment must be comptime-known",
        }) catch |err| switch (err) {
            error.GenericPoison => break :blk null,
            else => |e| return e,
        };
        const alignment = try sema.validate_align_allow_zero(block, align_src, try align_val.to_unsigned_int_advanced(sema));
        const default = target_util.default_function_alignment(target);
        break :blk if (alignment == default) .none else alignment;
    } else .none;

    const @"addrspace": ?std.builtin.AddressSpace = if (extra.data.bits.has_addrspace_body) blk: {
        const body_len = sema.code.extra[extra_index];
        extra_index += 1;
        const body = sema.code.body_slice(extra_index, body_len);
        extra_index += body.len;

        const addrspace_ty = Type.from_interned(.address_space_type);
        const val = try sema.resolve_generic_body(block, addrspace_src, body, inst, addrspace_ty, .{
            .needed_comptime_reason = "addrspace must be comptime-known",
        });
        if (val.is_generic_poison()) {
            break :blk null;
        }
        break :blk mod.to_enum(std.builtin.AddressSpace, val);
    } else if (extra.data.bits.has_addrspace_ref) blk: {
        const addrspace_ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
        extra_index += 1;
        const addrspace_ty = Type.from_interned(.address_space_type);
        const uncoerced_addrspace = sema.resolve_inst(addrspace_ref) catch |err| switch (err) {
            error.GenericPoison => break :blk null,
            else => |e| return e,
        };
        const coerced_addrspace = sema.coerce(block, addrspace_ty, uncoerced_addrspace, addrspace_src) catch |err| switch (err) {
            error.GenericPoison => break :blk null,
            else => |e| return e,
        };
        const addrspace_val = sema.resolve_const_defined_value(block, addrspace_src, coerced_addrspace, .{
            .needed_comptime_reason = "addrspace must be comptime-known",
        }) catch |err| switch (err) {
            error.GenericPoison => break :blk null,
            else => |e| return e,
        };
        break :blk mod.to_enum(std.builtin.AddressSpace, addrspace_val);
    } else target_util.default_address_space(target, .function);

    const section: Section = if (extra.data.bits.has_section_body) blk: {
        const body_len = sema.code.extra[extra_index];
        extra_index += 1;
        const body = sema.code.body_slice(extra_index, body_len);
        extra_index += body.len;

        const ty = Type.slice_const_u8;
        const val = try sema.resolve_generic_body(block, section_src, body, inst, ty, .{
            .needed_comptime_reason = "linksection must be comptime-known",
        });
        if (val.is_generic_poison()) {
            break :blk .generic;
        }
        break :blk .{ .explicit = try sema.slice_to_ip_string(block, section_src, val, .{
            .needed_comptime_reason = "linksection must be comptime-known",
        }) };
    } else if (extra.data.bits.has_section_ref) blk: {
        const section_ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
        extra_index += 1;
        const section_name = sema.resolve_const_string_intern(block, section_src, section_ref, .{
            .needed_comptime_reason = "linksection must be comptime-known",
        }) catch |err| switch (err) {
            error.GenericPoison => {
                break :blk .generic;
            },
            else => |e| return e,
        };
        break :blk .{ .explicit = section_name };
    } else .default;

    const cc: ?std.builtin.CallingConvention = if (extra.data.bits.has_cc_body) blk: {
        const body_len = sema.code.extra[extra_index];
        extra_index += 1;
        const body = sema.code.body_slice(extra_index, body_len);
        extra_index += body.len;

        const cc_ty = try sema.get_builtin_type("CallingConvention");
        const val = try sema.resolve_generic_body(block, cc_src, body, inst, cc_ty, .{
            .needed_comptime_reason = "calling convention must be comptime-known",
        });
        if (val.is_generic_poison()) {
            break :blk null;
        }
        break :blk mod.to_enum(std.builtin.CallingConvention, val);
    } else if (extra.data.bits.has_cc_ref) blk: {
        const cc_ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
        extra_index += 1;
        const cc_ty = Type.from_interned(.calling_convention_type);
        const uncoerced_cc = sema.resolve_inst(cc_ref) catch |err| switch (err) {
            error.GenericPoison => break :blk null,
            else => |e| return e,
        };
        const coerced_cc = sema.coerce(block, cc_ty, uncoerced_cc, cc_src) catch |err| switch (err) {
            error.GenericPoison => break :blk null,
            else => |e| return e,
        };
        const cc_val = sema.resolve_const_defined_value(block, cc_src, coerced_cc, .{
            .needed_comptime_reason = "calling convention must be comptime-known",
        }) catch |err| switch (err) {
            error.GenericPoison => break :blk null,
            else => |e| return e,
        };
        break :blk mod.to_enum(std.builtin.CallingConvention, cc_val);
    } else if (sema.owner_decl.is_exported and has_body)
        .C
    else
        .Unspecified;

    const ret_ty: Type = if (extra.data.bits.has_ret_ty_body) blk: {
        const body_len = sema.code.extra[extra_index];
        extra_index += 1;
        const body = sema.code.body_slice(extra_index, body_len);
        extra_index += body.len;

        const val = try sema.resolve_generic_body(block, ret_src, body, inst, Type.type, .{
            .needed_comptime_reason = "return type must be comptime-known",
        });
        const ty = val.to_type();
        break :blk ty;
    } else if (extra.data.bits.has_ret_ty_ref) blk: {
        const ret_ty_ref: Zir.Inst.Ref = @enumFromInt(sema.code.extra[extra_index]);
        extra_index += 1;
        const ret_ty_val = sema.resolve_inst_const(block, ret_src, ret_ty_ref, .{
            .needed_comptime_reason = "return type must be comptime-known",
        }) catch |err| switch (err) {
            error.GenericPoison => {
                break :blk Type.generic_poison;
            },
            else => |e| return e,
        };
        break :blk ret_ty_val.to_type();
    } else Type.void;

    const noalias_bits: u32 = if (extra.data.bits.has_any_noalias) blk: {
        const x = sema.code.extra[extra_index];
        extra_index += 1;
        break :blk x;
    } else 0;

    var src_locs: Zir.Inst.Func.SrcLocs = undefined;
    if (has_body) {
        extra_index += extra.data.body_len;
        src_locs = sema.code.extra_data(Zir.Inst.Func.SrcLocs, extra_index).data;
    }

    const is_var_args = extra.data.bits.is_var_args;
    const is_inferred_error = extra.data.bits.is_inferred_error;
    const is_extern = extra.data.bits.is_extern;
    const is_noinline = extra.data.bits.is_noinline;

    return sema.func_common(
        block,
        inst_data.src_node,
        inst,
        @"align",
        @"addrspace",
        section,
        cc,
        ret_ty,
        is_var_args,
        is_inferred_error,
        is_extern,
        has_body,
        src_locs,
        lib_name,
        noalias_bits,
        is_noinline,
    );
}

fn zir_cundef(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    const extra = sema.code.extra_data(Zir.Inst.UnNode, extended.operand).data;
    const src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };

    const name = try sema.resolve_const_string(block, src, extra.operand, .{
        .needed_comptime_reason = "name of macro being undefined must be comptime-known",
    });
    try block.c_import_buf.?.writer().print("#undef {s}\n", .{name});
    return .void_value;
}

fn zir_cinclude(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    const extra = sema.code.extra_data(Zir.Inst.UnNode, extended.operand).data;
    const src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };

    const name = try sema.resolve_const_string(block, src, extra.operand, .{
        .needed_comptime_reason = "path being included must be comptime-known",
    });
    try block.c_import_buf.?.writer().print("#include <{s}>\n", .{name});
    return .void_value;
}

fn zir_cdefine(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const extra = sema.code.extra_data(Zir.Inst.BinNode, extended.operand).data;
    const name_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const val_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = extra.node };

    const name = try sema.resolve_const_string(block, name_src, extra.lhs, .{
        .needed_comptime_reason = "name of macro being undefined must be comptime-known",
    });
    const rhs = try sema.resolve_inst(extra.rhs);
    if (sema.type_of(rhs).zig_type_tag(mod) != .Void) {
        const value = try sema.resolve_const_string(block, val_src, extra.rhs, .{
            .needed_comptime_reason = "value of macro being undefined must be comptime-known",
        });
        try block.c_import_buf.?.writer().print("#define {s} {s}\n", .{ name, value });
    } else {
        try block.c_import_buf.?.writer().print("#define {s}\n", .{name});
    }
    return .void_value;
}

fn zir_wasm_memory_size(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    const extra = sema.code.extra_data(Zir.Inst.UnNode, extended.operand).data;
    const index_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const builtin_src = LazySrcLoc.nodeOffset(extra.node);
    const target = sema.mod.get_target();
    if (!target.is_wasm()) {
        return sema.fail(block, builtin_src, "builtin @wasmMemorySize is available when targeting WebAssembly; targeted CPU architecture is {s}", .{@tag_name(target.cpu.arch)});
    }

    const index: u32 = @int_cast(try sema.resolve_int(block, index_src, extra.operand, Type.u32, .{
        .needed_comptime_reason = "wasm memory size index must be comptime-known",
    }));
    try sema.require_runtime_block(block, builtin_src, null);
    return block.add_inst(.{
        .tag = .wasm_memory_size,
        .data = .{ .pl_op = .{
            .operand = .none,
            .payload = index,
        } },
    });
}

fn zir_wasm_memory_grow(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    const extra = sema.code.extra_data(Zir.Inst.BinNode, extended.operand).data;
    const builtin_src = LazySrcLoc.nodeOffset(extra.node);
    const index_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const delta_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = extra.node };
    const target = sema.mod.get_target();
    if (!target.is_wasm()) {
        return sema.fail(block, builtin_src, "builtin @wasmMemoryGrow is available when targeting WebAssembly; targeted CPU architecture is {s}", .{@tag_name(target.cpu.arch)});
    }

    const index: u32 = @int_cast(try sema.resolve_int(block, index_src, extra.lhs, Type.u32, .{
        .needed_comptime_reason = "wasm memory size index must be comptime-known",
    }));
    const delta = try sema.coerce(block, Type.usize, try sema.resolve_inst(extra.rhs), delta_src);

    try sema.require_runtime_block(block, builtin_src, null);
    return block.add_inst(.{
        .tag = .wasm_memory_grow,
        .data = .{ .pl_op = .{
            .operand = delta,
            .payload = index,
        } },
    });
}

fn resolve_prefetch_options(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    zir_ref: Zir.Inst.Ref,
) CompileError!std.builtin.PrefetchOptions {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;
    const options_ty = try sema.get_builtin_type("PrefetchOptions");
    const options = try sema.coerce(block, options_ty, try sema.resolve_inst(zir_ref), src);

    const rw_src = sema.maybe_options_src(block, src, "rw");
    const locality_src = sema.maybe_options_src(block, src, "locality");
    const cache_src = sema.maybe_options_src(block, src, "cache");

    const rw = try sema.field_val(block, src, options, try ip.get_or_put_string(gpa, "rw", .no_embedded_nulls), rw_src);
    const rw_val = try sema.resolve_const_defined_value(block, rw_src, rw, .{
        .needed_comptime_reason = "prefetch read/write must be comptime-known",
    });

    const locality = try sema.field_val(block, src, options, try ip.get_or_put_string(gpa, "locality", .no_embedded_nulls), locality_src);
    const locality_val = try sema.resolve_const_defined_value(block, locality_src, locality, .{
        .needed_comptime_reason = "prefetch locality must be comptime-known",
    });

    const cache = try sema.field_val(block, src, options, try ip.get_or_put_string(gpa, "cache", .no_embedded_nulls), cache_src);
    const cache_val = try sema.resolve_const_defined_value(block, cache_src, cache, .{
        .needed_comptime_reason = "prefetch cache must be comptime-known",
    });

    return std.builtin.PrefetchOptions{
        .rw = mod.to_enum(std.builtin.PrefetchOptions.Rw, rw_val),
        .locality = @int_cast(try locality_val.to_unsigned_int_advanced(sema)),
        .cache = mod.to_enum(std.builtin.PrefetchOptions.Cache, cache_val),
    };
}

fn zir_prefetch(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    const extra = sema.code.extra_data(Zir.Inst.BinNode, extended.operand).data;
    const ptr_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const opts_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = extra.node };
    const ptr = try sema.resolve_inst(extra.lhs);
    try sema.check_ptr_operand(block, ptr_src, sema.type_of(ptr));

    const options = sema.resolve_prefetch_options(block, .unneeded, extra.rhs) catch |err| switch (err) {
        error.NeededSourceLocation => {
            _ = try sema.resolve_prefetch_options(block, opts_src, extra.rhs);
            unreachable;
        },
        else => |e| return e,
    };

    if (!block.is_comptime) {
        _ = try block.add_inst(.{
            .tag = .prefetch,
            .data = .{ .prefetch = .{
                .ptr = ptr,
                .rw = options.rw,
                .locality = options.locality,
                .cache = options.cache,
            } },
        });
    }

    return .void_value;
}

fn resolve_extern_options(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    zir_ref: Zir.Inst.Ref,
) CompileError!struct {
    name: InternPool.NullTerminatedString,
    library_name: InternPool.OptionalNullTerminatedString = .none,
    linkage: std.builtin.GlobalLinkage = .strong,
    is_thread_local: bool = false,
} {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;
    const options_inst = try sema.resolve_inst(zir_ref);
    const extern_options_ty = try sema.get_builtin_type("ExternOptions");
    const options = try sema.coerce(block, extern_options_ty, options_inst, src);

    const name_src = sema.maybe_options_src(block, src, "name");
    const library_src = sema.maybe_options_src(block, src, "library");
    const linkage_src = sema.maybe_options_src(block, src, "linkage");
    const thread_local_src = sema.maybe_options_src(block, src, "thread_local");

    const name_ref = try sema.field_val(block, src, options, try ip.get_or_put_string(gpa, "name", .no_embedded_nulls), name_src);
    const name = try sema.to_const_string(block, name_src, name_ref, .{
        .needed_comptime_reason = "name of the extern symbol must be comptime-known",
    });

    const library_name_inst = try sema.field_val(block, src, options, try ip.get_or_put_string(gpa, "library_name", .no_embedded_nulls), library_src);
    const library_name_val = try sema.resolve_const_defined_value(block, library_src, library_name_inst, .{
        .needed_comptime_reason = "library in which extern symbol is must be comptime-known",
    });

    const linkage_ref = try sema.field_val(block, src, options, try ip.get_or_put_string(gpa, "linkage", .no_embedded_nulls), linkage_src);
    const linkage_val = try sema.resolve_const_defined_value(block, linkage_src, linkage_ref, .{
        .needed_comptime_reason = "linkage of the extern symbol must be comptime-known",
    });
    const linkage = mod.to_enum(std.builtin.GlobalLinkage, linkage_val);

    const is_thread_local = try sema.field_val(block, src, options, try ip.get_or_put_string(gpa, "is_thread_local", .no_embedded_nulls), thread_local_src);
    const is_thread_local_val = try sema.resolve_const_defined_value(block, thread_local_src, is_thread_local, .{
        .needed_comptime_reason = "threadlocality of the extern symbol must be comptime-known",
    });

    const library_name = if (library_name_val.optional_value(mod)) |library_name_payload| library_name: {
        const library_name = try sema.to_const_string(block, library_src, Air.interned_to_ref(library_name_payload.to_intern()), .{
            .needed_comptime_reason = "library in which extern symbol is must be comptime-known",
        });
        if (library_name.len == 0) {
            return sema.fail(block, library_src, "library name cannot be empty", .{});
        }
        try sema.handle_extern_lib_name(block, library_src, library_name);
        break :library_name library_name;
    } else null;

    if (name.len == 0) {
        return sema.fail(block, name_src, "extern symbol name cannot be empty", .{});
    }

    if (linkage != .weak and linkage != .strong) {
        return sema.fail(block, linkage_src, "extern symbol must use strong or weak linkage", .{});
    }

    return .{
        .name = try ip.get_or_put_string(gpa, name, .no_embedded_nulls),
        .library_name = try ip.get_or_put_string_opt(gpa, library_name, .no_embedded_nulls),
        .linkage = linkage,
        .is_thread_local = is_thread_local_val.to_bool(),
    };
}

fn zir_builtin_extern(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const extra = sema.code.extra_data(Zir.Inst.BinNode, extended.operand).data;
    const ty_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const options_src: LazySrcLoc = .{ .node_offset_builtin_call_arg1 = extra.node };

    var ty = try sema.resolve_type(block, ty_src, extra.lhs);
    if (!ty.is_ptr_at_runtime(mod)) {
        return sema.fail(block, ty_src, "expected (optional) pointer", .{});
    }
    if (!try sema.validate_extern_type(ty, .other)) {
        const msg = msg: {
            const msg = try sema.err_msg(block, ty_src, "extern symbol cannot have type '{}'", .{ty.fmt(mod)});
            errdefer msg.destroy(sema.gpa);
            const src_decl = sema.mod.decl_ptr(block.src_decl);
            try sema.explain_why_type_is_not_extern(msg, src_decl.to_src_loc(ty_src, mod), ty, .other);
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    const options = sema.resolve_extern_options(block, .unneeded, extra.rhs) catch |err| switch (err) {
        error.NeededSourceLocation => {
            _ = try sema.resolve_extern_options(block, options_src, extra.rhs);
            unreachable;
        },
        else => |e| return e,
    };

    if (options.linkage == .weak and !ty.ptr_allows_zero(mod)) {
        ty = try mod.optional_type(ty.to_intern());
    }
    const ptr_info = ty.ptr_info(mod);

    const new_decl_index = try mod.allocate_new_decl(sema.owner_decl.src_namespace, sema.owner_decl.src_node);
    errdefer mod.destroy_decl(new_decl_index);
    const new_decl = mod.decl_ptr(new_decl_index);
    try mod.init_new_anon_decl(
        new_decl_index,
        sema.owner_decl.src_line,
        Value.from_interned(
            if (Type.from_interned(ptr_info.child).zig_type_tag(mod) == .Fn)
                try ip.get_extern_func(sema.gpa, .{
                    .ty = ptr_info.child,
                    .decl = new_decl_index,
                    .lib_name = options.library_name,
                })
            else
                try mod.intern(.{ .variable = .{
                    .ty = ptr_info.child,
                    .init = .none,
                    .decl = new_decl_index,
                    .lib_name = options.library_name,
                    .is_extern = true,
                    .is_const = ptr_info.flags.is_const,
                    .is_threadlocal = options.is_thread_local,
                    .is_weak_linkage = options.linkage == .weak,
                } }),
        ),
        options.name,
    );
    new_decl.owns_tv = true;
    // Note that this will queue the anon decl for codegen, so that the backend can
    // correctly handle the extern, including duplicate detection.
    try mod.finalize_anon_decl(new_decl_index);

    return Air.interned_to_ref((try mod.get_coerced(Value.from_interned((try mod.intern(.{ .ptr = .{
        .ty = switch (ip.index_to_key(ty.to_intern())) {
            .ptr_type => ty.to_intern(),
            .opt_type => |child_type| child_type,
            else => unreachable,
        },
        .base_addr = .{ .decl = new_decl_index },
        .byte_offset = 0,
    } }))), ty)).to_intern());
}

fn zir_work_item(
    sema: *Sema,
    block: *Block,
    extended: Zir.Inst.Extended.InstData,
    zir_tag: Zir.Inst.Extended,
) CompileError!Air.Inst.Ref {
    const extra = sema.code.extra_data(Zir.Inst.UnNode, extended.operand).data;
    const dimension_src: LazySrcLoc = .{ .node_offset_builtin_call_arg0 = extra.node };
    const builtin_src = LazySrcLoc.nodeOffset(extra.node);
    const target = sema.mod.get_target();

    switch (target.cpu.arch) {
        // TODO: Allow for other GPU targets.
        .amdgcn => {},
        else => {
            return sema.fail(block, builtin_src, "builtin only available on GPU targets; targeted architecture is {s}", .{@tag_name(target.cpu.arch)});
        },
    }

    const dimension: u32 = @int_cast(try sema.resolve_int(block, dimension_src, extra.operand, Type.u32, .{
        .needed_comptime_reason = "dimension must be comptime-known",
    }));
    try sema.require_runtime_block(block, builtin_src, null);

    return block.add_inst(.{
        .tag = switch (zir_tag) {
            .work_item_id => .work_item_id,
            .work_group_size => .work_group_size,
            .work_group_id => .work_group_id,
            else => unreachable,
        },
        .data = .{ .pl_op = .{
            .operand = .none,
            .payload = dimension,
        } },
    });
}

fn zir_in_comptime(
    sema: *Sema,
    block: *Block,
) CompileError!Air.Inst.Ref {
    _ = sema;
    return if (block.is_comptime) .bool_true else .bool_false;
}

fn require_runtime_block(sema: *Sema, block: *Block, src: LazySrcLoc, runtime_src: ?LazySrcLoc) !void {
    if (block.is_comptime) {
        const msg = msg: {
            const msg = try sema.err_msg(block, src, "unable to evaluate comptime expression", .{});
            errdefer msg.destroy(sema.gpa);

            if (runtime_src) |some| {
                try sema.err_note(block, some, msg, "operation is runtime due to this operand", .{});
            }
            if (block.comptime_reason) |some| {
                try some.explain(sema, msg);
            }
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }
}

/// Emit a compile error if type cannot be used for a runtime variable.
fn validate_var_type(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    var_ty: Type,
    is_extern: bool,
) CompileError!void {
    const mod = sema.mod;
    if (is_extern) {
        if (!try sema.validate_extern_type(var_ty, .other)) {
            const msg = msg: {
                const msg = try sema.err_msg(block, src, "extern variable cannot have type '{}'", .{var_ty.fmt(mod)});
                errdefer msg.destroy(sema.gpa);
                const src_decl = mod.decl_ptr(block.src_decl);
                try sema.explain_why_type_is_not_extern(msg, src_decl.to_src_loc(src, mod), var_ty, .other);
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        }
    } else {
        if (var_ty.zig_type_tag(mod) == .Opaque) {
            return sema.fail(
                block,
                src,
                "non-extern variable with opaque type '{}'",
                .{var_ty.fmt(mod)},
            );
        }
    }

    if (!try sema.type_requires_comptime(var_ty)) return;

    const msg = msg: {
        const msg = try sema.err_msg(block, src, "variable of type '{}' must be const or comptime", .{var_ty.fmt(mod)});
        errdefer msg.destroy(sema.gpa);

        const src_decl = mod.decl_ptr(block.src_decl);
        try sema.explain_why_type_is_comptime(msg, src_decl.to_src_loc(src, mod), var_ty);
        if (var_ty.zig_type_tag(mod) == .ComptimeInt or var_ty.zig_type_tag(mod) == .ComptimeFloat) {
            try sema.err_note(block, src, msg, "to modify this variable at runtime, it must be given an explicit fixed-size number type", .{});
        }

        break :msg msg;
    };
    return sema.fail_with_owned_error_msg(block, msg);
}

const TypeSet = std.AutoHashMapUnmanaged(InternPool.Index, void);

fn explain_why_type_is_comptime(
    sema: *Sema,
    msg: *Module.ErrorMsg,
    src_loc: Module.SrcLoc,
    ty: Type,
) CompileError!void {
    var type_set = TypeSet{};
    defer type_set.deinit(sema.gpa);

    try sema.resolve_type_fully(ty);
    return sema.explain_why_type_is_comptime_inner(msg, src_loc, ty, &type_set);
}

fn explain_why_type_is_comptime_inner(
    sema: *Sema,
    msg: *Module.ErrorMsg,
    src_loc: Module.SrcLoc,
    ty: Type,
    type_set: *TypeSet,
) CompileError!void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    switch (ty.zig_type_tag(mod)) {
        .Bool,
        .Int,
        .Float,
        .ErrorSet,
        .Enum,
        .Frame,
        .AnyFrame,
        .Void,
        => return,

        .Fn => {
            try mod.err_note_non_lazy(src_loc, msg, "use '*const {}' for a function pointer type", .{
                ty.fmt(sema.mod),
            });
        },

        .Type => {
            try mod.err_note_non_lazy(src_loc, msg, "types are not available at runtime", .{});
        },

        .ComptimeFloat,
        .ComptimeInt,
        .EnumLiteral,
        .NoReturn,
        .Undefined,
        .Null,
        => return,

        .Opaque => {
            try mod.err_note_non_lazy(src_loc, msg, "opaque type '{}' has undefined size", .{ty.fmt(sema.mod)});
        },

        .Array, .Vector => {
            try sema.explain_why_type_is_comptime_inner(msg, src_loc, ty.child_type(mod), type_set);
        },
        .Pointer => {
            const elem_ty = ty.elem_type2(mod);
            if (elem_ty.zig_type_tag(mod) == .Fn) {
                const fn_info = mod.type_to_func(elem_ty).?;
                if (fn_info.is_generic) {
                    try mod.err_note_non_lazy(src_loc, msg, "function is generic", .{});
                }
                switch (fn_info.cc) {
                    .Inline => try mod.err_note_non_lazy(src_loc, msg, "function has inline calling convention", .{}),
                    else => {},
                }
                if (Type.from_interned(fn_info.return_type).comptime_only(mod)) {
                    try mod.err_note_non_lazy(src_loc, msg, "function has a comptime-only return type", .{});
                }
                return;
            }
            try sema.explain_why_type_is_comptime_inner(msg, src_loc, ty.child_type(mod), type_set);
        },

        .Optional => {
            try sema.explain_why_type_is_comptime_inner(msg, src_loc, ty.optional_child(mod), type_set);
        },
        .ErrorUnion => {
            try sema.explain_why_type_is_comptime_inner(msg, src_loc, ty.error_union_payload(mod), type_set);
        },

        .Struct => {
            if ((try type_set.get_or_put(sema.gpa, ty.to_intern())).found_existing) return;

            if (mod.type_to_struct(ty)) |struct_type| {
                for (0..struct_type.field_types.len) |i| {
                    const field_ty = Type.from_interned(struct_type.field_types.get(ip)[i]);
                    const field_src_loc = mod.field_src_loc(struct_type.decl.unwrap().?, .{
                        .index = i,
                        .range = .type,
                    });

                    if (try sema.type_requires_comptime(field_ty)) {
                        try mod.err_note_non_lazy(field_src_loc, msg, "struct requires comptime because of this field", .{});
                        try sema.explain_why_type_is_comptime_inner(msg, field_src_loc, field_ty, type_set);
                    }
                }
            }
            // TODO tuples
        },

        .Union => {
            if ((try type_set.get_or_put(sema.gpa, ty.to_intern())).found_existing) return;

            if (mod.type_to_union(ty)) |union_obj| {
                for (0..union_obj.field_types.len) |i| {
                    const field_ty = Type.from_interned(union_obj.field_types.get(ip)[i]);
                    const field_src_loc = mod.field_src_loc(union_obj.decl, .{
                        .index = i,
                        .range = .type,
                    });

                    if (try sema.type_requires_comptime(field_ty)) {
                        try mod.err_note_non_lazy(field_src_loc, msg, "union requires comptime because of this field", .{});
                        try sema.explain_why_type_is_comptime_inner(msg, field_src_loc, field_ty, type_set);
                    }
                }
            }
        },
    }
}

const ExternPosition = enum {
    ret_ty,
    param_ty,
    union_field,
    struct_field,
    element,
    other,
};

/// Returns true if `ty` is allowed in extern types.
/// Does *NOT* require `ty` to be resolved in any way.
/// Calls `resolve_type_layout` for packed containers.
fn validate_extern_type(
    sema: *Sema,
    ty: Type,
    position: ExternPosition,
) !bool {
    const mod = sema.mod;
    switch (ty.zig_type_tag(mod)) {
        .Type,
        .ComptimeFloat,
        .ComptimeInt,
        .EnumLiteral,
        .Undefined,
        .Null,
        .ErrorUnion,
        .ErrorSet,
        .Frame,
        => return false,
        .Void => return position == .union_field or position == .ret_ty or position == .struct_field or position == .element,
        .NoReturn => return position == .ret_ty,
        .Opaque,
        .Bool,
        .Float,
        .AnyFrame,
        => return true,
        .Pointer => {
            if (ty.child_type(mod).zig_type_tag(mod) == .Fn) {
                return ty.is_const_ptr(mod) and try sema.validate_extern_type(ty.child_type(mod), .other);
            }
            return !(ty.is_slice(mod) or try sema.type_requires_comptime(ty));
        },
        .Int => switch (ty.int_info(mod).bits) {
            0, 8, 16, 32, 64, 128 => return true,
            else => return false,
        },
        .Fn => {
            if (position != .other) return false;
            const target = sema.mod.get_target();
            // For now we want to authorize PTX kernel to use zig objects, even if we end up exposing the ABI.
            // The goal is to experiment with more integrated CPU/GPU code.
            if (ty.fn_calling_convention(mod) == .Kernel and (target.cpu.arch == .nvptx or target.cpu.arch == .nvptx64)) {
                return true;
            }
            return !target_util.fn_call_conv_allows_zig_types(target, ty.fn_calling_convention(mod));
        },
        .Enum => {
            return sema.validate_extern_type(ty.int_tag_type(mod), position);
        },
        .Struct, .Union => switch (ty.container_layout(mod)) {
            .@"extern" => return true,
            .@"packed" => {
                const bit_size = try ty.bit_size_advanced(mod, sema);
                switch (bit_size) {
                    0, 8, 16, 32, 64, 128 => return true,
                    else => return false,
                }
            },
            .auto => return !(try sema.type_has_runtime_bits(ty)),
        },
        .Array => {
            if (position == .ret_ty or position == .param_ty) return false;
            return sema.validate_extern_type(ty.elem_type2(mod), .element);
        },
        .Vector => return sema.validate_extern_type(ty.elem_type2(mod), .element),
        .Optional => return ty.is_ptr_like_optional(mod),
    }
}

fn explain_why_type_is_not_extern(
    sema: *Sema,
    msg: *Module.ErrorMsg,
    src_loc: Module.SrcLoc,
    ty: Type,
    position: ExternPosition,
) CompileError!void {
    const mod = sema.mod;
    switch (ty.zig_type_tag(mod)) {
        .Opaque,
        .Bool,
        .Float,
        .AnyFrame,
        => return,

        .Type,
        .ComptimeFloat,
        .ComptimeInt,
        .EnumLiteral,
        .Undefined,
        .Null,
        .ErrorUnion,
        .ErrorSet,
        .Frame,
        => return,

        .Pointer => {
            if (ty.is_slice(mod)) {
                try mod.err_note_non_lazy(src_loc, msg, "slices have no guaranteed in-memory representation", .{});
            } else {
                const pointee_ty = ty.child_type(mod);
                if (!ty.is_const_ptr(mod) and pointee_ty.zig_type_tag(mod) == .Fn) {
                    try mod.err_note_non_lazy(src_loc, msg, "pointer to extern function must be 'const'", .{});
                } else if (try sema.type_requires_comptime(ty)) {
                    try mod.err_note_non_lazy(src_loc, msg, "pointer to comptime-only type '{}'", .{pointee_ty.fmt(sema.mod)});
                    try sema.explain_why_type_is_comptime(msg, src_loc, ty);
                }
                try sema.explain_why_type_is_not_extern(msg, src_loc, pointee_ty, .other);
            }
        },
        .Void => try mod.err_note_non_lazy(src_loc, msg, "'void' is a zero bit type; for C 'void' use 'anyopaque'", .{}),
        .NoReturn => try mod.err_note_non_lazy(src_loc, msg, "'noreturn' is only allowed as a return type", .{}),
        .Int => if (!std.math.is_power_of_two(ty.int_info(mod).bits)) {
            try mod.err_note_non_lazy(src_loc, msg, "only integers with 0 or power of two bits are extern compatible", .{});
        } else {
            try mod.err_note_non_lazy(src_loc, msg, "only integers with 0, 8, 16, 32, 64 and 128 bits are extern compatible", .{});
        },
        .Fn => {
            if (position != .other) {
                try mod.err_note_non_lazy(src_loc, msg, "type has no guaranteed in-memory representation", .{});
                try mod.err_note_non_lazy(src_loc, msg, "use '*const ' to make a function pointer type", .{});
                return;
            }
            switch (ty.fn_calling_convention(mod)) {
                .Unspecified => try mod.err_note_non_lazy(src_loc, msg, "extern function must specify calling convention", .{}),
                .Async => try mod.err_note_non_lazy(src_loc, msg, "async function cannot be extern", .{}),
                .Inline => try mod.err_note_non_lazy(src_loc, msg, "inline function cannot be extern", .{}),
                else => return,
            }
        },
        .Enum => {
            const tag_ty = ty.int_tag_type(mod);
            try mod.err_note_non_lazy(src_loc, msg, "enum tag type '{}' is not extern compatible", .{tag_ty.fmt(sema.mod)});
            try sema.explain_why_type_is_not_extern(msg, src_loc, tag_ty, position);
        },
        .Struct => try mod.err_note_non_lazy(src_loc, msg, "only extern structs and ABI sized packed structs are extern compatible", .{}),
        .Union => try mod.err_note_non_lazy(src_loc, msg, "only extern unions and ABI sized packed unions are extern compatible", .{}),
        .Array => {
            if (position == .ret_ty) {
                return mod.err_note_non_lazy(src_loc, msg, "arrays are not allowed as a return type", .{});
            } else if (position == .param_ty) {
                return mod.err_note_non_lazy(src_loc, msg, "arrays are not allowed as a parameter type", .{});
            }
            try sema.explain_why_type_is_not_extern(msg, src_loc, ty.elem_type2(mod), .element);
        },
        .Vector => try sema.explain_why_type_is_not_extern(msg, src_loc, ty.elem_type2(mod), .element),
        .Optional => try mod.err_note_non_lazy(src_loc, msg, "only pointer like optionals are extern compatible", .{}),
    }
}

/// Returns true if `ty` is allowed in packed types.
/// Does not require `ty` to be resolved in any way, but may resolve whether it is comptime-only.
fn validate_packed_type(sema: *Sema, ty: Type) !bool {
    const zcu = sema.mod;
    return switch (ty.zig_type_tag(zcu)) {
        .Type,
        .ComptimeFloat,
        .ComptimeInt,
        .EnumLiteral,
        .Undefined,
        .Null,
        .ErrorUnion,
        .ErrorSet,
        .Frame,
        .NoReturn,
        .Opaque,
        .AnyFrame,
        .Fn,
        .Array,
        => false,
        .Optional => return ty.is_ptr_like_optional(zcu),
        .Void,
        .Bool,
        .Float,
        .Int,
        .Vector,
        => true,
        .Enum => switch (zcu.intern_pool.load_enum_type(ty.to_intern()).tag_mode) {
            .auto => false,
            .explicit, .nonexhaustive => true,
        },
        .Pointer => !ty.is_slice(zcu) and !try sema.type_requires_comptime(ty),
        .Struct, .Union => ty.container_layout(zcu) == .@"packed",
    };
}

fn explain_why_type_is_not_packed(
    sema: *Sema,
    msg: *Module.ErrorMsg,
    src_loc: Module.SrcLoc,
    ty: Type,
) CompileError!void {
    const mod = sema.mod;
    switch (ty.zig_type_tag(mod)) {
        .Void,
        .Bool,
        .Float,
        .Int,
        .Vector,
        .Enum,
        => return,
        .Type,
        .ComptimeFloat,
        .ComptimeInt,
        .EnumLiteral,
        .Undefined,
        .Null,
        .Frame,
        .NoReturn,
        .Opaque,
        .ErrorUnion,
        .ErrorSet,
        .AnyFrame,
        .Optional,
        .Array,
        => try mod.err_note_non_lazy(src_loc, msg, "type has no guaranteed in-memory representation", .{}),
        .Pointer => if (ty.is_slice(mod)) {
            try mod.err_note_non_lazy(src_loc, msg, "slices have no guaranteed in-memory representation", .{});
        } else {
            try mod.err_note_non_lazy(src_loc, msg, "comptime-only pointer has no guaranteed in-memory representation", .{});
            try sema.explain_why_type_is_comptime(msg, src_loc, ty);
        },
        .Fn => {
            try mod.err_note_non_lazy(src_loc, msg, "type has no guaranteed in-memory representation", .{});
            try mod.err_note_non_lazy(src_loc, msg, "use '*const ' to make a function pointer type", .{});
        },
        .Struct => try mod.err_note_non_lazy(src_loc, msg, "only packed structs layout are allowed in packed types", .{}),
        .Union => try mod.err_note_non_lazy(src_loc, msg, "only packed unions layout are allowed in packed types", .{}),
    }
}

fn prepare_simple_panic(sema: *Sema, block: *Block) !void {
    const mod = sema.mod;

    if (mod.panic_func_index == .none) {
        const decl_index = (try sema.get_builtin_decl(block, "panic"));
        // decl_index may be an alias; we must find the decl that actually
        // owns the function.
        try sema.ensure_decl_analyzed(decl_index);
        const fn_val = try mod.decl_ptr(decl_index).value_or_fail();
        try sema.declare_dependency(.{ .decl_val = decl_index });
        assert(fn_val.type_of(mod).zig_type_tag(mod) == .Fn);
        assert(try sema.fn_has_runtime_bits(fn_val.type_of(mod)));
        try mod.ensure_func_body_analysis_queued(fn_val.to_intern());
        mod.panic_func_index = fn_val.to_intern();
    }

    if (mod.null_stack_trace == .none) {
        const stack_trace_ty = try sema.get_builtin_type("StackTrace");
        try sema.resolve_type_fields(stack_trace_ty);
        const target = mod.get_target();
        const ptr_stack_trace_ty = try sema.ptr_type(.{
            .child = stack_trace_ty.to_intern(),
            .flags = .{
                .address_space = target_util.default_address_space(target, .global_constant),
            },
        });
        const opt_ptr_stack_trace_ty = try mod.optional_type(ptr_stack_trace_ty.to_intern());
        mod.null_stack_trace = try mod.intern(.{ .opt = .{
            .ty = opt_ptr_stack_trace_ty.to_intern(),
            .val = .none,
        } });
    }
}

/// Backends depend on panic decls being available when lowering safety-checked
/// instructions. This function ensures the panic function will be available to
/// be called during that time.
fn prepare_panic_id(sema: *Sema, block: *Block, panic_id: Module.PanicId) !InternPool.DeclIndex {
    const mod = sema.mod;
    const gpa = sema.gpa;
    if (mod.panic_messages[@int_from_enum(panic_id)].unwrap()) |x| return x;

    try sema.prepare_simple_panic(block);

    const panic_messages_ty = try sema.get_builtin_type("panic_messages");
    const msg_decl_index = (sema.namespace_lookup(
        block,
        .unneeded,
        panic_messages_ty.get_namespace_index(mod),
        try mod.intern_pool.get_or_put_string(gpa, @tag_name(panic_id), .no_embedded_nulls),
    ) catch |err| switch (err) {
        error.AnalysisFail, error.NeededSourceLocation => @panic("std.builtin.panic_messages is corrupt"),
        error.GenericPoison, error.ComptimeReturn, error.ComptimeBreak => unreachable,
        error.OutOfMemory => |e| return e,
    }).?;
    try sema.ensure_decl_analyzed(msg_decl_index);
    mod.panic_messages[@int_from_enum(panic_id)] = msg_decl_index.to_optional();
    return msg_decl_index;
}

fn add_safety_check(
    sema: *Sema,
    parent_block: *Block,
    src: LazySrcLoc,
    ok: Air.Inst.Ref,
    panic_id: Module.PanicId,
) !void {
    const gpa = sema.gpa;
    assert(!parent_block.is_comptime);

    var fail_block: Block = .{
        .parent = parent_block,
        .sema = sema,
        .src_decl = parent_block.src_decl,
        .namespace = parent_block.namespace,
        .instructions = .{},
        .inlining = parent_block.inlining,
        .is_comptime = false,
    };

    defer fail_block.instructions.deinit(gpa);

    try sema.safety_panic(&fail_block, src, panic_id);
    try sema.add_safety_check_extra(parent_block, ok, &fail_block);
}

fn add_safety_check_extra(
    sema: *Sema,
    parent_block: *Block,
    ok: Air.Inst.Ref,
    fail_block: *Block,
) !void {
    const gpa = sema.gpa;

    try parent_block.instructions.ensure_unused_capacity(gpa, 1);

    try sema.air_extra.ensure_unused_capacity(gpa, @typeInfo(Air.Block).Struct.fields.len +
        1 + // The main block only needs space for the cond_br.
        @typeInfo(Air.CondBr).Struct.fields.len +
        1 + // The ok branch of the cond_br only needs space for the br.
        fail_block.instructions.items.len);

    try sema.air_instructions.ensure_unused_capacity(gpa, 3);
    const block_inst: Air.Inst.Index = @enumFromInt(sema.air_instructions.len);
    const cond_br_inst: Air.Inst.Index = @enumFromInt(@int_from_enum(block_inst) + 1);
    const br_inst: Air.Inst.Index = @enumFromInt(@int_from_enum(cond_br_inst) + 1);
    sema.air_instructions.append_assume_capacity(.{
        .tag = .block,
        .data = .{ .ty_pl = .{
            .ty = .void_type,
            .payload = sema.add_extra_assume_capacity(Air.Block{
                .body_len = 1,
            }),
        } },
    });
    sema.air_extra.append_assume_capacity(@int_from_enum(cond_br_inst));

    sema.air_instructions.append_assume_capacity(.{
        .tag = .cond_br,
        .data = .{ .pl_op = .{
            .operand = ok,
            .payload = sema.add_extra_assume_capacity(Air.CondBr{
                .then_body_len = 1,
                .else_body_len = @int_cast(fail_block.instructions.items.len),
            }),
        } },
    });
    sema.air_extra.append_assume_capacity(@int_from_enum(br_inst));
    sema.air_extra.append_slice_assume_capacity(@ptr_cast(fail_block.instructions.items));

    sema.air_instructions.append_assume_capacity(.{
        .tag = .br,
        .data = .{ .br = .{
            .block_inst = block_inst,
            .operand = .void_value,
        } },
    });

    parent_block.instructions.append_assume_capacity(block_inst);
}

fn panic_with_msg(sema: *Sema, block: *Block, src: LazySrcLoc, msg_inst: Air.Inst.Ref, operation: CallOperation) !void {
    const mod = sema.mod;

    if (!mod.backend_supports_feature(.panic_fn)) {
        _ = try block.add_no_op(.trap);
        return;
    }

    try sema.prepare_simple_panic(block);

    const panic_func = mod.func_info(mod.panic_func_index);
    const panic_fn = try sema.analyze_decl_val(block, src, panic_func.owner_decl);
    const null_stack_trace = Air.interned_to_ref(mod.null_stack_trace);

    const opt_usize_ty = try mod.optional_type(.usize_type);
    const null_ret_addr = Air.interned_to_ref((try mod.intern(.{ .opt = .{
        .ty = opt_usize_ty.to_intern(),
        .val = .none,
    } })));
    try sema.call_builtin(block, src, panic_fn, .auto, &.{ msg_inst, null_stack_trace, null_ret_addr }, operation);
}

fn panic_unwrap_error(
    sema: *Sema,
    parent_block: *Block,
    src: LazySrcLoc,
    operand: Air.Inst.Ref,
    unwrap_err_tag: Air.Inst.Tag,
    is_non_err_tag: Air.Inst.Tag,
) !void {
    assert(!parent_block.is_comptime);
    const ok = try parent_block.add_un_op(is_non_err_tag, operand);
    if (!sema.mod.comp.formatted_panics) {
        return sema.add_safety_check(parent_block, src, ok, .unwrap_error);
    }
    const gpa = sema.gpa;

    var fail_block: Block = .{
        .parent = parent_block,
        .sema = sema,
        .src_decl = parent_block.src_decl,
        .namespace = parent_block.namespace,
        .instructions = .{},
        .inlining = parent_block.inlining,
        .is_comptime = false,
    };

    defer fail_block.instructions.deinit(gpa);

    {
        if (!sema.mod.backend_supports_feature(.panic_unwrap_error)) {
            _ = try fail_block.add_no_op(.trap);
        } else {
            const panic_fn = try sema.get_builtin("panic_unwrap_error");
            const err = try fail_block.add_ty_op(unwrap_err_tag, Type.anyerror, operand);
            const err_return_trace = try sema.get_error_return_trace(&fail_block);
            const args: [2]Air.Inst.Ref = .{ err_return_trace, err };
            try sema.call_builtin(&fail_block, src, panic_fn, .auto, &args, .@"safety check");
        }
    }
    try sema.add_safety_check_extra(parent_block, ok, &fail_block);
}

fn panic_index_out_of_bounds(
    sema: *Sema,
    parent_block: *Block,
    src: LazySrcLoc,
    index: Air.Inst.Ref,
    len: Air.Inst.Ref,
    cmp_op: Air.Inst.Tag,
) !void {
    assert(!parent_block.is_comptime);
    const ok = try parent_block.add_bin_op(cmp_op, index, len);
    if (!sema.mod.comp.formatted_panics) {
        return sema.add_safety_check(parent_block, src, ok, .index_out_of_bounds);
    }
    try sema.safety_check_formatted(parent_block, src, ok, "panic_out_of_bounds", &.{ index, len });
}

fn panic_inactive_union_field(
    sema: *Sema,
    parent_block: *Block,
    src: LazySrcLoc,
    active_tag: Air.Inst.Ref,
    wanted_tag: Air.Inst.Ref,
) !void {
    assert(!parent_block.is_comptime);
    const ok = try parent_block.add_bin_op(.cmp_eq, active_tag, wanted_tag);
    if (!sema.mod.comp.formatted_panics) {
        return sema.add_safety_check(parent_block, src, ok, .inactive_union_field);
    }
    try sema.safety_check_formatted(parent_block, src, ok, "panic_inactive_union_field", &.{ active_tag, wanted_tag });
}

fn panic_sentinel_mismatch(
    sema: *Sema,
    parent_block: *Block,
    src: LazySrcLoc,
    maybe_sentinel: ?Value,
    sentinel_ty: Type,
    ptr: Air.Inst.Ref,
    sentinel_index: Air.Inst.Ref,
) !void {
    assert(!parent_block.is_comptime);
    const mod = sema.mod;
    const expected_sentinel_val = maybe_sentinel orelse return;
    const expected_sentinel = Air.interned_to_ref(expected_sentinel_val.to_intern());

    const ptr_ty = sema.type_of(ptr);
    const actual_sentinel = if (ptr_ty.is_slice(mod))
        try parent_block.add_bin_op(.slice_elem_val, ptr, sentinel_index)
    else blk: {
        const elem_ptr_ty = try sema.elem_ptr_type(ptr_ty, null);
        const sentinel_ptr = try parent_block.add_ptr_elem_ptr(ptr, sentinel_index, elem_ptr_ty);
        break :blk try parent_block.add_ty_op(.load, sentinel_ty, sentinel_ptr);
    };

    const ok = if (sentinel_ty.zig_type_tag(mod) == .Vector) ok: {
        const eql =
            try parent_block.add_cmp_vector(expected_sentinel, actual_sentinel, .eq);
        break :ok try parent_block.add_inst(.{
            .tag = .reduce,
            .data = .{ .reduce = .{
                .operand = eql,
                .operation = .And,
            } },
        });
    } else if (sentinel_ty.is_self_comparable(mod, true))
        try parent_block.add_bin_op(.cmp_eq, expected_sentinel, actual_sentinel)
    else {
        const panic_fn = try sema.get_builtin("check_non_scalar_sentinel");
        const args: [2]Air.Inst.Ref = .{ expected_sentinel, actual_sentinel };
        try sema.call_builtin(parent_block, src, panic_fn, .auto, &args, .@"safety check");
        return;
    };

    if (!sema.mod.comp.formatted_panics) {
        return sema.add_safety_check(parent_block, src, ok, .sentinel_mismatch);
    }
    try sema.safety_check_formatted(parent_block, src, ok, "panic_sentinel_mismatch", &.{ expected_sentinel, actual_sentinel });
}

fn safety_check_formatted(
    sema: *Sema,
    parent_block: *Block,
    src: LazySrcLoc,
    ok: Air.Inst.Ref,
    func: []const u8,
    args: []const Air.Inst.Ref,
) CompileError!void {
    assert(sema.mod.comp.formatted_panics);
    const gpa = sema.gpa;

    var fail_block: Block = .{
        .parent = parent_block,
        .sema = sema,
        .src_decl = parent_block.src_decl,
        .namespace = parent_block.namespace,
        .instructions = .{},
        .inlining = parent_block.inlining,
        .is_comptime = false,
    };

    defer fail_block.instructions.deinit(gpa);

    if (!sema.mod.backend_supports_feature(.safety_check_formatted)) {
        _ = try fail_block.add_no_op(.trap);
    } else {
        const panic_fn = try sema.get_builtin(func);
        try sema.call_builtin(&fail_block, src, panic_fn, .auto, args, .@"safety check");
    }
    try sema.add_safety_check_extra(parent_block, ok, &fail_block);
}

fn safety_panic(sema: *Sema, block: *Block, src: LazySrcLoc, panic_id: Module.PanicId) CompileError!void {
    const msg_decl_index = try sema.prepare_panic_id(block, panic_id);
    const msg_inst = try sema.analyze_decl_val(block, src, msg_decl_index);
    try sema.panic_with_msg(block, src, msg_inst, .@"safety check");
}

fn emit_backward_branch(sema: *Sema, block: *Block, src: LazySrcLoc) !void {
    sema.branch_count += 1;
    if (sema.branch_count > sema.branch_quota) {
        const msg = try sema.err_msg(
            block,
            src,
            "evaluation exceeded {d} backwards branches",
            .{sema.branch_quota},
        );
        try sema.err_note(
            block,
            src,
            msg,
            "use @setEvalBranchQuota() to raise the branch limit from {d}",
            .{sema.branch_quota},
        );
        return sema.fail_with_owned_error_msg(block, msg);
    }
}

fn field_val(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    object: Air.Inst.Ref,
    field_name: InternPool.NullTerminatedString,
    field_name_src: LazySrcLoc,
) CompileError!Air.Inst.Ref {
    // When editing this function, note that there is corresponding logic to be edited
    // in `field_ptr`. This function takes a value and returns a value.

    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const object_src = src; // TODO better source location
    const object_ty = sema.type_of(object);

    // Zig allows dereferencing a single pointer during field lookup. Note that
    // we don't actually need to generate the dereference some field lookups, like the
    // length of arrays and other comptime operations.
    const is_pointer_to = object_ty.is_single_pointer(mod);

    const inner_ty = if (is_pointer_to)
        object_ty.child_type(mod)
    else
        object_ty;

    switch (inner_ty.zig_type_tag(mod)) {
        .Array => {
            if (field_name.eql_slice("len", ip)) {
                return Air.interned_to_ref((try mod.int_value(Type.usize, inner_ty.array_len(mod))).to_intern());
            } else if (field_name.eql_slice("ptr", ip) and is_pointer_to) {
                const ptr_info = object_ty.ptr_info(mod);
                const result_ty = try sema.ptr_type(.{
                    .child = Type.from_interned(ptr_info.child).child_type(mod).to_intern(),
                    .sentinel = if (inner_ty.sentinel(mod)) |s| s.to_intern() else .none,
                    .flags = .{
                        .size = .Many,
                        .alignment = ptr_info.flags.alignment,
                        .is_const = ptr_info.flags.is_const,
                        .is_volatile = ptr_info.flags.is_volatile,
                        .is_allowzero = ptr_info.flags.is_allowzero,
                        .address_space = ptr_info.flags.address_space,
                        .vector_index = ptr_info.flags.vector_index,
                    },
                    .packed_offset = ptr_info.packed_offset,
                });
                return sema.coerce(block, result_ty, object, src);
            } else {
                return sema.fail(
                    block,
                    field_name_src,
                    "no member named '{}' in '{}'",
                    .{ field_name.fmt(ip), object_ty.fmt(mod) },
                );
            }
        },
        .Pointer => {
            const ptr_info = inner_ty.ptr_info(mod);
            if (ptr_info.flags.size == .Slice) {
                if (field_name.eql_slice("ptr", ip)) {
                    const slice = if (is_pointer_to)
                        try sema.analyze_load(block, src, object, object_src)
                    else
                        object;
                    return sema.analyze_slice_ptr(block, object_src, slice, inner_ty);
                } else if (field_name.eql_slice("len", ip)) {
                    const slice = if (is_pointer_to)
                        try sema.analyze_load(block, src, object, object_src)
                    else
                        object;
                    return sema.analyze_slice_len(block, src, slice);
                } else {
                    return sema.fail(
                        block,
                        field_name_src,
                        "no member named '{}' in '{}'",
                        .{ field_name.fmt(ip), object_ty.fmt(mod) },
                    );
                }
            }
        },
        .Type => {
            const dereffed_type = if (is_pointer_to)
                try sema.analyze_load(block, src, object, object_src)
            else
                object;

            const val = (try sema.resolve_defined_value(block, object_src, dereffed_type)).?;
            const child_type = val.to_type();

            if (child_type.type_decl_inst(mod)) |type_decl_inst| {
                try sema.declare_dependency(.{ .namespace_name = .{
                    .namespace = type_decl_inst,
                    .name = field_name,
                } });
            }

            switch (try child_type.zig_type_tag_or_poison(mod)) {
                .ErrorSet => {
                    switch (ip.index_to_key(child_type.to_intern())) {
                        .error_set_type => |error_set_type| blk: {
                            if (error_set_type.name_index(ip, field_name) != null) break :blk;
                            return sema.fail(block, src, "no error named '{}' in '{}'", .{
                                field_name.fmt(ip), child_type.fmt(mod),
                            });
                        },
                        .inferred_error_set_type => {
                            return sema.fail(block, src, "TODO handle inferred error sets here", .{});
                        },
                        .simple_type => |t| {
                            assert(t == .anyerror);
                            _ = try mod.get_error_value(field_name);
                        },
                        else => unreachable,
                    }

                    const error_set_type = if (!child_type.is_any_error(mod))
                        child_type
                    else
                        try mod.single_error_set_type(field_name);
                    return Air.interned_to_ref((try mod.intern(.{ .err = .{
                        .ty = error_set_type.to_intern(),
                        .name = field_name,
                    } })));
                },
                .Union => {
                    if (try sema.namespace_lookup_val(block, src, child_type.get_namespace_index(mod), field_name)) |inst| {
                        return inst;
                    }
                    try sema.resolve_type_fields(child_type);
                    if (child_type.union_tag_type(mod)) |enum_ty| {
                        if (enum_ty.enum_field_index(field_name, mod)) |field_index_usize| {
                            const field_index: u32 = @int_cast(field_index_usize);
                            return Air.interned_to_ref((try mod.enum_value_field_index(enum_ty, field_index)).to_intern());
                        }
                    }
                    return sema.fail_with_bad_member_access(block, child_type, field_name_src, field_name);
                },
                .Enum => {
                    if (try sema.namespace_lookup_val(block, src, child_type.get_namespace_index(mod), field_name)) |inst| {
                        return inst;
                    }
                    const field_index_usize = child_type.enum_field_index(field_name, mod) orelse
                        return sema.fail_with_bad_member_access(block, child_type, field_name_src, field_name);
                    const field_index: u32 = @int_cast(field_index_usize);
                    const enum_val = try mod.enum_value_field_index(child_type, field_index);
                    return Air.interned_to_ref(enum_val.to_intern());
                },
                .Struct, .Opaque => {
                    if (try sema.namespace_lookup_val(block, src, child_type.get_namespace_index(mod), field_name)) |inst| {
                        return inst;
                    }
                    return sema.fail_with_bad_member_access(block, child_type, src, field_name);
                },
                else => {
                    const msg = msg: {
                        const msg = try sema.err_msg(block, src, "type '{}' has no members", .{child_type.fmt(mod)});
                        errdefer msg.destroy(sema.gpa);
                        if (child_type.is_slice(mod)) try sema.err_note(block, src, msg, "slice values have 'len' and 'ptr' members", .{});
                        if (child_type.zig_type_tag(mod) == .Array) try sema.err_note(block, src, msg, "array values have 'len' member", .{});
                        break :msg msg;
                    };
                    return sema.fail_with_owned_error_msg(block, msg);
                },
            }
        },
        .Struct => if (is_pointer_to) {
            // Avoid loading the entire struct by fetching a pointer and loading that
            const field_ptr = try sema.struct_field_ptr(block, src, object, field_name, field_name_src, inner_ty, false);
            return sema.analyze_load(block, src, field_ptr, object_src);
        } else {
            return sema.struct_field_val(block, src, object, field_name, field_name_src, inner_ty);
        },
        .Union => if (is_pointer_to) {
            // Avoid loading the entire union by fetching a pointer and loading that
            const field_ptr = try sema.union_field_ptr(block, src, object, field_name, field_name_src, inner_ty, false);
            return sema.analyze_load(block, src, field_ptr, object_src);
        } else {
            return sema.union_field_val(block, src, object, field_name, field_name_src, inner_ty);
        },
        else => {},
    }
    return sema.fail_with_invalid_field_access(block, src, object_ty, field_name);
}

fn field_ptr(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    object_ptr: Air.Inst.Ref,
    field_name: InternPool.NullTerminatedString,
    field_name_src: LazySrcLoc,
    initializing: bool,
) CompileError!Air.Inst.Ref {
    // When editing this function, note that there is corresponding logic to be edited
    // in `field_val`. This function takes a pointer and returns a pointer.

    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const object_ptr_src = src; // TODO better source location
    const object_ptr_ty = sema.type_of(object_ptr);
    const object_ty = switch (object_ptr_ty.zig_type_tag(mod)) {
        .Pointer => object_ptr_ty.child_type(mod),
        else => return sema.fail(block, object_ptr_src, "expected pointer, found '{}'", .{object_ptr_ty.fmt(mod)}),
    };

    // Zig allows dereferencing a single pointer during field lookup. Note that
    // we don't actually need to generate the dereference some field lookups, like the
    // length of arrays and other comptime operations.
    const is_pointer_to = object_ty.is_single_pointer(mod);

    const inner_ty = if (is_pointer_to)
        object_ty.child_type(mod)
    else
        object_ty;

    switch (inner_ty.zig_type_tag(mod)) {
        .Array => {
            if (field_name.eql_slice("len", ip)) {
                const int_val = try mod.int_value(Type.usize, inner_ty.array_len(mod));
                return anon_decl_ref(sema, int_val.to_intern());
            } else if (field_name.eql_slice("ptr", ip) and is_pointer_to) {
                const ptr_info = object_ty.ptr_info(mod);
                const new_ptr_ty = try sema.ptr_type(.{
                    .child = Type.from_interned(ptr_info.child).child_type(mod).to_intern(),
                    .sentinel = if (object_ty.sentinel(mod)) |s| s.to_intern() else .none,
                    .flags = .{
                        .size = .Many,
                        .alignment = ptr_info.flags.alignment,
                        .is_const = ptr_info.flags.is_const,
                        .is_volatile = ptr_info.flags.is_volatile,
                        .is_allowzero = ptr_info.flags.is_allowzero,
                        .address_space = ptr_info.flags.address_space,
                        .vector_index = ptr_info.flags.vector_index,
                    },
                    .packed_offset = ptr_info.packed_offset,
                });
                const ptr_ptr_info = object_ptr_ty.ptr_info(mod);
                const result_ty = try sema.ptr_type(.{
                    .child = new_ptr_ty.to_intern(),
                    .sentinel = if (object_ptr_ty.sentinel(mod)) |s| s.to_intern() else .none,
                    .flags = .{
                        .alignment = ptr_ptr_info.flags.alignment,
                        .is_const = ptr_ptr_info.flags.is_const,
                        .is_volatile = ptr_ptr_info.flags.is_volatile,
                        .is_allowzero = ptr_ptr_info.flags.is_allowzero,
                        .address_space = ptr_ptr_info.flags.address_space,
                        .vector_index = ptr_ptr_info.flags.vector_index,
                    },
                    .packed_offset = ptr_ptr_info.packed_offset,
                });
                return sema.bit_cast(block, result_ty, object_ptr, src, null);
            } else {
                return sema.fail(
                    block,
                    field_name_src,
                    "no member named '{}' in '{}'",
                    .{ field_name.fmt(ip), object_ty.fmt(mod) },
                );
            }
        },
        .Pointer => if (inner_ty.is_slice(mod)) {
            const inner_ptr = if (is_pointer_to)
                try sema.analyze_load(block, src, object_ptr, object_ptr_src)
            else
                object_ptr;

            const attr_ptr_ty = if (is_pointer_to) object_ty else object_ptr_ty;

            if (field_name.eql_slice("ptr", ip)) {
                const slice_ptr_ty = inner_ty.slice_ptr_field_type(mod);

                const result_ty = try sema.ptr_type(.{
                    .child = slice_ptr_ty.to_intern(),
                    .flags = .{
                        .is_const = !attr_ptr_ty.ptr_is_mutable(mod),
                        .is_volatile = attr_ptr_ty.is_volatile_ptr(mod),
                        .address_space = attr_ptr_ty.ptr_address_space(mod),
                    },
                });

                if (try sema.resolve_defined_value(block, object_ptr_src, inner_ptr)) |val| {
                    return Air.interned_to_ref((try val.ptr_field(Value.slice_ptr_index, sema)).to_intern());
                }
                try sema.require_runtime_block(block, src, null);

                const field_ptr = try block.add_ty_op(.ptr_slice_ptr_ptr, result_ty, inner_ptr);
                try sema.check_known_alloc_ptr(block, inner_ptr, field_ptr);
                return field_ptr;
            } else if (field_name.eql_slice("len", ip)) {
                const result_ty = try sema.ptr_type(.{
                    .child = .usize_type,
                    .flags = .{
                        .is_const = !attr_ptr_ty.ptr_is_mutable(mod),
                        .is_volatile = attr_ptr_ty.is_volatile_ptr(mod),
                        .address_space = attr_ptr_ty.ptr_address_space(mod),
                    },
                });

                if (try sema.resolve_defined_value(block, object_ptr_src, inner_ptr)) |val| {
                    return Air.interned_to_ref((try val.ptr_field(Value.slice_len_index, sema)).to_intern());
                }
                try sema.require_runtime_block(block, src, null);

                const field_ptr = try block.add_ty_op(.ptr_slice_len_ptr, result_ty, inner_ptr);
                try sema.check_known_alloc_ptr(block, inner_ptr, field_ptr);
                return field_ptr;
            } else {
                return sema.fail(
                    block,
                    field_name_src,
                    "no member named '{}' in '{}'",
                    .{ field_name.fmt(ip), object_ty.fmt(mod) },
                );
            }
        },
        .Type => {
            _ = try sema.resolve_const_defined_value(block, .unneeded, object_ptr, undefined);
            const result = try sema.analyze_load(block, src, object_ptr, object_ptr_src);
            const inner = if (is_pointer_to)
                try sema.analyze_load(block, src, result, object_ptr_src)
            else
                result;

            const val = (sema.resolve_defined_value(block, src, inner) catch unreachable).?;
            const child_type = val.to_type();

            if (child_type.type_decl_inst(mod)) |type_decl_inst| {
                try sema.declare_dependency(.{ .namespace_name = .{
                    .namespace = type_decl_inst,
                    .name = field_name,
                } });
            }

            switch (child_type.zig_type_tag(mod)) {
                .ErrorSet => {
                    switch (ip.index_to_key(child_type.to_intern())) {
                        .error_set_type => |error_set_type| blk: {
                            if (error_set_type.name_index(ip, field_name) != null) {
                                break :blk;
                            }
                            return sema.fail(block, src, "no error named '{}' in '{}'", .{
                                field_name.fmt(ip), child_type.fmt(mod),
                            });
                        },
                        .inferred_error_set_type => {
                            return sema.fail(block, src, "TODO handle inferred error sets here", .{});
                        },
                        .simple_type => |t| {
                            assert(t == .anyerror);
                            _ = try mod.get_error_value(field_name);
                        },
                        else => unreachable,
                    }

                    const error_set_type = if (!child_type.is_any_error(mod))
                        child_type
                    else
                        try mod.single_error_set_type(field_name);
                    return anon_decl_ref(sema, try mod.intern(.{ .err = .{
                        .ty = error_set_type.to_intern(),
                        .name = field_name,
                    } }));
                },
                .Union => {
                    if (try sema.namespace_lookup_ref(block, src, child_type.get_namespace_index(mod), field_name)) |inst| {
                        return inst;
                    }
                    try sema.resolve_type_fields(child_type);
                    if (child_type.union_tag_type(mod)) |enum_ty| {
                        if (enum_ty.enum_field_index(field_name, mod)) |field_index| {
                            const field_index_u32: u32 = @int_cast(field_index);
                            const idx_val = try mod.enum_value_field_index(enum_ty, field_index_u32);
                            return anon_decl_ref(sema, idx_val.to_intern());
                        }
                    }
                    return sema.fail_with_bad_member_access(block, child_type, field_name_src, field_name);
                },
                .Enum => {
                    if (try sema.namespace_lookup_ref(block, src, child_type.get_namespace_index(mod), field_name)) |inst| {
                        return inst;
                    }
                    const field_index = child_type.enum_field_index(field_name, mod) orelse {
                        return sema.fail_with_bad_member_access(block, child_type, field_name_src, field_name);
                    };
                    const field_index_u32: u32 = @int_cast(field_index);
                    const idx_val = try mod.enum_value_field_index(child_type, field_index_u32);
                    return anon_decl_ref(sema, idx_val.to_intern());
                },
                .Struct, .Opaque => {
                    if (try sema.namespace_lookup_ref(block, src, child_type.get_namespace_index(mod), field_name)) |inst| {
                        return inst;
                    }
                    return sema.fail_with_bad_member_access(block, child_type, field_name_src, field_name);
                },
                else => return sema.fail(block, src, "type '{}' has no members", .{child_type.fmt(mod)}),
            }
        },
        .Struct => {
            const inner_ptr = if (is_pointer_to)
                try sema.analyze_load(block, src, object_ptr, object_ptr_src)
            else
                object_ptr;
            const field_ptr = try sema.struct_field_ptr(block, src, inner_ptr, field_name, field_name_src, inner_ty, initializing);
            try sema.check_known_alloc_ptr(block, inner_ptr, field_ptr);
            return field_ptr;
        },
        .Union => {
            const inner_ptr = if (is_pointer_to)
                try sema.analyze_load(block, src, object_ptr, object_ptr_src)
            else
                object_ptr;
            const field_ptr = try sema.union_field_ptr(block, src, inner_ptr, field_name, field_name_src, inner_ty, initializing);
            try sema.check_known_alloc_ptr(block, inner_ptr, field_ptr);
            return field_ptr;
        },
        else => {},
    }
    return sema.fail_with_invalid_field_access(block, src, object_ty, field_name);
}

const ResolvedFieldCallee = union(enum) {
    /// The LHS of the call was an actual field with this value.
    direct: Air.Inst.Ref,
    /// This is a method call, with the function and first argument given.
    method: struct {
        func_inst: Air.Inst.Ref,
        arg0_inst: Air.Inst.Ref,
    },
};

fn field_call_bind(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    raw_ptr: Air.Inst.Ref,
    field_name: InternPool.NullTerminatedString,
    field_name_src: LazySrcLoc,
) CompileError!ResolvedFieldCallee {
    // When editing this function, note that there is corresponding logic to be edited
    // in `field_val`. This function takes a pointer and returns a pointer.

    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const raw_ptr_src = src; // TODO better source location
    const raw_ptr_ty = sema.type_of(raw_ptr);
    const inner_ty = if (raw_ptr_ty.zig_type_tag(mod) == .Pointer and (raw_ptr_ty.ptr_size(mod) == .One or raw_ptr_ty.ptr_size(mod) == .C))
        raw_ptr_ty.child_type(mod)
    else
        return sema.fail(block, raw_ptr_src, "expected single pointer, found '{}'", .{raw_ptr_ty.fmt(mod)});

    // Optionally dereference a second pointer to get the concrete type.
    const is_double_ptr = inner_ty.zig_type_tag(mod) == .Pointer and inner_ty.ptr_size(mod) == .One;
    const concrete_ty = if (is_double_ptr) inner_ty.child_type(mod) else inner_ty;
    const ptr_ty = if (is_double_ptr) inner_ty else raw_ptr_ty;
    const object_ptr = if (is_double_ptr)
        try sema.analyze_load(block, src, raw_ptr, src)
    else
        raw_ptr;

    find_field: {
        switch (concrete_ty.zig_type_tag(mod)) {
            .Struct => {
                try sema.resolve_type_fields(concrete_ty);
                if (mod.type_to_struct(concrete_ty)) |struct_type| {
                    const field_index = struct_type.name_index(ip, field_name) orelse
                        break :find_field;
                    const field_ty = Type.from_interned(struct_type.field_types.get(ip)[field_index]);

                    return sema.finish_field_call_bind(block, src, ptr_ty, field_ty, field_index, object_ptr);
                } else if (concrete_ty.is_tuple(mod)) {
                    if (field_name.eql_slice("len", ip)) {
                        return .{ .direct = try mod.int_ref(Type.usize, concrete_ty.struct_field_count(mod)) };
                    }
                    if (field_name.to_unsigned(ip)) |field_index| {
                        if (field_index >= concrete_ty.struct_field_count(mod)) break :find_field;
                        return sema.finish_field_call_bind(block, src, ptr_ty, concrete_ty.struct_field_type(field_index, mod), field_index, object_ptr);
                    }
                } else {
                    const max = concrete_ty.struct_field_count(mod);
                    for (0..max) |i_usize| {
                        const i: u32 = @int_cast(i_usize);
                        if (field_name == concrete_ty.struct_field_name(i, mod).unwrap().?) {
                            return sema.finish_field_call_bind(block, src, ptr_ty, concrete_ty.struct_field_type(i, mod), i, object_ptr);
                        }
                    }
                }
            },
            .Union => {
                try sema.resolve_type_fields(concrete_ty);
                const union_obj = mod.type_to_union(concrete_ty).?;
                _ = union_obj.load_tag_type(ip).name_index(ip, field_name) orelse break :find_field;
                const field_ptr = try union_field_ptr(sema, block, src, object_ptr, field_name, field_name_src, concrete_ty, false);
                return .{ .direct = try sema.analyze_load(block, src, field_ptr, src) };
            },
            .Type => {
                const namespace = try sema.analyze_load(block, src, object_ptr, src);
                return .{ .direct = try sema.field_val(block, src, namespace, field_name, field_name_src) };
            },
            else => {},
        }
    }

    // If we get here, we need to look for a decl in the struct type instead.
    const found_decl = found_decl: {
        const namespace = concrete_ty.get_namespace(mod) orelse
            break :found_decl null;
        const decl_idx = (try sema.namespace_lookup(block, src, namespace, field_name)) orelse
            break :found_decl null;

        try sema.add_referenced_by(block, src, decl_idx);
        const decl_val = try sema.analyze_decl_val(block, src, decl_idx);
        const decl_type = sema.type_of(decl_val);
        if (mod.type_to_func(decl_type)) |func_type| f: {
            if (func_type.param_types.len == 0) break :f;

            const first_param_type = Type.from_interned(func_type.param_types.get(ip)[0]);
            if (first_param_type.is_generic_poison() or
                (first_param_type.zig_type_tag(mod) == .Pointer and
                (first_param_type.ptr_size(mod) == .One or
                first_param_type.ptr_size(mod) == .C) and
                first_param_type.child_type(mod).eql(concrete_ty, mod)))
            {
                // Note that if the param type is generic poison, we know that it must
                // specifically be `anytype` since it's the first parameter, meaning we
                // can safely assume it can be a pointer.
                // TODO: bound fn calls on rvalues should probably
                // generate a by-value argument somehow.
                return .{ .method = .{
                    .func_inst = decl_val,
                    .arg0_inst = object_ptr,
                } };
            } else if (first_param_type.eql(concrete_ty, mod)) {
                const deref = try sema.analyze_load(block, src, object_ptr, src);
                return .{ .method = .{
                    .func_inst = decl_val,
                    .arg0_inst = deref,
                } };
            } else if (first_param_type.zig_type_tag(mod) == .Optional) {
                const child = first_param_type.optional_child(mod);
                if (child.eql(concrete_ty, mod)) {
                    const deref = try sema.analyze_load(block, src, object_ptr, src);
                    return .{ .method = .{
                        .func_inst = decl_val,
                        .arg0_inst = deref,
                    } };
                } else if (child.zig_type_tag(mod) == .Pointer and
                    child.ptr_size(mod) == .One and
                    child.child_type(mod).eql(concrete_ty, mod))
                {
                    return .{ .method = .{
                        .func_inst = decl_val,
                        .arg0_inst = object_ptr,
                    } };
                }
            } else if (first_param_type.zig_type_tag(mod) == .ErrorUnion and
                first_param_type.error_union_payload(mod).eql(concrete_ty, mod))
            {
                const deref = try sema.analyze_load(block, src, object_ptr, src);
                return .{ .method = .{
                    .func_inst = decl_val,
                    .arg0_inst = deref,
                } };
            }
        }
        break :found_decl decl_idx;
    };

    const msg = msg: {
        const msg = try sema.err_msg(block, src, "no field or member function named '{}' in '{}'", .{
            field_name.fmt(ip),
            concrete_ty.fmt(mod),
        });
        errdefer msg.destroy(sema.gpa);
        try sema.add_declared_here_note(msg, concrete_ty);
        if (found_decl) |decl_idx| {
            const decl = mod.decl_ptr(decl_idx);
            try mod.err_note_non_lazy(decl.src_loc(mod), msg, "'{}' is not a member function", .{field_name.fmt(ip)});
        }
        if (concrete_ty.zig_type_tag(mod) == .ErrorUnion) {
            try sema.err_note(block, src, msg, "consider using 'try', 'catch', or 'if'", .{});
        }
        break :msg msg;
    };
    return sema.fail_with_owned_error_msg(block, msg);
}

fn finish_field_call_bind(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    ptr_ty: Type,
    field_ty: Type,
    field_index: u32,
    object_ptr: Air.Inst.Ref,
) CompileError!ResolvedFieldCallee {
    const mod = sema.mod;
    const ptr_field_ty = try sema.ptr_type(.{
        .child = field_ty.to_intern(),
        .flags = .{
            .is_const = !ptr_ty.ptr_is_mutable(mod),
            .address_space = ptr_ty.ptr_address_space(mod),
        },
    });

    const container_ty = ptr_ty.child_type(mod);
    if (container_ty.zig_type_tag(mod) == .Struct) {
        if (container_ty.struct_field_is_comptime(field_index, mod)) {
            try sema.resolve_struct_field_inits(container_ty);
            const default_val = (try container_ty.struct_field_value_comptime(mod, field_index)).?;
            return .{ .direct = Air.interned_to_ref(default_val.to_intern()) };
        }
    }

    if (try sema.resolve_defined_value(block, src, object_ptr)) |struct_ptr_val| {
        const ptr_val = try struct_ptr_val.ptr_field(field_index, sema);
        const pointer = Air.interned_to_ref(ptr_val.to_intern());
        return .{ .direct = try sema.analyze_load(block, src, pointer, src) };
    }

    try sema.require_runtime_block(block, src, null);
    const ptr_inst = try block.add_struct_field_ptr(object_ptr, field_index, ptr_field_ty);
    return .{ .direct = try sema.analyze_load(block, src, ptr_inst, src) };
}

fn namespace_lookup(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    opt_namespace: InternPool.OptionalNamespaceIndex,
    decl_name: InternPool.NullTerminatedString,
) CompileError!?InternPool.DeclIndex {
    const mod = sema.mod;
    const gpa = sema.gpa;
    if (try sema.lookup_in_namespace(block, src, opt_namespace, decl_name, true)) |decl_index| {
        const decl = mod.decl_ptr(decl_index);
        if (!decl.is_pub and decl.get_file_scope(mod) != block.get_file_scope(mod)) {
            const msg = msg: {
                const msg = try sema.err_msg(block, src, "'{}' is not marked 'pub'", .{
                    decl_name.fmt(&mod.intern_pool),
                });
                errdefer msg.destroy(gpa);
                try mod.err_note_non_lazy(decl.src_loc(mod), msg, "declared here", .{});
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        }
        return decl_index;
    }
    return null;
}

fn namespace_lookup_ref(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    opt_namespace: InternPool.OptionalNamespaceIndex,
    decl_name: InternPool.NullTerminatedString,
) CompileError!?Air.Inst.Ref {
    const decl = (try sema.namespace_lookup(block, src, opt_namespace, decl_name)) orelse return null;
    try sema.add_referenced_by(block, src, decl);
    return try sema.analyze_decl_ref(decl);
}

fn namespace_lookup_val(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    opt_namespace: InternPool.OptionalNamespaceIndex,
    decl_name: InternPool.NullTerminatedString,
) CompileError!?Air.Inst.Ref {
    const decl = (try sema.namespace_lookup(block, src, opt_namespace, decl_name)) orelse return null;
    return try sema.analyze_decl_val(block, src, decl);
}

fn struct_field_ptr(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    struct_ptr: Air.Inst.Ref,
    field_name: InternPool.NullTerminatedString,
    field_name_src: LazySrcLoc,
    struct_ty: Type,
    initializing: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    assert(struct_ty.zig_type_tag(mod) == .Struct);

    try sema.resolve_type_fields(struct_ty);
    try sema.resolve_struct_layout(struct_ty);

    if (struct_ty.is_tuple(mod)) {
        if (field_name.eql_slice("len", ip)) {
            const len_inst = try mod.int_ref(Type.usize, struct_ty.struct_field_count(mod));
            return sema.analyze_ref(block, src, len_inst);
        }
        const field_index = try sema.tuple_field_index(block, struct_ty, field_name, field_name_src);
        return sema.tuple_field_ptr(block, src, struct_ptr, field_name_src, field_index, initializing);
    } else if (struct_ty.is_anon_struct(mod)) {
        const field_index = try sema.anon_struct_field_index(block, struct_ty, field_name, field_name_src);
        return sema.tuple_field_ptr(block, src, struct_ptr, field_name_src, field_index, initializing);
    }

    const struct_type = mod.type_to_struct(struct_ty).?;

    const field_index = struct_type.name_index(ip, field_name) orelse
        return sema.fail_with_bad_struct_field_access(block, struct_type, field_name_src, field_name);

    return sema.struct_field_ptr_by_index(block, src, struct_ptr, field_index, field_name_src, struct_ty, initializing);
}

fn struct_field_ptr_by_index(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    struct_ptr: Air.Inst.Ref,
    field_index: u32,
    field_src: LazySrcLoc,
    struct_ty: Type,
    initializing: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    if (struct_ty.is_anon_struct(mod)) {
        return sema.tuple_field_ptr(block, src, struct_ptr, field_src, field_index, initializing);
    }

    if (try sema.resolve_defined_value(block, src, struct_ptr)) |struct_ptr_val| {
        const val = try struct_ptr_val.ptr_field(field_index, sema);
        return Air.interned_to_ref(val.to_intern());
    }

    const struct_type = mod.type_to_struct(struct_ty).?;
    const field_ty = struct_type.field_types.get(ip)[field_index];
    const struct_ptr_ty = sema.type_of(struct_ptr);
    const struct_ptr_ty_info = struct_ptr_ty.ptr_info(mod);

    var ptr_ty_data: InternPool.Key.PtrType = .{
        .child = field_ty,
        .flags = .{
            .is_const = struct_ptr_ty_info.flags.is_const,
            .is_volatile = struct_ptr_ty_info.flags.is_volatile,
            .address_space = struct_ptr_ty_info.flags.address_space,
        },
    };

    const parent_align = if (struct_ptr_ty_info.flags.alignment != .none)
        struct_ptr_ty_info.flags.alignment
    else
        try sema.type_abi_alignment(Type.from_interned(struct_ptr_ty_info.child));

    if (struct_type.layout == .@"packed") {
        switch (struct_ty.packed_struct_field_ptr_info(struct_ptr_ty, field_index, mod)) {
            .bit_ptr => |packed_offset| {
                ptr_ty_data.flags.alignment = parent_align;
                ptr_ty_data.packed_offset = packed_offset;
            },
            .byte_ptr => |ptr_info| {
                ptr_ty_data.flags.alignment = ptr_info.alignment;
            },
        }
    } else if (struct_type.layout == .@"extern") {
        // For extern structs, field alignment might be bigger than type's
        // natural alignment. Eg, in `extern struct { x: u32, y: u16 }` the
        // second field is aligned as u32.
        const field_offset = struct_ty.struct_field_offset(field_index, mod);
        ptr_ty_data.flags.alignment = if (parent_align == .none)
            .none
        else
            @enumFromInt(@min(@int_from_enum(parent_align), @ctz(field_offset)));
    } else {
        // Our alignment is capped at the field alignment.
        const field_align = try sema.struct_field_alignment(
            struct_type.field_align(ip, field_index),
            Type.from_interned(field_ty),
            struct_type.layout,
        );
        ptr_ty_data.flags.alignment = if (struct_ptr_ty_info.flags.alignment == .none)
            field_align
        else
            field_align.min(parent_align);
    }

    const ptr_field_ty = try sema.ptr_type(ptr_ty_data);

    if (struct_type.field_is_comptime(ip, field_index)) {
        try sema.resolve_struct_field_inits(struct_ty);
        const val = try mod.intern(.{ .ptr = .{
            .ty = ptr_field_ty.to_intern(),
            .base_addr = .{ .comptime_field = struct_type.field_inits.get(ip)[field_index] },
            .byte_offset = 0,
        } });
        return Air.interned_to_ref(val);
    }

    try sema.require_runtime_block(block, src, null);
    return block.add_struct_field_ptr(struct_ptr, field_index, ptr_field_ty);
}

fn struct_field_val(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    struct_byval: Air.Inst.Ref,
    field_name: InternPool.NullTerminatedString,
    field_name_src: LazySrcLoc,
    struct_ty: Type,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    assert(struct_ty.zig_type_tag(mod) == .Struct);

    try sema.resolve_type_fields(struct_ty);

    switch (ip.index_to_key(struct_ty.to_intern())) {
        .struct_type => {
            const struct_type = ip.load_struct_type(struct_ty.to_intern());
            if (struct_type.is_tuple(ip))
                return sema.tuple_field_val(block, src, struct_byval, field_name, field_name_src, struct_ty);

            const field_index = struct_type.name_index(ip, field_name) orelse
                return sema.fail_with_bad_struct_field_access(block, struct_type, field_name_src, field_name);
            if (struct_type.field_is_comptime(ip, field_index)) {
                try sema.resolve_struct_field_inits(struct_ty);
                return Air.interned_to_ref(struct_type.field_inits.get(ip)[field_index]);
            }

            const field_ty = Type.from_interned(struct_type.field_types.get(ip)[field_index]);
            if (try sema.type_has_one_possible_value(field_ty)) |field_val|
                return Air.interned_to_ref(field_val.to_intern());

            if (try sema.resolve_value(struct_byval)) |struct_val| {
                if (struct_val.is_undef(mod)) return mod.undef_ref(field_ty);
                if ((try sema.type_has_one_possible_value(field_ty))) |opv| {
                    return Air.interned_to_ref(opv.to_intern());
                }
                return Air.interned_to_ref((try struct_val.field_value(mod, field_index)).to_intern());
            }

            try sema.require_runtime_block(block, src, null);
            try sema.resolve_type_layout(field_ty);
            return block.add_struct_field_val(struct_byval, field_index, field_ty);
        },
        .anon_struct_type => |anon_struct| {
            if (anon_struct.names.len == 0) {
                return sema.tuple_field_val(block, src, struct_byval, field_name, field_name_src, struct_ty);
            } else {
                const field_index = try sema.anon_struct_field_index(block, struct_ty, field_name, field_name_src);
                return sema.tuple_field_val_by_index(block, src, struct_byval, field_index, struct_ty);
            }
        },
        else => unreachable,
    }
}

fn tuple_field_val(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    tuple_byval: Air.Inst.Ref,
    field_name: InternPool.NullTerminatedString,
    field_name_src: LazySrcLoc,
    tuple_ty: Type,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    if (field_name.eql_slice("len", &mod.intern_pool)) {
        return mod.int_ref(Type.usize, tuple_ty.struct_field_count(mod));
    }
    const field_index = try sema.tuple_field_index(block, tuple_ty, field_name, field_name_src);
    return sema.tuple_field_val_by_index(block, src, tuple_byval, field_index, tuple_ty);
}

/// Asserts that `field_name` is not "len".
fn tuple_field_index(
    sema: *Sema,
    block: *Block,
    tuple_ty: Type,
    field_name: InternPool.NullTerminatedString,
    field_name_src: LazySrcLoc,
) CompileError!u32 {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    assert(!field_name.eql_slice("len", ip));
    if (field_name.to_unsigned(ip)) |field_index| {
        if (field_index < tuple_ty.struct_field_count(mod)) return field_index;
        return sema.fail(block, field_name_src, "index '{}' out of bounds of tuple '{}'", .{
            field_name.fmt(ip), tuple_ty.fmt(mod),
        });
    }

    return sema.fail(block, field_name_src, "no field named '{}' in tuple '{}'", .{
        field_name.fmt(ip), tuple_ty.fmt(mod),
    });
}

fn tuple_field_val_by_index(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    tuple_byval: Air.Inst.Ref,
    field_index: u32,
    tuple_ty: Type,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const field_ty = tuple_ty.struct_field_type(field_index, mod);

    if (tuple_ty.struct_field_is_comptime(field_index, mod))
        try sema.resolve_struct_field_inits(tuple_ty);
    if (try tuple_ty.struct_field_value_comptime(mod, field_index)) |default_value| {
        return Air.interned_to_ref(default_value.to_intern());
    }

    if (try sema.resolve_value(tuple_byval)) |tuple_val| {
        if ((try sema.type_has_one_possible_value(field_ty))) |opv| {
            return Air.interned_to_ref(opv.to_intern());
        }
        return switch (mod.intern_pool.index_to_key(tuple_val.to_intern())) {
            .undef => mod.undef_ref(field_ty),
            .aggregate => |aggregate| Air.interned_to_ref(switch (aggregate.storage) {
                .bytes => |bytes| try mod.int_value(Type.u8, bytes.at(field_index, &mod.intern_pool)),
                .elems => |elems| Value.from_interned(elems[field_index]),
                .repeated_elem => |elem| Value.from_interned(elem),
            }.to_intern()),
            else => unreachable,
        };
    }

    try sema.require_runtime_block(block, src, null);
    try sema.resolve_type_layout(field_ty);
    return block.add_struct_field_val(tuple_byval, field_index, field_ty);
}

fn union_field_ptr(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    union_ptr: Air.Inst.Ref,
    field_name: InternPool.NullTerminatedString,
    field_name_src: LazySrcLoc,
    union_ty: Type,
    initializing: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;

    assert(union_ty.zig_type_tag(mod) == .Union);

    const union_ptr_ty = sema.type_of(union_ptr);
    const union_ptr_info = union_ptr_ty.ptr_info(mod);
    try sema.resolve_type_fields(union_ty);
    const union_obj = mod.type_to_union(union_ty).?;
    const field_index = try sema.union_field_index(block, union_ty, field_name, field_name_src);
    const field_ty = Type.from_interned(union_obj.field_types.get(ip)[field_index]);
    const ptr_field_ty = try sema.ptr_type(.{
        .child = field_ty.to_intern(),
        .flags = .{
            .is_const = union_ptr_info.flags.is_const,
            .is_volatile = union_ptr_info.flags.is_volatile,
            .address_space = union_ptr_info.flags.address_space,
            .alignment = if (union_obj.get_layout(ip) == .auto) blk: {
                const union_align = if (union_ptr_info.flags.alignment != .none)
                    union_ptr_info.flags.alignment
                else
                    try sema.type_abi_alignment(union_ty);
                const field_align = try sema.union_field_alignment(union_obj, field_index);
                break :blk union_align.min(field_align);
            } else union_ptr_info.flags.alignment,
        },
        .packed_offset = union_ptr_info.packed_offset,
    });
    const enum_field_index: u32 = @int_cast(Type.from_interned(union_obj.enum_tag_ty).enum_field_index(field_name, mod).?);

    if (initializing and field_ty.zig_type_tag(mod) == .NoReturn) {
        const msg = msg: {
            const msg = try sema.err_msg(block, src, "cannot initialize 'noreturn' field of union", .{});
            errdefer msg.destroy(sema.gpa);

            try sema.add_field_err_note(union_ty, field_index, msg, "field '{}' declared here", .{
                field_name.fmt(ip),
            });
            try sema.add_declared_here_note(msg, union_ty);
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    if (try sema.resolve_defined_value(block, src, union_ptr)) |union_ptr_val| ct: {
        switch (union_obj.get_layout(ip)) {
            .auto => if (initializing) {
                // Store to the union to initialize the tag.
                const field_tag = try mod.enum_value_field_index(Type.from_interned(union_obj.enum_tag_ty), enum_field_index);
                const payload_ty = Type.from_interned(union_obj.field_types.get(ip)[field_index]);
                const new_union_val = try mod.union_value(union_ty, field_tag, try mod.undef_value(payload_ty));
                try sema.store_ptr_val(block, src, union_ptr_val, new_union_val, union_ty);
            } else {
                const union_val = (try sema.pointer_deref(block, src, union_ptr_val, union_ptr_ty)) orelse
                    break :ct;
                if (union_val.is_undef(mod)) {
                    return sema.fail_with_use_of_undef(block, src);
                }
                const un = ip.index_to_key(union_val.to_intern()).un;
                const field_tag = try mod.enum_value_field_index(Type.from_interned(union_obj.enum_tag_ty), enum_field_index);
                const tag_matches = un.tag == field_tag.to_intern();
                if (!tag_matches) {
                    const msg = msg: {
                        const active_index = Type.from_interned(union_obj.enum_tag_ty).enum_tag_field_index(Value.from_interned(un.tag), mod).?;
                        const active_field_name = Type.from_interned(union_obj.enum_tag_ty).enum_field_name(active_index, mod);
                        const msg = try sema.err_msg(block, src, "access of union field '{}' while field '{}' is active", .{
                            field_name.fmt(ip),
                            active_field_name.fmt(ip),
                        });
                        errdefer msg.destroy(sema.gpa);
                        try sema.add_declared_here_note(msg, union_ty);
                        break :msg msg;
                    };
                    return sema.fail_with_owned_error_msg(block, msg);
                }
            },
            .@"packed", .@"extern" => {},
        }
        const field_ptr_val = try union_ptr_val.ptr_field(field_index, sema);
        return Air.interned_to_ref(field_ptr_val.to_intern());
    }

    try sema.require_runtime_block(block, src, null);
    if (!initializing and union_obj.get_layout(ip) == .auto and block.want_safety() and
        union_ty.union_tag_type_safety(mod) != null and union_obj.field_types.len > 1)
    {
        const wanted_tag_val = try mod.enum_value_field_index(Type.from_interned(union_obj.enum_tag_ty), enum_field_index);
        const wanted_tag = Air.interned_to_ref(wanted_tag_val.to_intern());
        // TODO would it be better if get_union_tag supported pointers to unions?
        const union_val = try block.add_ty_op(.load, union_ty, union_ptr);
        const active_tag = try block.add_ty_op(.get_union_tag, Type.from_interned(union_obj.enum_tag_ty), union_val);
        try sema.panic_inactive_union_field(block, src, active_tag, wanted_tag);
    }
    if (field_ty.zig_type_tag(mod) == .NoReturn) {
        _ = try block.add_no_op(.unreach);
        return .unreachable_value;
    }
    return block.add_struct_field_ptr(union_ptr, field_index, ptr_field_ty);
}

fn union_field_val(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    union_byval: Air.Inst.Ref,
    field_name: InternPool.NullTerminatedString,
    field_name_src: LazySrcLoc,
    union_ty: Type,
) CompileError!Air.Inst.Ref {
    const zcu = sema.mod;
    const ip = &zcu.intern_pool;
    assert(union_ty.zig_type_tag(zcu) == .Union);

    try sema.resolve_type_fields(union_ty);
    const union_obj = zcu.type_to_union(union_ty).?;
    const field_index = try sema.union_field_index(block, union_ty, field_name, field_name_src);
    const field_ty = Type.from_interned(union_obj.field_types.get(ip)[field_index]);
    const enum_field_index: u32 = @int_cast(Type.from_interned(union_obj.enum_tag_ty).enum_field_index(field_name, zcu).?);

    if (try sema.resolve_value(union_byval)) |union_val| {
        if (union_val.is_undef(zcu)) return zcu.undef_ref(field_ty);

        const un = ip.index_to_key(union_val.to_intern()).un;
        const field_tag = try zcu.enum_value_field_index(Type.from_interned(union_obj.enum_tag_ty), enum_field_index);
        const tag_matches = un.tag == field_tag.to_intern();
        switch (union_obj.get_layout(ip)) {
            .auto => {
                if (tag_matches) {
                    return Air.interned_to_ref(un.val);
                } else {
                    const msg = msg: {
                        const active_index = Type.from_interned(union_obj.enum_tag_ty).enum_tag_field_index(Value.from_interned(un.tag), zcu).?;
                        const active_field_name = Type.from_interned(union_obj.enum_tag_ty).enum_field_name(active_index, zcu);
                        const msg = try sema.err_msg(block, src, "access of union field '{}' while field '{}' is active", .{
                            field_name.fmt(ip), active_field_name.fmt(ip),
                        });
                        errdefer msg.destroy(sema.gpa);
                        try sema.add_declared_here_note(msg, union_ty);
                        break :msg msg;
                    };
                    return sema.fail_with_owned_error_msg(block, msg);
                }
            },
            .@"extern" => if (tag_matches) {
                // Fast path - no need to use bitcast logic.
                return Air.interned_to_ref(un.val);
            } else if (try sema.bitCastVal(union_val, field_ty, 0, 0, 0)) |field_val| {
                return Air.interned_to_ref(field_val.to_intern());
            },
            .@"packed" => if (tag_matches) {
                // Fast path - no need to use bitcast logic.
                return Air.interned_to_ref(un.val);
            } else if (try sema.bitCastVal(union_val, field_ty, 0, try union_ty.bit_size_advanced(zcu, sema), 0)) |field_val| {
                return Air.interned_to_ref(field_val.to_intern());
            },
        }
    }

    try sema.require_runtime_block(block, src, null);
    if (union_obj.get_layout(ip) == .auto and block.want_safety() and
        union_ty.union_tag_type_safety(zcu) != null and union_obj.field_types.len > 1)
    {
        const wanted_tag_val = try zcu.enum_value_field_index(Type.from_interned(union_obj.enum_tag_ty), enum_field_index);
        const wanted_tag = Air.interned_to_ref(wanted_tag_val.to_intern());
        const active_tag = try block.add_ty_op(.get_union_tag, Type.from_interned(union_obj.enum_tag_ty), union_byval);
        try sema.panic_inactive_union_field(block, src, active_tag, wanted_tag);
    }
    if (field_ty.zig_type_tag(zcu) == .NoReturn) {
        _ = try block.add_no_op(.unreach);
        return .unreachable_value;
    }
    try sema.resolve_type_layout(field_ty);
    return block.add_struct_field_val(union_byval, field_index, field_ty);
}

fn elem_ptr(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    indexable_ptr: Air.Inst.Ref,
    elem_index: Air.Inst.Ref,
    elem_index_src: LazySrcLoc,
    init: bool,
    oob_safety: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const indexable_ptr_src = src; // TODO better source location
    const indexable_ptr_ty = sema.type_of(indexable_ptr);

    const indexable_ty = switch (indexable_ptr_ty.zig_type_tag(mod)) {
        .Pointer => indexable_ptr_ty.child_type(mod),
        else => return sema.fail(block, indexable_ptr_src, "expected pointer, found '{}'", .{indexable_ptr_ty.fmt(mod)}),
    };
    try check_indexable(sema, block, src, indexable_ty);

    const elem_ptr = switch (indexable_ty.zig_type_tag(mod)) {
        .Array, .Vector => try sema.elem_ptr_array(block, src, indexable_ptr_src, indexable_ptr, elem_index_src, elem_index, init, oob_safety),
        .Struct => blk: {
            // Tuple field access.
            const index_val = try sema.resolve_const_defined_value(block, elem_index_src, elem_index, .{
                .needed_comptime_reason = "tuple field access index must be comptime-known",
            });
            const index: u32 = @int_cast(try index_val.to_unsigned_int_advanced(sema));
            break :blk try sema.tuple_field_ptr(block, src, indexable_ptr, elem_index_src, index, init);
        },
        else => {
            const indexable = try sema.analyze_load(block, indexable_ptr_src, indexable_ptr, indexable_ptr_src);
            return elem_ptr_one_layer_only(sema, block, src, indexable, elem_index, elem_index_src, init, oob_safety);
        },
    };

    try sema.check_known_alloc_ptr(block, indexable_ptr, elem_ptr);
    return elem_ptr;
}

/// Asserts that the type of indexable is pointer.
fn elem_ptr_one_layer_only(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    indexable: Air.Inst.Ref,
    elem_index: Air.Inst.Ref,
    elem_index_src: LazySrcLoc,
    init: bool,
    oob_safety: bool,
) CompileError!Air.Inst.Ref {
    const indexable_src = src; // TODO better source location
    const indexable_ty = sema.type_of(indexable);
    const mod = sema.mod;

    try check_indexable(sema, block, src, indexable_ty);

    switch (indexable_ty.ptr_size(mod)) {
        .Slice => return sema.elem_ptr_slice(block, src, indexable_src, indexable, elem_index_src, elem_index, oob_safety),
        .Many, .C => {
            const maybe_ptr_val = try sema.resolve_defined_value(block, indexable_src, indexable);
            const maybe_index_val = try sema.resolve_defined_value(block, elem_index_src, elem_index);
            const runtime_src = rs: {
                const ptr_val = maybe_ptr_val orelse break :rs indexable_src;
                const index_val = maybe_index_val orelse break :rs elem_index_src;
                const index: usize = @int_cast(try index_val.to_unsigned_int_advanced(sema));
                const elem_ptr = try ptr_val.ptr_elem(index, sema);
                return Air.interned_to_ref(elem_ptr.to_intern());
            };
            const result_ty = try sema.elem_ptr_type(indexable_ty, null);

            try sema.require_runtime_block(block, src, runtime_src);
            return block.add_ptr_elem_ptr(indexable, elem_index, result_ty);
        },
        .One => {
            const child_ty = indexable_ty.child_type(mod);
            const elem_ptr = switch (child_ty.zig_type_tag(mod)) {
                .Array, .Vector => try sema.elem_ptr_array(block, src, indexable_src, indexable, elem_index_src, elem_index, init, oob_safety),
                .Struct => blk: {
                    assert(child_ty.is_tuple(mod));
                    const index_val = try sema.resolve_const_defined_value(block, elem_index_src, elem_index, .{
                        .needed_comptime_reason = "tuple field access index must be comptime-known",
                    });
                    const index: u32 = @int_cast(try index_val.to_unsigned_int_advanced(sema));
                    break :blk try sema.tuple_field_ptr(block, indexable_src, indexable, elem_index_src, index, false);
                },
                else => unreachable, // Guaranteed by check_indexable
            };
            try sema.check_known_alloc_ptr(block, indexable, elem_ptr);
            return elem_ptr;
        },
    }
}

fn elem_val(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    indexable: Air.Inst.Ref,
    elem_index_uncasted: Air.Inst.Ref,
    elem_index_src: LazySrcLoc,
    oob_safety: bool,
) CompileError!Air.Inst.Ref {
    const indexable_src = src; // TODO better source location
    const indexable_ty = sema.type_of(indexable);
    const mod = sema.mod;

    try check_indexable(sema, block, src, indexable_ty);

    // TODO in case of a vector of pointers, we need to detect whether the element
    // index is a scalar or vector instead of unconditionally casting to usize.
    const elem_index = try sema.coerce(block, Type.usize, elem_index_uncasted, elem_index_src);

    switch (indexable_ty.zig_type_tag(mod)) {
        .Pointer => switch (indexable_ty.ptr_size(mod)) {
            .Slice => return sema.elem_val_slice(block, src, indexable_src, indexable, elem_index_src, elem_index, oob_safety),
            .Many, .C => {
                const maybe_indexable_val = try sema.resolve_defined_value(block, indexable_src, indexable);
                const maybe_index_val = try sema.resolve_defined_value(block, elem_index_src, elem_index);

                const runtime_src = rs: {
                    const indexable_val = maybe_indexable_val orelse break :rs indexable_src;
                    const index_val = maybe_index_val orelse break :rs elem_index_src;
                    const index: usize = @int_cast(try index_val.to_unsigned_int_advanced(sema));
                    const elem_ty = indexable_ty.elem_type2(mod);
                    const many_ptr_ty = try mod.many_const_ptr_type(elem_ty);
                    const many_ptr_val = try mod.get_coerced(indexable_val, many_ptr_ty);
                    const elem_ptr_ty = try mod.single_const_ptr_type(elem_ty);
                    const elem_ptr_val = try many_ptr_val.ptr_elem(index, sema);
                    if (try sema.pointer_deref(block, indexable_src, elem_ptr_val, elem_ptr_ty)) |elem_val| {
                        return Air.interned_to_ref((try mod.get_coerced(elem_val, elem_ty)).to_intern());
                    }
                    break :rs indexable_src;
                };

                try sema.require_runtime_block(block, src, runtime_src);
                return block.add_bin_op(.ptr_elem_val, indexable, elem_index);
            },
            .One => {
                arr_sent: {
                    const inner_ty = indexable_ty.child_type(mod);
                    if (inner_ty.zig_type_tag(mod) != .Array) break :arr_sent;
                    const sentinel = inner_ty.sentinel(mod) orelse break :arr_sent;
                    const index_val = try sema.resolve_defined_value(block, elem_index_src, elem_index) orelse break :arr_sent;
                    const index = try sema.usize_cast(block, src, try index_val.to_unsigned_int_advanced(sema));
                    if (index != inner_ty.array_len(mod)) break :arr_sent;
                    return Air.interned_to_ref(sentinel.to_intern());
                }
                const elem_ptr = try sema.elem_ptr(block, indexable_src, indexable, elem_index, elem_index_src, false, oob_safety);
                return sema.analyze_load(block, indexable_src, elem_ptr, elem_index_src);
            },
        },
        .Array => return sema.elem_val_array(block, src, indexable_src, indexable, elem_index_src, elem_index, oob_safety),
        .Vector => {
            // TODO: If the index is a vector, the result should be a vector.
            return sema.elem_val_array(block, src, indexable_src, indexable, elem_index_src, elem_index, oob_safety);
        },
        .Struct => {
            // Tuple field access.
            const index_val = try sema.resolve_const_defined_value(block, elem_index_src, elem_index, .{
                .needed_comptime_reason = "tuple field access index must be comptime-known",
            });
            const index: u32 = @int_cast(try index_val.to_unsigned_int_advanced(sema));
            return sema.tuple_field(block, indexable_src, indexable, elem_index_src, index);
        },
        else => unreachable,
    }
}

fn validate_runtime_elem_access(
    sema: *Sema,
    block: *Block,
    elem_index_src: LazySrcLoc,
    elem_ty: Type,
    parent_ty: Type,
    parent_src: LazySrcLoc,
) CompileError!void {
    const mod = sema.mod;
    if (try sema.type_requires_comptime(elem_ty)) {
        const msg = msg: {
            const msg = try sema.err_msg(
                block,
                elem_index_src,
                "values of type '{}' must be comptime-known, but index value is runtime-known",
                .{parent_ty.fmt(mod)},
            );
            errdefer msg.destroy(sema.gpa);

            const src_decl = mod.decl_ptr(block.src_decl);
            try sema.explain_why_type_is_comptime(msg, src_decl.to_src_loc(parent_src, mod), parent_ty);

            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }
}

fn tuple_field_ptr(
    sema: *Sema,
    block: *Block,
    tuple_ptr_src: LazySrcLoc,
    tuple_ptr: Air.Inst.Ref,
    field_index_src: LazySrcLoc,
    field_index: u32,
    init: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const tuple_ptr_ty = sema.type_of(tuple_ptr);
    const tuple_ty = tuple_ptr_ty.child_type(mod);
    try sema.resolve_type_fields(tuple_ty);
    const field_count = tuple_ty.struct_field_count(mod);

    if (field_count == 0) {
        return sema.fail(block, tuple_ptr_src, "indexing into empty tuple is not allowed", .{});
    }

    if (field_index >= field_count) {
        return sema.fail(block, field_index_src, "index {d} outside tuple of length {d}", .{
            field_index, field_count,
        });
    }

    const field_ty = tuple_ty.struct_field_type(field_index, mod);
    const ptr_field_ty = try sema.ptr_type(.{
        .child = field_ty.to_intern(),
        .flags = .{
            .is_const = !tuple_ptr_ty.ptr_is_mutable(mod),
            .is_volatile = tuple_ptr_ty.is_volatile_ptr(mod),
            .address_space = tuple_ptr_ty.ptr_address_space(mod),
        },
    });

    if (tuple_ty.struct_field_is_comptime(field_index, mod))
        try sema.resolve_struct_field_inits(tuple_ty);

    if (try tuple_ty.struct_field_value_comptime(mod, field_index)) |default_val| {
        return Air.interned_to_ref((try mod.intern(.{ .ptr = .{
            .ty = ptr_field_ty.to_intern(),
            .base_addr = .{ .comptime_field = default_val.to_intern() },
            .byte_offset = 0,
        } })));
    }

    if (try sema.resolve_value(tuple_ptr)) |tuple_ptr_val| {
        const field_ptr_val = try tuple_ptr_val.ptr_field(field_index, sema);
        return Air.interned_to_ref(field_ptr_val.to_intern());
    }

    if (!init) {
        try sema.validate_runtime_elem_access(block, field_index_src, field_ty, tuple_ty, tuple_ptr_src);
    }

    try sema.require_runtime_block(block, tuple_ptr_src, null);
    return block.add_struct_field_ptr(tuple_ptr, field_index, ptr_field_ty);
}

fn tuple_field(
    sema: *Sema,
    block: *Block,
    tuple_src: LazySrcLoc,
    tuple: Air.Inst.Ref,
    field_index_src: LazySrcLoc,
    field_index: u32,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const tuple_ty = sema.type_of(tuple);
    try sema.resolve_type_fields(tuple_ty);
    const field_count = tuple_ty.struct_field_count(mod);

    if (field_count == 0) {
        return sema.fail(block, tuple_src, "indexing into empty tuple is not allowed", .{});
    }

    if (field_index >= field_count) {
        return sema.fail(block, field_index_src, "index {d} outside tuple of length {d}", .{
            field_index, field_count,
        });
    }

    const field_ty = tuple_ty.struct_field_type(field_index, mod);

    if (tuple_ty.struct_field_is_comptime(field_index, mod))
        try sema.resolve_struct_field_inits(tuple_ty);
    if (try tuple_ty.struct_field_value_comptime(mod, field_index)) |default_value| {
        return Air.interned_to_ref(default_value.to_intern()); // comptime field
    }

    if (try sema.resolve_value(tuple)) |tuple_val| {
        if (tuple_val.is_undef(mod)) return mod.undef_ref(field_ty);
        return Air.interned_to_ref((try tuple_val.field_value(mod, field_index)).to_intern());
    }

    try sema.validate_runtime_elem_access(block, field_index_src, field_ty, tuple_ty, tuple_src);

    try sema.require_runtime_block(block, tuple_src, null);
    try sema.resolve_type_layout(field_ty);
    return block.add_struct_field_val(tuple, field_index, field_ty);
}

fn elem_val_array(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    array_src: LazySrcLoc,
    array: Air.Inst.Ref,
    elem_index_src: LazySrcLoc,
    elem_index: Air.Inst.Ref,
    oob_safety: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const array_ty = sema.type_of(array);
    const array_sent = array_ty.sentinel(mod);
    const array_len = array_ty.array_len(mod);
    const array_len_s = array_len + @int_from_bool(array_sent != null);
    const elem_ty = array_ty.child_type(mod);

    if (array_len_s == 0) {
        return sema.fail(block, array_src, "indexing into empty array is not allowed", .{});
    }

    const maybe_undef_array_val = try sema.resolve_value(array);
    // index must be defined since it can access out of bounds
    const maybe_index_val = try sema.resolve_defined_value(block, elem_index_src, elem_index);

    if (maybe_index_val) |index_val| {
        const index: usize = @int_cast(try index_val.to_unsigned_int_advanced(sema));
        if (array_sent) |s| {
            if (index == array_len) {
                return Air.interned_to_ref(s.to_intern());
            }
        }
        if (index >= array_len_s) {
            const sentinel_label: []const u8 = if (array_sent != null) " +1 (sentinel)" else "";
            return sema.fail(block, elem_index_src, "index {d} outside array of length {d}{s}", .{ index, array_len, sentinel_label });
        }
    }
    if (maybe_undef_array_val) |array_val| {
        if (array_val.is_undef(mod)) {
            return mod.undef_ref(elem_ty);
        }
        if (maybe_index_val) |index_val| {
            const index: usize = @int_cast(try index_val.to_unsigned_int_advanced(sema));
            const elem_val = try array_val.elem_value(mod, index);
            return Air.interned_to_ref(elem_val.to_intern());
        }
    }

    try sema.validate_runtime_elem_access(block, elem_index_src, elem_ty, array_ty, array_src);

    const runtime_src = if (maybe_undef_array_val != null) elem_index_src else array_src;
    if (oob_safety and block.want_safety()) {
        // Runtime check is only needed if unable to comptime check
        if (maybe_index_val == null) {
            const len_inst = try mod.int_ref(Type.usize, array_len);
            const cmp_op: Air.Inst.Tag = if (array_sent != null) .cmp_lte else .cmp_lt;
            try sema.panic_index_out_of_bounds(block, src, elem_index, len_inst, cmp_op);
        }
    }

    if (try sema.type_has_one_possible_value(elem_ty)) |elem_val|
        return Air.interned_to_ref(elem_val.to_intern());

    try sema.require_runtime_block(block, src, runtime_src);
    try sema.queue_full_type_resolution(array_ty);
    return block.add_bin_op(.array_elem_val, array, elem_index);
}

fn elem_ptr_array(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    array_ptr_src: LazySrcLoc,
    array_ptr: Air.Inst.Ref,
    elem_index_src: LazySrcLoc,
    elem_index: Air.Inst.Ref,
    init: bool,
    oob_safety: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const array_ptr_ty = sema.type_of(array_ptr);
    const array_ty = array_ptr_ty.child_type(mod);
    const array_sent = array_ty.sentinel(mod) != null;
    const array_len = array_ty.array_len(mod);
    const array_len_s = array_len + @int_from_bool(array_sent);

    if (array_len_s == 0) {
        return sema.fail(block, array_ptr_src, "indexing into empty array is not allowed", .{});
    }

    const maybe_undef_array_ptr_val = try sema.resolve_value(array_ptr);
    // The index must not be undefined since it can be out of bounds.
    const offset: ?usize = if (try sema.resolve_defined_value(block, elem_index_src, elem_index)) |index_val| o: {
        const index = try sema.usize_cast(block, elem_index_src, try index_val.to_unsigned_int_advanced(sema));
        if (index >= array_len_s) {
            const sentinel_label: []const u8 = if (array_sent) " +1 (sentinel)" else "";
            return sema.fail(block, elem_index_src, "index {d} outside array of length {d}{s}", .{ index, array_len, sentinel_label });
        }
        break :o index;
    } else null;

    const elem_ptr_ty = try sema.elem_ptr_type(array_ptr_ty, offset);

    if (maybe_undef_array_ptr_val) |array_ptr_val| {
        if (array_ptr_val.is_undef(mod)) {
            return mod.undef_ref(elem_ptr_ty);
        }
        if (offset) |index| {
            const elem_ptr = try array_ptr_val.ptr_elem(index, sema);
            return Air.interned_to_ref(elem_ptr.to_intern());
        }
    }

    if (!init) {
        try sema.validate_runtime_elem_access(block, elem_index_src, array_ty.elem_type2(mod), array_ty, array_ptr_src);
    }

    const runtime_src = if (maybe_undef_array_ptr_val != null) elem_index_src else array_ptr_src;
    try sema.require_runtime_block(block, src, runtime_src);

    // Runtime check is only needed if unable to comptime check.
    if (oob_safety and block.want_safety() and offset == null) {
        const len_inst = try mod.int_ref(Type.usize, array_len);
        const cmp_op: Air.Inst.Tag = if (array_sent) .cmp_lte else .cmp_lt;
        try sema.panic_index_out_of_bounds(block, src, elem_index, len_inst, cmp_op);
    }

    return block.add_ptr_elem_ptr(array_ptr, elem_index, elem_ptr_ty);
}

fn elem_val_slice(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    slice_src: LazySrcLoc,
    slice: Air.Inst.Ref,
    elem_index_src: LazySrcLoc,
    elem_index: Air.Inst.Ref,
    oob_safety: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const slice_ty = sema.type_of(slice);
    const slice_sent = slice_ty.sentinel(mod) != null;
    const elem_ty = slice_ty.elem_type2(mod);
    var runtime_src = slice_src;

    // slice must be defined since it can dereferenced as null
    const maybe_slice_val = try sema.resolve_defined_value(block, slice_src, slice);
    // index must be defined since it can index out of bounds
    const maybe_index_val = try sema.resolve_defined_value(block, elem_index_src, elem_index);

    if (maybe_slice_val) |slice_val| {
        runtime_src = elem_index_src;
        const slice_len = try slice_val.slice_len(sema);
        const slice_len_s = slice_len + @int_from_bool(slice_sent);
        if (slice_len_s == 0) {
            return sema.fail(block, slice_src, "indexing into empty slice is not allowed", .{});
        }
        if (maybe_index_val) |index_val| {
            const index: usize = @int_cast(try index_val.to_unsigned_int_advanced(sema));
            if (index >= slice_len_s) {
                const sentinel_label: []const u8 = if (slice_sent) " +1 (sentinel)" else "";
                return sema.fail(block, elem_index_src, "index {d} outside slice of length {d}{s}", .{ index, slice_len, sentinel_label });
            }
            const elem_ptr_ty = try sema.elem_ptr_type(slice_ty, index);
            const elem_ptr_val = try slice_val.ptr_elem(index, sema);
            if (try sema.pointer_deref(block, slice_src, elem_ptr_val, elem_ptr_ty)) |elem_val| {
                return Air.interned_to_ref(elem_val.to_intern());
            }
            runtime_src = slice_src;
        }
    }

    try sema.validate_runtime_elem_access(block, elem_index_src, elem_ty, slice_ty, slice_src);

    try sema.require_runtime_block(block, src, runtime_src);
    if (oob_safety and block.want_safety()) {
        const len_inst = if (maybe_slice_val) |slice_val|
            try mod.int_ref(Type.usize, try slice_val.slice_len(sema))
        else
            try block.add_ty_op(.slice_len, Type.usize, slice);
        const cmp_op: Air.Inst.Tag = if (slice_sent) .cmp_lte else .cmp_lt;
        try sema.panic_index_out_of_bounds(block, src, elem_index, len_inst, cmp_op);
    }
    try sema.queue_full_type_resolution(sema.type_of(slice));
    return block.add_bin_op(.slice_elem_val, slice, elem_index);
}

fn elem_ptr_slice(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    slice_src: LazySrcLoc,
    slice: Air.Inst.Ref,
    elem_index_src: LazySrcLoc,
    elem_index: Air.Inst.Ref,
    oob_safety: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const slice_ty = sema.type_of(slice);
    const slice_sent = slice_ty.sentinel(mod) != null;

    const maybe_undef_slice_val = try sema.resolve_value(slice);
    // The index must not be undefined since it can be out of bounds.
    const offset: ?usize = if (try sema.resolve_defined_value(block, elem_index_src, elem_index)) |index_val| o: {
        const index = try sema.usize_cast(block, elem_index_src, try index_val.to_unsigned_int_advanced(sema));
        break :o index;
    } else null;

    const elem_ptr_ty = try sema.elem_ptr_type(slice_ty, offset);

    if (maybe_undef_slice_val) |slice_val| {
        if (slice_val.is_undef(mod)) {
            return mod.undef_ref(elem_ptr_ty);
        }
        const slice_len = try slice_val.slice_len(sema);
        const slice_len_s = slice_len + @int_from_bool(slice_sent);
        if (slice_len_s == 0) {
            return sema.fail(block, slice_src, "indexing into empty slice is not allowed", .{});
        }
        if (offset) |index| {
            if (index >= slice_len_s) {
                const sentinel_label: []const u8 = if (slice_sent) " +1 (sentinel)" else "";
                return sema.fail(block, elem_index_src, "index {d} outside slice of length {d}{s}", .{ index, slice_len, sentinel_label });
            }
            const elem_ptr_val = try slice_val.ptr_elem(index, sema);
            return Air.interned_to_ref(elem_ptr_val.to_intern());
        }
    }

    try sema.validate_runtime_elem_access(block, elem_index_src, elem_ptr_ty, slice_ty, slice_src);

    const runtime_src = if (maybe_undef_slice_val != null) elem_index_src else slice_src;
    try sema.require_runtime_block(block, src, runtime_src);
    if (oob_safety and block.want_safety()) {
        const len_inst = len: {
            if (maybe_undef_slice_val) |slice_val|
                if (!slice_val.is_undef(mod))
                    break :len try mod.int_ref(Type.usize, try slice_val.slice_len(sema));
            break :len try block.add_ty_op(.slice_len, Type.usize, slice);
        };
        const cmp_op: Air.Inst.Tag = if (slice_sent) .cmp_lte else .cmp_lt;
        try sema.panic_index_out_of_bounds(block, src, elem_index, len_inst, cmp_op);
    }
    return block.add_slice_elem_ptr(slice, elem_index, elem_ptr_ty);
}

fn coerce(
    sema: *Sema,
    block: *Block,
    dest_ty_unresolved: Type,
    inst: Air.Inst.Ref,
    inst_src: LazySrcLoc,
) CompileError!Air.Inst.Ref {
    return sema.coerce_extra(block, dest_ty_unresolved, inst, inst_src, .{}) catch |err| switch (err) {
        error.NotCoercible => unreachable,
        else => |e| return e,
    };
}

const CoersionError = CompileError || error{
    /// When coerce is called recursively, this error should be returned instead of using `fail`
    /// to ensure correct types in compile errors.
    NotCoercible,
};

const CoerceOpts = struct {
    /// Should coerce_extra emit error messages.
    report_err: bool = true,
    /// Ignored if `report_err == false`.
    is_ret: bool = false,
    /// Should coercion to comptime_int emit an error message.
    no_cast_to_comptime_int: bool = false,

    param_src: struct {
        func_inst: Air.Inst.Ref = .none,
        param_i: u32 = undefined,

        fn get(info: @This(), sema: *Sema) !?Module.SrcLoc {
            if (info.func_inst == .none) return null;
            const mod = sema.mod;
            const fn_decl = (try sema.func_decl_src(info.func_inst)) orelse return null;
            const param_src = Module.param_src(0, mod, fn_decl, info.param_i);
            if (param_src == .node_offset_param) {
                return Module.SrcLoc{
                    .file_scope = fn_decl.get_file_scope(mod),
                    .parent_decl_node = fn_decl.src_node,
                    .lazy = LazySrcLoc.nodeOffset(param_src.node_offset_param),
                };
            }
            return fn_decl.to_src_loc(param_src, mod);
        }
    } = .{},
};

fn coerce_extra(
    sema: *Sema,
    block: *Block,
    dest_ty: Type,
    inst: Air.Inst.Ref,
    inst_src: LazySrcLoc,
    opts: CoerceOpts,
) CoersionError!Air.Inst.Ref {
    if (dest_ty.is_generic_poison()) return inst;
    const zcu = sema.mod;
    const dest_ty_src = inst_src; // TODO better source location
    try sema.resolve_type_fields(dest_ty);
    const inst_ty = sema.type_of(inst);
    try sema.resolve_type_fields(inst_ty);
    const target = zcu.get_target();
    // If the types are the same, we can return the operand.
    if (dest_ty.eql(inst_ty, zcu))
        return inst;

    const maybe_inst_val = try sema.resolve_value(inst);

    var in_memory_result = try sema.coerce_in_memory_allowed(block, dest_ty, inst_ty, false, target, dest_ty_src, inst_src);
    if (in_memory_result == .ok) {
        if (maybe_inst_val) |val| {
            return sema.coerce_in_memory(val, dest_ty);
        }
        try sema.require_runtime_block(block, inst_src, null);
        try sema.queue_full_type_resolution(dest_ty);
        const new_val = try block.add_bit_cast(dest_ty, inst);
        try sema.check_known_alloc_ptr(block, inst, new_val);
        return new_val;
    }

    switch (dest_ty.zig_type_tag(zcu)) {
        .Optional => optional: {
            if (maybe_inst_val) |val| {
                // undefined sets the optional bit also to undefined.
                if (val.to_intern() == .undef) {
                    return zcu.undef_ref(dest_ty);
                }

                // null to ?T
                if (val.to_intern() == .null_value) {
                    return Air.interned_to_ref((try zcu.intern(.{ .opt = .{
                        .ty = dest_ty.to_intern(),
                        .val = .none,
                    } })));
                }
            }

            // cast from ?*T and ?[*]T to ?*anyopaque
            // but don't do it if the source type is a double pointer
            if (dest_ty.is_ptr_like_optional(zcu) and
                dest_ty.elem_type2(zcu).to_intern() == .anyopaque_type and
                inst_ty.is_ptr_at_runtime(zcu))
            anyopaque_check: {
                if (!sema.check_ptr_attributes(dest_ty, inst_ty, &in_memory_result)) break :optional;
                const elem_ty = inst_ty.elem_type2(zcu);
                if (elem_ty.zig_type_tag(zcu) == .Pointer or elem_ty.is_ptr_like_optional(zcu)) {
                    in_memory_result = .{ .double_ptr_to_anyopaque = .{
                        .actual = inst_ty,
                        .wanted = dest_ty,
                    } };
                    break :optional;
                }
                // Let the logic below handle wrapping the optional now that
                // it has been checked to correctly coerce.
                if (!inst_ty.is_ptr_like_optional(zcu)) break :anyopaque_check;
                return sema.coerce_compatible_ptrs(block, dest_ty, inst, inst_src);
            }

            // T to ?T
            const child_type = dest_ty.optional_child(zcu);
            const intermediate = sema.coerce_extra(block, child_type, inst, inst_src, .{ .report_err = false }) catch |err| switch (err) {
                error.NotCoercible => {
                    if (in_memory_result == .no_match) {
                        // Try to give more useful notes
                        in_memory_result = try sema.coerce_in_memory_allowed(block, child_type, inst_ty, false, target, dest_ty_src, inst_src);
                    }
                    break :optional;
                },
                else => |e| return e,
            };
            return try sema.wrap_optional(block, dest_ty, intermediate, inst_src);
        },
        .Pointer => pointer: {
            const dest_info = dest_ty.ptr_info(zcu);

            // Function body to function pointer.
            if (inst_ty.zig_type_tag(zcu) == .Fn) {
                const fn_val = try sema.resolve_const_defined_value(block, .unneeded, inst, undefined);
                const fn_decl = fn_val.pointer_decl(zcu).?;
                const inst_as_ptr = try sema.analyze_decl_ref(fn_decl);
                return sema.coerce(block, dest_ty, inst_as_ptr, inst_src);
            }

            // *T to *[1]T
            single_item: {
                if (dest_info.flags.size != .One) break :single_item;
                if (!inst_ty.is_single_pointer(zcu)) break :single_item;
                if (!sema.check_ptr_attributes(dest_ty, inst_ty, &in_memory_result)) break :pointer;
                const ptr_elem_ty = inst_ty.child_type(zcu);
                const array_ty = Type.from_interned(dest_info.child);
                if (array_ty.zig_type_tag(zcu) != .Array) break :single_item;
                const array_elem_ty = array_ty.child_type(zcu);
                if (array_ty.array_len(zcu) != 1) break :single_item;
                const dest_is_mut = !dest_info.flags.is_const;
                switch (try sema.coerce_in_memory_allowed(block, array_elem_ty, ptr_elem_ty, dest_is_mut, target, dest_ty_src, inst_src)) {
                    .ok => {},
                    else => break :single_item,
                }
                return sema.coerce_compatible_ptrs(block, dest_ty, inst, inst_src);
            }

            // Coercions where the source is a single pointer to an array.
            src_array_ptr: {
                if (!inst_ty.is_single_pointer(zcu)) break :src_array_ptr;
                if (!sema.check_ptr_attributes(dest_ty, inst_ty, &in_memory_result)) break :pointer;
                const array_ty = inst_ty.child_type(zcu);
                if (array_ty.zig_type_tag(zcu) != .Array) break :src_array_ptr;
                const array_elem_type = array_ty.child_type(zcu);
                const dest_is_mut = !dest_info.flags.is_const;

                const dst_elem_type = Type.from_interned(dest_info.child);
                const elem_res = try sema.coerce_in_memory_allowed(block, dst_elem_type, array_elem_type, dest_is_mut, target, dest_ty_src, inst_src);
                switch (elem_res) {
                    .ok => {},
                    else => {
                        in_memory_result = .{ .ptr_child = .{
                            .child = try elem_res.dupe(sema.arena),
                            .actual = array_elem_type,
                            .wanted = dst_elem_type,
                        } };
                        break :src_array_ptr;
                    },
                }

                if (dest_info.sentinel != .none) {
                    if (array_ty.sentinel(zcu)) |inst_sent| {
                        if (Air.interned_to_ref(dest_info.sentinel) !=
                            try sema.coerce_in_memory(inst_sent, dst_elem_type))
                        {
                            in_memory_result = .{ .ptr_sentinel = .{
                                .actual = inst_sent,
                                .wanted = Value.from_interned(dest_info.sentinel),
                                .ty = dst_elem_type,
                            } };
                            break :src_array_ptr;
                        }
                    } else {
                        in_memory_result = .{ .ptr_sentinel = .{
                            .actual = Value.@"unreachable",
                            .wanted = Value.from_interned(dest_info.sentinel),
                            .ty = dst_elem_type,
                        } };
                        break :src_array_ptr;
                    }
                }

                switch (dest_info.flags.size) {
                    .Slice => {
                        // *[N]T to []T
                        return sema.coerce_array_ptr_to_slice(block, dest_ty, inst, inst_src);
                    },
                    .C => {
                        // *[N]T to [*c]T
                        return sema.coerce_compatible_ptrs(block, dest_ty, inst, inst_src);
                    },
                    .Many => {
                        // *[N]T to [*]T
                        return sema.coerce_compatible_ptrs(block, dest_ty, inst, inst_src);
                    },
                    .One => {},
                }
            }

            // coercion from C pointer
            if (inst_ty.is_cptr(zcu)) src_c_ptr: {
                if (dest_info.flags.size == .Slice) break :src_c_ptr;
                if (!sema.check_ptr_attributes(dest_ty, inst_ty, &in_memory_result)) break :src_c_ptr;
                // In this case we must add a safety check because the C pointer
                // could be null.
                const src_elem_ty = inst_ty.child_type(zcu);
                const dest_is_mut = !dest_info.flags.is_const;
                const dst_elem_type = Type.from_interned(dest_info.child);
                switch (try sema.coerce_in_memory_allowed(block, dst_elem_type, src_elem_ty, dest_is_mut, target, dest_ty_src, inst_src)) {
                    .ok => {},
                    else => break :src_c_ptr,
                }
                return sema.coerce_compatible_ptrs(block, dest_ty, inst, inst_src);
            }

            // cast from *T and [*]T to *anyopaque
            // but don't do it if the source type is a double pointer
            if (dest_info.child == .anyopaque_type and inst_ty.zig_type_tag(zcu) == .Pointer) to_anyopaque: {
                if (!sema.check_ptr_attributes(dest_ty, inst_ty, &in_memory_result)) break :pointer;
                const elem_ty = inst_ty.elem_type2(zcu);
                if (elem_ty.zig_type_tag(zcu) == .Pointer or elem_ty.is_ptr_like_optional(zcu)) {
                    in_memory_result = .{ .double_ptr_to_anyopaque = .{
                        .actual = inst_ty,
                        .wanted = dest_ty,
                    } };
                    break :pointer;
                }
                if (dest_ty.is_slice(zcu)) break :to_anyopaque;
                if (inst_ty.is_slice(zcu)) {
                    in_memory_result = .{ .slice_to_anyopaque = .{
                        .actual = inst_ty,
                        .wanted = dest_ty,
                    } };
                    break :pointer;
                }
                return sema.coerce_compatible_ptrs(block, dest_ty, inst, inst_src);
            }

            switch (dest_info.flags.size) {
                // coercion to C pointer
                .C => switch (inst_ty.zig_type_tag(zcu)) {
                    .Null => return Air.interned_to_ref(try zcu.intern(.{ .ptr = .{
                        .ty = dest_ty.to_intern(),
                        .base_addr = .int,
                        .byte_offset = 0,
                    } })),
                    .ComptimeInt => {
                        const addr = sema.coerce_extra(block, Type.usize, inst, inst_src, .{ .report_err = false }) catch |err| switch (err) {
                            error.NotCoercible => break :pointer,
                            else => |e| return e,
                        };
                        return try sema.coerce_compatible_ptrs(block, dest_ty, addr, inst_src);
                    },
                    .Int => {
                        const ptr_size_ty = switch (inst_ty.int_info(zcu).signedness) {
                            .signed => Type.isize,
                            .unsigned => Type.usize,
                        };
                        const addr = sema.coerce_extra(block, ptr_size_ty, inst, inst_src, .{ .report_err = false }) catch |err| switch (err) {
                            error.NotCoercible => {
                                // Try to give more useful notes
                                in_memory_result = try sema.coerce_in_memory_allowed(block, ptr_size_ty, inst_ty, false, target, dest_ty_src, inst_src);
                                break :pointer;
                            },
                            else => |e| return e,
                        };
                        return try sema.coerce_compatible_ptrs(block, dest_ty, addr, inst_src);
                    },
                    .Pointer => p: {
                        if (!sema.check_ptr_attributes(dest_ty, inst_ty, &in_memory_result)) break :p;
                        const inst_info = inst_ty.ptr_info(zcu);
                        switch (try sema.coerce_in_memory_allowed(
                            block,
                            Type.from_interned(dest_info.child),
                            Type.from_interned(inst_info.child),
                            !dest_info.flags.is_const,
                            target,
                            dest_ty_src,
                            inst_src,
                        )) {
                            .ok => {},
                            else => break :p,
                        }
                        if (inst_info.flags.size == .Slice) {
                            assert(dest_info.sentinel == .none);
                            if (inst_info.sentinel == .none or
                                inst_info.sentinel != (try zcu.int_value(Type.from_interned(inst_info.child), 0)).to_intern())
                                break :p;

                            const slice_ptr = try sema.analyze_slice_ptr(block, inst_src, inst, inst_ty);
                            return sema.coerce_compatible_ptrs(block, dest_ty, slice_ptr, inst_src);
                        }
                        return sema.coerce_compatible_ptrs(block, dest_ty, inst, inst_src);
                    },
                    else => {},
                },
                .One => switch (Type.from_interned(dest_info.child).zig_type_tag(zcu)) {
                    .Union => {
                        // pointer to anonymous struct to pointer to union
                        if (inst_ty.is_single_pointer(zcu) and
                            inst_ty.child_type(zcu).is_anon_struct(zcu) and
                            sema.check_ptr_attributes(dest_ty, inst_ty, &in_memory_result))
                        {
                            return sema.coerce_anon_struct_to_union_ptrs(block, dest_ty, dest_ty_src, inst, inst_src);
                        }
                    },
                    .Struct => {
                        // pointer to anonymous struct to pointer to struct
                        if (inst_ty.is_single_pointer(zcu) and
                            inst_ty.child_type(zcu).is_anon_struct(zcu) and
                            sema.check_ptr_attributes(dest_ty, inst_ty, &in_memory_result))
                        {
                            return sema.coerce_anon_struct_to_struct_ptrs(block, dest_ty, dest_ty_src, inst, inst_src) catch |err| switch (err) {
                                error.NotCoercible => break :pointer,
                                else => |e| return e,
                            };
                        }
                    },
                    .Array => {
                        // pointer to tuple to pointer to array
                        if (inst_ty.is_single_pointer(zcu) and
                            inst_ty.child_type(zcu).is_tuple(zcu) and
                            sema.check_ptr_attributes(dest_ty, inst_ty, &in_memory_result))
                        {
                            return sema.coerce_tuple_to_array_ptrs(block, dest_ty, dest_ty_src, inst, inst_src);
                        }
                    },
                    else => {},
                },
                .Slice => to_slice: {
                    if (inst_ty.zig_type_tag(zcu) == .Array) {
                        return sema.fail(
                            block,
                            inst_src,
                            "array literal requires address-of operator (&) to coerce to slice type '{}'",
                            .{dest_ty.fmt(zcu)},
                        );
                    }

                    if (!inst_ty.is_single_pointer(zcu)) break :to_slice;
                    const inst_child_ty = inst_ty.child_type(zcu);
                    if (!inst_child_ty.is_tuple(zcu)) break :to_slice;

                    // empty tuple to zero-length slice
                    // note that this allows coercing to a mutable slice.
                    if (inst_child_ty.struct_field_count(zcu) == 0) {
                        const align_val = try dest_ty.ptr_alignment_advanced(zcu, sema);
                        return Air.interned_to_ref(try zcu.intern(.{ .slice = .{
                            .ty = dest_ty.to_intern(),
                            .ptr = try zcu.intern(.{ .ptr = .{
                                .ty = dest_ty.slice_ptr_field_type(zcu).to_intern(),
                                .base_addr = .int,
                                .byte_offset = align_val.to_byte_units().?,
                            } }),
                            .len = .zero_usize,
                        } }));
                    }

                    // pointer to tuple to slice
                    if (!dest_info.flags.is_const) {
                        const err_msg = err_msg: {
                            const err_msg = try sema.err_msg(block, inst_src, "cannot cast pointer to tuple to '{}'", .{dest_ty.fmt(zcu)});
                            errdefer err_msg.destroy(sema.gpa);
                            try sema.err_note(block, dest_ty_src, err_msg, "pointers to tuples can only coerce to constant pointers", .{});
                            break :err_msg err_msg;
                        };
                        return sema.fail_with_owned_error_msg(block, err_msg);
                    }
                    return sema.coerce_tuple_to_slice_ptrs(block, dest_ty, dest_ty_src, inst, inst_src);
                },
                .Many => p: {
                    if (!inst_ty.is_slice(zcu)) break :p;
                    if (!sema.check_ptr_attributes(dest_ty, inst_ty, &in_memory_result)) break :p;
                    const inst_info = inst_ty.ptr_info(zcu);

                    switch (try sema.coerce_in_memory_allowed(
                        block,
                        Type.from_interned(dest_info.child),
                        Type.from_interned(inst_info.child),
                        !dest_info.flags.is_const,
                        target,
                        dest_ty_src,
                        inst_src,
                    )) {
                        .ok => {},
                        else => break :p,
                    }

                    if (dest_info.sentinel == .none or inst_info.sentinel == .none or
                        Air.interned_to_ref(dest_info.sentinel) !=
                        try sema.coerce_in_memory(Value.from_interned(inst_info.sentinel), Type.from_interned(dest_info.child)))
                        break :p;

                    const slice_ptr = try sema.analyze_slice_ptr(block, inst_src, inst, inst_ty);
                    return sema.coerce_compatible_ptrs(block, dest_ty, slice_ptr, inst_src);
                },
            }
        },
        .Int, .ComptimeInt => switch (inst_ty.zig_type_tag(zcu)) {
            .Float, .ComptimeFloat => float: {
                const val = maybe_inst_val orelse {
                    if (dest_ty.zig_type_tag(zcu) == .ComptimeInt) {
                        if (!opts.report_err) return error.NotCoercible;
                        return sema.fail_with_needed_comptime(block, inst_src, .{
                            .needed_comptime_reason = "value being casted to 'comptime_int' must be comptime-known",
                        });
                    }
                    break :float;
                };
                const result_val = try sema.int_from_float(block, inst_src, val, inst_ty, dest_ty, .exact);
                return Air.interned_to_ref(result_val.to_intern());
            },
            .Int, .ComptimeInt => {
                if (maybe_inst_val) |val| {
                    // comptime-known integer to other number
                    if (!(try sema.int_fits_in_type(val, dest_ty, null))) {
                        if (!opts.report_err) return error.NotCoercible;
                        return sema.fail(block, inst_src, "type '{}' cannot represent integer value '{}'", .{ dest_ty.fmt(zcu), val.fmt_value(zcu, sema) });
                    }
                    return switch (zcu.intern_pool.index_to_key(val.to_intern())) {
                        .undef => try zcu.undef_ref(dest_ty),
                        .int => |int| Air.interned_to_ref(
                            try zcu.intern_pool.get_coerced_ints(zcu.gpa, int, dest_ty.to_intern()),
                        ),
                        else => unreachable,
                    };
                }
                if (dest_ty.zig_type_tag(zcu) == .ComptimeInt) {
                    if (!opts.report_err) return error.NotCoercible;
                    if (opts.no_cast_to_comptime_int) return inst;
                    return sema.fail_with_needed_comptime(block, inst_src, .{
                        .needed_comptime_reason = "value being casted to 'comptime_int' must be comptime-known",
                    });
                }

                // integer widening
                const dst_info = dest_ty.int_info(zcu);
                const src_info = inst_ty.int_info(zcu);
                if ((src_info.signedness == dst_info.signedness and dst_info.bits >= src_info.bits) or
                    // small enough unsigned ints can get casted to large enough signed ints
                    (dst_info.signedness == .signed and dst_info.bits > src_info.bits))
                {
                    try sema.require_runtime_block(block, inst_src, null);
                    return block.add_ty_op(.intcast, dest_ty, inst);
                }
            },
            else => {},
        },
        .Float, .ComptimeFloat => switch (inst_ty.zig_type_tag(zcu)) {
            .ComptimeFloat => {
                const val = try sema.resolve_const_defined_value(block, .unneeded, inst, undefined);
                const result_val = try val.float_cast(dest_ty, zcu);
                return Air.interned_to_ref(result_val.to_intern());
            },
            .Float => {
                if (maybe_inst_val) |val| {
                    const result_val = try val.float_cast(dest_ty, zcu);
                    if (!val.eql(try result_val.float_cast(inst_ty, zcu), inst_ty, zcu)) {
                        return sema.fail(
                            block,
                            inst_src,
                            "type '{}' cannot represent float value '{}'",
                            .{ dest_ty.fmt(zcu), val.fmt_value(zcu, sema) },
                        );
                    }
                    return Air.interned_to_ref(result_val.to_intern());
                } else if (dest_ty.zig_type_tag(zcu) == .ComptimeFloat) {
                    if (!opts.report_err) return error.NotCoercible;
                    return sema.fail_with_needed_comptime(block, inst_src, .{
                        .needed_comptime_reason = "value being casted to 'comptime_float' must be comptime-known",
                    });
                }

                // float widening
                const src_bits = inst_ty.float_bits(target);
                const dst_bits = dest_ty.float_bits(target);
                if (dst_bits >= src_bits) {
                    try sema.require_runtime_block(block, inst_src, null);
                    return block.add_ty_op(.fpext, dest_ty, inst);
                }
            },
            .Int, .ComptimeInt => int: {
                const val = maybe_inst_val orelse {
                    if (dest_ty.zig_type_tag(zcu) == .ComptimeFloat) {
                        if (!opts.report_err) return error.NotCoercible;
                        return sema.fail_with_needed_comptime(block, inst_src, .{
                            .needed_comptime_reason = "value being casted to 'comptime_float' must be comptime-known",
                        });
                    }
                    break :int;
                };
                const result_val = try val.float_from_int_advanced(sema.arena, inst_ty, dest_ty, zcu, sema);
                // TODO implement this compile error
                //const int_again_val = try result_val.int_from_float(sema.arena, inst_ty);
                //if (!int_again_val.eql(val, inst_ty, zcu)) {
                //    return sema.fail(
                //        block,
                //        inst_src,
                //        "type '{}' cannot represent integer value '{}'",
                //        .{ dest_ty.fmt(zcu), val },
                //    );
                //}
                return Air.interned_to_ref(result_val.to_intern());
            },
            else => {},
        },
        .Enum => switch (inst_ty.zig_type_tag(zcu)) {
            .EnumLiteral => {
                // enum literal to enum
                const val = try sema.resolve_const_defined_value(block, .unneeded, inst, undefined);
                const string = zcu.intern_pool.index_to_key(val.to_intern()).enum_literal;
                const field_index = dest_ty.enum_field_index(string, zcu) orelse {
                    return sema.fail(block, inst_src, "no field named '{}' in enum '{}'", .{
                        string.fmt(&zcu.intern_pool), dest_ty.fmt(zcu),
                    });
                };
                return Air.interned_to_ref((try zcu.enum_value_field_index(dest_ty, @int_cast(field_index))).to_intern());
            },
            .Union => blk: {
                // union to its own tag type
                const union_tag_ty = inst_ty.union_tag_type(zcu) orelse break :blk;
                if (union_tag_ty.eql(dest_ty, zcu)) {
                    return sema.union_to_tag(block, dest_ty, inst, inst_src);
                }
            },
            else => {},
        },
        .ErrorUnion => switch (inst_ty.zig_type_tag(zcu)) {
            .ErrorUnion => eu: {
                if (maybe_inst_val) |inst_val| {
                    switch (inst_val.to_intern()) {
                        .undef => return zcu.undef_ref(dest_ty),
                        else => switch (zcu.intern_pool.index_to_key(inst_val.to_intern())) {
                            .error_union => |error_union| switch (error_union.val) {
                                .err_name => |err_name| {
                                    const error_set_ty = inst_ty.error_union_set(zcu);
                                    const error_set_val = Air.interned_to_ref((try zcu.intern(.{ .err = .{
                                        .ty = error_set_ty.to_intern(),
                                        .name = err_name,
                                    } })));
                                    return sema.wrap_error_union_set(block, dest_ty, error_set_val, inst_src);
                                },
                                .payload => |payload| {
                                    const payload_val = Air.interned_to_ref(payload);
                                    return sema.wrap_error_union_payload(block, dest_ty, payload_val, inst_src) catch |err| switch (err) {
                                        error.NotCoercible => break :eu,
                                        else => |e| return e,
                                    };
                                },
                            },
                            else => unreachable,
                        },
                    }
                }
            },
            .ErrorSet => {
                // E to E!T
                return sema.wrap_error_union_set(block, dest_ty, inst, inst_src);
            },
            else => eu: {
                // T to E!T
                return sema.wrap_error_union_payload(block, dest_ty, inst, inst_src) catch |err| switch (err) {
                    error.NotCoercible => break :eu,
                    else => |e| return e,
                };
            },
        },
        .Union => switch (inst_ty.zig_type_tag(zcu)) {
            .Enum, .EnumLiteral => return sema.coerce_enum_to_union(block, dest_ty, dest_ty_src, inst, inst_src),
            .Struct => {
                if (inst_ty.is_anon_struct(zcu)) {
                    return sema.coerce_anon_struct_to_union(block, dest_ty, dest_ty_src, inst, inst_src);
                }
            },
            else => {},
        },
        .Array => switch (inst_ty.zig_type_tag(zcu)) {
            .Array => array_to_array: {
                // Array coercions are allowed only if the child is IMC and the sentinel is unchanged or removed.
                if (.ok != try sema.coerce_in_memory_allowed(
                    block,
                    dest_ty.child_type(zcu),
                    inst_ty.child_type(zcu),
                    false,
                    target,
                    dest_ty_src,
                    inst_src,
                )) {
                    break :array_to_array;
                }

                if (dest_ty.sentinel(zcu)) |dest_sent| {
                    const src_sent = inst_ty.sentinel(zcu) orelse break :array_to_array;
                    if (dest_sent.to_intern() != (try zcu.get_coerced(src_sent, dest_ty.child_type(zcu))).to_intern()) {
                        break :array_to_array;
                    }
                }

                return sema.coerce_array_like(block, dest_ty, dest_ty_src, inst, inst_src);
            },
            .Vector => return sema.coerce_array_like(block, dest_ty, dest_ty_src, inst, inst_src),
            .Struct => {
                if (inst == .empty_struct) {
                    return sema.array_init_empty(block, inst_src, dest_ty);
                }
                if (inst_ty.is_tuple(zcu)) {
                    return sema.coerce_tuple_to_array(block, dest_ty, dest_ty_src, inst, inst_src);
                }
            },
            else => {},
        },
        .Vector => switch (inst_ty.zig_type_tag(zcu)) {
            .Array, .Vector => return sema.coerce_array_like(block, dest_ty, dest_ty_src, inst, inst_src),
            .Struct => {
                if (inst_ty.is_tuple(zcu)) {
                    return sema.coerce_tuple_to_array(block, dest_ty, dest_ty_src, inst, inst_src);
                }
            },
            else => {},
        },
        .Struct => blk: {
            if (inst == .empty_struct) {
                return sema.struct_init_empty(block, dest_ty, dest_ty_src, inst_src);
            }
            if (inst_ty.is_tuple_or_anon_struct(zcu)) {
                return sema.coerce_tuple_to_struct(block, dest_ty, inst, inst_src) catch |err| switch (err) {
                    error.NotCoercible => break :blk,
                    else => |e| return e,
                };
            }
        },
        else => {},
    }

    // undefined to anything. We do this after the big switch above so that
    // special logic has a chance to run first, such as `*[N]T` to `[]T` which
    // should initialize the length field of the slice.
    if (maybe_inst_val) |val| if (val.to_intern() == .undef) return zcu.undef_ref(dest_ty);

    if (!opts.report_err) return error.NotCoercible;

    if (opts.is_ret and dest_ty.zig_type_tag(zcu) == .NoReturn) {
        const msg = msg: {
            const msg = try sema.err_msg(block, inst_src, "function declared 'noreturn' returns", .{});
            errdefer msg.destroy(sema.gpa);

            const ret_ty_src: LazySrcLoc = .{ .node_offset_fn_type_ret_ty = 0 };
            const src_decl = zcu.func_owner_decl_ptr(sema.func_index);
            try zcu.err_note_non_lazy(src_decl.to_src_loc(ret_ty_src, zcu), msg, "'noreturn' declared here", .{});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    const msg = msg: {
        const msg = try sema.err_msg(block, inst_src, "expected type '{}', found '{}'", .{ dest_ty.fmt(zcu), inst_ty.fmt(zcu) });
        errdefer msg.destroy(sema.gpa);

        // E!T to T
        if (inst_ty.zig_type_tag(zcu) == .ErrorUnion and
            (try sema.coerce_in_memory_allowed(block, inst_ty.error_union_payload(zcu), dest_ty, false, target, dest_ty_src, inst_src)) == .ok)
        {
            try sema.err_note(block, inst_src, msg, "cannot convert error union to payload type", .{});
            try sema.err_note(block, inst_src, msg, "consider using 'try', 'catch', or 'if'", .{});
        }

        // ?T to T
        if (inst_ty.zig_type_tag(zcu) == .Optional and
            (try sema.coerce_in_memory_allowed(block, inst_ty.optional_child(zcu), dest_ty, false, target, dest_ty_src, inst_src)) == .ok)
        {
            try sema.err_note(block, inst_src, msg, "cannot convert optional to payload type", .{});
            try sema.err_note(block, inst_src, msg, "consider using '.?', 'orelse', or 'if'", .{});
        }

        try in_memory_result.report(sema, block, inst_src, msg);

        // Add notes about function return type
        if (opts.is_ret and
            zcu.test_functions.get(zcu.func_owner_decl_index(sema.func_index)) == null)
        {
            const ret_ty_src: LazySrcLoc = .{ .node_offset_fn_type_ret_ty = 0 };
            const src_decl = zcu.func_owner_decl_ptr(sema.func_index);
            if (inst_ty.is_error(zcu) and !dest_ty.is_error(zcu)) {
                try zcu.err_note_non_lazy(src_decl.to_src_loc(ret_ty_src, zcu), msg, "function cannot return an error", .{});
            } else {
                try zcu.err_note_non_lazy(src_decl.to_src_loc(ret_ty_src, zcu), msg, "function return type declared here", .{});
            }
        }

        if (try opts.param_src.get(sema)) |param_src| {
            try zcu.err_note_non_lazy(param_src, msg, "parameter type declared here", .{});
        }

        // TODO maybe add "cannot store an error in type '{}'" note

        break :msg msg;
    };
    return sema.fail_with_owned_error_msg(block, msg);
}

fn coerce_in_memory(
    sema: *Sema,
    val: Value,
    dst_ty: Type,
) CompileError!Air.Inst.Ref {
    return Air.interned_to_ref((try sema.mod.get_coerced(val, dst_ty)).to_intern());
}

const InMemoryCoercionResult = union(enum) {
    ok,
    no_match: Pair,
    int_not_coercible: Int,
    error_union_payload: PairAndChild,
    array_len: IntPair,
    array_sentinel: Sentinel,
    array_elem: PairAndChild,
    vector_len: IntPair,
    vector_elem: PairAndChild,
    optional_shape: Pair,
    optional_child: PairAndChild,
    from_anyerror,
    missing_error: []const InternPool.NullTerminatedString,
    /// true if wanted is var args
    fn_var_args: bool,
    /// true if wanted is generic
    fn_generic: bool,
    fn_param_count: IntPair,
    fn_param_noalias: IntPair,
    fn_param_comptime: ComptimeParam,
    fn_param: Param,
    fn_cc: CC,
    fn_return_type: PairAndChild,
    ptr_child: PairAndChild,
    ptr_addrspace: AddressSpace,
    ptr_sentinel: Sentinel,
    ptr_size: Size,
    ptr_qualifiers: Qualifiers,
    ptr_allowzero: Pair,
    ptr_bit_range: BitRange,
    ptr_alignment: AlignPair,
    double_ptr_to_anyopaque: Pair,
    slice_to_anyopaque: Pair,

    const Pair = struct {
        actual: Type,
        wanted: Type,
    };

    const PairAndChild = struct {
        child: *InMemoryCoercionResult,
        actual: Type,
        wanted: Type,
    };

    const Param = struct {
        child: *InMemoryCoercionResult,
        actual: Type,
        wanted: Type,
        index: u64,
    };

    const ComptimeParam = struct {
        index: u64,
        wanted: bool,
    };

    const Sentinel = struct {
        // unreachable_value indicates no sentinel
        actual: Value,
        wanted: Value,
        ty: Type,
    };

    const Int = struct {
        actual_signedness: std.builtin.Signedness,
        wanted_signedness: std.builtin.Signedness,
        actual_bits: u16,
        wanted_bits: u16,
    };

    const IntPair = struct {
        actual: u64,
        wanted: u64,
    };

    const AlignPair = struct {
        actual: Alignment,
        wanted: Alignment,
    };

    const Size = struct {
        actual: std.builtin.Type.Pointer.Size,
        wanted: std.builtin.Type.Pointer.Size,
    };

    const Qualifiers = struct {
        actual_const: bool,
        wanted_const: bool,
        actual_volatile: bool,
        wanted_volatile: bool,
    };

    const AddressSpace = struct {
        actual: std.builtin.AddressSpace,
        wanted: std.builtin.AddressSpace,
    };

    const CC = struct {
        actual: std.builtin.CallingConvention,
        wanted: std.builtin.CallingConvention,
    };

    const BitRange = struct {
        actual_host: u16,
        wanted_host: u16,
        actual_offset: u16,
        wanted_offset: u16,
    };

    fn dupe(child: *const InMemoryCoercionResult, arena: Allocator) !*InMemoryCoercionResult {
        const res = try arena.create(InMemoryCoercionResult);
        res.* = child.*;
        return res;
    }

    fn report(res: *const InMemoryCoercionResult, sema: *Sema, block: *Block, src: LazySrcLoc, msg: *Module.ErrorMsg) !void {
        const mod = sema.mod;
        var cur = res;
        while (true) switch (cur.*) {
            .ok => unreachable,
            .no_match => |types| {
                try sema.add_declared_here_note(msg, types.wanted);
                try sema.add_declared_here_note(msg, types.actual);
                break;
            },
            .int_not_coercible => |int| {
                try sema.err_note(block, src, msg, "{s} {d}-bit int cannot represent all possible {s} {d}-bit values", .{
                    @tag_name(int.wanted_signedness), int.wanted_bits, @tag_name(int.actual_signedness), int.actual_bits,
                });
                break;
            },
            .error_union_payload => |pair| {
                try sema.err_note(block, src, msg, "error union payload '{}' cannot cast into error union payload '{}'", .{
                    pair.actual.fmt(mod), pair.wanted.fmt(mod),
                });
                cur = pair.child;
            },
            .array_len => |lens| {
                try sema.err_note(block, src, msg, "array of length {d} cannot cast into an array of length {d}", .{
                    lens.actual, lens.wanted,
                });
                break;
            },
            .array_sentinel => |sentinel| {
                if (sentinel.actual.to_intern() != .unreachable_value) {
                    try sema.err_note(block, src, msg, "array sentinel '{}' cannot cast into array sentinel '{}'", .{
                        sentinel.actual.fmt_value(mod, sema), sentinel.wanted.fmt_value(mod, sema),
                    });
                } else {
                    try sema.err_note(block, src, msg, "destination array requires '{}' sentinel", .{
                        sentinel.wanted.fmt_value(mod, sema),
                    });
                }
                break;
            },
            .array_elem => |pair| {
                try sema.err_note(block, src, msg, "array element type '{}' cannot cast into array element type '{}'", .{
                    pair.actual.fmt(mod), pair.wanted.fmt(mod),
                });
                cur = pair.child;
            },
            .vector_len => |lens| {
                try sema.err_note(block, src, msg, "vector of length {d} cannot cast into a vector of length {d}", .{
                    lens.actual, lens.wanted,
                });
                break;
            },
            .vector_elem => |pair| {
                try sema.err_note(block, src, msg, "vector element type '{}' cannot cast into vector element type '{}'", .{
                    pair.actual.fmt(mod), pair.wanted.fmt(mod),
                });
                cur = pair.child;
            },
            .optional_shape => |pair| {
                try sema.err_note(block, src, msg, "optional type child '{}' cannot cast into optional type child '{}'", .{
                    pair.actual.optional_child(mod).fmt(mod), pair.wanted.optional_child(mod).fmt(mod),
                });
                break;
            },
            .optional_child => |pair| {
                try sema.err_note(block, src, msg, "optional type child '{}' cannot cast into optional type child '{}'", .{
                    pair.actual.fmt(mod), pair.wanted.fmt(mod),
                });
                cur = pair.child;
            },
            .from_anyerror => {
                try sema.err_note(block, src, msg, "global error set cannot cast into a smaller set", .{});
                break;
            },
            .missing_error => |missing_errors| {
                for (missing_errors) |err| {
                    try sema.err_note(block, src, msg, "'error.{}' not a member of destination error set", .{err.fmt(&mod.intern_pool)});
                }
                break;
            },
            .fn_var_args => |wanted_var_args| {
                if (wanted_var_args) {
                    try sema.err_note(block, src, msg, "non-variadic function cannot cast into a variadic function", .{});
                } else {
                    try sema.err_note(block, src, msg, "variadic function cannot cast into a non-variadic function", .{});
                }
                break;
            },
            .fn_generic => |wanted_generic| {
                if (wanted_generic) {
                    try sema.err_note(block, src, msg, "non-generic function cannot cast into a generic function", .{});
                } else {
                    try sema.err_note(block, src, msg, "generic function cannot cast into a non-generic function", .{});
                }
                break;
            },
            .fn_param_count => |lens| {
                try sema.err_note(block, src, msg, "function with {d} parameters cannot cast into a function with {d} parameters", .{
                    lens.actual, lens.wanted,
                });
                break;
            },
            .fn_param_noalias => |param| {
                var index: u6 = 0;
                var actual_noalias = false;
                while (true) : (index += 1) {
                    const actual: u1 = @truncate(param.actual >> index);
                    const wanted: u1 = @truncate(param.wanted >> index);
                    if (actual != wanted) {
                        actual_noalias = actual == 1;
                        break;
                    }
                }
                if (!actual_noalias) {
                    try sema.err_note(block, src, msg, "regular parameter {d} cannot cast into a noalias parameter", .{index});
                } else {
                    try sema.err_note(block, src, msg, "noalias parameter {d} cannot cast into a regular parameter", .{index});
                }
                break;
            },
            .fn_param_comptime => |param| {
                if (param.wanted) {
                    try sema.err_note(block, src, msg, "non-comptime parameter {d} cannot cast into a comptime parameter", .{param.index});
                } else {
                    try sema.err_note(block, src, msg, "comptime parameter {d} cannot cast into a non-comptime parameter", .{param.index});
                }
                break;
            },
            .fn_param => |param| {
                try sema.err_note(block, src, msg, "parameter {d} '{}' cannot cast into '{}'", .{
                    param.index, param.actual.fmt(mod), param.wanted.fmt(mod),
                });
                cur = param.child;
            },
            .fn_cc => |cc| {
                try sema.err_note(block, src, msg, "calling convention '{s}' cannot cast into calling convention '{s}'", .{ @tag_name(cc.actual), @tag_name(cc.wanted) });
                break;
            },
            .fn_return_type => |pair| {
                try sema.err_note(block, src, msg, "return type '{}' cannot cast into return type '{}'", .{
                    pair.actual.fmt(mod), pair.wanted.fmt(mod),
                });
                cur = pair.child;
            },
            .ptr_child => |pair| {
                try sema.err_note(block, src, msg, "pointer type child '{}' cannot cast into pointer type child '{}'", .{
                    pair.actual.fmt(mod), pair.wanted.fmt(mod),
                });
                cur = pair.child;
            },
            .ptr_addrspace => |@"addrspace"| {
                try sema.err_note(block, src, msg, "address space '{s}' cannot cast into address space '{s}'", .{ @tag_name(@"addrspace".actual), @tag_name(@"addrspace".wanted) });
                break;
            },
            .ptr_sentinel => |sentinel| {
                if (sentinel.actual.to_intern() != .unreachable_value) {
                    try sema.err_note(block, src, msg, "pointer sentinel '{}' cannot cast into pointer sentinel '{}'", .{
                        sentinel.actual.fmt_value(mod, sema), sentinel.wanted.fmt_value(mod, sema),
                    });
                } else {
                    try sema.err_note(block, src, msg, "destination pointer requires '{}' sentinel", .{
                        sentinel.wanted.fmt_value(mod, sema),
                    });
                }
                break;
            },
            .ptr_size => |size| {
                try sema.err_note(block, src, msg, "a {s} pointer cannot cast into a {s} pointer", .{ pointer_size_string(size.actual), pointer_size_string(size.wanted) });
                break;
            },
            .ptr_qualifiers => |qualifiers| {
                const ok_const = !qualifiers.actual_const or qualifiers.wanted_const;
                const ok_volatile = !qualifiers.actual_volatile or qualifiers.wanted_volatile;
                if (!ok_const) {
                    try sema.err_note(block, src, msg, "cast discards const qualifier", .{});
                } else if (!ok_volatile) {
                    try sema.err_note(block, src, msg, "cast discards volatile qualifier", .{});
                }
                break;
            },
            .ptr_allowzero => |pair| {
                const wanted_allow_zero = pair.wanted.ptr_allows_zero(mod);
                const actual_allow_zero = pair.actual.ptr_allows_zero(mod);
                if (actual_allow_zero and !wanted_allow_zero) {
                    try sema.err_note(block, src, msg, "'{}' could have null values which are illegal in type '{}'", .{
                        pair.actual.fmt(mod), pair.wanted.fmt(mod),
                    });
                } else {
                    try sema.err_note(block, src, msg, "mutable '{}' allows illegal null values stored to type '{}'", .{
                        pair.actual.fmt(mod), pair.wanted.fmt(mod),
                    });
                }
                break;
            },
            .ptr_bit_range => |bit_range| {
                if (bit_range.actual_host != bit_range.wanted_host) {
                    try sema.err_note(block, src, msg, "pointer host size '{}' cannot cast into pointer host size '{}'", .{
                        bit_range.actual_host, bit_range.wanted_host,
                    });
                }
                if (bit_range.actual_offset != bit_range.wanted_offset) {
                    try sema.err_note(block, src, msg, "pointer bit offset '{}' cannot cast into pointer bit offset '{}'", .{
                        bit_range.actual_offset, bit_range.wanted_offset,
                    });
                }
                break;
            },
            .ptr_alignment => |pair| {
                try sema.err_note(block, src, msg, "pointer alignment '{d}' cannot cast into pointer alignment '{d}'", .{
                    pair.actual.to_byte_units() orelse 0, pair.wanted.to_byte_units() orelse 0,
                });
                break;
            },
            .double_ptr_to_anyopaque => |pair| {
                try sema.err_note(block, src, msg, "cannot implicitly cast double pointer '{}' to anyopaque pointer '{}'", .{
                    pair.actual.fmt(mod), pair.wanted.fmt(mod),
                });
                break;
            },
            .slice_to_anyopaque => |pair| {
                try sema.err_note(block, src, msg, "cannot implicitly cast slice '{}' to anyopaque pointer '{}'", .{
                    pair.actual.fmt(mod), pair.wanted.fmt(mod),
                });
                try sema.err_note(block, src, msg, "consider using '.ptr'", .{});
                break;
            },
        };
    }
};

fn pointer_size_string(size: std.builtin.Type.Pointer.Size) []const u8 {
    return switch (size) {
        .One => "single",
        .Many => "many",
        .C => "C",
        .Slice => unreachable,
    };
}

/// If pointers have the same representation in runtime memory, a bitcast AIR instruction
/// may be used for the coercion.
/// * `const` attribute can be gained
/// * `volatile` attribute can be gained
/// * `allowzero` attribute can be gained (whether from explicit attribute, C pointer, or optional pointer) but only if !dest_is_mut
/// * alignment can be decreased
/// * bit offset attributes must match exactly
/// * `*`/`[*]` must match exactly, but `[*c]` matches either one
/// * sentinel-terminated pointers can coerce into `[*]`
pub fn coerce_in_memory_allowed(
    sema: *Sema,
    block: *Block,
    dest_ty: Type,
    src_ty: Type,
    dest_is_mut: bool,
    target: std.Target,
    dest_src: LazySrcLoc,
    src_src: LazySrcLoc,
) CompileError!InMemoryCoercionResult {
    const mod = sema.mod;

    if (dest_ty.eql(src_ty, mod))
        return .ok;

    const dest_tag = dest_ty.zig_type_tag(mod);
    const src_tag = src_ty.zig_type_tag(mod);

    // Differently-named integers with the same number of bits.
    if (dest_tag == .Int and src_tag == .Int) {
        const dest_info = dest_ty.int_info(mod);
        const src_info = src_ty.int_info(mod);

        if (dest_info.signedness == src_info.signedness and
            dest_info.bits == src_info.bits)
        {
            return .ok;
        }

        if ((src_info.signedness == dest_info.signedness and dest_info.bits < src_info.bits) or
            // small enough unsigned ints can get casted to large enough signed ints
            (dest_info.signedness == .signed and (src_info.signedness == .unsigned or dest_info.bits <= src_info.bits)) or
            (dest_info.signedness == .unsigned and src_info.signedness == .signed))
        {
            return InMemoryCoercionResult{ .int_not_coercible = .{
                .actual_signedness = src_info.signedness,
                .wanted_signedness = dest_info.signedness,
                .actual_bits = src_info.bits,
                .wanted_bits = dest_info.bits,
            } };
        }
    }

    // Differently-named floats with the same number of bits.
    if (dest_tag == .Float and src_tag == .Float) {
        const dest_bits = dest_ty.float_bits(target);
        const src_bits = src_ty.float_bits(target);
        if (dest_bits == src_bits) {
            return .ok;
        }
    }

    // Pointers / Pointer-like Optionals
    const maybe_dest_ptr_ty = try sema.type_ptr_or_optional_ptr_ty(dest_ty);
    const maybe_src_ptr_ty = try sema.type_ptr_or_optional_ptr_ty(src_ty);
    if (maybe_dest_ptr_ty) |dest_ptr_ty| {
        if (maybe_src_ptr_ty) |src_ptr_ty| {
            return try sema.coerce_in_memory_allowed_ptrs(block, dest_ty, src_ty, dest_ptr_ty, src_ptr_ty, dest_is_mut, target, dest_src, src_src);
        }
    }

    // Slices
    if (dest_ty.is_slice(mod) and src_ty.is_slice(mod)) {
        return try sema.coerce_in_memory_allowed_ptrs(block, dest_ty, src_ty, dest_ty, src_ty, dest_is_mut, target, dest_src, src_src);
    }

    // Functions
    if (dest_tag == .Fn and src_tag == .Fn) {
        return try sema.coerce_in_memory_allowed_fns(block, dest_ty, src_ty, target, dest_src, src_src);
    }

    // Error Unions
    if (dest_tag == .ErrorUnion and src_tag == .ErrorUnion) {
        const dest_payload = dest_ty.error_union_payload(mod);
        const src_payload = src_ty.error_union_payload(mod);
        const child = try sema.coerce_in_memory_allowed(block, dest_payload, src_payload, dest_is_mut, target, dest_src, src_src);
        if (child != .ok) {
            return InMemoryCoercionResult{ .error_union_payload = .{
                .child = try child.dupe(sema.arena),
                .actual = src_payload,
                .wanted = dest_payload,
            } };
        }
        return try sema.coerce_in_memory_allowed(block, dest_ty.error_union_set(mod), src_ty.error_union_set(mod), dest_is_mut, target, dest_src, src_src);
    }

    // Error Sets
    if (dest_tag == .ErrorSet and src_tag == .ErrorSet) {
        return try sema.coerce_in_memory_allowed_error_sets(block, dest_ty, src_ty, dest_src, src_src);
    }

    // Arrays
    if (dest_tag == .Array and src_tag == .Array) {
        const dest_info = dest_ty.array_info(mod);
        const src_info = src_ty.array_info(mod);
        if (dest_info.len != src_info.len) {
            return InMemoryCoercionResult{ .array_len = .{
                .actual = src_info.len,
                .wanted = dest_info.len,
            } };
        }

        const child = try sema.coerce_in_memory_allowed(block, dest_info.elem_type, src_info.elem_type, dest_is_mut, target, dest_src, src_src);
        if (child != .ok) {
            return InMemoryCoercionResult{ .array_elem = .{
                .child = try child.dupe(sema.arena),
                .actual = src_info.elem_type,
                .wanted = dest_info.elem_type,
            } };
        }
        const ok_sent = (dest_info.sentinel == null and src_info.sentinel == null) or
            (src_info.sentinel != null and
            dest_info.sentinel != null and
            dest_info.sentinel.?.eql(
            try mod.get_coerced(src_info.sentinel.?, dest_info.elem_type),
            dest_info.elem_type,
            mod,
        ));
        if (!ok_sent) {
            return InMemoryCoercionResult{ .array_sentinel = .{
                .actual = src_info.sentinel orelse Value.@"unreachable",
                .wanted = dest_info.sentinel orelse Value.@"unreachable",
                .ty = dest_info.elem_type,
            } };
        }
        return .ok;
    }

    // Vectors
    if (dest_tag == .Vector and src_tag == .Vector) {
        const dest_len = dest_ty.vector_len(mod);
        const src_len = src_ty.vector_len(mod);
        if (dest_len != src_len) {
            return InMemoryCoercionResult{ .vector_len = .{
                .actual = src_len,
                .wanted = dest_len,
            } };
        }

        const dest_elem_ty = dest_ty.scalar_type(mod);
        const src_elem_ty = src_ty.scalar_type(mod);
        const child = try sema.coerce_in_memory_allowed(block, dest_elem_ty, src_elem_ty, dest_is_mut, target, dest_src, src_src);
        if (child != .ok) {
            return InMemoryCoercionResult{ .vector_elem = .{
                .child = try child.dupe(sema.arena),
                .actual = src_elem_ty,
                .wanted = dest_elem_ty,
            } };
        }

        return .ok;
    }

    // Arrays <-> Vectors
    if ((dest_tag == .Vector and src_tag == .Array) or
        (dest_tag == .Array and src_tag == .Vector))
    {
        const dest_len = dest_ty.array_len(mod);
        const src_len = src_ty.array_len(mod);
        if (dest_len != src_len) {
            return InMemoryCoercionResult{ .array_len = .{
                .actual = src_len,
                .wanted = dest_len,
            } };
        }

        const dest_elem_ty = dest_ty.child_type(mod);
        const src_elem_ty = src_ty.child_type(mod);
        const child = try sema.coerce_in_memory_allowed(block, dest_elem_ty, src_elem_ty, dest_is_mut, target, dest_src, src_src);
        if (child != .ok) {
            return InMemoryCoercionResult{ .array_elem = .{
                .child = try child.dupe(sema.arena),
                .actual = src_elem_ty,
                .wanted = dest_elem_ty,
            } };
        }

        if (dest_tag == .Array) {
            const dest_info = dest_ty.array_info(mod);
            if (dest_info.sentinel != null) {
                return InMemoryCoercionResult{ .array_sentinel = .{
                    .actual = Value.@"unreachable",
                    .wanted = dest_info.sentinel.?,
                    .ty = dest_info.elem_type,
                } };
            }
        }

        // The memory layout of @Vector(N, iM) is the same as the integer type i(N*M),
        // that is to say, the padding bits are not in the same place as the array [N]iM.
        // If there's no padding, the bitcast is possible.
        const elem_bit_size = dest_elem_ty.bit_size(mod);
        const elem_abi_byte_size = dest_elem_ty.abi_size(mod);
        if (elem_abi_byte_size * 8 == elem_bit_size)
            return .ok;
    }

    // Optionals
    if (dest_tag == .Optional and src_tag == .Optional) {
        if ((maybe_dest_ptr_ty != null) != (maybe_src_ptr_ty != null)) {
            return InMemoryCoercionResult{ .optional_shape = .{
                .actual = src_ty,
                .wanted = dest_ty,
            } };
        }
        const dest_child_type = dest_ty.optional_child(mod);
        const src_child_type = src_ty.optional_child(mod);

        const child = try sema.coerce_in_memory_allowed(block, dest_child_type, src_child_type, dest_is_mut, target, dest_src, src_src);
        if (child != .ok) {
            return InMemoryCoercionResult{ .optional_child = .{
                .child = try child.dupe(sema.arena),
                .actual = src_child_type,
                .wanted = dest_child_type,
            } };
        }

        return .ok;
    }

    // Tuples (with in-memory-coercible fields)
    if (dest_ty.is_tuple(mod) and src_ty.is_tuple(mod)) tuple: {
        if (dest_ty.container_layout(mod) != src_ty.container_layout(mod)) break :tuple;
        if (dest_ty.struct_field_count(mod) != src_ty.struct_field_count(mod)) break :tuple;
        const field_count = dest_ty.struct_field_count(mod);
        for (0..field_count) |field_idx| {
            if (dest_ty.struct_field_is_comptime(field_idx, mod) != src_ty.struct_field_is_comptime(field_idx, mod)) break :tuple;
            if (dest_ty.struct_field_align(field_idx, mod) != src_ty.struct_field_align(field_idx, mod)) break :tuple;
            const dest_field_ty = dest_ty.struct_field_type(field_idx, mod);
            const src_field_ty = src_ty.struct_field_type(field_idx, mod);
            const field = try sema.coerce_in_memory_allowed(block, dest_field_ty, src_field_ty, dest_is_mut, target, dest_src, src_src);
            if (field != .ok) break :tuple;
        }
        return .ok;
    }

    return InMemoryCoercionResult{ .no_match = .{
        .actual = dest_ty,
        .wanted = src_ty,
    } };
}

fn coerce_in_memory_allowed_error_sets(
    sema: *Sema,
    block: *Block,
    dest_ty: Type,
    src_ty: Type,
    dest_src: LazySrcLoc,
    src_src: LazySrcLoc,
) !InMemoryCoercionResult {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;

    // Coercion to `anyerror`. Note that this check can return false negatives
    // in case the error sets did not get resolved.
    if (dest_ty.is_any_error(mod)) {
        return .ok;
    }

    if (dest_ty.to_intern() == .adhoc_inferred_error_set_type) {
        // We are trying to coerce an error set to the current function's
        // inferred error set.
        const dst_ies = sema.fn_ret_ty_ies.?;
        try dst_ies.add_error_set(src_ty, ip, sema.arena);
        return .ok;
    }

    if (ip.is_inferred_error_set_type(dest_ty.to_intern())) {
        const dst_ies_func_index = ip.ies_func_index(dest_ty.to_intern());
        if (sema.fn_ret_ty_ies) |dst_ies| {
            if (dst_ies.func == dst_ies_func_index) {
                // We are trying to coerce an error set to the current function's
                // inferred error set.
                try dst_ies.add_error_set(src_ty, ip, sema.arena);
                return .ok;
            }
        }
        switch (try sema.resolve_inferred_error_set(block, dest_src, dest_ty.to_intern())) {
            // is_any_error might have changed from a false negative to a true
            // positive after resolution.
            .anyerror_type => return .ok,
            else => {},
        }
    }

    var missing_error_buf = std.ArrayList(InternPool.NullTerminatedString).init(gpa);
    defer missing_error_buf.deinit();

    switch (src_ty.to_intern()) {
        .anyerror_type => switch (ip.index_to_key(dest_ty.to_intern())) {
            .simple_type => unreachable, // filtered out above
            .error_set_type, .inferred_error_set_type => return .from_anyerror,
            else => unreachable,
        },

        else => switch (ip.index_to_key(src_ty.to_intern())) {
            .inferred_error_set_type => {
                const resolved_src_ty = try sema.resolve_inferred_error_set(block, src_src, src_ty.to_intern());
                // src anyerror status might have changed after the resolution.
                if (resolved_src_ty == .anyerror_type) {
                    // dest_ty.is_any_error(mod) == true is already checked for at this point.
                    return .from_anyerror;
                }

                for (ip.index_to_key(resolved_src_ty).error_set_type.names.get(ip)) |key| {
                    if (!Type.error_set_has_field_ip(ip, dest_ty.to_intern(), key)) {
                        try missing_error_buf.append(key);
                    }
                }

                if (missing_error_buf.items.len != 0) {
                    return InMemoryCoercionResult{
                        .missing_error = try sema.arena.dupe(InternPool.NullTerminatedString, missing_error_buf.items),
                    };
                }

                return .ok;
            },
            .error_set_type => |error_set_type| {
                for (error_set_type.names.get(ip)) |name| {
                    if (!Type.error_set_has_field_ip(ip, dest_ty.to_intern(), name)) {
                        try missing_error_buf.append(name);
                    }
                }

                if (missing_error_buf.items.len != 0) {
                    return InMemoryCoercionResult{
                        .missing_error = try sema.arena.dupe(InternPool.NullTerminatedString, missing_error_buf.items),
                    };
                }

                return .ok;
            },
            else => unreachable,
        },
    }
}

fn coerce_in_memory_allowed_fns(
    sema: *Sema,
    block: *Block,
    dest_ty: Type,
    src_ty: Type,
    target: std.Target,
    dest_src: LazySrcLoc,
    src_src: LazySrcLoc,
) !InMemoryCoercionResult {
    const mod = sema.mod;
    const ip = &mod.intern_pool;

    const dest_info = mod.type_to_func(dest_ty).?;
    const src_info = mod.type_to_func(src_ty).?;

    {
        if (dest_info.is_var_args != src_info.is_var_args) {
            return InMemoryCoercionResult{ .fn_var_args = dest_info.is_var_args };
        }

        if (dest_info.is_generic != src_info.is_generic) {
            return InMemoryCoercionResult{ .fn_generic = dest_info.is_generic };
        }

        if (dest_info.cc != src_info.cc) {
            return InMemoryCoercionResult{ .fn_cc = .{
                .actual = src_info.cc,
                .wanted = dest_info.cc,
            } };
        }

        switch (src_info.return_type) {
            .noreturn_type, .generic_poison_type => {},
            else => {
                const dest_return_type = Type.from_interned(dest_info.return_type);
                const src_return_type = Type.from_interned(src_info.return_type);
                const rt = try sema.coerce_in_memory_allowed(block, dest_return_type, src_return_type, false, target, dest_src, src_src);
                if (rt != .ok) {
                    return InMemoryCoercionResult{ .fn_return_type = .{
                        .child = try rt.dupe(sema.arena),
                        .actual = src_return_type,
                        .wanted = dest_return_type,
                    } };
                }
            },
        }
    }

    const params_len = params_len: {
        if (dest_info.param_types.len != src_info.param_types.len) {
            return InMemoryCoercionResult{ .fn_param_count = .{
                .actual = src_info.param_types.len,
                .wanted = dest_info.param_types.len,
            } };
        }

        if (dest_info.noalias_bits != src_info.noalias_bits) {
            return InMemoryCoercionResult{ .fn_param_noalias = .{
                .actual = src_info.noalias_bits,
                .wanted = dest_info.noalias_bits,
            } };
        }

        break :params_len dest_info.param_types.len;
    };

    for (0..params_len) |param_i| {
        const dest_param_ty = Type.from_interned(dest_info.param_types.get(ip)[param_i]);
        const src_param_ty = Type.from_interned(src_info.param_types.get(ip)[param_i]);

        const param_i_small: u5 = @int_cast(param_i);
        if (dest_info.param_is_comptime(param_i_small) != src_info.param_is_comptime(param_i_small)) {
            return InMemoryCoercionResult{ .fn_param_comptime = .{
                .index = param_i,
                .wanted = dest_info.param_is_comptime(param_i_small),
            } };
        }

        switch (src_param_ty.to_intern()) {
            .generic_poison_type => {},
            else => {
                // Note: Cast direction is reversed here.
                const param = try sema.coerce_in_memory_allowed(block, src_param_ty, dest_param_ty, false, target, dest_src, src_src);
                if (param != .ok) {
                    return InMemoryCoercionResult{ .fn_param = .{
                        .child = try param.dupe(sema.arena),
                        .actual = src_param_ty,
                        .wanted = dest_param_ty,
                        .index = param_i,
                    } };
                }
            },
        }
    }

    return .ok;
}

fn coerce_in_memory_allowed_ptrs(
    sema: *Sema,
    block: *Block,
    dest_ty: Type,
    src_ty: Type,
    dest_ptr_ty: Type,
    src_ptr_ty: Type,
    dest_is_mut: bool,
    target: std.Target,
    dest_src: LazySrcLoc,
    src_src: LazySrcLoc,
) !InMemoryCoercionResult {
    const zcu = sema.mod;
    const dest_info = dest_ptr_ty.ptr_info(zcu);
    const src_info = src_ptr_ty.ptr_info(zcu);

    const ok_ptr_size = src_info.flags.size == dest_info.flags.size or
        src_info.flags.size == .C or dest_info.flags.size == .C;
    if (!ok_ptr_size) {
        return InMemoryCoercionResult{ .ptr_size = .{
            .actual = src_info.flags.size,
            .wanted = dest_info.flags.size,
        } };
    }

    const ok_cv_qualifiers =
        (!src_info.flags.is_const or dest_info.flags.is_const) and
        (!src_info.flags.is_volatile or dest_info.flags.is_volatile);

    if (!ok_cv_qualifiers) {
        return InMemoryCoercionResult{ .ptr_qualifiers = .{
            .actual_const = src_info.flags.is_const,
            .wanted_const = dest_info.flags.is_const,
            .actual_volatile = src_info.flags.is_volatile,
            .wanted_volatile = dest_info.flags.is_volatile,
        } };
    }

    if (dest_info.flags.address_space != src_info.flags.address_space) {
        return InMemoryCoercionResult{ .ptr_addrspace = .{
            .actual = src_info.flags.address_space,
            .wanted = dest_info.flags.address_space,
        } };
    }

    const dest_child = Type.from_interned(dest_info.child);
    const src_child = Type.from_interned(src_info.child);
    const child = try sema.coerce_in_memory_allowed(block, dest_child, src_child, !dest_info.flags.is_const, target, dest_src, src_src);
    if (child != .ok) allow: {
        // As a special case, we also allow coercing `*[n:s]T` to `*[n]T`, akin to dropping the sentinel from a slice.
        // `*[n:s]T` cannot coerce in memory to `*[n]T` since they have different sizes.
        if (src_child.zig_type_tag(zcu) == .Array and dest_child.zig_type_tag(zcu) == .Array and
            src_child.sentinel(zcu) != null and dest_child.sentinel(zcu) == null and
            .ok == try sema.coerce_in_memory_allowed(block, dest_child.child_type(zcu), src_child.child_type(zcu), !dest_info.flags.is_const, target, dest_src, src_src))
        {
            break :allow;
        }
        return InMemoryCoercionResult{ .ptr_child = .{
            .child = try child.dupe(sema.arena),
            .actual = Type.from_interned(src_info.child),
            .wanted = Type.from_interned(dest_info.child),
        } };
    }

    const dest_allow_zero = dest_ty.ptr_allows_zero(zcu);
    const src_allow_zero = src_ty.ptr_allows_zero(zcu);

    const ok_allows_zero = (dest_allow_zero and
        (src_allow_zero or !dest_is_mut)) or
        (!dest_allow_zero and !src_allow_zero);
    if (!ok_allows_zero) {
        return InMemoryCoercionResult{ .ptr_allowzero = .{
            .actual = src_ty,
            .wanted = dest_ty,
        } };
    }

    if (src_info.packed_offset.host_size != dest_info.packed_offset.host_size or
        src_info.packed_offset.bit_offset != dest_info.packed_offset.bit_offset)
    {
        return InMemoryCoercionResult{ .ptr_bit_range = .{
            .actual_host = src_info.packed_offset.host_size,
            .wanted_host = dest_info.packed_offset.host_size,
            .actual_offset = src_info.packed_offset.bit_offset,
            .wanted_offset = dest_info.packed_offset.bit_offset,
        } };
    }

    const ok_sent = dest_info.sentinel == .none or src_info.flags.size == .C or
        (src_info.sentinel != .none and
        dest_info.sentinel == try zcu.intern_pool.get_coerced(sema.gpa, src_info.sentinel, dest_info.child));
    if (!ok_sent) {
        return InMemoryCoercionResult{ .ptr_sentinel = .{
            .actual = switch (src_info.sentinel) {
                .none => Value.@"unreachable",
                else => Value.from_interned(src_info.sentinel),
            },
            .wanted = switch (dest_info.sentinel) {
                .none => Value.@"unreachable",
                else => Value.from_interned(dest_info.sentinel),
            },
            .ty = Type.from_interned(dest_info.child),
        } };
    }

    // If both pointers have alignment 0, it means they both want ABI alignment.
    // In this case, if they share the same child type, no need to resolve
    // pointee type alignment. Otherwise both pointee types must have their alignment
    // resolved and we compare the alignment numerically.
    if (src_info.flags.alignment != .none or dest_info.flags.alignment != .none or
        dest_info.child != src_info.child)
    {
        const src_align = if (src_info.flags.alignment != .none)
            src_info.flags.alignment
        else
            try sema.type_abi_alignment(Type.from_interned(src_info.child));

        const dest_align = if (dest_info.flags.alignment != .none)
            dest_info.flags.alignment
        else
            try sema.type_abi_alignment(Type.from_interned(dest_info.child));

        if (dest_align.compare(.gt, src_align)) {
            return InMemoryCoercionResult{ .ptr_alignment = .{
                .actual = src_align,
                .wanted = dest_align,
            } };
        }
    }

    return .ok;
}

fn coerce_var_arg_param(
    sema: *Sema,
    block: *Block,
    inst: Air.Inst.Ref,
    inst_src: LazySrcLoc,
) !Air.Inst.Ref {
    if (block.is_typeof) return inst;

    const mod = sema.mod;
    const uncasted_ty = sema.type_of(inst);
    const coerced = switch (uncasted_ty.zig_type_tag(mod)) {
        // TODO consider casting to c_int/f64 if they fit
        .ComptimeInt, .ComptimeFloat => return sema.fail(
            block,
            inst_src,
            "integer and float literals passed to variadic function must be casted to a fixed-size number type",
            .{},
        ),
        .Fn => fn_ptr: {
            const fn_val = try sema.resolve_const_defined_value(block, .unneeded, inst, undefined);
            const fn_decl = fn_val.pointer_decl(mod).?;
            break :fn_ptr try sema.analyze_decl_ref(fn_decl);
        },
        .Array => return sema.fail(block, inst_src, "arrays must be passed by reference to variadic function", .{}),
        .Float => float: {
            const target = sema.mod.get_target();
            const double_bits = target.c_type_bit_size(.double);
            const inst_bits = uncasted_ty.float_bits(sema.mod.get_target());
            if (inst_bits >= double_bits) break :float inst;
            switch (double_bits) {
                32 => break :float try sema.coerce(block, Type.f32, inst, inst_src),
                64 => break :float try sema.coerce(block, Type.f64, inst, inst_src),
                else => unreachable,
            }
        },
        else => if (uncasted_ty.is_abi_int(mod)) int: {
            if (!try sema.validate_extern_type(uncasted_ty, .param_ty)) break :int inst;
            const target = sema.mod.get_target();
            const uncasted_info = uncasted_ty.int_info(mod);
            if (uncasted_info.bits <= target.c_type_bit_size(switch (uncasted_info.signedness) {
                .signed => .int,
                .unsigned => .uint,
            })) break :int try sema.coerce(block, switch (uncasted_info.signedness) {
                .signed => Type.c_int,
                .unsigned => Type.c_uint,
            }, inst, inst_src);
            if (uncasted_info.bits <= target.c_type_bit_size(switch (uncasted_info.signedness) {
                .signed => .long,
                .unsigned => .ulong,
            })) break :int try sema.coerce(block, switch (uncasted_info.signedness) {
                .signed => Type.c_long,
                .unsigned => Type.c_ulong,
            }, inst, inst_src);
            if (uncasted_info.bits <= target.c_type_bit_size(switch (uncasted_info.signedness) {
                .signed => .longlong,
                .unsigned => .ulonglong,
            })) break :int try sema.coerce(block, switch (uncasted_info.signedness) {
                .signed => Type.c_longlong,
                .unsigned => Type.c_ulonglong,
            }, inst, inst_src);
            break :int inst;
        } else inst,
    };

    const coerced_ty = sema.type_of(coerced);
    if (!try sema.validate_extern_type(coerced_ty, .param_ty)) {
        const msg = msg: {
            const msg = try sema.err_msg(block, inst_src, "cannot pass '{}' to variadic function", .{coerced_ty.fmt(sema.mod)});
            errdefer msg.destroy(sema.gpa);

            const src_decl = sema.mod.decl_ptr(block.src_decl);
            try sema.explain_why_type_is_not_extern(msg, src_decl.to_src_loc(inst_src, mod), coerced_ty, .param_ty);

            try sema.add_declared_here_note(msg, coerced_ty);
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }
    return coerced;
}

// TODO migrate callsites to use store_ptr2 instead.
fn store_ptr(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    ptr: Air.Inst.Ref,
    uncasted_operand: Air.Inst.Ref,
) CompileError!void {
    const air_tag: Air.Inst.Tag = if (block.want_safety()) .store_safe else .store;
    return sema.store_ptr2(block, src, ptr, src, uncasted_operand, src, air_tag);
}

fn store_ptr2(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    ptr: Air.Inst.Ref,
    ptr_src: LazySrcLoc,
    uncasted_operand: Air.Inst.Ref,
    operand_src: LazySrcLoc,
    air_tag: Air.Inst.Tag,
) CompileError!void {
    const mod = sema.mod;
    const ptr_ty = sema.type_of(ptr);
    if (ptr_ty.is_const_ptr(mod))
        return sema.fail(block, ptr_src, "cannot assign to constant", .{});

    const elem_ty = ptr_ty.child_type(mod);

    // To generate better code for tuples, we detect a tuple operand here, and
    // analyze field loads and stores directly. This avoids an extra allocation + memcpy
    // which would occur if we used `coerce`.
    // However, we avoid this mechanism if the destination element type is a tuple,
    // because the regular store will be better for this case.
    // If the destination type is a struct we don't want this mechanism to trigger, because
    // this code does not handle tuple-to-struct coercion which requires dealing with missing
    // fields.
    const operand_ty = sema.type_of(uncasted_operand);
    if (operand_ty.is_tuple(mod) and elem_ty.zig_type_tag(mod) == .Array) {
        const field_count = operand_ty.struct_field_count(mod);
        var i: u32 = 0;
        while (i < field_count) : (i += 1) {
            const elem_src = operand_src; // TODO better source location
            const elem = try sema.tuple_field(block, operand_src, uncasted_operand, elem_src, i);
            const elem_index = try mod.int_ref(Type.usize, i);
            const elem_ptr = try sema.elem_ptr(block, ptr_src, ptr, elem_index, elem_src, false, true);
            try sema.store_ptr2(block, src, elem_ptr, elem_src, elem, elem_src, .store);
        }
        return;
    }

    // TODO do the same thing for anon structs as for tuples above.
    // However, beware of the need to handle missing/extra fields.

    const is_ret = air_tag == .ret_ptr;

    // Detect if we are storing an array operand to a bitcasted vector pointer.
    // If so, we instead reach through the bitcasted pointer to the vector pointer,
    // bitcast the array operand to a vector, and then lower this as a store of
    // a vector value to a vector pointer. This generally results in better code,
    // as well as working around an LLVM bug:
    // https://github.com/ziglang/zig/issues/11154
    if (sema.obtain_bit_casted_vector_ptr(ptr)) |vector_ptr| {
        const vector_ty = sema.type_of(vector_ptr).child_type(mod);
        const vector = sema.coerce_extra(block, vector_ty, uncasted_operand, operand_src, .{ .is_ret = is_ret }) catch |err| switch (err) {
            error.NotCoercible => unreachable,
            else => |e| return e,
        };
        try sema.store_ptr2(block, src, vector_ptr, ptr_src, vector, operand_src, .store);
        return;
    }

    const operand = sema.coerce_extra(block, elem_ty, uncasted_operand, operand_src, .{ .is_ret = is_ret }) catch |err| switch (err) {
        error.NotCoercible => unreachable,
        else => |e| return e,
    };
    const maybe_operand_val = try sema.resolve_value(operand);

    const runtime_src = if (try sema.resolve_defined_value(block, ptr_src, ptr)) |ptr_val| rs: {
        const operand_val = maybe_operand_val orelse {
            try sema.check_ptr_is_not_comptime_mutable(block, ptr_val, ptr_src, operand_src);
            break :rs operand_src;
        };
        if (sema.is_comptime_mutable_ptr(ptr_val)) {
            try sema.store_ptr_val(block, src, ptr_val, operand_val, elem_ty);
            return;
        } else break :rs ptr_src;
    } else ptr_src;

    // We do this after the possible comptime store above, for the case of field_ptr stores
    // to unions because we want the comptime tag to be set, even if the field type is void.
    if ((try sema.type_has_one_possible_value(elem_ty)) != null) {
        return;
    }

    try sema.require_runtime_block(block, src, runtime_src);
    try sema.queue_full_type_resolution(elem_ty);

    if (ptr_ty.ptr_info(mod).flags.vector_index == .runtime) {
        const ptr_inst = ptr.to_index().?;
        const air_tags = sema.air_instructions.items(.tag);
        if (air_tags[@int_from_enum(ptr_inst)] == .ptr_elem_ptr) {
            const ty_pl = sema.air_instructions.items(.data)[@int_from_enum(ptr_inst)].ty_pl;
            const bin_op = sema.get_tmp_air().extra_data(Air.Bin, ty_pl.payload).data;
            _ = try block.add_inst(.{
                .tag = .vector_store_elem,
                .data = .{ .vector_store_elem = .{
                    .vector_ptr = bin_op.lhs,
                    .payload = try block.sema.add_extra(Air.Bin{
                        .lhs = bin_op.rhs,
                        .rhs = operand,
                    }),
                } },
            });
            return;
        }
        return sema.fail(block, ptr_src, "unable to determine vector element index of type '{}'", .{
            ptr_ty.fmt(sema.mod),
        });
    }

    const store_inst = if (is_ret)
        try block.add_bin_op(.store, ptr, operand)
    else
        try block.add_bin_op(air_tag, ptr, operand);

    try sema.check_comptime_known_store(block, store_inst, operand_src);

    return;
}

/// Given an AIR store instruction, checks whether we are performing a
/// comptime-known store to a local alloc, and updates `maybe_comptime_allocs`
/// accordingly.
/// Handles calling `validate_runtime_value` if the store is runtime for any reason.
fn check_comptime_known_store(sema: *Sema, block: *Block, store_inst_ref: Air.Inst.Ref, store_src: LazySrcLoc) !void {
    const store_inst = store_inst_ref.to_index().?;
    const inst_data = sema.air_instructions.items(.data)[@int_from_enum(store_inst)].bin_op;
    const ptr = inst_data.lhs.to_index() orelse return;
    const operand = inst_data.rhs;

    known: {
        const maybe_base_alloc = sema.base_allocs.get(ptr) orelse break :known;
        const maybe_comptime_alloc = sema.maybe_comptime_allocs.get_ptr(maybe_base_alloc) orelse break :known;

        if ((try sema.resolve_value(operand)) != null and
            block.runtime_index == maybe_comptime_alloc.runtime_index)
        {
            try maybe_comptime_alloc.stores.append(sema.arena, .{
                .inst = store_inst,
                .src_decl = block.src_decl,
                .src = store_src,
            });
            return;
        }

        // We're newly discovering that this alloc is runtime-known.
        try sema.mark_maybe_comptime_alloc_runtime(block, maybe_base_alloc);
    }

    try sema.validate_runtime_value(block, store_src, operand);
}

/// Given an AIR instruction transforming a pointer (struct_field_ptr,
/// ptr_elem_ptr, bitcast, etc), checks whether the base pointer refers to a
/// local alloc, and updates `base_allocs` accordingly.
fn check_known_alloc_ptr(sema: *Sema, block: *Block, base_ptr: Air.Inst.Ref, new_ptr: Air.Inst.Ref) !void {
    const base_ptr_inst = base_ptr.to_index() orelse return;
    const new_ptr_inst = new_ptr.to_index() orelse return;
    const alloc_inst = sema.base_allocs.get(base_ptr_inst) orelse return;
    try sema.base_allocs.put(sema.gpa, new_ptr_inst, alloc_inst);

    switch (sema.air_instructions.items(.tag)[@int_from_enum(new_ptr_inst)]) {
        .optional_payload_ptr_set, .errunion_payload_ptr_set => {
            const maybe_comptime_alloc = sema.maybe_comptime_allocs.get_ptr(alloc_inst) orelse return;

            // This is functionally a store, since it writes the optional payload bit.
            // Thus, if it is behind a runtime condition, we must mark the alloc as runtime appropriately.
            if (block.runtime_index != maybe_comptime_alloc.runtime_index) {
                return sema.mark_maybe_comptime_alloc_runtime(block, alloc_inst);
            }

            try maybe_comptime_alloc.stores.append(sema.arena, .{
                .inst = new_ptr_inst,
                .src_decl = block.src_decl,
                .src = .unneeded,
            });
        },
        .ptr_elem_ptr => {
            const tmp_air = sema.get_tmp_air();
            const pl_idx = tmp_air.instructions.items(.data)[@int_from_enum(new_ptr_inst)].ty_pl.payload;
            const bin = tmp_air.extra_data(Air.Bin, pl_idx).data;
            const index_ref = bin.rhs;

            // If the index value is runtime-known, this pointer is also runtime-known, so
            // we must in turn make the alloc value runtime-known.
            if (null == try sema.resolve_value(index_ref)) {
                try sema.mark_maybe_comptime_alloc_runtime(block, alloc_inst);
            }
        },
        else => {},
    }
}

fn mark_maybe_comptime_alloc_runtime(sema: *Sema, block: *Block, alloc_inst: Air.Inst.Index) CompileError!void {
    const maybe_comptime_alloc = (sema.maybe_comptime_allocs.fetch_remove(alloc_inst) orelse return).value;
    // Since the alloc has been determined to be runtime, we must check that
    // all other stores to it are permitted to be runtime values.
    const mod = sema.mod;
    const slice = maybe_comptime_alloc.stores.slice();
    for (slice.items(.inst), slice.items(.src_decl), slice.items(.src)) |other_inst, other_src_decl, other_src| {
        if (other_src == .unneeded) {
            switch (sema.air_instructions.items(.tag)[@int_from_enum(other_inst)]) {
                .set_union_tag, .optional_payload_ptr_set, .errunion_payload_ptr_set => continue,
                else => unreachable, // assertion failure
            }
        }
        const other_data = sema.air_instructions.items(.data)[@int_from_enum(other_inst)].bin_op;
        const other_operand = other_data.rhs;
        if (!sema.check_runtime_value(other_operand)) {
            return sema.fail_with_owned_error_msg(block, msg: {
                const other_src_resolved = mod.decl_ptr(other_src_decl).to_src_loc(other_src, mod);
                const msg = try Module.ErrorMsg.create(sema.gpa, other_src_resolved, "runtime value contains reference to comptime var", .{});
                errdefer msg.destroy(sema.gpa);
                try mod.err_note_non_lazy(other_src_resolved, msg, "comptime var pointers are not available at runtime", .{});
                break :msg msg;
            });
        }
    }
}

/// Traverse an arbitrary number of bitcasted pointers and return the underyling vector
/// pointer. Only if the final element type matches the vector element type, and the
/// lengths match.
fn obtain_bit_casted_vector_ptr(sema: *Sema, ptr: Air.Inst.Ref) ?Air.Inst.Ref {
    const mod = sema.mod;
    const array_ty = sema.type_of(ptr).child_type(mod);
    if (array_ty.zig_type_tag(mod) != .Array) return null;
    var ptr_ref = ptr;
    var ptr_inst = ptr_ref.to_index() orelse return null;
    const air_datas = sema.air_instructions.items(.data);
    const air_tags = sema.air_instructions.items(.tag);
    const vector_ty = while (air_tags[@int_from_enum(ptr_inst)] == .bitcast) {
        ptr_ref = air_datas[@int_from_enum(ptr_inst)].ty_op.operand;
        if (!sema.is_known_zig_type(ptr_ref, .Pointer)) return null;
        const child_ty = sema.type_of(ptr_ref).child_type(mod);
        if (child_ty.zig_type_tag(mod) == .Vector) break child_ty;
        ptr_inst = ptr_ref.to_index() orelse return null;
    } else return null;

    // We have a pointer-to-array and a pointer-to-vector. If the elements and
    // lengths match, return the result.
    if (array_ty.child_type(mod).eql(vector_ty.child_type(mod), sema.mod) and
        array_ty.array_len(mod) == vector_ty.vector_len(mod))
    {
        return ptr_ref;
    } else {
        return null;
    }
}

/// Call when you have Value objects rather than Air instructions, and you want to
/// assert the store must be done at comptime.
fn store_ptr_val(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    ptr_val: Value,
    operand_val: Value,
    operand_ty: Type,
) !void {
    const zcu = sema.mod;
    const ip = &zcu.intern_pool;
    // TODO: audit use sites to eliminate this coercion
    const coerced_operand_val = try zcu.get_coerced(operand_val, operand_ty);
    // TODO: audit use sites to eliminate this coercion
    const ptr_ty = try zcu.ptr_type(info: {
        var info = ptr_val.type_of(zcu).ptr_info(zcu);
        info.child = operand_ty.to_intern();
        break :info info;
    });
    const coerced_ptr_val = try zcu.get_coerced(ptr_val, ptr_ty);

    switch (try sema.store_comptime_ptr(block, src, coerced_ptr_val, coerced_operand_val)) {
        .success => {},
        .runtime_store => unreachable, // use sites check this
        // TODO use fail_with_invalid_comptime_field_store
        .comptime_field_mismatch => return sema.fail(
            block,
            src,
            "value stored in comptime field does not match the default value of the field",
            .{},
        ),
        .undef => return sema.fail_with_use_of_undef(block, src),
        .err_payload => |err_name| return sema.fail(block, src, "attempt to unwrap error: {}", .{err_name.fmt(ip)}),
        .null_payload => return sema.fail(block, src, "attempt to use null value", .{}),
        .inactive_union_field => return sema.fail(block, src, "access of inactive union field", .{}),
        .needed_well_defined => |ty| return sema.fail(
            block,
            src,
            "comptime dereference requires '{}' to have a well-defined layout",
            .{ty.fmt(zcu)},
        ),
        .out_of_bounds => |ty| return sema.fail(
            block,
            src,
            "dereference of '{}' exceeds bounds of containing decl of type '{}'",
            .{ ptr_ty.fmt(zcu), ty.fmt(zcu) },
        ),
        .exceeds_host_size => return sema.fail(block, src, "bit-pointer target exceeds host size", .{}),
    }
}

fn bit_cast(
    sema: *Sema,
    block: *Block,
    dest_ty: Type,
    inst: Air.Inst.Ref,
    inst_src: LazySrcLoc,
    operand_src: ?LazySrcLoc,
) CompileError!Air.Inst.Ref {
    const zcu = sema.mod;
    try sema.resolve_type_layout(dest_ty);

    const old_ty = sema.type_of(inst);
    try sema.resolve_type_layout(old_ty);

    const dest_bits = dest_ty.bit_size(zcu);
    const old_bits = old_ty.bit_size(zcu);

    if (old_bits != dest_bits) {
        return sema.fail(block, inst_src, "@bit_cast size mismatch: destination type '{}' has {d} bits but source type '{}' has {d} bits", .{
            dest_ty.fmt(zcu),
            dest_bits,
            old_ty.fmt(zcu),
            old_bits,
        });
    }

    if (try sema.resolve_value(inst)) |val| {
        if (val.is_undef(zcu))
            return zcu.undef_ref(dest_ty);
        if (old_ty.zig_type_tag(zcu) == .ErrorSet and dest_ty.zig_type_tag(zcu) == .ErrorSet) {
            // Special case: we sometimes call `bit_cast` on error set values, but they
            // don't have a well-defined layout, so we can't use `bitCastVal` on them.
            return Air.interned_to_ref((try zcu.get_coerced(val, dest_ty)).to_intern());
        }
        if (try sema.bitCastVal(val, dest_ty, 0, 0, 0)) |result_val| {
            return Air.interned_to_ref(result_val.to_intern());
        }
    }
    try sema.require_runtime_block(block, inst_src, operand_src);
    try sema.validate_runtime_value(block, inst_src, inst);
    return block.add_bit_cast(dest_ty, inst);
}

fn coerce_array_ptr_to_slice(
    sema: *Sema,
    block: *Block,
    dest_ty: Type,
    inst: Air.Inst.Ref,
    inst_src: LazySrcLoc,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    if (try sema.resolve_value(inst)) |val| {
        const ptr_array_ty = sema.type_of(inst);
        const array_ty = ptr_array_ty.child_type(mod);
        const slice_ptr_ty = dest_ty.slice_ptr_field_type(mod);
        const slice_ptr = try mod.get_coerced(val, slice_ptr_ty);
        const slice_val = try mod.intern(.{ .slice = .{
            .ty = dest_ty.to_intern(),
            .ptr = slice_ptr.to_intern(),
            .len = (try mod.int_value(Type.usize, array_ty.array_len(mod))).to_intern(),
        } });
        return Air.interned_to_ref(slice_val);
    }
    try sema.require_runtime_block(block, inst_src, null);
    return block.add_ty_op(.array_to_slice, dest_ty, inst);
}

fn check_ptr_attributes(sema: *Sema, dest_ty: Type, inst_ty: Type, in_memory_result: *InMemoryCoercionResult) bool {
    const mod = sema.mod;
    const dest_info = dest_ty.ptr_info(mod);
    const inst_info = inst_ty.ptr_info(mod);
    const len0 = (Type.from_interned(inst_info.child).zig_type_tag(mod) == .Array and (Type.from_interned(inst_info.child).array_len_including_sentinel(mod) == 0 or
        (Type.from_interned(inst_info.child).array_len(mod) == 0 and dest_info.sentinel == .none and dest_info.flags.size != .C and dest_info.flags.size != .Many))) or
        (Type.from_interned(inst_info.child).is_tuple(mod) and Type.from_interned(inst_info.child).struct_field_count(mod) == 0);

    const ok_cv_qualifiers =
        ((!inst_info.flags.is_const or dest_info.flags.is_const) or len0) and
        (!inst_info.flags.is_volatile or dest_info.flags.is_volatile);

    if (!ok_cv_qualifiers) {
        in_memory_result.* = .{ .ptr_qualifiers = .{
            .actual_const = inst_info.flags.is_const,
            .wanted_const = dest_info.flags.is_const,
            .actual_volatile = inst_info.flags.is_volatile,
            .wanted_volatile = dest_info.flags.is_volatile,
        } };
        return false;
    }
    if (dest_info.flags.address_space != inst_info.flags.address_space) {
        in_memory_result.* = .{ .ptr_addrspace = .{
            .actual = inst_info.flags.address_space,
            .wanted = dest_info.flags.address_space,
        } };
        return false;
    }
    if (inst_info.flags.alignment == .none and dest_info.flags.alignment == .none) return true;
    if (len0) return true;

    const inst_align = if (inst_info.flags.alignment != .none)
        inst_info.flags.alignment
    else
        Type.from_interned(inst_info.child).abi_alignment(mod);

    const dest_align = if (dest_info.flags.alignment != .none)
        dest_info.flags.alignment
    else
        Type.from_interned(dest_info.child).abi_alignment(mod);

    if (dest_align.compare(.gt, inst_align)) {
        in_memory_result.* = .{ .ptr_alignment = .{
            .actual = inst_align,
            .wanted = dest_align,
        } };
        return false;
    }
    return true;
}

fn coerce_compatible_ptrs(
    sema: *Sema,
    block: *Block,
    dest_ty: Type,
    inst: Air.Inst.Ref,
    inst_src: LazySrcLoc,
) !Air.Inst.Ref {
    const mod = sema.mod;
    const inst_ty = sema.type_of(inst);
    if (try sema.resolve_value(inst)) |val| {
        if (!val.is_undef(mod) and val.is_null(mod) and !dest_ty.is_allowzero_ptr(mod)) {
            return sema.fail(block, inst_src, "null pointer casted to type '{}'", .{dest_ty.fmt(sema.mod)});
        }
        // The comptime Value representation is compatible with both types.
        return Air.interned_to_ref(
            (try mod.get_coerced(val, dest_ty)).to_intern(),
        );
    }
    try sema.require_runtime_block(block, inst_src, null);
    const inst_allows_zero = inst_ty.zig_type_tag(mod) != .Pointer or inst_ty.ptr_allows_zero(mod);
    if (block.want_safety() and inst_allows_zero and !dest_ty.ptr_allows_zero(mod) and
        (try sema.type_has_runtime_bits(dest_ty.elem_type2(mod)) or dest_ty.elem_type2(mod).zig_type_tag(mod) == .Fn))
    {
        const actual_ptr = if (inst_ty.is_slice(mod))
            try sema.analyze_slice_ptr(block, inst_src, inst, inst_ty)
        else
            inst;
        const ptr_int = try block.add_un_op(.int_from_ptr, actual_ptr);
        const is_non_zero = try block.add_bin_op(.cmp_neq, ptr_int, .zero_usize);
        const ok = if (inst_ty.is_slice(mod)) ok: {
            const len = try sema.analyze_slice_len(block, inst_src, inst);
            const len_zero = try block.add_bin_op(.cmp_eq, len, .zero_usize);
            break :ok try block.add_bin_op(.bool_or, len_zero, is_non_zero);
        } else is_non_zero;
        try sema.add_safety_check(block, inst_src, ok, .cast_to_null);
    }
    const new_ptr = try sema.bit_cast(block, dest_ty, inst, inst_src, null);
    try sema.check_known_alloc_ptr(block, inst, new_ptr);
    return new_ptr;
}

fn coerce_enum_to_union(
    sema: *Sema,
    block: *Block,
    union_ty: Type,
    union_ty_src: LazySrcLoc,
    inst: Air.Inst.Ref,
    inst_src: LazySrcLoc,
) !Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const inst_ty = sema.type_of(inst);

    const tag_ty = union_ty.union_tag_type(mod) orelse {
        const msg = msg: {
            const msg = try sema.err_msg(block, inst_src, "expected type '{}', found '{}'", .{
                union_ty.fmt(sema.mod), inst_ty.fmt(sema.mod),
            });
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, union_ty_src, msg, "cannot coerce enum to untagged union", .{});
            try sema.add_declared_here_note(msg, union_ty);
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    };

    const enum_tag = try sema.coerce(block, tag_ty, inst, inst_src);
    if (try sema.resolve_defined_value(block, inst_src, enum_tag)) |val| {
        const field_index = union_ty.union_tag_field_index(val, sema.mod) orelse {
            return sema.fail(block, inst_src, "union '{}' has no tag with value '{}'", .{
                union_ty.fmt(sema.mod), val.fmt_value(sema.mod, sema),
            });
        };

        const union_obj = mod.type_to_union(union_ty).?;
        const field_ty = Type.from_interned(union_obj.field_types.get(ip)[field_index]);
        try sema.resolve_type_fields(field_ty);
        if (field_ty.zig_type_tag(mod) == .NoReturn) {
            const msg = msg: {
                const msg = try sema.err_msg(block, inst_src, "cannot initialize 'noreturn' field of union", .{});
                errdefer msg.destroy(sema.gpa);

                const field_name = union_obj.load_tag_type(ip).names.get(ip)[field_index];
                try sema.add_field_err_note(union_ty, field_index, msg, "field '{}' declared here", .{
                    field_name.fmt(ip),
                });
                try sema.add_declared_here_note(msg, union_ty);
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        }
        const opv = (try sema.type_has_one_possible_value(field_ty)) orelse {
            const msg = msg: {
                const field_name = union_obj.load_tag_type(ip).names.get(ip)[field_index];
                const msg = try sema.err_msg(block, inst_src, "coercion from enum '{}' to union '{}' must initialize '{}' field '{}'", .{
                    inst_ty.fmt(sema.mod),  union_ty.fmt(sema.mod),
                    field_ty.fmt(sema.mod), field_name.fmt(ip),
                });
                errdefer msg.destroy(sema.gpa);

                try sema.add_field_err_note(union_ty, field_index, msg, "field '{}' declared here", .{
                    field_name.fmt(ip),
                });
                try sema.add_declared_here_note(msg, union_ty);
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        };

        return Air.interned_to_ref((try mod.union_value(union_ty, val, opv)).to_intern());
    }

    try sema.require_runtime_block(block, inst_src, null);

    if (tag_ty.is_nonexhaustive_enum(mod)) {
        const msg = msg: {
            const msg = try sema.err_msg(block, inst_src, "runtime coercion to union '{}' from non-exhaustive enum", .{
                union_ty.fmt(sema.mod),
            });
            errdefer msg.destroy(sema.gpa);
            try sema.add_declared_here_note(msg, tag_ty);
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    const union_obj = mod.type_to_union(union_ty).?;
    {
        var msg: ?*Module.ErrorMsg = null;
        errdefer if (msg) |some| some.destroy(sema.gpa);

        for (union_obj.field_types.get(ip), 0..) |field_ty, field_index| {
            if (Type.from_interned(field_ty).zig_type_tag(mod) == .NoReturn) {
                const err_msg = msg orelse try sema.err_msg(
                    block,
                    inst_src,
                    "runtime coercion from enum '{}' to union '{}' which has a 'noreturn' field",
                    .{ tag_ty.fmt(sema.mod), union_ty.fmt(sema.mod) },
                );
                msg = err_msg;

                try sema.add_field_err_note(union_ty, field_index, err_msg, "'noreturn' field here", .{});
            }
        }
        if (msg) |some| {
            msg = null;
            try sema.add_declared_here_note(some, union_ty);
            return sema.fail_with_owned_error_msg(block, some);
        }
    }

    // If the union has all fields 0 bits, the union value is just the enum value.
    if (union_ty.union_has_all_zero_bit_field_types(mod)) {
        return block.add_bit_cast(union_ty, enum_tag);
    }

    const msg = msg: {
        const msg = try sema.err_msg(
            block,
            inst_src,
            "runtime coercion from enum '{}' to union '{}' which has non-void fields",
            .{ tag_ty.fmt(sema.mod), union_ty.fmt(sema.mod) },
        );
        errdefer msg.destroy(sema.gpa);

        for (0..union_obj.field_types.len) |field_index| {
            const field_name = union_obj.load_tag_type(ip).names.get(ip)[field_index];
            const field_ty = Type.from_interned(union_obj.field_types.get(ip)[field_index]);
            if (!(try sema.type_has_runtime_bits(field_ty))) continue;
            try sema.add_field_err_note(union_ty, field_index, msg, "field '{}' has type '{}'", .{
                field_name.fmt(ip),
                field_ty.fmt(sema.mod),
            });
        }
        try sema.add_declared_here_note(msg, union_ty);
        break :msg msg;
    };
    return sema.fail_with_owned_error_msg(block, msg);
}

fn coerce_anon_struct_to_union(
    sema: *Sema,
    block: *Block,
    union_ty: Type,
    union_ty_src: LazySrcLoc,
    inst: Air.Inst.Ref,
    inst_src: LazySrcLoc,
) !Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const inst_ty = sema.type_of(inst);
    const field_info: union(enum) {
        name: InternPool.NullTerminatedString,
        count: usize,
    } = switch (ip.index_to_key(inst_ty.to_intern())) {
        .anon_struct_type => |anon_struct_type| if (anon_struct_type.names.len == 1)
            .{ .name = anon_struct_type.names.get(ip)[0] }
        else
            .{ .count = anon_struct_type.names.len },
        .struct_type => name: {
            const field_names = ip.load_struct_type(inst_ty.to_intern()).field_names.get(ip);
            break :name if (field_names.len == 1)
                .{ .name = field_names[0] }
            else
                .{ .count = field_names.len };
        },
        else => unreachable,
    };
    switch (field_info) {
        .name => |field_name| {
            const init = try sema.struct_field_val(block, inst_src, inst, field_name, inst_src, inst_ty);
            return sema.union_init(block, init, inst_src, union_ty, union_ty_src, field_name, inst_src);
        },
        .count => |field_count| {
            assert(field_count != 1);
            const msg = msg: {
                const msg = if (field_count > 1) try sema.err_msg(
                    block,
                    inst_src,
                    "cannot initialize multiple union fields at once; unions can only have one active field",
                    .{},
                ) else try sema.err_msg(
                    block,
                    inst_src,
                    "union initializer must initialize one field",
                    .{},
                );
                errdefer msg.destroy(sema.gpa);

                // TODO add notes for where the anon struct was created to point out
                // the extra fields.

                try sema.add_declared_here_note(msg, union_ty);
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        },
    }
}

fn coerce_anon_struct_to_union_ptrs(
    sema: *Sema,
    block: *Block,
    ptr_union_ty: Type,
    union_ty_src: LazySrcLoc,
    ptr_anon_struct: Air.Inst.Ref,
    anon_struct_src: LazySrcLoc,
) !Air.Inst.Ref {
    const mod = sema.mod;
    const union_ty = ptr_union_ty.child_type(mod);
    const anon_struct = try sema.analyze_load(block, anon_struct_src, ptr_anon_struct, anon_struct_src);
    const union_inst = try sema.coerce_anon_struct_to_union(block, union_ty, union_ty_src, anon_struct, anon_struct_src);
    return sema.analyze_ref(block, union_ty_src, union_inst);
}

fn coerce_anon_struct_to_struct_ptrs(
    sema: *Sema,
    block: *Block,
    ptr_struct_ty: Type,
    struct_ty_src: LazySrcLoc,
    ptr_anon_struct: Air.Inst.Ref,
    anon_struct_src: LazySrcLoc,
) !Air.Inst.Ref {
    const mod = sema.mod;
    const struct_ty = ptr_struct_ty.child_type(mod);
    const anon_struct = try sema.analyze_load(block, anon_struct_src, ptr_anon_struct, anon_struct_src);
    const struct_inst = try sema.coerce_tuple_to_struct(block, struct_ty, anon_struct, anon_struct_src);
    return sema.analyze_ref(block, struct_ty_src, struct_inst);
}

/// If the lengths match, coerces element-wise.
fn coerce_array_like(
    sema: *Sema,
    block: *Block,
    dest_ty: Type,
    dest_ty_src: LazySrcLoc,
    inst: Air.Inst.Ref,
    inst_src: LazySrcLoc,
) !Air.Inst.Ref {
    const mod = sema.mod;
    const inst_ty = sema.type_of(inst);
    const target = mod.get_target();

    // try coercion of the whole array
    const in_memory_result = try sema.coerce_in_memory_allowed(block, dest_ty, inst_ty, false, target, dest_ty_src, inst_src);
    if (in_memory_result == .ok) {
        if (try sema.resolve_value(inst)) |inst_val| {
            // These types share the same comptime value representation.
            return sema.coerce_in_memory(inst_val, dest_ty);
        }
        try sema.require_runtime_block(block, inst_src, null);
        return block.add_bit_cast(dest_ty, inst);
    }

    // otherwise, try element by element
    const inst_len = inst_ty.array_len(mod);
    const dest_len = try sema.usize_cast(block, dest_ty_src, dest_ty.array_len(mod));
    if (dest_len != inst_len) {
        const msg = msg: {
            const msg = try sema.err_msg(block, inst_src, "expected type '{}', found '{}'", .{
                dest_ty.fmt(mod), inst_ty.fmt(mod),
            });
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, dest_ty_src, msg, "destination has length {d}", .{dest_len});
            try sema.err_note(block, inst_src, msg, "source has length {d}", .{inst_len});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    const dest_elem_ty = dest_ty.child_type(mod);
    if (dest_ty.is_vector(mod) and inst_ty.is_vector(mod) and (try sema.resolve_value(inst)) == null) {
        const inst_elem_ty = inst_ty.child_type(mod);
        switch (dest_elem_ty.zig_type_tag(mod)) {
            .Int => if (inst_elem_ty.is_int(mod)) {
                // integer widening
                const dst_info = dest_elem_ty.int_info(mod);
                const src_info = inst_elem_ty.int_info(mod);
                if ((src_info.signedness == dst_info.signedness and dst_info.bits >= src_info.bits) or
                    // small enough unsigned ints can get casted to large enough signed ints
                    (dst_info.signedness == .signed and dst_info.bits > src_info.bits))
                {
                    try sema.require_runtime_block(block, inst_src, null);
                    return block.add_ty_op(.intcast, dest_ty, inst);
                }
            },
            .Float => if (inst_elem_ty.is_runtime_float()) {
                // float widening
                const src_bits = inst_elem_ty.float_bits(target);
                const dst_bits = dest_elem_ty.float_bits(target);
                if (dst_bits >= src_bits) {
                    try sema.require_runtime_block(block, inst_src, null);
                    return block.add_ty_op(.fpext, dest_ty, inst);
                }
            },
            else => {},
        }
    }

    const element_vals = try sema.arena.alloc(InternPool.Index, dest_len);
    const element_refs = try sema.arena.alloc(Air.Inst.Ref, dest_len);
    var runtime_src: ?LazySrcLoc = null;

    for (element_vals, element_refs, 0..) |*val, *ref, i| {
        const index_ref = Air.interned_to_ref((try mod.int_value(Type.usize, i)).to_intern());
        const src = inst_src; // TODO better source location
        const elem_src = inst_src; // TODO better source location
        const elem_ref = try sema.elem_val_array(block, src, inst_src, inst, elem_src, index_ref, true);
        const coerced = try sema.coerce(block, dest_elem_ty, elem_ref, elem_src);
        ref.* = coerced;
        if (runtime_src == null) {
            if (try sema.resolve_value(coerced)) |elem_val| {
                val.* = elem_val.to_intern();
            } else {
                runtime_src = elem_src;
            }
        }
    }

    if (runtime_src) |rs| {
        try sema.require_runtime_block(block, inst_src, rs);
        return block.add_aggregate_init(dest_ty, element_refs);
    }

    return Air.interned_to_ref((try mod.intern(.{ .aggregate = .{
        .ty = dest_ty.to_intern(),
        .storage = .{ .elems = element_vals },
    } })));
}

/// If the lengths match, coerces element-wise.
fn coerce_tuple_to_array(
    sema: *Sema,
    block: *Block,
    dest_ty: Type,
    dest_ty_src: LazySrcLoc,
    inst: Air.Inst.Ref,
    inst_src: LazySrcLoc,
) !Air.Inst.Ref {
    const mod = sema.mod;
    const inst_ty = sema.type_of(inst);
    const inst_len = inst_ty.array_len(mod);
    const dest_len = dest_ty.array_len(mod);

    if (dest_len != inst_len) {
        const msg = msg: {
            const msg = try sema.err_msg(block, inst_src, "expected type '{}', found '{}'", .{
                dest_ty.fmt(sema.mod), inst_ty.fmt(sema.mod),
            });
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, dest_ty_src, msg, "destination has length {d}", .{dest_len});
            try sema.err_note(block, inst_src, msg, "source has length {d}", .{inst_len});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }

    const dest_elems = try sema.usize_cast(block, dest_ty_src, dest_len);
    const element_vals = try sema.arena.alloc(InternPool.Index, dest_elems);
    const element_refs = try sema.arena.alloc(Air.Inst.Ref, dest_elems);
    const dest_elem_ty = dest_ty.child_type(mod);

    var runtime_src: ?LazySrcLoc = null;
    for (element_vals, element_refs, 0..) |*val, *ref, i_usize| {
        const i: u32 = @int_cast(i_usize);
        if (i_usize == inst_len) {
            const sentinel_val = dest_ty.sentinel(mod).?;
            val.* = sentinel_val.to_intern();
            ref.* = Air.interned_to_ref(sentinel_val.to_intern());
            break;
        }
        const elem_src = inst_src; // TODO better source location
        const elem_ref = try sema.tuple_field(block, inst_src, inst, elem_src, i);
        const coerced = try sema.coerce(block, dest_elem_ty, elem_ref, elem_src);
        ref.* = coerced;
        if (runtime_src == null) {
            if (try sema.resolve_value(coerced)) |elem_val| {
                val.* = elem_val.to_intern();
            } else {
                runtime_src = elem_src;
            }
        }
    }

    if (runtime_src) |rs| {
        try sema.require_runtime_block(block, inst_src, rs);
        return block.add_aggregate_init(dest_ty, element_refs);
    }

    return Air.interned_to_ref((try mod.intern(.{ .aggregate = .{
        .ty = dest_ty.to_intern(),
        .storage = .{ .elems = element_vals },
    } })));
}

/// If the lengths match, coerces element-wise.
fn coerce_tuple_to_slice_ptrs(
    sema: *Sema,
    block: *Block,
    slice_ty: Type,
    slice_ty_src: LazySrcLoc,
    ptr_tuple: Air.Inst.Ref,
    tuple_src: LazySrcLoc,
) !Air.Inst.Ref {
    const mod = sema.mod;
    const tuple_ty = sema.type_of(ptr_tuple).child_type(mod);
    const tuple = try sema.analyze_load(block, tuple_src, ptr_tuple, tuple_src);
    const slice_info = slice_ty.ptr_info(mod);
    const array_ty = try mod.array_type(.{
        .len = tuple_ty.struct_field_count(mod),
        .sentinel = slice_info.sentinel,
        .child = slice_info.child,
    });
    const array_inst = try sema.coerce_tuple_to_array(block, array_ty, slice_ty_src, tuple, tuple_src);
    if (slice_info.flags.alignment != .none) {
        return sema.fail(block, slice_ty_src, "TODO: override the alignment of the array decl we create here", .{});
    }
    const ptr_array = try sema.analyze_ref(block, slice_ty_src, array_inst);
    return sema.coerce_array_ptr_to_slice(block, slice_ty, ptr_array, slice_ty_src);
}

/// If the lengths match, coerces element-wise.
fn coerce_tuple_to_array_ptrs(
    sema: *Sema,
    block: *Block,
    ptr_array_ty: Type,
    array_ty_src: LazySrcLoc,
    ptr_tuple: Air.Inst.Ref,
    tuple_src: LazySrcLoc,
) !Air.Inst.Ref {
    const mod = sema.mod;
    const tuple = try sema.analyze_load(block, tuple_src, ptr_tuple, tuple_src);
    const ptr_info = ptr_array_ty.ptr_info(mod);
    const array_ty = Type.from_interned(ptr_info.child);
    const array_inst = try sema.coerce_tuple_to_array(block, array_ty, array_ty_src, tuple, tuple_src);
    if (ptr_info.flags.alignment != .none) {
        return sema.fail(block, array_ty_src, "TODO: override the alignment of the array decl we create here", .{});
    }
    const ptr_array = try sema.analyze_ref(block, array_ty_src, array_inst);
    return ptr_array;
}

/// Handles both tuples and anon struct literals. Coerces field-wise. Reports
/// errors for both extra fields and missing fields.
fn coerce_tuple_to_struct(
    sema: *Sema,
    block: *Block,
    struct_ty: Type,
    inst: Air.Inst.Ref,
    inst_src: LazySrcLoc,
) !Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    try sema.resolve_type_fields(struct_ty);
    try sema.resolve_struct_field_inits(struct_ty);

    if (struct_ty.is_tuple_or_anon_struct(mod)) {
        return sema.coerce_tuple_to_tuple(block, struct_ty, inst, inst_src);
    }

    const struct_type = mod.type_to_struct(struct_ty).?;
    const field_vals = try sema.arena.alloc(InternPool.Index, struct_type.field_types.len);
    const field_refs = try sema.arena.alloc(Air.Inst.Ref, field_vals.len);
    @memset(field_refs, .none);

    const inst_ty = sema.type_of(inst);
    var runtime_src: ?LazySrcLoc = null;
    const field_count = switch (ip.index_to_key(inst_ty.to_intern())) {
        .anon_struct_type => |anon_struct_type| anon_struct_type.types.len,
        .struct_type => ip.load_struct_type(inst_ty.to_intern()).field_types.len,
        else => unreachable,
    };
    for (0..field_count) |tuple_field_index| {
        const field_src = inst_src; // TODO better source location
        const field_name: InternPool.NullTerminatedString = switch (ip.index_to_key(inst_ty.to_intern())) {
            .anon_struct_type => |anon_struct_type| if (anon_struct_type.names.len > 0)
                anon_struct_type.names.get(ip)[tuple_field_index]
            else
                try ip.get_or_put_string_fmt(sema.gpa, "{d}", .{tuple_field_index}, .no_embedded_nulls),
            .struct_type => ip.load_struct_type(inst_ty.to_intern()).field_names.get(ip)[tuple_field_index],
            else => unreachable,
        };
        const struct_field_index = try sema.struct_field_index(block, struct_ty, field_name, field_src);
        const struct_field_ty = Type.from_interned(struct_type.field_types.get(ip)[struct_field_index]);
        const elem_ref = try sema.tuple_field(block, inst_src, inst, field_src, @int_cast(tuple_field_index));
        const coerced = try sema.coerce(block, struct_field_ty, elem_ref, field_src);
        field_refs[struct_field_index] = coerced;
        if (struct_type.field_is_comptime(ip, struct_field_index)) {
            const init_val = (try sema.resolve_value(coerced)) orelse {
                return sema.fail_with_needed_comptime(block, field_src, .{
                    .needed_comptime_reason = "value stored in comptime field must be comptime-known",
                });
            };

            const field_init = Value.from_interned(struct_type.field_inits.get(ip)[struct_field_index]);
            if (!init_val.eql(field_init, struct_field_ty, sema.mod)) {
                return sema.fail_with_invalid_comptime_field_store(block, field_src, inst_ty, tuple_field_index);
            }
        }
        if (runtime_src == null) {
            if (try sema.resolve_value(coerced)) |field_val| {
                field_vals[struct_field_index] = field_val.to_intern();
            } else {
                runtime_src = field_src;
            }
        }
    }

    // Populate default field values and report errors for missing fields.
    var root_msg: ?*Module.ErrorMsg = null;
    errdefer if (root_msg) |msg| msg.destroy(sema.gpa);

    for (field_refs, 0..) |*field_ref, i| {
        if (field_ref.* != .none) continue;

        const field_name = struct_type.field_names.get(ip)[i];
        const field_default_val = struct_type.field_init(ip, i);
        const field_src = inst_src; // TODO better source location
        if (field_default_val == .none) {
            const template = "missing struct field: {}";
            const args = .{field_name.fmt(ip)};
            if (root_msg) |msg| {
                try sema.err_note(block, field_src, msg, template, args);
            } else {
                root_msg = try sema.err_msg(block, field_src, template, args);
            }
            continue;
        }
        if (runtime_src == null) {
            field_vals[i] = field_default_val;
        } else {
            field_ref.* = Air.interned_to_ref(field_default_val);
        }
    }

    if (root_msg) |msg| {
        try sema.add_declared_here_note(msg, struct_ty);
        root_msg = null;
        return sema.fail_with_owned_error_msg(block, msg);
    }

    if (runtime_src) |rs| {
        try sema.require_runtime_block(block, inst_src, rs);
        return block.add_aggregate_init(struct_ty, field_refs);
    }

    const struct_val = try mod.intern(.{ .aggregate = .{
        .ty = struct_ty.to_intern(),
        .storage = .{ .elems = field_vals },
    } });
    // TODO: figure out InternPool removals for incremental compilation
    //errdefer ip.remove(struct_val);

    return Air.interned_to_ref(struct_val);
}

fn coerce_tuple_to_tuple(
    sema: *Sema,
    block: *Block,
    tuple_ty: Type,
    inst: Air.Inst.Ref,
    inst_src: LazySrcLoc,
) !Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const dest_field_count = switch (ip.index_to_key(tuple_ty.to_intern())) {
        .anon_struct_type => |anon_struct_type| anon_struct_type.types.len,
        .struct_type => ip.load_struct_type(tuple_ty.to_intern()).field_types.len,
        else => unreachable,
    };
    const field_vals = try sema.arena.alloc(InternPool.Index, dest_field_count);
    const field_refs = try sema.arena.alloc(Air.Inst.Ref, field_vals.len);
    @memset(field_refs, .none);

    const inst_ty = sema.type_of(inst);
    const src_field_count = switch (ip.index_to_key(inst_ty.to_intern())) {
        .anon_struct_type => |anon_struct_type| anon_struct_type.types.len,
        .struct_type => ip.load_struct_type(inst_ty.to_intern()).field_types.len,
        else => unreachable,
    };
    if (src_field_count > dest_field_count) return error.NotCoercible;

    var runtime_src: ?LazySrcLoc = null;
    for (0..dest_field_count) |field_index_usize| {
        const field_i: u32 = @int_cast(field_index_usize);
        const field_src = inst_src; // TODO better source location
        const field_name: InternPool.NullTerminatedString = switch (ip.index_to_key(inst_ty.to_intern())) {
            .anon_struct_type => |anon_struct_type| if (anon_struct_type.names.len > 0)
                anon_struct_type.names.get(ip)[field_i]
            else
                try ip.get_or_put_string_fmt(sema.gpa, "{d}", .{field_i}, .no_embedded_nulls),
            .struct_type => s: {
                const struct_type = ip.load_struct_type(inst_ty.to_intern());
                if (struct_type.field_names.len > 0) {
                    break :s struct_type.field_names.get(ip)[field_i];
                } else {
                    break :s try ip.get_or_put_string_fmt(sema.gpa, "{d}", .{field_i}, .no_embedded_nulls);
                }
            },
            else => unreachable,
        };

        if (field_name.eql_slice("len", ip))
            return sema.fail(block, field_src, "cannot assign to 'len' field of tuple", .{});

        const field_ty = switch (ip.index_to_key(tuple_ty.to_intern())) {
            .anon_struct_type => |anon_struct_type| anon_struct_type.types.get(ip)[field_index_usize],
            .struct_type => ip.load_struct_type(tuple_ty.to_intern()).field_types.get(ip)[field_index_usize],
            else => unreachable,
        };
        const default_val = switch (ip.index_to_key(tuple_ty.to_intern())) {
            .anon_struct_type => |anon_struct_type| anon_struct_type.values.get(ip)[field_index_usize],
            .struct_type => ip.load_struct_type(tuple_ty.to_intern()).field_init(ip, field_index_usize),
            else => unreachable,
        };

        const field_index = try sema.tuple_field_index(block, tuple_ty, field_name, field_src);

        const elem_ref = try sema.tuple_field(block, inst_src, inst, field_src, field_i);
        const coerced = try sema.coerce(block, Type.from_interned(field_ty), elem_ref, field_src);
        field_refs[field_index] = coerced;
        if (default_val != .none) {
            const init_val = (try sema.resolve_value(coerced)) orelse {
                return sema.fail_with_needed_comptime(block, field_src, .{
                    .needed_comptime_reason = "value stored in comptime field must be comptime-known",
                });
            };

            if (!init_val.eql(Value.from_interned(default_val), Type.from_interned(field_ty), sema.mod)) {
                return sema.fail_with_invalid_comptime_field_store(block, field_src, inst_ty, field_i);
            }
        }
        if (runtime_src == null) {
            if (try sema.resolve_value(coerced)) |field_val| {
                field_vals[field_index] = field_val.to_intern();
            } else {
                runtime_src = field_src;
            }
        }
    }

    // Populate default field values and report errors for missing fields.
    var root_msg: ?*Module.ErrorMsg = null;
    errdefer if (root_msg) |msg| msg.destroy(sema.gpa);

    for (field_refs, 0..) |*field_ref, i_usize| {
        const i: u32 = @int_cast(i_usize);
        if (field_ref.* != .none) continue;

        const default_val = switch (ip.index_to_key(tuple_ty.to_intern())) {
            .anon_struct_type => |anon_struct_type| anon_struct_type.values.get(ip)[i],
            .struct_type => ip.load_struct_type(tuple_ty.to_intern()).field_init(ip, i),
            else => unreachable,
        };

        const field_src = inst_src; // TODO better source location
        if (default_val == .none) {
            const field_name = tuple_ty.struct_field_name(i, mod).unwrap() orelse {
                const template = "missing tuple field: {d}";
                if (root_msg) |msg| {
                    try sema.err_note(block, field_src, msg, template, .{i});
                } else {
                    root_msg = try sema.err_msg(block, field_src, template, .{i});
                }
                continue;
            };
            const template = "missing struct field: {}";
            const args = .{field_name.fmt(ip)};
            if (root_msg) |msg| {
                try sema.err_note(block, field_src, msg, template, args);
            } else {
                root_msg = try sema.err_msg(block, field_src, template, args);
            }
            continue;
        }
        if (runtime_src == null) {
            field_vals[i] = default_val;
        } else {
            field_ref.* = Air.interned_to_ref(default_val);
        }
    }

    if (root_msg) |msg| {
        try sema.add_declared_here_note(msg, tuple_ty);
        root_msg = null;
        return sema.fail_with_owned_error_msg(block, msg);
    }

    if (runtime_src) |rs| {
        try sema.require_runtime_block(block, inst_src, rs);
        return block.add_aggregate_init(tuple_ty, field_refs);
    }

    return Air.interned_to_ref((try mod.intern(.{ .aggregate = .{
        .ty = tuple_ty.to_intern(),
        .storage = .{ .elems = field_vals },
    } })));
}

fn analyze_decl_val(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    decl_index: InternPool.DeclIndex,
) CompileError!Air.Inst.Ref {
    try sema.add_referenced_by(block, src, decl_index);
    if (sema.decl_val_table.get(decl_index)) |result| {
        return result;
    }
    const decl_ref = try sema.analyze_decl_ref_inner(decl_index, false);
    const result = try sema.analyze_load(block, src, decl_ref, src);
    if (result.to_interned() != null) {
        if (!block.is_typeof) {
            try sema.decl_val_table.put(sema.gpa, decl_index, result);
        }
    }
    return result;
}

fn add_referenced_by(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    decl_index: InternPool.DeclIndex,
) !void {
    if (sema.mod.comp.reference_trace == 0) return;
    if (src == .unneeded) {
        // We can't use NeededSourceLocation, since sites handling that assume it means a compile
        // error. Our long-term strategy here is to gradually transition from NeededSourceLocation
        // into having more LazySrcLoc tags. In the meantime, let release compilers just ignore this
        // reference (a slightly-incomplete error is better than a crash!), but trigger a panic in
        // debug so we can fix this case.
        if (std.debug.runtime_safety) unreachable else return;
    }
    try sema.mod.reference_table.put(sema.gpa, decl_index, .{
        .referencer = block.src_decl,
        .src = src,
    });
}

pub fn ensure_decl_analyzed(sema: *Sema, decl_index: InternPool.DeclIndex) CompileError!void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const decl = mod.decl_ptr(decl_index);
    if (decl.analysis == .in_progress) {
        const msg = try Module.ErrorMsg.create(sema.gpa, decl.src_loc(mod), "dependency loop detected", .{});
        return sema.fail_with_owned_error_msg(null, msg);
    }

    mod.ensure_decl_analyzed(decl_index) catch |err| {
        if (sema.owner_func_index != .none) {
            ip.func_analysis(sema.owner_func_index).state = .dependency_failure;
        } else {
            sema.owner_decl.analysis = .dependency_failure;
        }
        return err;
    };
}

fn ensure_func_body_analyzed(sema: *Sema, func: InternPool.Index) CompileError!void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    mod.ensure_func_body_analyzed(func) catch |err| {
        if (sema.owner_func_index != .none) {
            ip.func_analysis(sema.owner_func_index).state = .dependency_failure;
        } else {
            sema.owner_decl.analysis = .dependency_failure;
        }
        return err;
    };
}

fn opt_ref_value(sema: *Sema, opt_val: ?Value) !Value {
    const mod = sema.mod;
    const ptr_anyopaque_ty = try mod.single_const_ptr_type(Type.anyopaque);
    return Value.from_interned((try mod.intern(.{ .opt = .{
        .ty = (try mod.optional_type(ptr_anyopaque_ty.to_intern())).to_intern(),
        .val = if (opt_val) |val| (try mod.get_coerced(
            Value.from_interned(try sema.ref_value(val.to_intern())),
            ptr_anyopaque_ty,
        )).to_intern() else .none,
    } })));
}

fn analyze_decl_ref(sema: *Sema, decl_index: InternPool.DeclIndex) CompileError!Air.Inst.Ref {
    return sema.analyze_decl_ref_inner(decl_index, true);
}

/// Analyze a reference to the decl at the given index. Ensures the underlying decl is analyzed, but
/// only triggers analysis for function bodies if `analyze_fn_body` is true. If it's possible for a
/// decl_ref to end up in runtime code, the function body must be analyzed: `analyze_decl_ref` wraps
/// this function with `analyze_fn_body` set to true.
fn analyze_decl_ref_inner(sema: *Sema, decl_index: InternPool.DeclIndex, analyze_fn_body: bool) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    try sema.ensure_decl_analyzed(decl_index);

    const decl_val = try mod.decl_ptr(decl_index).value_or_fail();
    const owner_decl = mod.decl_ptr(switch (mod.intern_pool.index_to_key(decl_val.to_intern())) {
        .variable => |variable| variable.decl,
        .extern_func => |extern_func| extern_func.decl,
        .func => |func| func.owner_decl,
        else => decl_index,
    });
    // TODO: if this is a `decl_ref` of a non-variable decl, only depend on decl type
    try sema.declare_dependency(.{ .decl_val = decl_index });
    const ptr_ty = try sema.ptr_type(.{
        .child = decl_val.type_of(mod).to_intern(),
        .flags = .{
            .alignment = owner_decl.alignment,
            .is_const = if (decl_val.get_variable(mod)) |variable| variable.is_const else true,
            .address_space = owner_decl.@"addrspace",
        },
    });
    if (analyze_fn_body) {
        try sema.maybe_queue_func_body_analysis(decl_index);
    }
    return Air.interned_to_ref((try mod.intern(.{ .ptr = .{
        .ty = ptr_ty.to_intern(),
        .base_addr = .{ .decl = decl_index },
        .byte_offset = 0,
    } })));
}

fn maybe_queue_func_body_analysis(sema: *Sema, decl_index: InternPool.DeclIndex) !void {
    const mod = sema.mod;
    const decl = mod.decl_ptr(decl_index);
    const decl_val = try decl.value_or_fail();
    if (!mod.intern_pool.is_func_body(decl_val.to_intern())) return;
    if (!try sema.fn_has_runtime_bits(decl_val.type_of(mod))) return;
    try mod.ensure_func_body_analysis_queued(decl_val.to_intern());
}

fn analyze_ref(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    operand: Air.Inst.Ref,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const operand_ty = sema.type_of(operand);

    if (try sema.resolve_value(operand)) |val| {
        switch (mod.intern_pool.index_to_key(val.to_intern())) {
            .extern_func => |extern_func| return sema.analyze_decl_ref(extern_func.decl),
            .func => |func| return sema.analyze_decl_ref(func.owner_decl),
            else => return anon_decl_ref(sema, val.to_intern()),
        }
    }

    try sema.require_runtime_block(block, src, null);
    const address_space = target_util.default_address_space(mod.get_target(), .local);
    const ptr_type = try sema.ptr_type(.{
        .child = operand_ty.to_intern(),
        .flags = .{
            .is_const = true,
            .address_space = address_space,
        },
    });
    const mut_ptr_type = try sema.ptr_type(.{
        .child = operand_ty.to_intern(),
        .flags = .{ .address_space = address_space },
    });
    const alloc = try block.add_ty(.alloc, mut_ptr_type);
    try sema.store_ptr(block, src, alloc, operand);

    // TODO: Replace with sema.coerce when that supports adding pointer constness.
    return sema.bit_cast(block, ptr_type, alloc, src, null);
}

fn analyze_load(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    ptr: Air.Inst.Ref,
    ptr_src: LazySrcLoc,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ptr_ty = sema.type_of(ptr);
    const elem_ty = switch (ptr_ty.zig_type_tag(mod)) {
        .Pointer => ptr_ty.child_type(mod),
        else => return sema.fail(block, ptr_src, "expected pointer, found '{}'", .{ptr_ty.fmt(sema.mod)}),
    };
    if (elem_ty.zig_type_tag(mod) == .Opaque) {
        return sema.fail(block, ptr_src, "cannot load opaque type '{}'", .{elem_ty.fmt(mod)});
    }

    if (try sema.type_has_one_possible_value(elem_ty)) |opv| {
        return Air.interned_to_ref(opv.to_intern());
    }

    if (try sema.resolve_defined_value(block, ptr_src, ptr)) |ptr_val| {
        if (try sema.pointer_deref(block, src, ptr_val, ptr_ty)) |elem_val| {
            return Air.interned_to_ref(elem_val.to_intern());
        }
    }

    if (ptr_ty.ptr_info(mod).flags.vector_index == .runtime) {
        const ptr_inst = ptr.to_index().?;
        const air_tags = sema.air_instructions.items(.tag);
        if (air_tags[@int_from_enum(ptr_inst)] == .ptr_elem_ptr) {
            const ty_pl = sema.air_instructions.items(.data)[@int_from_enum(ptr_inst)].ty_pl;
            const bin_op = sema.get_tmp_air().extra_data(Air.Bin, ty_pl.payload).data;
            return block.add_bin_op(.ptr_elem_val, bin_op.lhs, bin_op.rhs);
        }
        return sema.fail(block, ptr_src, "unable to determine vector element index of type '{}'", .{
            ptr_ty.fmt(sema.mod),
        });
    }

    return block.add_ty_op(.load, elem_ty, ptr);
}

fn analyze_slice_ptr(
    sema: *Sema,
    block: *Block,
    slice_src: LazySrcLoc,
    slice: Air.Inst.Ref,
    slice_ty: Type,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const result_ty = slice_ty.slice_ptr_field_type(mod);
    if (try sema.resolve_value(slice)) |val| {
        if (val.is_undef(mod)) return mod.undef_ref(result_ty);
        return Air.interned_to_ref(val.slice_ptr(mod).to_intern());
    }
    try sema.require_runtime_block(block, slice_src, null);
    return block.add_ty_op(.slice_ptr, result_ty, slice);
}

fn analyze_optional_slice_ptr(
    sema: *Sema,
    block: *Block,
    opt_slice_src: LazySrcLoc,
    opt_slice: Air.Inst.Ref,
    opt_slice_ty: Type,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const result_ty = opt_slice_ty.optional_child(mod).slice_ptr_field_type(mod);

    if (try sema.resolve_value(opt_slice)) |opt_val| {
        if (opt_val.is_undef(mod)) return mod.undef_ref(result_ty);
        const slice_ptr: InternPool.Index = if (opt_val.optional_value(mod)) |val|
            val.slice_ptr(mod).to_intern()
        else
            .null_value;

        return Air.interned_to_ref(slice_ptr);
    }

    try sema.require_runtime_block(block, opt_slice_src, null);

    const slice = try block.add_ty_op(.optional_payload, opt_slice_ty, opt_slice);
    return block.add_ty_op(.slice_ptr, result_ty, slice);
}

fn analyze_slice_len(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    slice_inst: Air.Inst.Ref,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    if (try sema.resolve_value(slice_inst)) |slice_val| {
        if (slice_val.is_undef(mod)) {
            return mod.undef_ref(Type.usize);
        }
        return mod.int_ref(Type.usize, try slice_val.slice_len(sema));
    }
    try sema.require_runtime_block(block, src, null);
    return block.add_ty_op(.slice_len, Type.usize, slice_inst);
}

fn analyze_is_null(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    operand: Air.Inst.Ref,
    invert_logic: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const result_ty = Type.bool;
    if (try sema.resolve_value(operand)) |opt_val| {
        if (opt_val.is_undef(mod)) {
            return mod.undef_ref(result_ty);
        }
        const is_null = opt_val.is_null(mod);
        const bool_value = if (invert_logic) !is_null else is_null;
        return if (bool_value) .bool_true else .bool_false;
    }

    const inverted_non_null_res: Air.Inst.Ref = if (invert_logic) .bool_true else .bool_false;
    const operand_ty = sema.type_of(operand);
    if (operand_ty.zig_type_tag(mod) == .Optional and operand_ty.optional_child(mod).zig_type_tag(mod) == .NoReturn) {
        return inverted_non_null_res;
    }
    if (operand_ty.zig_type_tag(mod) != .Optional and !operand_ty.is_ptr_like_optional(mod)) {
        return inverted_non_null_res;
    }
    try sema.require_runtime_block(block, src, null);
    const air_tag: Air.Inst.Tag = if (invert_logic) .is_non_null else .is_null;
    return block.add_un_op(air_tag, operand);
}

fn analyze_ptr_is_non_err_comptime_only(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    operand: Air.Inst.Ref,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ptr_ty = sema.type_of(operand);
    assert(ptr_ty.zig_type_tag(mod) == .Pointer);
    const child_ty = ptr_ty.child_type(mod);

    const child_tag = child_ty.zig_type_tag(mod);
    if (child_tag != .ErrorSet and child_tag != .ErrorUnion) return .bool_true;
    if (child_tag == .ErrorSet) return .bool_false;
    assert(child_tag == .ErrorUnion);

    _ = block;
    _ = src;

    return .none;
}

fn analyze_is_non_err_comptime_only(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    operand: Air.Inst.Ref,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const operand_ty = sema.type_of(operand);
    const ot = operand_ty.zig_type_tag(mod);
    if (ot != .ErrorSet and ot != .ErrorUnion) return .bool_true;
    if (ot == .ErrorSet) return .bool_false;
    assert(ot == .ErrorUnion);

    const payload_ty = operand_ty.error_union_payload(mod);
    if (payload_ty.zig_type_tag(mod) == .NoReturn) {
        return .bool_false;
    }

    if (operand.to_index()) |operand_inst| {
        switch (sema.air_instructions.items(.tag)[@int_from_enum(operand_inst)]) {
            .wrap_errunion_payload => return .bool_true,
            .wrap_errunion_err => return .bool_false,
            else => {},
        }
    } else if (operand == .undef) {
        return mod.undef_ref(Type.bool);
    } else if (@int_from_enum(operand) < InternPool.static_len) {
        // None of the ref tags can be errors.
        return .bool_true;
    }

    const maybe_operand_val = try sema.resolve_value(operand);

    // exception if the error union error set is known to be empty,
    // we allow the comparison but always make it comptime-known.
    const set_ty = ip.error_union_set(operand_ty.to_intern());
    switch (set_ty) {
        .anyerror_type => {},
        .adhoc_inferred_error_set_type => if (sema.fn_ret_ty_ies) |ies| blk: {
            // If the error set is empty, we must return a comptime true or false.
            // However we want to avoid unnecessarily resolving an inferred error set
            // in case it is already non-empty.
            switch (ies.resolved) {
                .anyerror_type => break :blk,
                .none => {},
                else => |i| if (ip.index_to_key(i).error_set_type.names.len != 0) break :blk,
            }

            if (maybe_operand_val != null) break :blk;

            // Try to avoid resolving inferred error set if possible.
            if (ies.errors.count() != 0) return .none;
            switch (ies.resolved) {
                .anyerror_type => return .none,
                .none => {},
                else => switch (ip.index_to_key(ies.resolved).error_set_type.names.len) {
                    0 => return .bool_true,
                    else => return .none,
                },
            }
            // We do not have a comptime answer because this inferred error
            // set is not resolved, and an instruction later in this function
            // body may or may not cause an error to be added to this set.
            return .none;
        },
        else => switch (ip.index_to_key(set_ty)) {
            .error_set_type => |error_set_type| {
                if (error_set_type.names.len == 0) return .bool_true;
            },
            .inferred_error_set_type => |func_index| blk: {
                // If the error set is empty, we must return a comptime true or false.
                // However we want to avoid unnecessarily resolving an inferred error set
                // in case it is already non-empty.
                switch (ip.func_ies_resolved(func_index).*) {
                    .anyerror_type => break :blk,
                    .none => {},
                    else => |i| if (ip.index_to_key(i).error_set_type.names.len != 0) break :blk,
                }
                if (maybe_operand_val != null) break :blk;
                if (sema.fn_ret_ty_ies) |ies| {
                    if (ies.func == func_index) {
                        // Try to avoid resolving inferred error set if possible.
                        if (ies.errors.count() != 0) return .none;
                        switch (ies.resolved) {
                            .anyerror_type => return .none,
                            .none => {},
                            else => switch (ip.index_to_key(ies.resolved).error_set_type.names.len) {
                                0 => return .bool_true,
                                else => return .none,
                            },
                        }
                        // We do not have a comptime answer because this inferred error
                        // set is not resolved, and an instruction later in this function
                        // body may or may not cause an error to be added to this set.
                        return .none;
                    }
                }
                const resolved_ty = try sema.resolve_inferred_error_set(block, src, set_ty);
                if (resolved_ty == .anyerror_type)
                    break :blk;
                if (ip.index_to_key(resolved_ty).error_set_type.names.len == 0)
                    return .bool_true;
            },
            else => unreachable,
        },
    }

    if (maybe_operand_val) |err_union| {
        if (err_union.is_undef(mod)) {
            return mod.undef_ref(Type.bool);
        }
        if (err_union.get_error_name(mod) == .none) {
            return .bool_true;
        } else {
            return .bool_false;
        }
    }
    return .none;
}

fn analyze_is_non_err(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    operand: Air.Inst.Ref,
) CompileError!Air.Inst.Ref {
    const result = try sema.analyze_is_non_err_comptime_only(block, src, operand);
    if (result == .none) {
        try sema.require_runtime_block(block, src, null);
        return block.add_un_op(.is_non_err, operand);
    } else {
        return result;
    }
}

fn analyze_ptr_is_non_err(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    operand: Air.Inst.Ref,
) CompileError!Air.Inst.Ref {
    const result = try sema.analyze_ptr_is_non_err_comptime_only(block, src, operand);
    if (result == .none) {
        try sema.require_runtime_block(block, src, null);
        return block.add_un_op(.is_non_err_ptr, operand);
    } else {
        return result;
    }
}

fn analyze_slice(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    ptr_ptr: Air.Inst.Ref,
    uncasted_start: Air.Inst.Ref,
    uncasted_end_opt: Air.Inst.Ref,
    sentinel_opt: Air.Inst.Ref,
    sentinel_src: LazySrcLoc,
    ptr_src: LazySrcLoc,
    start_src: LazySrcLoc,
    end_src: LazySrcLoc,
    by_length: bool,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    // Slice expressions can operate on a variable whose type is an array. This requires
    // the slice operand to be a pointer. In the case of a non-array, it will be a double pointer.
    const ptr_ptr_ty = sema.type_of(ptr_ptr);
    const ptr_ptr_child_ty = switch (ptr_ptr_ty.zig_type_tag(mod)) {
        .Pointer => ptr_ptr_ty.child_type(mod),
        else => return sema.fail(block, ptr_src, "expected pointer, found '{}'", .{ptr_ptr_ty.fmt(mod)}),
    };

    var array_ty = ptr_ptr_child_ty;
    var slice_ty = ptr_ptr_ty;
    var ptr_or_slice = ptr_ptr;
    var elem_ty: Type = undefined;
    var ptr_sentinel: ?Value = null;
    switch (ptr_ptr_child_ty.zig_type_tag(mod)) {
        .Array => {
            ptr_sentinel = ptr_ptr_child_ty.sentinel(mod);
            elem_ty = ptr_ptr_child_ty.child_type(mod);
        },
        .Pointer => switch (ptr_ptr_child_ty.ptr_size(mod)) {
            .One => {
                const double_child_ty = ptr_ptr_child_ty.child_type(mod);
                ptr_or_slice = try sema.analyze_load(block, src, ptr_ptr, ptr_src);
                if (double_child_ty.zig_type_tag(mod) == .Array) {
                    ptr_sentinel = double_child_ty.sentinel(mod);
                    slice_ty = ptr_ptr_child_ty;
                    array_ty = double_child_ty;
                    elem_ty = double_child_ty.child_type(mod);
                } else {
                    const bounds_error_message = "slice of single-item pointer must have comptime-known bounds [0..0], [0..1], or [1..1]";
                    if (uncasted_end_opt == .none) {
                        return sema.fail(block, src, bounds_error_message, .{});
                    }
                    const start_value = try sema.resolve_const_defined_value(
                        block,
                        start_src,
                        uncasted_start,
                        .{ .needed_comptime_reason = bounds_error_message },
                    );

                    const end_value = try sema.resolve_const_defined_value(
                        block,
                        end_src,
                        uncasted_end_opt,
                        .{ .needed_comptime_reason = bounds_error_message },
                    );

                    if (try sema.compare_scalar(start_value, .neq, end_value, Type.comptime_int)) {
                        if (try sema.compare_scalar(start_value, .neq, Value.zero_comptime_int, Type.comptime_int)) {
                            const msg = msg: {
                                const msg = try sema.err_msg(block, start_src, bounds_error_message, .{});
                                errdefer msg.destroy(sema.gpa);
                                try sema.err_note(
                                    block,
                                    start_src,
                                    msg,
                                    "expected '{}', found '{}'",
                                    .{
                                        Value.zero_comptime_int.fmt_value(mod, sema),
                                        start_value.fmt_value(mod, sema),
                                    },
                                );
                                break :msg msg;
                            };
                            return sema.fail_with_owned_error_msg(block, msg);
                        } else if (try sema.compare_scalar(end_value, .neq, Value.one_comptime_int, Type.comptime_int)) {
                            const msg = msg: {
                                const msg = try sema.err_msg(block, end_src, bounds_error_message, .{});
                                errdefer msg.destroy(sema.gpa);
                                try sema.err_note(
                                    block,
                                    end_src,
                                    msg,
                                    "expected '{}', found '{}'",
                                    .{
                                        Value.one_comptime_int.fmt_value(mod, sema),
                                        end_value.fmt_value(mod, sema),
                                    },
                                );
                                break :msg msg;
                            };
                            return sema.fail_with_owned_error_msg(block, msg);
                        }
                    } else {
                        if (try sema.compare_scalar(end_value, .gt, Value.one_comptime_int, Type.comptime_int)) {
                            return sema.fail(
                                block,
                                end_src,
                                "end index {} out of bounds for slice of single-item pointer",
                                .{end_value.fmt_value(mod, sema)},
                            );
                        }
                    }

                    array_ty = try mod.array_type(.{
                        .len = 1,
                        .child = double_child_ty.to_intern(),
                    });
                    const ptr_info = ptr_ptr_child_ty.ptr_info(mod);
                    slice_ty = try mod.ptr_type(.{
                        .child = array_ty.to_intern(),
                        .flags = .{
                            .alignment = ptr_info.flags.alignment,
                            .is_const = ptr_info.flags.is_const,
                            .is_allowzero = ptr_info.flags.is_allowzero,
                            .is_volatile = ptr_info.flags.is_volatile,
                            .address_space = ptr_info.flags.address_space,
                        },
                    });
                    elem_ty = double_child_ty;
                }
            },
            .Many, .C => {
                ptr_sentinel = ptr_ptr_child_ty.sentinel(mod);
                ptr_or_slice = try sema.analyze_load(block, src, ptr_ptr, ptr_src);
                slice_ty = ptr_ptr_child_ty;
                array_ty = ptr_ptr_child_ty;
                elem_ty = ptr_ptr_child_ty.child_type(mod);

                if (ptr_ptr_child_ty.ptr_size(mod) == .C) {
                    if (try sema.resolve_defined_value(block, ptr_src, ptr_or_slice)) |ptr_val| {
                        if (ptr_val.is_null(mod)) {
                            return sema.fail(block, src, "slice of null pointer", .{});
                        }
                    }
                }
            },
            .Slice => {
                ptr_sentinel = ptr_ptr_child_ty.sentinel(mod);
                ptr_or_slice = try sema.analyze_load(block, src, ptr_ptr, ptr_src);
                slice_ty = ptr_ptr_child_ty;
                array_ty = ptr_ptr_child_ty;
                elem_ty = ptr_ptr_child_ty.child_type(mod);
            },
        },
        else => return sema.fail(block, src, "slice of non-array type '{}'", .{ptr_ptr_child_ty.fmt(mod)}),
    }

    const ptr = if (slice_ty.is_slice(mod))
        try sema.analyze_slice_ptr(block, ptr_src, ptr_or_slice, slice_ty)
    else if (array_ty.zig_type_tag(mod) == .Array) ptr: {
        var manyptr_ty_key = mod.intern_pool.index_to_key(slice_ty.to_intern()).ptr_type;
        assert(manyptr_ty_key.child == array_ty.to_intern());
        assert(manyptr_ty_key.flags.size == .One);
        manyptr_ty_key.child = elem_ty.to_intern();
        manyptr_ty_key.flags.size = .Many;
        break :ptr try sema.coerce_compatible_ptrs(block, try sema.ptr_type(manyptr_ty_key), ptr_or_slice, ptr_src);
    } else ptr_or_slice;

    const start = try sema.coerce(block, Type.usize, uncasted_start, start_src);
    const new_ptr = try sema.analyze_ptr_arithmetic(block, src, ptr, start, .ptr_add, ptr_src, start_src);
    const new_ptr_ty = sema.type_of(new_ptr);

    // true if and only if the end index of the slice, implicitly or explicitly, equals
    // the length of the underlying object being sliced. we might learn the length of the
    // underlying object because it is an array (which has the length in the type), or
    // we might learn of the length because it is a comptime-known slice value.
    var end_is_len = uncasted_end_opt == .none;
    const end = e: {
        if (array_ty.zig_type_tag(mod) == .Array) {
            const len_val = try mod.int_value(Type.usize, array_ty.array_len(mod));

            if (!end_is_len) {
                const end = if (by_length) end: {
                    const len = try sema.coerce(block, Type.usize, uncasted_end_opt, end_src);
                    const uncasted_end = try sema.analyze_arithmetic(block, .add, start, len, src, start_src, end_src, false);
                    break :end try sema.coerce(block, Type.usize, uncasted_end, end_src);
                } else try sema.coerce(block, Type.usize, uncasted_end_opt, end_src);
                if (try sema.resolve_defined_value(block, end_src, end)) |end_val| {
                    const len_s_val = try mod.int_value(
                        Type.usize,
                        array_ty.array_len_including_sentinel(mod),
                    );
                    if (!(try sema.compare_all(end_val, .lte, len_s_val, Type.usize))) {
                        const sentinel_label: []const u8 = if (array_ty.sentinel(mod) != null)
                            " +1 (sentinel)"
                        else
                            "";

                        return sema.fail(
                            block,
                            end_src,
                            "end index {} out of bounds for array of length {}{s}",
                            .{
                                end_val.fmt_value(mod, sema),
                                len_val.fmt_value(mod, sema),
                                sentinel_label,
                            },
                        );
                    }

                    // end_is_len is only true if we are NOT using the sentinel
                    // length. For sentinel-length, we don't want the type to
                    // contain the sentinel.
                    if (end_val.eql(len_val, Type.usize, mod)) {
                        end_is_len = true;
                    }
                }
                break :e end;
            }

            break :e Air.interned_to_ref(len_val.to_intern());
        } else if (slice_ty.is_slice(mod)) {
            if (!end_is_len) {
                const end = if (by_length) end: {
                    const len = try sema.coerce(block, Type.usize, uncasted_end_opt, end_src);
                    const uncasted_end = try sema.analyze_arithmetic(block, .add, start, len, src, start_src, end_src, false);
                    break :end try sema.coerce(block, Type.usize, uncasted_end, end_src);
                } else try sema.coerce(block, Type.usize, uncasted_end_opt, end_src);
                if (try sema.resolve_defined_value(block, end_src, end)) |end_val| {
                    if (try sema.resolve_value(ptr_or_slice)) |slice_val| {
                        if (slice_val.is_undef(mod)) {
                            return sema.fail(block, src, "slice of undefined", .{});
                        }
                        const has_sentinel = slice_ty.sentinel(mod) != null;
                        const slice_len = try slice_val.slice_len(sema);
                        const len_plus_sent = slice_len + @int_from_bool(has_sentinel);
                        const slice_len_val_with_sentinel = try mod.int_value(Type.usize, len_plus_sent);
                        if (!(try sema.compare_all(end_val, .lte, slice_len_val_with_sentinel, Type.usize))) {
                            const sentinel_label: []const u8 = if (has_sentinel)
                                " +1 (sentinel)"
                            else
                                "";

                            return sema.fail(
                                block,
                                end_src,
                                "end index {} out of bounds for slice of length {d}{s}",
                                .{
                                    end_val.fmt_value(mod, sema),
                                    try slice_val.slice_len(sema),
                                    sentinel_label,
                                },
                            );
                        }

                        // If the slice has a sentinel, we consider end_is_len
                        // is only true if it equals the length WITHOUT the
                        // sentinel, so we don't add a sentinel type.
                        const slice_len_val = try mod.int_value(Type.usize, slice_len);
                        if (end_val.eql(slice_len_val, Type.usize, mod)) {
                            end_is_len = true;
                        }
                    }
                }
                break :e end;
            }
            break :e try sema.analyze_slice_len(block, src, ptr_or_slice);
        }
        if (!end_is_len) {
            if (by_length) {
                const len = try sema.coerce(block, Type.usize, uncasted_end_opt, end_src);
                const uncasted_end = try sema.analyze_arithmetic(block, .add, start, len, src, start_src, end_src, false);
                break :e try sema.coerce(block, Type.usize, uncasted_end, end_src);
            } else break :e try sema.coerce(block, Type.usize, uncasted_end_opt, end_src);
        }
        return sema.analyze_ptr_arithmetic(block, src, ptr, start, .ptr_add, ptr_src, start_src);
    };

    const sentinel = s: {
        if (sentinel_opt != .none) {
            const casted = try sema.coerce(block, elem_ty, sentinel_opt, sentinel_src);
            break :s try sema.resolve_const_defined_value(block, sentinel_src, casted, .{
                .needed_comptime_reason = "slice sentinel must be comptime-known",
            });
        }
        // If we are slicing to the end of something that is sentinel-terminated
        // then the resulting slice type is also sentinel-terminated.
        if (end_is_len) {
            if (ptr_sentinel) |sent| {
                break :s sent;
            }
        }
        break :s null;
    };
    const slice_sentinel = if (sentinel_opt != .none) sentinel else null;

    var checked_start_lte_end = by_length;
    var runtime_src: ?LazySrcLoc = null;

    // requirement: start <= end
    if (try sema.resolve_defined_value(block, end_src, end)) |end_val| {
        if (try sema.resolve_defined_value(block, start_src, start)) |start_val| {
            if (!by_length and !(try sema.compare_all(start_val, .lte, end_val, Type.usize))) {
                return sema.fail(
                    block,
                    start_src,
                    "start index {} is larger than end index {}",
                    .{
                        start_val.fmt_value(mod, sema),
                        end_val.fmt_value(mod, sema),
                    },
                );
            }
            checked_start_lte_end = true;
            if (try sema.resolve_value(new_ptr)) |ptr_val| sentinel_check: {
                const expected_sentinel = sentinel orelse break :sentinel_check;
                const start_int = start_val.get_unsigned_int(mod).?;
                const end_int = end_val.get_unsigned_int(mod).?;
                const sentinel_index = try sema.usize_cast(block, end_src, end_int - start_int);

                const many_ptr_ty = try mod.many_const_ptr_type(elem_ty);
                const many_ptr_val = try mod.get_coerced(ptr_val, many_ptr_ty);
                const elem_ptr = try many_ptr_val.ptr_elem(sentinel_index, sema);
                const res = try sema.pointer_deref_extra(block, src, elem_ptr);
                const actual_sentinel = switch (res) {
                    .runtime_load => break :sentinel_check,
                    .val => |v| v,
                    .needed_well_defined => |ty| return sema.fail(
                        block,
                        src,
                        "comptime dereference requires '{}' to have a well-defined layout",
                        .{ty.fmt(mod)},
                    ),
                    .out_of_bounds => |ty| return sema.fail(
                        block,
                        end_src,
                        "slice end index {d} exceeds bounds of containing decl of type '{}'",
                        .{ end_int, ty.fmt(mod) },
                    ),
                };

                if (!actual_sentinel.eql(expected_sentinel, elem_ty, mod)) {
                    const msg = msg: {
                        const msg = try sema.err_msg(block, src, "value in memory does not match slice sentinel", .{});
                        errdefer msg.destroy(sema.gpa);
                        try sema.err_note(block, src, msg, "expected '{}', found '{}'", .{
                            expected_sentinel.fmt_value(mod, sema),
                            actual_sentinel.fmt_value(mod, sema),
                        });

                        break :msg msg;
                    };
                    return sema.fail_with_owned_error_msg(block, msg);
                }
            } else {
                runtime_src = ptr_src;
            }
        } else {
            runtime_src = start_src;
        }
    } else {
        runtime_src = end_src;
    }

    if (!checked_start_lte_end and block.want_safety() and !block.is_comptime) {
        // requirement: start <= end
        assert(!block.is_comptime);
        try sema.require_runtime_block(block, src, runtime_src.?);
        const ok = try block.add_bin_op(.cmp_lte, start, end);
        if (!sema.mod.comp.formatted_panics) {
            try sema.add_safety_check(block, src, ok, .start_index_greater_than_end);
        } else {
            try sema.safety_check_formatted(block, src, ok, "panic_start_greater_than_end", &.{ start, end });
        }
    }
    const new_len = if (by_length)
        try sema.coerce(block, Type.usize, uncasted_end_opt, end_src)
    else
        try sema.analyze_arithmetic(block, .sub, end, start, src, end_src, start_src, false);
    const opt_new_len_val = try sema.resolve_defined_value(block, src, new_len);

    const new_ptr_ty_info = new_ptr_ty.ptr_info(mod);
    const new_allowzero = new_ptr_ty_info.flags.is_allowzero and sema.type_of(ptr).ptr_size(mod) != .C;

    if (opt_new_len_val) |new_len_val| {
        const new_len_int = try new_len_val.to_unsigned_int_advanced(sema);

        const return_ty = try sema.ptr_type(.{
            .child = (try mod.array_type(.{
                .len = new_len_int,
                .sentinel = if (sentinel) |s| s.to_intern() else .none,
                .child = elem_ty.to_intern(),
            })).to_intern(),
            .flags = .{
                .alignment = new_ptr_ty_info.flags.alignment,
                .is_const = new_ptr_ty_info.flags.is_const,
                .is_allowzero = new_allowzero,
                .is_volatile = new_ptr_ty_info.flags.is_volatile,
                .address_space = new_ptr_ty_info.flags.address_space,
            },
        });

        const opt_new_ptr_val = try sema.resolve_value(new_ptr);
        const new_ptr_val = opt_new_ptr_val orelse {
            const result = try block.add_bit_cast(return_ty, new_ptr);
            if (block.want_safety()) {
                // requirement: slicing C ptr is non-null
                if (ptr_ptr_child_ty.is_cptr(mod)) {
                    const is_non_null = try sema.analyze_is_null(block, ptr_src, ptr, true);
                    try sema.add_safety_check(block, src, is_non_null, .unwrap_null);
                }

                bounds_check: {
                    const actual_len = if (array_ty.zig_type_tag(mod) == .Array)
                        try mod.int_ref(Type.usize, array_ty.array_len_including_sentinel(mod))
                    else if (slice_ty.is_slice(mod)) l: {
                        const slice_len_inst = try block.add_ty_op(.slice_len, Type.usize, ptr_or_slice);
                        break :l if (slice_ty.sentinel(mod) == null)
                            slice_len_inst
                        else
                            try sema.analyze_arithmetic(block, .add, slice_len_inst, .one, src, end_src, end_src, true);
                    } else break :bounds_check;

                    const actual_end = if (slice_sentinel != null)
                        try sema.analyze_arithmetic(block, .add, end, .one, src, end_src, end_src, true)
                    else
                        end;

                    try sema.panic_index_out_of_bounds(block, src, actual_end, actual_len, .cmp_lte);
                }

                // requirement: result[new_len] == slice_sentinel
                try sema.panic_sentinel_mismatch(block, src, slice_sentinel, elem_ty, result, new_len);
            }
            return result;
        };

        if (!new_ptr_val.is_undef(mod)) {
            return Air.interned_to_ref((try mod.get_coerced(new_ptr_val, return_ty)).to_intern());
        }

        // Special case: @as([]i32, undefined)[x..x]
        if (new_len_int == 0) {
            return mod.undef_ref(return_ty);
        }

        return sema.fail(block, src, "non-zero length slice of undefined pointer", .{});
    }

    const return_ty = try sema.ptr_type(.{
        .child = elem_ty.to_intern(),
        .sentinel = if (sentinel) |s| s.to_intern() else .none,
        .flags = .{
            .size = .Slice,
            .alignment = new_ptr_ty_info.flags.alignment,
            .is_const = new_ptr_ty_info.flags.is_const,
            .is_volatile = new_ptr_ty_info.flags.is_volatile,
            .is_allowzero = new_allowzero,
            .address_space = new_ptr_ty_info.flags.address_space,
        },
    });

    try sema.require_runtime_block(block, src, runtime_src.?);
    if (block.want_safety()) {
        // requirement: slicing C ptr is non-null
        if (ptr_ptr_child_ty.is_cptr(mod)) {
            const is_non_null = try sema.analyze_is_null(block, ptr_src, ptr, true);
            try sema.add_safety_check(block, src, is_non_null, .unwrap_null);
        }

        // requirement: end <= len
        const opt_len_inst = if (array_ty.zig_type_tag(mod) == .Array)
            try mod.int_ref(Type.usize, array_ty.array_len_including_sentinel(mod))
        else if (slice_ty.is_slice(mod)) blk: {
            if (try sema.resolve_defined_value(block, src, ptr_or_slice)) |slice_val| {
                // we don't need to add one for sentinels because the
                // underlying value data includes the sentinel
                break :blk try mod.int_ref(Type.usize, try slice_val.slice_len(sema));
            }

            const slice_len_inst = try block.add_ty_op(.slice_len, Type.usize, ptr_or_slice);
            if (slice_ty.sentinel(mod) == null) break :blk slice_len_inst;

            // we have to add one because slice lengths don't include the sentinel
            break :blk try sema.analyze_arithmetic(block, .add, slice_len_inst, .one, src, end_src, end_src, true);
        } else null;
        if (opt_len_inst) |len_inst| {
            const actual_end = if (slice_sentinel != null)
                try sema.analyze_arithmetic(block, .add, end, .one, src, end_src, end_src, true)
            else
                end;
            try sema.panic_index_out_of_bounds(block, src, actual_end, len_inst, .cmp_lte);
        }

        // requirement: start <= end
        try sema.panic_index_out_of_bounds(block, src, start, end, .cmp_lte);
    }
    const result = try block.add_inst(.{
        .tag = .slice,
        .data = .{ .ty_pl = .{
            .ty = Air.interned_to_ref(return_ty.to_intern()),
            .payload = try sema.add_extra(Air.Bin{
                .lhs = new_ptr,
                .rhs = new_len,
            }),
        } },
    });
    if (block.want_safety()) {
        // requirement: result[new_len] == slice_sentinel
        try sema.panic_sentinel_mismatch(block, src, slice_sentinel, elem_ty, result, new_len);
    }
    return result;
}

/// Asserts that lhs and rhs types are both numeric.
fn cmp_numeric(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    uncasted_lhs: Air.Inst.Ref,
    uncasted_rhs: Air.Inst.Ref,
    op: std.math.CompareOperator,
    lhs_src: LazySrcLoc,
    rhs_src: LazySrcLoc,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const lhs_ty = sema.type_of(uncasted_lhs);
    const rhs_ty = sema.type_of(uncasted_rhs);

    assert(lhs_ty.is_numeric(mod));
    assert(rhs_ty.is_numeric(mod));

    const lhs_ty_tag = lhs_ty.zig_type_tag(mod);
    const rhs_ty_tag = rhs_ty.zig_type_tag(mod);
    const target = mod.get_target();

    // One exception to heterogeneous comparison: comptime_float needs to
    // coerce to fixed-width float.

    const lhs = if (lhs_ty_tag == .ComptimeFloat and rhs_ty_tag == .Float)
        try sema.coerce(block, rhs_ty, uncasted_lhs, lhs_src)
    else
        uncasted_lhs;

    const rhs = if (lhs_ty_tag == .Float and rhs_ty_tag == .ComptimeFloat)
        try sema.coerce(block, lhs_ty, uncasted_rhs, rhs_src)
    else
        uncasted_rhs;

    const runtime_src: LazySrcLoc = src: {
        if (try sema.resolve_value(lhs)) |lhs_val| {
            if (try sema.resolve_value(rhs)) |rhs_val| {
                // Compare ints: const vs. undefined (or vice versa)
                if (!lhs_val.is_undef(mod) and (lhs_ty.is_int(mod) or lhs_ty_tag == .ComptimeInt) and rhs_ty.is_int(mod) and rhs_val.is_undef(mod)) {
                    if (try sema.compare_ints_only_possible_result(try sema.resolve_lazy_value(lhs_val), op, rhs_ty)) |res| {
                        return if (res) .bool_true else .bool_false;
                    }
                } else if (!rhs_val.is_undef(mod) and (rhs_ty.is_int(mod) or rhs_ty_tag == .ComptimeInt) and lhs_ty.is_int(mod) and lhs_val.is_undef(mod)) {
                    if (try sema.compare_ints_only_possible_result(try sema.resolve_lazy_value(rhs_val), op.reverse(), lhs_ty)) |res| {
                        return if (res) .bool_true else .bool_false;
                    }
                }

                if (lhs_val.is_undef(mod) or rhs_val.is_undef(mod)) {
                    return mod.undef_ref(Type.bool);
                }
                if (lhs_val.is_nan(mod) or rhs_val.is_nan(mod)) {
                    return if (op == std.math.CompareOperator.neq) .bool_true else .bool_false;
                }
                return if (try Value.compare_hetero_advanced(lhs_val, op, rhs_val, mod, sema))
                    .bool_true
                else
                    .bool_false;
            } else {
                if (!lhs_val.is_undef(mod) and (lhs_ty.is_int(mod) or lhs_ty_tag == .ComptimeInt) and rhs_ty.is_int(mod)) {
                    // Compare ints: const vs. var
                    if (try sema.compare_ints_only_possible_result(try sema.resolve_lazy_value(lhs_val), op, rhs_ty)) |res| {
                        return if (res) .bool_true else .bool_false;
                    }
                }
                break :src rhs_src;
            }
        } else {
            if (try sema.resolve_value_resolve_lazy(rhs)) |rhs_val| {
                if (!rhs_val.is_undef(mod) and (rhs_ty.is_int(mod) or rhs_ty_tag == .ComptimeInt) and lhs_ty.is_int(mod)) {
                    // Compare ints: var vs. const
                    if (try sema.compare_ints_only_possible_result(try sema.resolve_lazy_value(rhs_val), op.reverse(), lhs_ty)) |res| {
                        return if (res) .bool_true else .bool_false;
                    }
                }
            }
            break :src lhs_src;
        }
    };

    // TODO handle comparisons against lazy zero values
    // Some values can be compared against zero without being runtime-known or without forcing
    // a full resolution of their value, for example `@size_of(@Frame(function))` is known to
    // always be nonzero, and we benefit from not forcing the full evaluation and stack frame layout
    // of this function if we don't need to.
    try sema.require_runtime_block(block, src, runtime_src);

    // For floats, emit a float comparison instruction.
    const lhs_is_float = switch (lhs_ty_tag) {
        .Float, .ComptimeFloat => true,
        else => false,
    };
    const rhs_is_float = switch (rhs_ty_tag) {
        .Float, .ComptimeFloat => true,
        else => false,
    };

    if (lhs_is_float and rhs_is_float) {
        // Smaller fixed-width floats coerce to larger fixed-width floats.
        // comptime_float coerces to fixed-width float.
        const dest_ty = x: {
            if (lhs_ty_tag == .ComptimeFloat) {
                break :x rhs_ty;
            } else if (rhs_ty_tag == .ComptimeFloat) {
                break :x lhs_ty;
            }
            if (lhs_ty.float_bits(target) >= rhs_ty.float_bits(target)) {
                break :x lhs_ty;
            } else {
                break :x rhs_ty;
            }
        };
        const casted_lhs = try sema.coerce(block, dest_ty, lhs, lhs_src);
        const casted_rhs = try sema.coerce(block, dest_ty, rhs, rhs_src);
        return block.add_bin_op(Air.Inst.Tag.from_cmp_op(op, block.float_mode == .optimized), casted_lhs, casted_rhs);
    }
    // For mixed unsigned integer sizes, implicit cast both operands to the larger integer.
    // For mixed signed and unsigned integers, implicit cast both operands to a signed
    // integer with + 1 bit.
    // For mixed floats and integers, extract the integer part from the float, cast that to
    // a signed integer with mantissa bits + 1, and if there was any non-integral part of the float,
    // add/subtract 1.
    const lhs_is_signed = if (try sema.resolve_defined_value(block, lhs_src, lhs)) |lhs_val|
        !(try lhs_val.compare_all_with_zero_advanced(.gte, sema))
    else
        (lhs_ty.is_runtime_float() or lhs_ty.is_signed_int(mod));
    const rhs_is_signed = if (try sema.resolve_defined_value(block, rhs_src, rhs)) |rhs_val|
        !(try rhs_val.compare_all_with_zero_advanced(.gte, sema))
    else
        (rhs_ty.is_runtime_float() or rhs_ty.is_signed_int(mod));
    const dest_int_is_signed = lhs_is_signed or rhs_is_signed;

    var dest_float_type: ?Type = null;

    var lhs_bits: usize = undefined;
    if (try sema.resolve_value_resolve_lazy(lhs)) |lhs_val| {
        if (lhs_val.is_undef(mod))
            return mod.undef_ref(Type.bool);
        if (lhs_val.is_nan(mod)) switch (op) {
            .neq => return .bool_true,
            else => return .bool_false,
        };
        if (lhs_val.is_inf(mod)) switch (op) {
            .neq => return .bool_true,
            .eq => return .bool_false,
            .gt, .gte => return if (lhs_val.is_negative_inf(mod)) .bool_false else .bool_true,
            .lt, .lte => return if (lhs_val.is_negative_inf(mod)) .bool_true else .bool_false,
        };
        if (!rhs_is_signed) {
            switch (lhs_val.order_against_zero(mod)) {
                .gt => {},
                .eq => switch (op) { // LHS = 0, RHS is unsigned
                    .lte => return .bool_true,
                    .gt => return .bool_false,
                    else => {},
                },
                .lt => switch (op) { // LHS < 0, RHS is unsigned
                    .neq, .lt, .lte => return .bool_true,
                    .eq, .gt, .gte => return .bool_false,
                },
            }
        }
        if (lhs_is_float) {
            if (lhs_val.float_has_fraction(mod)) {
                switch (op) {
                    .eq => return .bool_false,
                    .neq => return .bool_true,
                    else => {},
                }
            }

            var bigint = try float128_int_part_to_big_int(sema.gpa, lhs_val.to_float(f128, mod));
            defer bigint.deinit();
            if (lhs_val.float_has_fraction(mod)) {
                if (lhs_is_signed) {
                    try bigint.add_scalar(&bigint, -1);
                } else {
                    try bigint.add_scalar(&bigint, 1);
                }
            }
            lhs_bits = bigint.to_const().bit_count_twos_comp();
        } else {
            lhs_bits = lhs_val.int_bit_count_twos_comp(mod);
        }
        lhs_bits += @int_from_bool(!lhs_is_signed and dest_int_is_signed);
    } else if (lhs_is_float) {
        dest_float_type = lhs_ty;
    } else {
        const int_info = lhs_ty.int_info(mod);
        lhs_bits = int_info.bits + @int_from_bool(int_info.signedness == .unsigned and dest_int_is_signed);
    }

    var rhs_bits: usize = undefined;
    if (try sema.resolve_value_resolve_lazy(rhs)) |rhs_val| {
        if (rhs_val.is_undef(mod))
            return mod.undef_ref(Type.bool);
        if (rhs_val.is_nan(mod)) switch (op) {
            .neq => return .bool_true,
            else => return .bool_false,
        };
        if (rhs_val.is_inf(mod)) switch (op) {
            .neq => return .bool_true,
            .eq => return .bool_false,
            .gt, .gte => return if (rhs_val.is_negative_inf(mod)) .bool_true else .bool_false,
            .lt, .lte => return if (rhs_val.is_negative_inf(mod)) .bool_false else .bool_true,
        };
        if (!lhs_is_signed) {
            switch (rhs_val.order_against_zero(mod)) {
                .gt => {},
                .eq => switch (op) { // RHS = 0, LHS is unsigned
                    .gte => return .bool_true,
                    .lt => return .bool_false,
                    else => {},
                },
                .lt => switch (op) { // RHS < 0, LHS is unsigned
                    .neq, .gt, .gte => return .bool_true,
                    .eq, .lt, .lte => return .bool_false,
                },
            }
        }
        if (rhs_is_float) {
            if (rhs_val.float_has_fraction(mod)) {
                switch (op) {
                    .eq => return .bool_false,
                    .neq => return .bool_true,
                    else => {},
                }
            }

            var bigint = try float128_int_part_to_big_int(sema.gpa, rhs_val.to_float(f128, mod));
            defer bigint.deinit();
            if (rhs_val.float_has_fraction(mod)) {
                if (rhs_is_signed) {
                    try bigint.add_scalar(&bigint, -1);
                } else {
                    try bigint.add_scalar(&bigint, 1);
                }
            }
            rhs_bits = bigint.to_const().bit_count_twos_comp();
        } else {
            rhs_bits = rhs_val.int_bit_count_twos_comp(mod);
        }
        rhs_bits += @int_from_bool(!rhs_is_signed and dest_int_is_signed);
    } else if (rhs_is_float) {
        dest_float_type = rhs_ty;
    } else {
        const int_info = rhs_ty.int_info(mod);
        rhs_bits = int_info.bits + @int_from_bool(int_info.signedness == .unsigned and dest_int_is_signed);
    }

    const dest_ty = if (dest_float_type) |ft| ft else blk: {
        const max_bits = @max(lhs_bits, rhs_bits);
        const casted_bits = std.math.cast(u16, max_bits) orelse return sema.fail(block, src, "{d} exceeds maximum integer bit count", .{max_bits});
        const signedness: std.builtin.Signedness = if (dest_int_is_signed) .signed else .unsigned;
        break :blk try mod.int_type(signedness, casted_bits);
    };
    const casted_lhs = try sema.coerce(block, dest_ty, lhs, lhs_src);
    const casted_rhs = try sema.coerce(block, dest_ty, rhs, rhs_src);

    return block.add_bin_op(Air.Inst.Tag.from_cmp_op(op, block.float_mode == .optimized), casted_lhs, casted_rhs);
}

/// Asserts that LHS value is an int or comptime int and not undefined, and
/// that RHS type is an int. Given a const LHS and an unknown RHS, attempt to
/// determine whether `op` has a guaranteed result.
/// If it cannot be determined, returns null.
/// Otherwise returns a bool for the guaranteed comparison operation.
fn compare_ints_only_possible_result(
    sema: *Sema,
    lhs_val: Value,
    op: std.math.CompareOperator,
    rhs_ty: Type,
) Allocator.Error!?bool {
    const mod = sema.mod;
    const rhs_info = rhs_ty.int_info(mod);
    const vs_zero = lhs_val.order_against_zero_advanced(mod, sema) catch unreachable;
    const is_zero = vs_zero == .eq;
    const is_negative = vs_zero == .lt;
    const is_positive = vs_zero == .gt;

    // Anything vs. zero-sized type has guaranteed outcome.
    if (rhs_info.bits == 0) return switch (op) {
        .eq, .lte, .gte => is_zero,
        .neq, .lt, .gt => !is_zero,
    };

    // Special case for i1, which can only be 0 or -1.
    // Zero and positive ints have guaranteed outcome.
    if (rhs_info.bits == 1 and rhs_info.signedness == .signed) {
        if (is_positive) return switch (op) {
            .gt, .gte, .neq => true,
            .lt, .lte, .eq => false,
        };
        if (is_zero) return switch (op) {
            .gte => true,
            .lt => false,
            .gt, .lte, .eq, .neq => null,
        };
    }

    // Negative vs. unsigned has guaranteed outcome.
    if (rhs_info.signedness == .unsigned and is_negative) return switch (op) {
        .eq, .gt, .gte => false,
        .neq, .lt, .lte => true,
    };

    const sign_adj = @int_from_bool(!is_negative and rhs_info.signedness == .signed);
    const req_bits = lhs_val.int_bit_count_twos_comp(mod) + sign_adj;

    // No sized type can have more than 65535 bits.
    // The RHS type operand is either a runtime value or sized (but undefined) constant.
    if (req_bits > 65535) return switch (op) {
        .lt, .lte => is_negative,
        .gt, .gte => is_positive,
        .eq => false,
        .neq => true,
    };
    const fits = req_bits <= rhs_info.bits;

    // Oversized int has guaranteed outcome.
    switch (op) {
        .eq => return if (!fits) false else null,
        .neq => return if (!fits) true else null,
        .lt, .lte => if (!fits) return is_negative,
        .gt, .gte => if (!fits) return !is_negative,
    }

    // For any other comparison, we need to know if the LHS value is
    // equal to the maximum or minimum possible value of the RHS type.
    const is_min, const is_max = edge: {
        if (is_zero and rhs_info.signedness == .unsigned) break :edge .{ true, false };

        if (req_bits != rhs_info.bits) break :edge .{ false, false };

        const ty = try mod.int_type(
            if (is_negative) .signed else .unsigned,
            @int_cast(req_bits),
        );
        const pop_count = lhs_val.pop_count(ty, mod);

        if (is_negative) {
            break :edge .{ pop_count == 1, false };
        } else {
            break :edge .{ false, pop_count == req_bits - sign_adj };
        }
    };

    assert(fits);
    return switch (op) {
        .lt => if (is_max) false else null,
        .lte => if (is_min) true else null,
        .gt => if (is_min) false else null,
        .gte => if (is_max) true else null,
        .eq, .neq => unreachable,
    };
}

/// Asserts that lhs and rhs types are both vectors.
fn cmp_vector(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    lhs: Air.Inst.Ref,
    rhs: Air.Inst.Ref,
    op: std.math.CompareOperator,
    lhs_src: LazySrcLoc,
    rhs_src: LazySrcLoc,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;
    const lhs_ty = sema.type_of(lhs);
    const rhs_ty = sema.type_of(rhs);
    assert(lhs_ty.zig_type_tag(mod) == .Vector);
    assert(rhs_ty.zig_type_tag(mod) == .Vector);
    try sema.check_vectorizable_binary_operands(block, src, lhs_ty, rhs_ty, lhs_src, rhs_src);

    const resolved_ty = try sema.resolve_peer_types(block, src, &.{ lhs, rhs }, .{ .override = &.{ lhs_src, rhs_src } });
    const casted_lhs = try sema.coerce(block, resolved_ty, lhs, lhs_src);
    const casted_rhs = try sema.coerce(block, resolved_ty, rhs, rhs_src);

    const result_ty = try mod.vector_type(.{
        .len = lhs_ty.vector_len(mod),
        .child = .bool_type,
    });

    const runtime_src: LazySrcLoc = src: {
        if (try sema.resolve_value(casted_lhs)) |lhs_val| {
            if (try sema.resolve_value(casted_rhs)) |rhs_val| {
                if (lhs_val.is_undef(mod) or rhs_val.is_undef(mod)) {
                    return mod.undef_ref(result_ty);
                }
                const cmp_val = try sema.compare_vector(lhs_val, op, rhs_val, resolved_ty);
                return Air.interned_to_ref(cmp_val.to_intern());
            } else {
                break :src rhs_src;
            }
        } else {
            break :src lhs_src;
        }
    };

    try sema.require_runtime_block(block, src, runtime_src);
    return block.add_cmp_vector(casted_lhs, casted_rhs, op);
}

fn wrap_optional(
    sema: *Sema,
    block: *Block,
    dest_ty: Type,
    inst: Air.Inst.Ref,
    inst_src: LazySrcLoc,
) !Air.Inst.Ref {
    if (try sema.resolve_value(inst)) |val| {
        return Air.interned_to_ref((try sema.mod.intern(.{ .opt = .{
            .ty = dest_ty.to_intern(),
            .val = val.to_intern(),
        } })));
    }

    try sema.require_runtime_block(block, inst_src, null);
    return block.add_ty_op(.wrap_optional, dest_ty, inst);
}

fn wrap_error_union_payload(
    sema: *Sema,
    block: *Block,
    dest_ty: Type,
    inst: Air.Inst.Ref,
    inst_src: LazySrcLoc,
) !Air.Inst.Ref {
    const mod = sema.mod;
    const dest_payload_ty = dest_ty.error_union_payload(mod);
    const coerced = try sema.coerce_extra(block, dest_payload_ty, inst, inst_src, .{ .report_err = false });
    if (try sema.resolve_value(coerced)) |val| {
        return Air.interned_to_ref((try mod.intern(.{ .error_union = .{
            .ty = dest_ty.to_intern(),
            .val = .{ .payload = val.to_intern() },
        } })));
    }
    try sema.require_runtime_block(block, inst_src, null);
    try sema.queue_full_type_resolution(dest_payload_ty);
    return block.add_ty_op(.wrap_errunion_payload, dest_ty, coerced);
}

fn wrap_error_union_set(
    sema: *Sema,
    block: *Block,
    dest_ty: Type,
    inst: Air.Inst.Ref,
    inst_src: LazySrcLoc,
) !Air.Inst.Ref {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const inst_ty = sema.type_of(inst);
    const dest_err_set_ty = dest_ty.error_union_set(mod);
    if (try sema.resolve_value(inst)) |val| {
        const expected_name = mod.intern_pool.index_to_key(val.to_intern()).err.name;
        switch (dest_err_set_ty.to_intern()) {
            .anyerror_type => {},
            .adhoc_inferred_error_set_type => ok: {
                const ies = sema.fn_ret_ty_ies.?;
                switch (ies.resolved) {
                    .anyerror_type => break :ok,
                    .none => if (.ok == try sema.coerce_in_memory_allowed_error_sets(block, dest_err_set_ty, inst_ty, inst_src, inst_src)) {
                        break :ok;
                    },
                    else => |i| if (ip.index_to_key(i).error_set_type.name_index(ip, expected_name) != null) {
                        break :ok;
                    },
                }
                return sema.fail_with_error_set_code_missing(block, inst_src, dest_err_set_ty, inst_ty);
            },
            else => switch (ip.index_to_key(dest_err_set_ty.to_intern())) {
                .error_set_type => |error_set_type| ok: {
                    if (error_set_type.name_index(ip, expected_name) != null) break :ok;
                    return sema.fail_with_error_set_code_missing(block, inst_src, dest_err_set_ty, inst_ty);
                },
                .inferred_error_set_type => |func_index| ok: {
                    // We carefully do this in an order that avoids unnecessarily
                    // resolving the destination error set type.
                    switch (ip.func_ies_resolved(func_index).*) {
                        .anyerror_type => break :ok,
                        .none => if (.ok == try sema.coerce_in_memory_allowed_error_sets(block, dest_err_set_ty, inst_ty, inst_src, inst_src)) {
                            break :ok;
                        },
                        else => |i| if (ip.index_to_key(i).error_set_type.name_index(ip, expected_name) != null) {
                            break :ok;
                        },
                    }

                    return sema.fail_with_error_set_code_missing(block, inst_src, dest_err_set_ty, inst_ty);
                },
                else => unreachable,
            },
        }
        return Air.interned_to_ref((try mod.intern(.{ .error_union = .{
            .ty = dest_ty.to_intern(),
            .val = .{ .err_name = expected_name },
        } })));
    }

    try sema.require_runtime_block(block, inst_src, null);
    const coerced = try sema.coerce(block, dest_err_set_ty, inst, inst_src);
    return block.add_ty_op(.wrap_errunion_err, dest_ty, coerced);
}

fn union_to_tag(
    sema: *Sema,
    block: *Block,
    enum_ty: Type,
    un: Air.Inst.Ref,
    un_src: LazySrcLoc,
) !Air.Inst.Ref {
    const mod = sema.mod;
    if ((try sema.type_has_one_possible_value(enum_ty))) |opv| {
        return Air.interned_to_ref(opv.to_intern());
    }
    if (try sema.resolve_value(un)) |un_val| {
        const tag_val = un_val.union_tag(mod).?;
        if (tag_val.is_undef(mod))
            return try mod.undef_ref(enum_ty);
        return Air.interned_to_ref(tag_val.to_intern());
    }
    try sema.require_runtime_block(block, un_src, null);
    return block.add_ty_op(.get_union_tag, enum_ty, un);
}

const PeerResolveStrategy = enum {
    /// The type is not known.
    /// If refined no further, this is equivalent to `exact`.
    unknown,
    /// The type may be an error set or error union.
    /// If refined no further, it is an error set.
    error_set,
    /// The type must be some error union.
    error_union,
    /// The type may be @TypeOf(null), an optional or a C pointer.
    /// If refined no further, it is @TypeOf(null).
    nullable,
    /// The type must be some optional or a C pointer.
    /// If refined no further, it is an optional.
    optional,
    /// The type must be either an array or a vector.
    /// If refined no further, it is an array.
    array,
    /// The type must be a vector.
    vector,
    /// The type must be a C pointer.
    c_ptr,
    /// The type must be a pointer (C or not).
    /// If refined no further, it is a non-C pointer.
    ptr,
    /// The type must be a function or a pointer to a function.
    /// If refined no further, it is a function.
    func,
    /// The type must be an enum literal, or some specific enum or union. Which one is decided
    /// afterwards based on the types in question.
    enum_or_union,
    /// The type must be some integer or float type.
    /// If refined no further, it is `comptime_int`.
    comptime_int,
    /// The type must be some float type.
    /// If refined no further, it is `comptime_float`.
    comptime_float,
    /// The type must be some float or fixed-width integer type.
    /// If refined no further, it is some fixed-width integer type.
    fixed_int,
    /// The type must be some fixed-width float type.
    fixed_float,
    /// The type must be a struct literal or tuple type.
    coercible_struct,
    /// The peers must all be of the same type.
    exact,

    /// Given two strategies, find a strategy that satisfies both, if one exists. If no such
    /// strategy exists, any strategy may be returned; an error will be emitted when the caller
    /// attempts to use the strategy to resolve the type.
    /// Strategy `a` comes from the peer in `reason_peer`, while strategy `b` comes from the peer at
    /// index `b_peer_idx`. `reason_peer` is updated to reflect the reason for the new strategy.
    fn merge(a: PeerResolveStrategy, b: PeerResolveStrategy, reason_peer: *usize, b_peer_idx: usize) PeerResolveStrategy {
        // Our merging should be order-independent. Thus, even though the union order is arbitrary,
        // by sorting the tags and switching first on the smaller, we have half as many cases to
        // worry about (since we avoid the duplicates).
        const s0_is_a = @int_from_enum(a) <= @int_from_enum(b);
        const s0 = if (s0_is_a) a else b;
        const s1 = if (s0_is_a) b else a;

        const ReasonMethod = enum {
            all_s0,
            all_s1,
            either,
        };

        const reason_method: ReasonMethod, const strat: PeerResolveStrategy = switch (s0) {
            .unknown => .{ .all_s1, s1 },
            .error_set => switch (s1) {
                .error_set => .{ .either, .error_set },
                else => .{ .all_s0, .error_union },
            },
            .error_union => switch (s1) {
                .error_union => .{ .either, .error_union },
                else => .{ .all_s0, .error_union },
            },
            .nullable => switch (s1) {
                .nullable => .{ .either, .nullable },
                .c_ptr => .{ .all_s1, .c_ptr },
                else => .{ .all_s0, .optional },
            },
            .optional => switch (s1) {
                .optional => .{ .either, .optional },
                .c_ptr => .{ .all_s1, .c_ptr },
                else => .{ .all_s0, .optional },
            },
            .array => switch (s1) {
                .array => .{ .either, .array },
                .vector => .{ .all_s1, .vector },
                else => .{ .all_s0, .array },
            },
            .vector => switch (s1) {
                .vector => .{ .either, .vector },
                else => .{ .all_s0, .vector },
            },
            .c_ptr => switch (s1) {
                .c_ptr => .{ .either, .c_ptr },
                else => .{ .all_s0, .c_ptr },
            },
            .ptr => switch (s1) {
                .ptr => .{ .either, .ptr },
                else => .{ .all_s0, .ptr },
            },
            .func => switch (s1) {
                .func => .{ .either, .func },
                else => .{ .all_s1, s1 }, // doesn't override anything later
            },
            .enum_or_union => switch (s1) {
                .enum_or_union => .{ .either, .enum_or_union },
                else => .{ .all_s0, .enum_or_union },
            },
            .comptime_int => switch (s1) {
                .comptime_int => .{ .either, .comptime_int },
                else => .{ .all_s1, s1 }, // doesn't override anything later
            },
            .comptime_float => switch (s1) {
                .comptime_float => .{ .either, .comptime_float },
                else => .{ .all_s1, s1 }, // doesn't override anything later
            },
            .fixed_int => switch (s1) {
                .fixed_int => .{ .either, .fixed_int },
                else => .{ .all_s1, s1 }, // doesn't override anything later
            },
            .fixed_float => switch (s1) {
                .fixed_float => .{ .either, .fixed_float },
                else => .{ .all_s1, s1 }, // doesn't override anything later
            },
            .coercible_struct => switch (s1) {
                .exact => .{ .all_s1, .exact },
                else => .{ .all_s0, .coercible_struct },
            },
            .exact => .{ .all_s0, .exact },
        };

        switch (reason_method) {
            .all_s0 => {
                if (!s0_is_a) {
                    reason_peer.* = b_peer_idx;
                }
            },
            .all_s1 => {
                if (s0_is_a) {
                    reason_peer.* = b_peer_idx;
                }
            },
            .either => {
                // Prefer the earliest peer
                reason_peer.* = @min(reason_peer.*, b_peer_idx);
            },
        }

        return strat;
    }

    fn select(ty: Type, mod: *Module) PeerResolveStrategy {
        return switch (ty.zig_type_tag(mod)) {
            .Type, .Void, .Bool, .Opaque, .Frame, .AnyFrame => .exact,
            .NoReturn, .Undefined => .unknown,
            .Null => .nullable,
            .ComptimeInt => .comptime_int,
            .Int => .fixed_int,
            .ComptimeFloat => .comptime_float,
            .Float => .fixed_float,
            .Pointer => if (ty.ptr_info(mod).flags.size == .C) .c_ptr else .ptr,
            .Array => .array,
            .Vector => .vector,
            .Optional => .optional,
            .ErrorSet => .error_set,
            .ErrorUnion => .error_union,
            .EnumLiteral, .Enum, .Union => .enum_or_union,
            .Struct => if (ty.is_tuple_or_anon_struct(mod)) .coercible_struct else .exact,
            .Fn => .func,
        };
    }
};

const PeerResolveResult = union(enum) {
    /// The peer type resolution was successful, and resulted in the given type.
    success: Type,
    /// There was some generic conflict between two peers.
    conflict: struct {
        peer_idx_a: usize,
        peer_idx_b: usize,
    },
    /// There was an error when resolving the type of a struct or tuple field.
    field_error: struct {
        /// The name of the field which caused the failure.
        field_name: InternPool.NullTerminatedString,
        /// The type of this field in each peer.
        field_types: []Type,
        /// The error from resolving the field type. Guaranteed not to be `success`.
        sub_result: *PeerResolveResult,
    },

    fn report(
        result: PeerResolveResult,
        sema: *Sema,
        block: *Block,
        src: LazySrcLoc,
        instructions: []const Air.Inst.Ref,
        candidate_srcs: Module.PeerTypeCandidateSrc,
    ) !*Module.ErrorMsg {
        const mod = sema.mod;
        const decl_ptr = mod.decl_ptr(block.src_decl);

        var opt_msg: ?*Module.ErrorMsg = null;
        errdefer if (opt_msg) |msg| msg.destroy(sema.gpa);

        // If we mention fields we'll want to include field types, so put peer types in a buffer
        var peer_tys = try sema.arena.alloc(Type, instructions.len);
        for (peer_tys, instructions) |*ty, inst| {
            ty.* = sema.type_of(inst);
        }

        var cur = result;
        while (true) {
            var conflict_idx: [2]usize = undefined;

            switch (cur) {
                .success => unreachable,
                .conflict => |conflict| {
                    // Fall through to two-peer conflict handling below
                    conflict_idx = .{
                        conflict.peer_idx_a,
                        conflict.peer_idx_b,
                    };
                },
                .field_error => |field_error| {
                    const fmt = "struct field '{}' has conflicting types";
                    const args = .{field_error.field_name.fmt(&mod.intern_pool)};
                    if (opt_msg) |msg| {
                        try sema.err_note(block, src, msg, fmt, args);
                    } else {
                        opt_msg = try sema.err_msg(block, src, fmt, args);
                    }

                    // Continue on to child error
                    cur = field_error.sub_result.*;
                    peer_tys = field_error.field_types;
                    continue;
                },
            }

            // This is the path for reporting a generic conflict between two peers.

            if (conflict_idx[1] < conflict_idx[0]) {
                // b comes first in source, so it's better if it comes first in the error
                std.mem.swap(usize, &conflict_idx[0], &conflict_idx[1]);
            }

            const conflict_tys: [2]Type = .{
                peer_tys[conflict_idx[0]],
                peer_tys[conflict_idx[1]],
            };
            const conflict_srcs: [2]?LazySrcLoc = .{
                candidate_srcs.resolve(mod, decl_ptr, conflict_idx[0]),
                candidate_srcs.resolve(mod, decl_ptr, conflict_idx[1]),
            };

            const fmt = "incompatible types: '{}' and '{}'";
            const args = .{
                conflict_tys[0].fmt(mod),
                conflict_tys[1].fmt(mod),
            };
            const msg = if (opt_msg) |msg| msg: {
                try sema.err_note(block, src, msg, fmt, args);
                break :msg msg;
            } else msg: {
                const msg = try sema.err_msg(block, src, fmt, args);
                opt_msg = msg;
                break :msg msg;
            };

            if (conflict_srcs[0]) |src_loc| try sema.err_note(block, src_loc, msg, "type '{}' here", .{conflict_tys[0].fmt(mod)});
            if (conflict_srcs[1]) |src_loc| try sema.err_note(block, src_loc, msg, "type '{}' here", .{conflict_tys[1].fmt(mod)});

            // No child error
            break;
        }

        return opt_msg.?;
    }
};

fn resolve_peer_types(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    instructions: []const Air.Inst.Ref,
    candidate_srcs: Module.PeerTypeCandidateSrc,
) !Type {
    switch (instructions.len) {
        0 => return Type.noreturn,
        1 => return sema.type_of(instructions[0]),
        else => {},
    }

    const peer_tys = try sema.arena.alloc(?Type, instructions.len);
    const peer_vals = try sema.arena.alloc(?Value, instructions.len);

    for (instructions, peer_tys, peer_vals) |inst, *ty, *val| {
        ty.* = sema.type_of(inst);
        val.* = try sema.resolve_value(inst);
    }

    switch (try sema.resolve_peer_types_inner(block, src, peer_tys, peer_vals)) {
        .success => |ty| return ty,
        else => |result| {
            const msg = try result.report(sema, block, src, instructions, candidate_srcs);
            return sema.fail_with_owned_error_msg(block, msg);
        },
    }
}

fn resolve_peer_types_inner(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    peer_tys: []?Type,
    peer_vals: []?Value,
) !PeerResolveResult {
    const mod = sema.mod;
    const ip = &mod.intern_pool;

    var strat_reason: usize = 0;
    var s: PeerResolveStrategy = .unknown;
    for (peer_tys, 0..) |opt_ty, i| {
        const ty = opt_ty orelse continue;
        s = s.merge(PeerResolveStrategy.select(ty, mod), &strat_reason, i);
    }

    if (s == .unknown) {
        // The whole thing was noreturn or undefined - try to do an exact match
        s = .exact;
    } else {
        // There was something other than noreturn and undefined, so we can ignore those peers
        for (peer_tys) |*ty_ptr| {
            const ty = ty_ptr.* orelse continue;
            switch (ty.zig_type_tag(mod)) {
                .NoReturn, .Undefined => ty_ptr.* = null,
                else => {},
            }
        }
    }

    const target = mod.get_target();

    switch (s) {
        .unknown => unreachable,

        .error_set => {
            var final_set: ?Type = null;
            for (peer_tys, 0..) |opt_ty, i| {
                const ty = opt_ty orelse continue;
                if (ty.zig_type_tag(mod) != .ErrorSet) return .{ .conflict = .{
                    .peer_idx_a = strat_reason,
                    .peer_idx_b = i,
                } };
                if (final_set) |cur_set| {
                    final_set = try sema.maybe_merge_error_sets(block, src, cur_set, ty);
                } else {
                    final_set = ty;
                }
            }
            return .{ .success = final_set.? };
        },

        .error_union => {
            var final_set: ?Type = null;
            for (peer_tys, peer_vals) |*ty_ptr, *val_ptr| {
                const ty = ty_ptr.* orelse continue;
                const set_ty = switch (ty.zig_type_tag(mod)) {
                    .ErrorSet => blk: {
                        ty_ptr.* = null; // no payload to decide on
                        val_ptr.* = null;
                        break :blk ty;
                    },
                    .ErrorUnion => blk: {
                        const set_ty = ty.error_union_set(mod);
                        ty_ptr.* = ty.error_union_payload(mod);
                        if (val_ptr.*) |eu_val| switch (ip.index_to_key(eu_val.to_intern())) {
                            .error_union => |eu| switch (eu.val) {
                                .payload => |payload_ip| val_ptr.* = Value.from_interned(payload_ip),
                                .err_name => val_ptr.* = null,
                            },
                            .undef => val_ptr.* = Value.from_interned((try sema.mod.intern(.{ .undef = ty_ptr.*.?.to_intern() }))),
                            else => unreachable,
                        };
                        break :blk set_ty;
                    },
                    else => continue, // whole type is the payload
                };
                if (final_set) |cur_set| {
                    final_set = try sema.maybe_merge_error_sets(block, src, cur_set, set_ty);
                } else {
                    final_set = set_ty;
                }
            }
            assert(final_set != null);
            const final_payload = switch (try sema.resolve_peer_types_inner(
                block,
                src,
                peer_tys,
                peer_vals,
            )) {
                .success => |ty| ty,
                else => |result| return result,
            };
            return .{ .success = try mod.error_union_type(final_set.?, final_payload) };
        },

        .nullable => {
            for (peer_tys, 0..) |opt_ty, i| {
                const ty = opt_ty orelse continue;
                if (!ty.eql(Type.null, mod)) return .{ .conflict = .{
                    .peer_idx_a = strat_reason,
                    .peer_idx_b = i,
                } };
            }
            return .{ .success = Type.null };
        },

        .optional => {
            for (peer_tys, peer_vals) |*ty_ptr, *val_ptr| {
                const ty = ty_ptr.* orelse continue;
                switch (ty.zig_type_tag(mod)) {
                    .Null => {
                        ty_ptr.* = null;
                        val_ptr.* = null;
                    },
                    .Optional => {
                        ty_ptr.* = ty.optional_child(mod);
                        if (val_ptr.*) |opt_val| val_ptr.* = if (!opt_val.is_undef(mod)) opt_val.optional_value(mod) else null;
                    },
                    else => {},
                }
            }
            const child_ty = switch (try sema.resolve_peer_types_inner(
                block,
                src,
                peer_tys,
                peer_vals,
            )) {
                .success => |ty| ty,
                else => |result| return result,
            };
            return .{ .success = try mod.optional_type(child_ty.to_intern()) };
        },

        .array => {
            // Index of the first non-null peer
            var opt_first_idx: ?usize = null;
            // Index of the first array or vector peer (i.e. not a tuple)
            var opt_first_arr_idx: ?usize = null;
            // Set to non-null once we see any peer, even a tuple
            var len: u64 = undefined;
            var sentinel: ?Value = undefined;
            // Only set once we see a non-tuple peer
            var elem_ty: Type = undefined;

            for (peer_tys, 0..) |*ty_ptr, i| {
                const ty = ty_ptr.* orelse continue;

                if (!ty.is_array_or_vector(mod)) {
                    // We allow tuples of the correct length. We won't validate their elem type, since the elements can be coerced.
                    const arr_like = sema.type_is_array_like(ty) orelse return .{ .conflict = .{
                        .peer_idx_a = strat_reason,
                        .peer_idx_b = i,
                    } };

                    if (opt_first_idx) |first_idx| {
                        if (arr_like.len != len) return .{ .conflict = .{
                            .peer_idx_a = first_idx,
                            .peer_idx_b = i,
                        } };
                    } else {
                        opt_first_idx = i;
                        len = arr_like.len;
                    }

                    sentinel = null;

                    continue;
                }

                const first_arr_idx = opt_first_arr_idx orelse {
                    if (opt_first_idx == null) {
                        opt_first_idx = i;
                        len = ty.array_len(mod);
                        sentinel = ty.sentinel(mod);
                    }
                    opt_first_arr_idx = i;
                    elem_ty = ty.child_type(mod);
                    continue;
                };

                if (ty.array_len(mod) != len) return .{ .conflict = .{
                    .peer_idx_a = first_arr_idx,
                    .peer_idx_b = i,
                } };

                const peer_elem_ty = ty.child_type(mod);
                if (!peer_elem_ty.eql(elem_ty, mod)) coerce: {
                    const peer_elem_coerces_to_elem =
                        try sema.coerce_in_memory_allowed(block, elem_ty, peer_elem_ty, false, mod.get_target(), src, src);
                    if (peer_elem_coerces_to_elem == .ok) {
                        break :coerce;
                    }

                    const elem_coerces_to_peer_elem =
                        try sema.coerce_in_memory_allowed(block, peer_elem_ty, elem_ty, false, mod.get_target(), src, src);
                    if (elem_coerces_to_peer_elem == .ok) {
                        elem_ty = peer_elem_ty;
                        break :coerce;
                    }

                    return .{ .conflict = .{
                        .peer_idx_a = first_arr_idx,
                        .peer_idx_b = i,
                    } };
                }

                if (sentinel) |cur_sent| {
                    if (ty.sentinel(mod)) |peer_sent| {
                        if (!peer_sent.eql(cur_sent, elem_ty, mod)) sentinel = null;
                    } else {
                        sentinel = null;
                    }
                }
            }

            // There should always be at least one array or vector peer
            assert(opt_first_arr_idx != null);

            return .{ .success = try mod.array_type(.{
                .len = len,
                .child = elem_ty.to_intern(),
                .sentinel = if (sentinel) |sent_val| sent_val.to_intern() else .none,
            }) };
        },

        .vector => {
            var len: ?u64 = null;
            var first_idx: usize = undefined;
            for (peer_tys, peer_vals, 0..) |*ty_ptr, *val_ptr, i| {
                const ty = ty_ptr.* orelse continue;

                if (!ty.is_array_or_vector(mod)) {
                    // Allow tuples of the correct length
                    const arr_like = sema.type_is_array_like(ty) orelse return .{ .conflict = .{
                        .peer_idx_a = strat_reason,
                        .peer_idx_b = i,
                    } };

                    if (len) |expect_len| {
                        if (arr_like.len != expect_len) return .{ .conflict = .{
                            .peer_idx_a = first_idx,
                            .peer_idx_b = i,
                        } };
                    } else {
                        len = arr_like.len;
                        first_idx = i;
                    }

                    // Tuples won't participate in the child type resolution. We'll resolve without
                    // them, and if the tuples have a bad type, we'll get a coercion error later.
                    ty_ptr.* = null;
                    val_ptr.* = null;

                    continue;
                }

                if (len) |expect_len| {
                    if (ty.array_len(mod) != expect_len) return .{ .conflict = .{
                        .peer_idx_a = first_idx,
                        .peer_idx_b = i,
                    } };
                } else {
                    len = ty.array_len(mod);
                    first_idx = i;
                }

                ty_ptr.* = ty.child_type(mod);
                val_ptr.* = null; // multiple child vals, so we can't easily use them in PTR
            }

            const child_ty = switch (try sema.resolve_peer_types_inner(
                block,
                src,
                peer_tys,
                peer_vals,
            )) {
                .success => |ty| ty,
                else => |result| return result,
            };

            return .{ .success = try mod.vector_type(.{
                .len = @int_cast(len.?),
                .child = child_ty.to_intern(),
            }) };
        },

        .c_ptr => {
            var opt_ptr_info: ?InternPool.Key.PtrType = null;
            var first_idx: usize = undefined;
            for (peer_tys, peer_vals, 0..) |opt_ty, opt_val, i| {
                const ty = opt_ty orelse continue;
                switch (ty.zig_type_tag(mod)) {
                    .ComptimeInt => continue, // comptime-known integers can always coerce to C pointers
                    .Int => {
                        if (opt_val != null) {
                            // Always allow the coercion for comptime-known ints
                            continue;
                        } else {
                            // Runtime-known, so check if the type is no bigger than a usize
                            const ptr_bits = target.ptr_bit_width();
                            const bits = ty.int_info(mod).bits;
                            if (bits <= ptr_bits) continue;
                        }
                    },
                    .Null => continue,
                    else => {},
                }

                if (!ty.is_ptr_at_runtime(mod)) return .{ .conflict = .{
                    .peer_idx_a = strat_reason,
                    .peer_idx_b = i,
                } };

                // Goes through optionals
                const peer_info = ty.ptr_info(mod);

                var ptr_info = opt_ptr_info orelse {
                    opt_ptr_info = peer_info;
                    opt_ptr_info.?.flags.size = .C;
                    first_idx = i;
                    continue;
                };

                // Try peer -> cur, then cur -> peer
                ptr_info.child = ((try sema.resolve_pair_in_memory_coercible(block, src, Type.from_interned(ptr_info.child), Type.from_interned(peer_info.child))) orelse {
                    return .{ .conflict = .{
                        .peer_idx_a = first_idx,
                        .peer_idx_b = i,
                    } };
                }).to_intern();

                if (ptr_info.sentinel != .none and peer_info.sentinel != .none) {
                    const peer_sent = try ip.get_coerced(sema.gpa, ptr_info.sentinel, ptr_info.child);
                    const ptr_sent = try ip.get_coerced(sema.gpa, peer_info.sentinel, ptr_info.child);
                    if (ptr_sent == peer_sent) {
                        ptr_info.sentinel = ptr_sent;
                    } else {
                        ptr_info.sentinel = .none;
                    }
                } else {
                    ptr_info.sentinel = .none;
                }

                // Note that the align can be always non-zero; Module.ptr_type will canonicalize it
                ptr_info.flags.alignment = InternPool.Alignment.min(
                    if (ptr_info.flags.alignment != .none)
                        ptr_info.flags.alignment
                    else
                        Type.from_interned(ptr_info.child).abi_alignment(mod),

                    if (peer_info.flags.alignment != .none)
                        peer_info.flags.alignment
                    else
                        Type.from_interned(peer_info.child).abi_alignment(mod),
                );
                if (ptr_info.flags.address_space != peer_info.flags.address_space) {
                    return .{ .conflict = .{
                        .peer_idx_a = first_idx,
                        .peer_idx_b = i,
                    } };
                }

                if (ptr_info.packed_offset.bit_offset != peer_info.packed_offset.bit_offset or
                    ptr_info.packed_offset.host_size != peer_info.packed_offset.host_size)
                {
                    return .{ .conflict = .{
                        .peer_idx_a = first_idx,
                        .peer_idx_b = i,
                    } };
                }

                ptr_info.flags.is_const = ptr_info.flags.is_const or peer_info.flags.is_const;
                ptr_info.flags.is_volatile = ptr_info.flags.is_volatile or peer_info.flags.is_volatile;

                opt_ptr_info = ptr_info;
            }
            return .{ .success = try sema.ptr_type(opt_ptr_info.?) };
        },

        .ptr => {
            // If we've resolved to a `[]T` but then see a `[*]T`, we can resolve to a `[*]T` only
            // if there were no actual slices. Else, we want the slice index to report a conflict.
            var opt_slice_idx: ?usize = null;

            var opt_ptr_info: ?InternPool.Key.PtrType = null;
            var first_idx: usize = undefined;
            var other_idx: usize = undefined; // We sometimes need a second peer index to report a generic error

            for (peer_tys, 0..) |opt_ty, i| {
                const ty = opt_ty orelse continue;
                const peer_info: InternPool.Key.PtrType = switch (ty.zig_type_tag(mod)) {
                    .Pointer => ty.ptr_info(mod),
                    .Fn => .{
                        .child = ty.to_intern(),
                        .flags = .{
                            .address_space = target_util.default_address_space(target, .global_constant),
                        },
                    },
                    else => return .{ .conflict = .{
                        .peer_idx_a = strat_reason,
                        .peer_idx_b = i,
                    } },
                };

                switch (peer_info.flags.size) {
                    .One, .Many => {},
                    .Slice => opt_slice_idx = i,
                    .C => return .{ .conflict = .{
                        .peer_idx_a = strat_reason,
                        .peer_idx_b = i,
                    } },
                }

                var ptr_info = opt_ptr_info orelse {
                    opt_ptr_info = peer_info;
                    first_idx = i;
                    continue;
                };

                other_idx = i;

                // We want to return this in a lot of cases, so alias it here for convenience
                const generic_err: PeerResolveResult = .{ .conflict = .{
                    .peer_idx_a = first_idx,
                    .peer_idx_b = i,
                } };

                // Note that the align can be always non-zero; Type.ptr will canonicalize it
                ptr_info.flags.alignment = Alignment.min(
                    if (ptr_info.flags.alignment != .none)
                        ptr_info.flags.alignment
                    else
                        try sema.type_abi_alignment(Type.from_interned(ptr_info.child)),

                    if (peer_info.flags.alignment != .none)
                        peer_info.flags.alignment
                    else
                        try sema.type_abi_alignment(Type.from_interned(peer_info.child)),
                );

                if (ptr_info.flags.address_space != peer_info.flags.address_space) {
                    return generic_err;
                }

                if (ptr_info.packed_offset.bit_offset != peer_info.packed_offset.bit_offset or
                    ptr_info.packed_offset.host_size != peer_info.packed_offset.host_size)
                {
                    return generic_err;
                }

                ptr_info.flags.is_const = ptr_info.flags.is_const or peer_info.flags.is_const;
                ptr_info.flags.is_volatile = ptr_info.flags.is_volatile or peer_info.flags.is_volatile;

                const peer_sentinel: InternPool.Index = switch (peer_info.flags.size) {
                    .One => switch (ip.index_to_key(peer_info.child)) {
                        .array_type => |array_type| array_type.sentinel,
                        else => .none,
                    },
                    .Many, .Slice => peer_info.sentinel,
                    .C => unreachable,
                };

                const cur_sentinel: InternPool.Index = switch (ptr_info.flags.size) {
                    .One => switch (ip.index_to_key(ptr_info.child)) {
                        .array_type => |array_type| array_type.sentinel,
                        else => .none,
                    },
                    .Many, .Slice => ptr_info.sentinel,
                    .C => unreachable,
                };

                // We abstract array handling slightly so that tuple pointers can work like array pointers
                const peer_pointee_array = sema.type_is_array_like(Type.from_interned(peer_info.child));
                const cur_pointee_array = sema.type_is_array_like(Type.from_interned(ptr_info.child));

                // This switch is just responsible for deciding the size and pointee (not including
                // single-pointer array sentinel).
                good: {
                    switch (peer_info.flags.size) {
                        .One => switch (ptr_info.flags.size) {
                            .One => {
                                if (try sema.resolve_pair_in_memory_coercible(block, src, Type.from_interned(ptr_info.child), Type.from_interned(peer_info.child))) |pointee| {
                                    ptr_info.child = pointee.to_intern();
                                    break :good;
                                }

                                const cur_arr = cur_pointee_array orelse return generic_err;
                                const peer_arr = peer_pointee_array orelse return generic_err;

                                if (try sema.resolve_pair_in_memory_coercible(block, src, cur_arr.elem_ty, peer_arr.elem_ty)) |elem_ty| {
                                    // *[n:x]T + *[n:y]T = *[n]T
                                    if (cur_arr.len == peer_arr.len) {
                                        ptr_info.child = (try mod.array_type(.{
                                            .len = cur_arr.len,
                                            .child = elem_ty.to_intern(),
                                        })).to_intern();
                                        break :good;
                                    }
                                    // *[a]T + *[b]T = []T
                                    ptr_info.flags.size = .Slice;
                                    ptr_info.child = elem_ty.to_intern();
                                    break :good;
                                }

                                if (peer_arr.elem_ty.to_intern() == .noreturn_type) {
                                    // *struct{} + *[a]T = []T
                                    ptr_info.flags.size = .Slice;
                                    ptr_info.child = cur_arr.elem_ty.to_intern();
                                    break :good;
                                }

                                if (cur_arr.elem_ty.to_intern() == .noreturn_type) {
                                    // *[a]T + *struct{} = []T
                                    ptr_info.flags.size = .Slice;
                                    ptr_info.child = peer_arr.elem_ty.to_intern();
                                    break :good;
                                }

                                return generic_err;
                            },
                            .Many => {
                                // Only works for *[n]T + [*]T -> [*]T
                                const arr = peer_pointee_array orelse return generic_err;
                                if (try sema.resolve_pair_in_memory_coercible(block, src, Type.from_interned(ptr_info.child), arr.elem_ty)) |pointee| {
                                    ptr_info.child = pointee.to_intern();
                                    break :good;
                                }
                                if (arr.elem_ty.to_intern() == .noreturn_type) {
                                    // *struct{} + [*]T -> [*]T
                                    break :good;
                                }
                                return generic_err;
                            },
                            .Slice => {
                                // Only works for *[n]T + []T -> []T
                                const arr = peer_pointee_array orelse return generic_err;
                                if (try sema.resolve_pair_in_memory_coercible(block, src, Type.from_interned(ptr_info.child), arr.elem_ty)) |pointee| {
                                    ptr_info.child = pointee.to_intern();
                                    break :good;
                                }
                                if (arr.elem_ty.to_intern() == .noreturn_type) {
                                    // *struct{} + []T -> []T
                                    break :good;
                                }
                                return generic_err;
                            },
                            .C => unreachable,
                        },
                        .Many => switch (ptr_info.flags.size) {
                            .One => {
                                // Only works for [*]T + *[n]T -> [*]T
                                const arr = cur_pointee_array orelse return generic_err;
                                if (try sema.resolve_pair_in_memory_coercible(block, src, arr.elem_ty, Type.from_interned(peer_info.child))) |pointee| {
                                    ptr_info.flags.size = .Many;
                                    ptr_info.child = pointee.to_intern();
                                    break :good;
                                }
                                if (arr.elem_ty.to_intern() == .noreturn_type) {
                                    // [*]T + *struct{} -> [*]T
                                    ptr_info.flags.size = .Many;
                                    ptr_info.child = peer_info.child;
                                    break :good;
                                }
                                return generic_err;
                            },
                            .Many => {
                                if (try sema.resolve_pair_in_memory_coercible(block, src, Type.from_interned(ptr_info.child), Type.from_interned(peer_info.child))) |pointee| {
                                    ptr_info.child = pointee.to_intern();
                                    break :good;
                                }
                                return generic_err;
                            },
                            .Slice => {
                                // Only works if no peers are actually slices
                                if (opt_slice_idx) |slice_idx| {
                                    return .{ .conflict = .{
                                        .peer_idx_a = slice_idx,
                                        .peer_idx_b = i,
                                    } };
                                }
                                // Okay, then works for [*]T + "[]T" -> [*]T
                                if (try sema.resolve_pair_in_memory_coercible(block, src, Type.from_interned(ptr_info.child), Type.from_interned(peer_info.child))) |pointee| {
                                    ptr_info.flags.size = .Many;
                                    ptr_info.child = pointee.to_intern();
                                    break :good;
                                }
                                return generic_err;
                            },
                            .C => unreachable,
                        },
                        .Slice => switch (ptr_info.flags.size) {
                            .One => {
                                // Only works for []T + *[n]T -> []T
                                const arr = cur_pointee_array orelse return generic_err;
                                if (try sema.resolve_pair_in_memory_coercible(block, src, arr.elem_ty, Type.from_interned(peer_info.child))) |pointee| {
                                    ptr_info.flags.size = .Slice;
                                    ptr_info.child = pointee.to_intern();
                                    break :good;
                                }
                                if (arr.elem_ty.to_intern() == .noreturn_type) {
                                    // []T + *struct{} -> []T
                                    ptr_info.flags.size = .Slice;
                                    ptr_info.child = peer_info.child;
                                    break :good;
                                }
                                return generic_err;
                            },
                            .Many => {
                                // Impossible! (current peer is an actual slice)
                                return generic_err;
                            },
                            .Slice => {
                                if (try sema.resolve_pair_in_memory_coercible(block, src, Type.from_interned(ptr_info.child), Type.from_interned(peer_info.child))) |pointee| {
                                    ptr_info.child = pointee.to_intern();
                                    break :good;
                                }
                                return generic_err;
                            },
                            .C => unreachable,
                        },
                        .C => unreachable,
                    }
                }

                const sentinel_ty = switch (ptr_info.flags.size) {
                    .One => switch (ip.index_to_key(ptr_info.child)) {
                        .array_type => |array_type| array_type.child,
                        else => ptr_info.child,
                    },
                    .Many, .Slice, .C => ptr_info.child,
                };

                sentinel: {
                    no_sentinel: {
                        if (peer_sentinel == .none) break :no_sentinel;
                        if (cur_sentinel == .none) break :no_sentinel;
                        const peer_sent_coerced = try ip.get_coerced(sema.gpa, peer_sentinel, sentinel_ty);
                        const cur_sent_coerced = try ip.get_coerced(sema.gpa, cur_sentinel, sentinel_ty);
                        if (peer_sent_coerced != cur_sent_coerced) break :no_sentinel;
                        // Sentinels match
                        if (ptr_info.flags.size == .One) switch (ip.index_to_key(ptr_info.child)) {
                            .array_type => |array_type| ptr_info.child = (try mod.array_type(.{
                                .len = array_type.len,
                                .child = array_type.child,
                                .sentinel = cur_sent_coerced,
                            })).to_intern(),
                            else => unreachable,
                        } else {
                            ptr_info.sentinel = cur_sent_coerced;
                        }
                        break :sentinel;
                    }
                    // Clear existing sentinel
                    ptr_info.sentinel = .none;
                    switch (ip.index_to_key(ptr_info.child)) {
                        .array_type => |array_type| ptr_info.child = (try mod.array_type(.{
                            .len = array_type.len,
                            .child = array_type.child,
                            .sentinel = .none,
                        })).to_intern(),
                        else => {},
                    }
                }

                opt_ptr_info = ptr_info;
            }

            // Before we succeed, check the pointee type. If we tried to apply PTR to (for instance)
            // &.{} and &.{}, we'll currently have a pointer type of `*[0]noreturn` - we wanted to
            // coerce the empty struct to a specific type, but no peer provided one. We need to
            // detect this case and emit an error.
            const pointee = opt_ptr_info.?.child;
            switch (pointee) {
                .noreturn_type => return .{ .conflict = .{
                    .peer_idx_a = first_idx,
                    .peer_idx_b = other_idx,
                } },
                else => switch (ip.index_to_key(pointee)) {
                    .array_type => |array_type| if (array_type.child == .noreturn_type) return .{ .conflict = .{
                        .peer_idx_a = first_idx,
                        .peer_idx_b = other_idx,
                    } },
                    else => {},
                },
            }

            return .{ .success = try sema.ptr_type(opt_ptr_info.?) };
        },

        .func => {
            var opt_cur_ty: ?Type = null;
            var first_idx: usize = undefined;
            for (peer_tys, 0..) |opt_ty, i| {
                const ty = opt_ty orelse continue;
                const cur_ty = opt_cur_ty orelse {
                    opt_cur_ty = ty;
                    first_idx = i;
                    continue;
                };
                if (ty.zig_type_tag(mod) != .Fn) return .{ .conflict = .{
                    .peer_idx_a = strat_reason,
                    .peer_idx_b = i,
                } };
                // ty -> cur_ty
                if (.ok == try sema.coerce_in_memory_allowed_fns(block, cur_ty, ty, target, src, src)) {
                    continue;
                }
                // cur_ty -> ty
                if (.ok == try sema.coerce_in_memory_allowed_fns(block, ty, cur_ty, target, src, src)) {
                    opt_cur_ty = ty;
                    continue;
                }
                return .{ .conflict = .{
                    .peer_idx_a = first_idx,
                    .peer_idx_b = i,
                } };
            }
            return .{ .success = opt_cur_ty.? };
        },

        .enum_or_union => {
            var opt_cur_ty: ?Type = null;
            // The peer index which gave the current type
            var cur_ty_idx: usize = undefined;

            for (peer_tys, 0..) |opt_ty, i| {
                const ty = opt_ty orelse continue;
                switch (ty.zig_type_tag(mod)) {
                    .EnumLiteral, .Enum, .Union => {},
                    else => return .{ .conflict = .{
                        .peer_idx_a = strat_reason,
                        .peer_idx_b = i,
                    } },
                }
                const cur_ty = opt_cur_ty orelse {
                    opt_cur_ty = ty;
                    cur_ty_idx = i;
                    continue;
                };

                // We want to return this in a lot of cases, so alias it here for convenience
                const generic_err: PeerResolveResult = .{ .conflict = .{
                    .peer_idx_a = cur_ty_idx,
                    .peer_idx_b = i,
                } };

                switch (cur_ty.zig_type_tag(mod)) {
                    .EnumLiteral => {
                        opt_cur_ty = ty;
                        cur_ty_idx = i;
                    },
                    .Enum => switch (ty.zig_type_tag(mod)) {
                        .EnumLiteral => {},
                        .Enum => {
                            if (!ty.eql(cur_ty, mod)) return generic_err;
                        },
                        .Union => {
                            const tag_ty = ty.union_tag_type_hypothetical(mod);
                            if (!tag_ty.eql(cur_ty, mod)) return generic_err;
                            opt_cur_ty = ty;
                            cur_ty_idx = i;
                        },
                        else => unreachable,
                    },
                    .Union => switch (ty.zig_type_tag(mod)) {
                        .EnumLiteral => {},
                        .Enum => {
                            const cur_tag_ty = cur_ty.union_tag_type_hypothetical(mod);
                            if (!ty.eql(cur_tag_ty, mod)) return generic_err;
                        },
                        .Union => {
                            if (!ty.eql(cur_ty, mod)) return generic_err;
                        },
                        else => unreachable,
                    },
                    else => unreachable,
                }
            }
            return .{ .success = opt_cur_ty.? };
        },

        .comptime_int => {
            for (peer_tys, 0..) |opt_ty, i| {
                const ty = opt_ty orelse continue;
                switch (ty.zig_type_tag(mod)) {
                    .ComptimeInt => {},
                    else => return .{ .conflict = .{
                        .peer_idx_a = strat_reason,
                        .peer_idx_b = i,
                    } },
                }
            }
            return .{ .success = Type.comptime_int };
        },

        .comptime_float => {
            for (peer_tys, 0..) |opt_ty, i| {
                const ty = opt_ty orelse continue;
                switch (ty.zig_type_tag(mod)) {
                    .ComptimeInt, .ComptimeFloat => {},
                    else => return .{ .conflict = .{
                        .peer_idx_a = strat_reason,
                        .peer_idx_b = i,
                    } },
                }
            }
            return .{ .success = Type.comptime_float };
        },

        .fixed_int => {
            var idx_unsigned: ?usize = null;
            var idx_signed: ?usize = null;

            // TODO: this is for compatibility with legacy behavior. See beneath the loop.
            var any_comptime_known = false;

            for (peer_tys, peer_vals, 0..) |opt_ty, *ptr_opt_val, i| {
                const ty = opt_ty orelse continue;
                const opt_val = ptr_opt_val.*;

                const peer_tag = ty.zig_type_tag(mod);
                switch (peer_tag) {
                    .ComptimeInt => {
                        // If the value is undefined, we can't refine to a fixed-width int
                        if (opt_val == null or opt_val.?.is_undef(mod)) return .{ .conflict = .{
                            .peer_idx_a = strat_reason,
                            .peer_idx_b = i,
                        } };
                        any_comptime_known = true;
                        ptr_opt_val.* = try sema.resolve_lazy_value(opt_val.?);
                        continue;
                    },
                    .Int => {},
                    else => return .{ .conflict = .{
                        .peer_idx_a = strat_reason,
                        .peer_idx_b = i,
                    } },
                }

                if (opt_val != null) any_comptime_known = true;

                const info = ty.int_info(mod);

                const idx_ptr = switch (info.signedness) {
                    .unsigned => &idx_unsigned,
                    .signed => &idx_signed,
                };

                const largest_idx = idx_ptr.* orelse {
                    idx_ptr.* = i;
                    continue;
                };

                const cur_info = peer_tys[largest_idx].?.int_info(mod);
                if (info.bits > cur_info.bits) {
                    idx_ptr.* = i;
                }
            }

            if (idx_signed == null) {
                return .{ .success = peer_tys[idx_unsigned.?].? };
            }

            if (idx_unsigned == null) {
                return .{ .success = peer_tys[idx_signed.?].? };
            }

            const unsigned_info = peer_tys[idx_unsigned.?].?.int_info(mod);
            const signed_info = peer_tys[idx_signed.?].?.int_info(mod);
            if (signed_info.bits > unsigned_info.bits) {
                return .{ .success = peer_tys[idx_signed.?].? };
            }

            // TODO: this is for compatibility with legacy behavior. Before this version of PTR was
            // implemented, the algorithm very often returned false positives, with the expectation
            // that you'd just hit a coercion error later. One of these was that for integers, the
            // largest type would always be returned, even if it couldn't fit everything. This had
            // an unintentional consequence to semantics, which is that if values were known at
            // comptime, they would be coerced down to the smallest type where possible. This
            // behavior is unintuitive and order-dependent, so in my opinion should be eliminated,
            // but for now we'll retain compatibility.
            if (any_comptime_known) {
                if (unsigned_info.bits > signed_info.bits) {
                    return .{ .success = peer_tys[idx_unsigned.?].? };
                }
                const idx = @min(idx_unsigned.?, idx_signed.?);
                return .{ .success = peer_tys[idx].? };
            }

            return .{ .conflict = .{
                .peer_idx_a = idx_unsigned.?,
                .peer_idx_b = idx_signed.?,
            } };
        },

        .fixed_float => {
            var opt_cur_ty: ?Type = null;

            for (peer_tys, peer_vals, 0..) |opt_ty, opt_val, i| {
                const ty = opt_ty orelse continue;
                switch (ty.zig_type_tag(mod)) {
                    .ComptimeFloat, .ComptimeInt => {},
                    .Int => {
                        if (opt_val == null) return .{ .conflict = .{
                            .peer_idx_a = strat_reason,
                            .peer_idx_b = i,
                        } };
                    },
                    .Float => {
                        if (opt_cur_ty) |cur_ty| {
                            if (cur_ty.eql(ty, mod)) continue;
                            // Recreate the type so we eliminate any c_longdouble
                            const bits = @max(cur_ty.float_bits(target), ty.float_bits(target));
                            opt_cur_ty = switch (bits) {
                                16 => Type.f16,
                                32 => Type.f32,
                                64 => Type.f64,
                                80 => Type.f80,
                                128 => Type.f128,
                                else => unreachable,
                            };
                        } else {
                            opt_cur_ty = ty;
                        }
                    },
                    else => return .{ .conflict = .{
                        .peer_idx_a = strat_reason,
                        .peer_idx_b = i,
                    } },
                }
            }

            // Note that fixed_float is only chosen if there is at least one fixed-width float peer,
            // so opt_cur_ty must be non-null.
            return .{ .success = opt_cur_ty.? };
        },

        .coercible_struct => {
            // First, check that every peer has the same approximate structure (field count and names)

            var opt_first_idx: ?usize = null;
            var is_tuple: bool = undefined;
            var field_count: usize = undefined;
            // Only defined for non-tuples.
            var field_names: []InternPool.NullTerminatedString = undefined;

            for (peer_tys, 0..) |opt_ty, i| {
                const ty = opt_ty orelse continue;

                if (!ty.is_tuple_or_anon_struct(mod)) {
                    return .{ .conflict = .{
                        .peer_idx_a = strat_reason,
                        .peer_idx_b = i,
                    } };
                }

                const first_idx = opt_first_idx orelse {
                    opt_first_idx = i;
                    is_tuple = ty.is_tuple(mod);
                    field_count = ty.struct_field_count(mod);
                    if (!is_tuple) {
                        const names = ip.index_to_key(ty.to_intern()).anon_struct_type.names.get(ip);
                        field_names = try sema.arena.dupe(InternPool.NullTerminatedString, names);
                    }
                    continue;
                };

                if (ty.is_tuple(mod) != is_tuple or ty.struct_field_count(mod) != field_count) {
                    return .{ .conflict = .{
                        .peer_idx_a = first_idx,
                        .peer_idx_b = i,
                    } };
                }

                if (!is_tuple) {
                    for (field_names, 0..) |expected, field_index_usize| {
                        const field_index: u32 = @int_cast(field_index_usize);
                        const actual = ty.struct_field_name(field_index, mod).unwrap().?;
                        if (actual == expected) continue;
                        return .{ .conflict = .{
                            .peer_idx_a = first_idx,
                            .peer_idx_b = i,
                        } };
                    }
                }
            }

            assert(opt_first_idx != null);

            // Now, we'll recursively resolve the field types
            const field_types = try sema.arena.alloc(InternPool.Index, field_count);
            // Values for `comptime` fields - `.none` used for non-comptime fields
            const field_vals = try sema.arena.alloc(InternPool.Index, field_count);
            const sub_peer_tys = try sema.arena.alloc(?Type, peer_tys.len);
            const sub_peer_vals = try sema.arena.alloc(?Value, peer_vals.len);

            for (field_types, field_vals, 0..) |*field_ty, *field_val, field_index| {
                // Fill buffers with types and values of the field
                for (peer_tys, peer_vals, sub_peer_tys, sub_peer_vals) |opt_ty, opt_val, *peer_field_ty, *peer_field_val| {
                    const ty = opt_ty orelse {
                        peer_field_ty.* = null;
                        peer_field_val.* = null;
                        continue;
                    };
                    peer_field_ty.* = ty.struct_field_type(field_index, mod);
                    peer_field_val.* = if (opt_val) |val| try val.field_value(mod, field_index) else null;
                }

                // Resolve field type recursively
                field_ty.* = switch (try sema.resolve_peer_types_inner(block, src, sub_peer_tys, sub_peer_vals)) {
                    .success => |ty| ty.to_intern(),
                    else => |result| {
                        const result_buf = try sema.arena.create(PeerResolveResult);
                        result_buf.* = result;
                        const field_name = if (is_tuple)
                            try ip.get_or_put_string_fmt(sema.gpa, "{d}", .{field_index}, .no_embedded_nulls)
                        else
                            field_names[field_index];

                        // The error info needs the field types, but we can't reuse sub_peer_tys
                        // since the recursive call may have clobbered it.
                        const peer_field_tys = try sema.arena.alloc(Type, peer_tys.len);
                        for (peer_tys, peer_field_tys) |opt_ty, *peer_field_ty| {
                            // Already-resolved types won't be referenced by the error so it's fine
                            // to leave them undefined.
                            const ty = opt_ty orelse continue;
                            peer_field_ty.* = ty.struct_field_type(field_index, mod);
                        }

                        return .{ .field_error = .{
                            .field_name = field_name,
                            .field_types = peer_field_tys,
                            .sub_result = result_buf,
                        } };
                    },
                };

                // Decide if this is a comptime field. If it is comptime in all peers, and the
                // coerced comptime values are all the same, we say it is comptime, else not.

                var comptime_val: ?Value = null;
                for (peer_tys) |opt_ty| {
                    const struct_ty = opt_ty orelse continue;
                    try sema.resolve_struct_field_inits(struct_ty);

                    const uncoerced_field_val = try struct_ty.struct_field_value_comptime(mod, field_index) orelse {
                        comptime_val = null;
                        break;
                    };
                    const uncoerced_field = Air.interned_to_ref(uncoerced_field_val.to_intern());
                    const coerced_inst = sema.coerce_extra(block, Type.from_interned(field_ty.*), uncoerced_field, src, .{ .report_err = false }) catch |err| switch (err) {
                        // It's possible for PTR to give false positives. Just give up on making this a comptime field, we'll get an error later anyway
                        error.NotCoercible => {
                            comptime_val = null;
                            break;
                        },
                        else => |e| return e,
                    };
                    const coerced_val = (try sema.resolve_value(coerced_inst)) orelse continue;
                    const existing = comptime_val orelse {
                        comptime_val = coerced_val;
                        continue;
                    };
                    if (!coerced_val.eql(existing, Type.from_interned(field_ty.*), mod)) {
                        comptime_val = null;
                        break;
                    }
                }

                field_val.* = if (comptime_val) |v| v.to_intern() else .none;
            }

            const final_ty = try ip.get_anon_struct_type(mod.gpa, .{
                .types = field_types,
                .names = if (is_tuple) &.{} else field_names,
                .values = field_vals,
            });

            return .{ .success = Type.from_interned(final_ty) };
        },

        .exact => {
            var expect_ty: ?Type = null;
            var first_idx: usize = undefined;
            for (peer_tys, 0..) |opt_ty, i| {
                const ty = opt_ty orelse continue;
                if (expect_ty) |expect| {
                    if (!ty.eql(expect, mod)) return .{ .conflict = .{
                        .peer_idx_a = first_idx,
                        .peer_idx_b = i,
                    } };
                } else {
                    expect_ty = ty;
                    first_idx = i;
                }
            }
            return .{ .success = expect_ty.? };
        },
    }
}

fn maybe_merge_error_sets(sema: *Sema, block: *Block, src: LazySrcLoc, e0: Type, e1: Type) !Type {
    // e0 -> e1
    if (.ok == try sema.coerce_in_memory_allowed_error_sets(block, e1, e0, src, src)) {
        return e1;
    }

    // e1 -> e0
    if (.ok == try sema.coerce_in_memory_allowed_error_sets(block, e0, e1, src, src)) {
        return e0;
    }

    return sema.error_set_merge(e0, e1);
}

fn resolve_pair_in_memory_coercible(sema: *Sema, block: *Block, src: LazySrcLoc, ty_a: Type, ty_b: Type) !?Type {
    // ty_b -> ty_a
    if (.ok == try sema.coerce_in_memory_allowed(block, ty_a, ty_b, true, sema.mod.get_target(), src, src)) {
        return ty_a;
    }

    // ty_a -> ty_b
    if (.ok == try sema.coerce_in_memory_allowed(block, ty_b, ty_a, true, sema.mod.get_target(), src, src)) {
        return ty_b;
    }

    return null;
}

const ArrayLike = struct {
    len: u64,
    /// `noreturn` indicates that this type is `struct{}` so can coerce to anything
    elem_ty: Type,
};
fn type_is_array_like(sema: *Sema, ty: Type) ?ArrayLike {
    const mod = sema.mod;
    return switch (ty.zig_type_tag(mod)) {
        .Array => .{
            .len = ty.array_len(mod),
            .elem_ty = ty.child_type(mod),
        },
        .Struct => {
            const field_count = ty.struct_field_count(mod);
            if (field_count == 0) return .{
                .len = 0,
                .elem_ty = Type.noreturn,
            };
            if (!ty.is_tuple(mod)) return null;
            const elem_ty = ty.struct_field_type(0, mod);
            for (1..field_count) |i| {
                if (!ty.struct_field_type(i, mod).eql(elem_ty, mod)) {
                    return null;
                }
            }
            return .{
                .len = field_count,
                .elem_ty = elem_ty,
            };
        },
        else => null,
    };
}

pub fn resolve_ies(sema: *Sema, block: *Block, src: LazySrcLoc) CompileError!void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;

    if (sema.fn_ret_ty_ies) |ies| {
        try sema.resolve_inferred_error_set_ptr(block, src, ies);
        assert(ies.resolved != .none);
        ip.func_ies_resolved(sema.func_index).* = ies.resolved;
    }
}

pub fn resolve_fn_types(sema: *Sema, fn_ty: Type) CompileError!void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const fn_ty_info = mod.type_to_func(fn_ty).?;

    try sema.resolve_type_fully(Type.from_interned(fn_ty_info.return_type));

    if (mod.comp.config.any_error_tracing and
        Type.from_interned(fn_ty_info.return_type).is_error(mod))
    {
        // Ensure the type exists so that backends can assume that.
        _ = try sema.get_builtin_type("StackTrace");
    }

    for (0..fn_ty_info.param_types.len) |i| {
        try sema.resolve_type_fully(Type.from_interned(fn_ty_info.param_types.get(ip)[i]));
    }
}

/// Make it so that calling hash() and eql() on `val` will not assert due
/// to a type not having its layout resolved.
fn resolve_lazy_value(sema: *Sema, val: Value) CompileError!Value {
    const mod = sema.mod;
    switch (mod.intern_pool.index_to_key(val.to_intern())) {
        .int => |int| switch (int.storage) {
            .u64, .i64, .big_int => return val,
            .lazy_align, .lazy_size => return mod.int_value(
                Type.from_interned(int.ty),
                (try val.get_unsigned_int_advanced(mod, sema)).?,
            ),
        },
        .slice => |slice| {
            const ptr = try sema.resolve_lazy_value(Value.from_interned(slice.ptr));
            const len = try sema.resolve_lazy_value(Value.from_interned(slice.len));
            if (ptr.to_intern() == slice.ptr and len.to_intern() == slice.len) return val;
            return Value.from_interned(try mod.intern(.{ .slice = .{
                .ty = slice.ty,
                .ptr = ptr.to_intern(),
                .len = len.to_intern(),
            } }));
        },
        .ptr => |ptr| {
            switch (ptr.base_addr) {
                .decl, .comptime_alloc, .anon_decl, .int => return val,
                .comptime_field => |field_val| {
                    const resolved_field_val =
                        (try sema.resolve_lazy_value(Value.from_interned(field_val))).to_intern();
                    return if (resolved_field_val == field_val)
                        val
                    else
                        Value.from_interned((try mod.intern(.{ .ptr = .{
                            .ty = ptr.ty,
                            .base_addr = .{ .comptime_field = resolved_field_val },
                            .byte_offset = ptr.byte_offset,
                        } })));
                },
                .eu_payload, .opt_payload => |base| {
                    const resolved_base = (try sema.resolve_lazy_value(Value.from_interned(base))).to_intern();
                    return if (resolved_base == base)
                        val
                    else
                        Value.from_interned((try mod.intern(.{ .ptr = .{
                            .ty = ptr.ty,
                            .base_addr = switch (ptr.base_addr) {
                                .eu_payload => .{ .eu_payload = resolved_base },
                                .opt_payload => .{ .opt_payload = resolved_base },
                                else => unreachable,
                            },
                            .byte_offset = ptr.byte_offset,
                        } })));
                },
                .arr_elem, .field => |base_index| {
                    const resolved_base = (try sema.resolve_lazy_value(Value.from_interned(base_index.base))).to_intern();
                    return if (resolved_base == base_index.base)
                        val
                    else
                        Value.from_interned((try mod.intern(.{ .ptr = .{
                            .ty = ptr.ty,
                            .base_addr = switch (ptr.base_addr) {
                                .arr_elem => .{ .arr_elem = .{
                                    .base = resolved_base,
                                    .index = base_index.index,
                                } },
                                .field => .{ .field = .{
                                    .base = resolved_base,
                                    .index = base_index.index,
                                } },
                                else => unreachable,
                            },
                            .byte_offset = ptr.byte_offset,
                        } })));
                },
            }
        },
        .aggregate => |aggregate| switch (aggregate.storage) {
            .bytes => return val,
            .elems => |elems| {
                var resolved_elems: []InternPool.Index = &.{};
                for (elems, 0..) |elem, i| {
                    const resolved_elem = (try sema.resolve_lazy_value(Value.from_interned(elem))).to_intern();
                    if (resolved_elems.len == 0 and resolved_elem != elem) {
                        resolved_elems = try sema.arena.alloc(InternPool.Index, elems.len);
                        @memcpy(resolved_elems[0..i], elems[0..i]);
                    }
                    if (resolved_elems.len > 0) resolved_elems[i] = resolved_elem;
                }
                return if (resolved_elems.len == 0) val else Value.from_interned((try mod.intern(.{ .aggregate = .{
                    .ty = aggregate.ty,
                    .storage = .{ .elems = resolved_elems },
                } })));
            },
            .repeated_elem => |elem| {
                const resolved_elem = (try sema.resolve_lazy_value(Value.from_interned(elem))).to_intern();
                return if (resolved_elem == elem) val else Value.from_interned((try mod.intern(.{ .aggregate = .{
                    .ty = aggregate.ty,
                    .storage = .{ .repeated_elem = resolved_elem },
                } })));
            },
        },
        .un => |un| {
            const resolved_tag = if (un.tag == .none)
                .none
            else
                (try sema.resolve_lazy_value(Value.from_interned(un.tag))).to_intern();
            const resolved_val = (try sema.resolve_lazy_value(Value.from_interned(un.val))).to_intern();
            return if (resolved_tag == un.tag and resolved_val == un.val)
                val
            else
                Value.from_interned((try mod.intern(.{ .un = .{
                    .ty = un.ty,
                    .tag = resolved_tag,
                    .val = resolved_val,
                } })));
        },
        else => return val,
    }
}

pub fn resolve_type_layout(sema: *Sema, ty: Type) CompileError!void {
    const mod = sema.mod;
    switch (mod.intern_pool.index_to_key(ty.to_intern())) {
        .simple_type => |simple_type| return sema.resolve_simple_type(simple_type),
        else => {},
    }
    switch (ty.zig_type_tag(mod)) {
        .Struct => return sema.resolve_struct_layout(ty),
        .Union => return sema.resolve_union_layout(ty),
        .Array => {
            if (ty.array_len_including_sentinel(mod) == 0) return;
            const elem_ty = ty.child_type(mod);
            return sema.resolve_type_layout(elem_ty);
        },
        .Optional => {
            const payload_ty = ty.optional_child(mod);
            // In case of querying the ABI alignment of this optional, we will ask
            // for has_runtime_bits() of the payload type, so we need "requires comptime"
            // to be known already before this function returns.
            _ = try sema.type_requires_comptime(payload_ty);
            return sema.resolve_type_layout(payload_ty);
        },
        .ErrorUnion => {
            const payload_ty = ty.error_union_payload(mod);
            return sema.resolve_type_layout(payload_ty);
        },
        .Fn => {
            const info = mod.type_to_func(ty).?;
            if (info.is_generic) {
                // Resolving of generic function types is deferred to when
                // the function is instantiated.
                return;
            }
            const ip = &mod.intern_pool;
            for (0..info.param_types.len) |i| {
                const param_ty = info.param_types.get(ip)[i];
                try sema.resolve_type_layout(Type.from_interned(param_ty));
            }
            try sema.resolve_type_layout(Type.from_interned(info.return_type));
        },
        else => {},
    }
}

/// Resolve a struct's alignment only without triggering resolution of its layout.
/// Asserts that the alignment is not yet resolved and the layout is non-packed.
pub fn resolve_struct_alignment(
    sema: *Sema,
    ty: InternPool.Index,
    struct_type: InternPool.LoadedStructType,
) CompileError!Alignment {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const target = mod.get_target();

    assert(struct_type.flags_ptr(ip).alignment == .none);
    assert(struct_type.layout != .@"packed");

    if (struct_type.flags_ptr(ip).field_types_wip) {
        // We'll guess "pointer-aligned", if the struct has an
        // underaligned pointer field then some allocations
        // might require explicit alignment.
        struct_type.flags_ptr(ip).assumed_pointer_aligned = true;
        const result = Alignment.from_byte_units(@div_exact(target.ptr_bit_width(), 8));
        struct_type.flags_ptr(ip).alignment = result;
        return result;
    }

    try sema.resolve_type_fields_struct(ty, struct_type);

    if (struct_type.set_alignment_wip(ip)) {
        // We'll guess "pointer-aligned", if the struct has an
        // underaligned pointer field then some allocations
        // might require explicit alignment.
        struct_type.flags_ptr(ip).assumed_pointer_aligned = true;
        const result = Alignment.from_byte_units(@div_exact(target.ptr_bit_width(), 8));
        struct_type.flags_ptr(ip).alignment = result;
        return result;
    }
    defer struct_type.clear_alignment_wip(ip);

    var result: Alignment = .@"1";

    for (0..struct_type.field_types.len) |i| {
        const field_ty = Type.from_interned(struct_type.field_types.get(ip)[i]);
        if (struct_type.field_is_comptime(ip, i) or try sema.type_requires_comptime(field_ty))
            continue;
        const field_align = try sema.struct_field_alignment(
            struct_type.field_align(ip, i),
            field_ty,
            struct_type.layout,
        );
        result = result.max_strict(field_align);
    }

    struct_type.flags_ptr(ip).alignment = result;
    return result;
}

fn resolve_struct_layout(sema: *Sema, ty: Type) CompileError!void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const struct_type = mod.type_to_struct(ty) orelse return;

    if (struct_type.have_layout(ip))
        return;

    try sema.resolve_type_fields(ty);

    if (struct_type.layout == .@"packed") {
        try sema_backing_int_type(mod, struct_type);
        return;
    }

    if (struct_type.set_layout_wip(ip)) {
        const msg = try Module.ErrorMsg.create(
            sema.gpa,
            mod.decl_ptr(struct_type.decl.unwrap().?).src_loc(mod),
            "struct '{}' depends on itself",
            .{ty.fmt(mod)},
        );
        return sema.fail_with_owned_error_msg(null, msg);
    }
    defer struct_type.clear_layout_wip(ip);

    const aligns = try sema.arena.alloc(Alignment, struct_type.field_types.len);
    const sizes = try sema.arena.alloc(u64, struct_type.field_types.len);

    var big_align: Alignment = .@"1";

    for (aligns, sizes, 0..) |*field_align, *field_size, i| {
        const field_ty = Type.from_interned(struct_type.field_types.get(ip)[i]);
        if (struct_type.field_is_comptime(ip, i) or try sema.type_requires_comptime(field_ty)) {
            struct_type.offsets.get(ip)[i] = 0;
            field_size.* = 0;
            field_align.* = .none;
            continue;
        }

        field_size.* = sema.type_abi_size(field_ty) catch |err| switch (err) {
            error.AnalysisFail => {
                const msg = sema.err orelse return err;
                try sema.add_field_err_note(ty, i, msg, "while checking this field", .{});
                return err;
            },
            else => return err,
        };
        field_align.* = try sema.struct_field_alignment(
            struct_type.field_align(ip, i),
            field_ty,
            struct_type.layout,
        );
        big_align = big_align.max_strict(field_align.*);
    }

    if (struct_type.flags_ptr(ip).assumed_runtime_bits and !(try sema.type_has_runtime_bits(ty))) {
        const msg = try Module.ErrorMsg.create(
            sema.gpa,
            mod.decl_ptr(struct_type.decl.unwrap().?).src_loc(mod),
            "struct layout depends on it having runtime bits",
            .{},
        );
        return sema.fail_with_owned_error_msg(null, msg);
    }

    if (struct_type.flags_ptr(ip).assumed_pointer_aligned and
        big_align.compare_strict(.neq, Alignment.from_byte_units(@div_exact(mod.get_target().ptr_bit_width(), 8))))
    {
        const msg = try Module.ErrorMsg.create(
            sema.gpa,
            mod.decl_ptr(struct_type.decl.unwrap().?).src_loc(mod),
            "struct layout depends on being pointer aligned",
            .{},
        );
        return sema.fail_with_owned_error_msg(null, msg);
    }

    if (struct_type.has_reordered_fields()) {
        const runtime_order = struct_type.runtime_order.get(ip);

        for (runtime_order, 0..) |*ro, i| {
            const field_ty = Type.from_interned(struct_type.field_types.get(ip)[i]);
            if (struct_type.field_is_comptime(ip, i) or try sema.type_requires_comptime(field_ty)) {
                ro.* = .omitted;
            } else {
                ro.* = @enumFromInt(i);
            }
        }

        const RuntimeOrder = InternPool.LoadedStructType.RuntimeOrder;

        const AlignSortContext = struct {
            aligns: []const Alignment,

            fn less_than(ctx: @This(), a: RuntimeOrder, b: RuntimeOrder) bool {
                if (a == .omitted) return false;
                if (b == .omitted) return true;
                const a_align = ctx.aligns[@int_from_enum(a)];
                const b_align = ctx.aligns[@int_from_enum(b)];
                return a_align.compare(.gt, b_align);
            }
        };
        if (struct_type.is_tuple(ip) or !mod.backend_supports_feature(.field_reordering)) {
            // TODO: don't handle tuples differently. This logic exists only because it
            // uncovers latent bugs if removed. Fix the latent bugs and remove this logic!
            // Likewise, implement field reordering support in all the backends!
            // This logic does not reorder fields; it only moves the omitted ones to the end
            // so that logic elsewhere does not need to special-case tuples.
            var i: usize = 0;
            var off: usize = 0;
            while (i + off < runtime_order.len) {
                if (runtime_order[i + off] == .omitted) {
                    off += 1;
                    continue;
                }
                runtime_order[i] = runtime_order[i + off];
                i += 1;
            }
            @memset(runtime_order[i..], .omitted);
        } else {
            mem.sort_unstable(RuntimeOrder, runtime_order, AlignSortContext{
                .aligns = aligns,
            }, AlignSortContext.less_than);
        }
    }

    // Calculate size, alignment, and field offsets.
    const offsets = struct_type.offsets.get(ip);
    var it = struct_type.iterate_runtime_order(ip);
    var offset: u64 = 0;
    while (it.next()) |i| {
        offsets[i] = @int_cast(aligns[i].forward(offset));
        offset = offsets[i] + sizes[i];
    }
    struct_type.size(ip).* = @int_cast(big_align.forward(offset));
    const flags = struct_type.flags_ptr(ip);
    flags.alignment = big_align;
    flags.layout_resolved = true;
    _ = try sema.type_requires_comptime(ty);
}

fn sema_backing_int_type(mod: *Module, struct_type: InternPool.LoadedStructType) CompileError!void {
    const gpa = mod.gpa;
    const ip = &mod.intern_pool;

    const decl_index = struct_type.decl.unwrap().?;
    const decl = mod.decl_ptr(decl_index);

    const zir = mod.namespace_ptr(struct_type.namespace.unwrap().?).file_scope.zir;

    var analysis_arena = std.heap.ArenaAllocator.init(gpa);
    defer analysis_arena.deinit();

    var comptime_err_ret_trace = std.ArrayList(Module.SrcLoc).init(gpa);
    defer comptime_err_ret_trace.deinit();

    var sema: Sema = .{
        .mod = mod,
        .gpa = gpa,
        .arena = analysis_arena.allocator(),
        .code = zir,
        .owner_decl = decl,
        .owner_decl_index = decl_index,
        .func_index = .none,
        .func_is_naked = false,
        .fn_ret_ty = Type.void,
        .fn_ret_ty_ies = null,
        .owner_func_index = .none,
        .comptime_err_ret_trace = &comptime_err_ret_trace,
    };
    defer sema.deinit();

    var block: Block = .{
        .parent = null,
        .sema = &sema,
        .src_decl = decl_index,
        .namespace = struct_type.namespace.unwrap() orelse decl.src_namespace,
        .instructions = .{},
        .inlining = null,
        .is_comptime = true,
    };
    defer assert(block.instructions.items.len == 0);

    const fields_bit_sum = blk: {
        var accumulator: u64 = 0;
        for (0..struct_type.field_types.len) |i| {
            const field_ty = Type.from_interned(struct_type.field_types.get(ip)[i]);
            accumulator += try field_ty.bit_size_advanced(mod, &sema);
        }
        break :blk accumulator;
    };

    const zir_index = struct_type.zir_index.unwrap().?.resolve(ip);
    const extended = zir.instructions.items(.data)[@int_from_enum(zir_index)].extended;
    assert(extended.opcode == .struct_decl);
    const small: Zir.Inst.StructDecl.Small = @bit_cast(extended.small);

    if (small.has_backing_int) {
        var extra_index: usize = extended.operand + @typeInfo(Zir.Inst.StructDecl).Struct.fields.len;
        const captures_len = if (small.has_captures_len) blk: {
            const captures_len = zir.extra[extra_index];
            extra_index += 1;
            break :blk captures_len;
        } else 0;
        extra_index += @int_from_bool(small.has_fields_len);
        extra_index += @int_from_bool(small.has_decls_len);

        extra_index += captures_len;

        const backing_int_body_len = zir.extra[extra_index];
        extra_index += 1;

        const backing_int_src: LazySrcLoc = .{ .node_offset_container_tag = 0 };
        const backing_int_ty = blk: {
            if (backing_int_body_len == 0) {
                const backing_int_ref: Zir.Inst.Ref = @enumFromInt(zir.extra[extra_index]);
                break :blk try sema.resolve_type(&block, backing_int_src, backing_int_ref);
            } else {
                const body = zir.body_slice(extra_index, backing_int_body_len);
                const ty_ref = try sema.resolve_inline_body(&block, body, zir_index);
                break :blk try sema.analyze_as_type(&block, backing_int_src, ty_ref);
            }
        };

        try sema.check_backing_int_type(&block, backing_int_src, backing_int_ty, fields_bit_sum);
        struct_type.backing_int_type(ip).* = backing_int_ty.to_intern();
    } else {
        if (fields_bit_sum > std.math.max_int(u16)) {
            return sema.fail(&block, LazySrcLoc.nodeOffset(0), "size of packed struct '{d}' exceeds maximum bit width of 65535", .{fields_bit_sum});
        }
        const backing_int_ty = try mod.int_type(.unsigned, @int_cast(fields_bit_sum));
        struct_type.backing_int_type(ip).* = backing_int_ty.to_intern();
    }
}

fn check_backing_int_type(sema: *Sema, block: *Block, src: LazySrcLoc, backing_int_ty: Type, fields_bit_sum: u64) CompileError!void {
    const mod = sema.mod;

    if (!backing_int_ty.is_int(mod)) {
        return sema.fail(block, src, "expected backing integer type, found '{}'", .{backing_int_ty.fmt(sema.mod)});
    }
    if (backing_int_ty.bit_size(mod) != fields_bit_sum) {
        return sema.fail(
            block,
            src,
            "backing integer type '{}' has bit size {} but the struct fields have a total bit size of {}",
            .{ backing_int_ty.fmt(sema.mod), backing_int_ty.bit_size(mod), fields_bit_sum },
        );
    }
}

fn check_indexable(sema: *Sema, block: *Block, src: LazySrcLoc, ty: Type) !void {
    const mod = sema.mod;
    if (!ty.is_indexable(mod)) {
        const msg = msg: {
            const msg = try sema.err_msg(block, src, "type '{}' does not support indexing", .{ty.fmt(sema.mod)});
            errdefer msg.destroy(sema.gpa);
            try sema.err_note(block, src, msg, "operand must be an array, slice, tuple, or vector", .{});
            break :msg msg;
        };
        return sema.fail_with_owned_error_msg(block, msg);
    }
}

fn check_mem_operand(sema: *Sema, block: *Block, src: LazySrcLoc, ty: Type) !void {
    const mod = sema.mod;
    if (ty.zig_type_tag(mod) == .Pointer) {
        switch (ty.ptr_size(mod)) {
            .Slice, .Many, .C => return,
            .One => {
                const elem_ty = ty.child_type(mod);
                if (elem_ty.zig_type_tag(mod) == .Array) return;
                // TODO https://github.com/ziglang/zig/issues/15479
                // if (elem_ty.is_tuple()) return;
            },
        }
    }
    const msg = msg: {
        const msg = try sema.err_msg(block, src, "type '{}' is not an indexable pointer", .{ty.fmt(sema.mod)});
        errdefer msg.destroy(sema.gpa);
        try sema.err_note(block, src, msg, "operand must be a slice, a many pointer or a pointer to an array", .{});
        break :msg msg;
    };
    return sema.fail_with_owned_error_msg(block, msg);
}

/// Resolve a unions's alignment only without triggering resolution of its layout.
/// Asserts that the alignment is not yet resolved.
pub fn resolve_union_alignment(
    sema: *Sema,
    ty: Type,
    union_type: InternPool.LoadedUnionType,
) CompileError!Alignment {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const target = mod.get_target();

    assert(!union_type.have_layout(ip));

    if (union_type.flags_ptr(ip).status == .field_types_wip) {
        // We'll guess "pointer-aligned", if the union has an
        // underaligned pointer field then some allocations
        // might require explicit alignment.
        union_type.flags_ptr(ip).assumed_pointer_aligned = true;
        const result = Alignment.from_byte_units(@div_exact(target.ptr_bit_width(), 8));
        union_type.flags_ptr(ip).alignment = result;
        return result;
    }

    try sema.resolve_type_fields_union(ty, union_type);

    var max_align: Alignment = .@"1";
    for (0..union_type.field_types.len) |field_index| {
        const field_ty = Type.from_interned(union_type.field_types.get(ip)[field_index]);
        if (!(try sema.type_has_runtime_bits(field_ty))) continue;

        const explicit_align = union_type.field_align(ip, field_index);
        const field_align = if (explicit_align != .none)
            explicit_align
        else
            try sema.type_abi_alignment(field_ty);

        max_align = max_align.max(field_align);
    }

    union_type.flags_ptr(ip).alignment = max_align;
    return max_align;
}

/// This logic must be kept in sync with `Module.get_union_layout`.
fn resolve_union_layout(sema: *Sema, ty: Type) CompileError!void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;

    try sema.resolve_type_fields_union(ty, ip.load_union_type(ty.ip_index));

    // Load again, since the tag type might have changed due to resolution.
    const union_type = ip.load_union_type(ty.ip_index);

    switch (union_type.flags_ptr(ip).status) {
        .none, .have_field_types => {},
        .field_types_wip, .layout_wip => {
            const msg = try Module.ErrorMsg.create(
                sema.gpa,
                mod.decl_ptr(union_type.decl).src_loc(mod),
                "union '{}' depends on itself",
                .{ty.fmt(mod)},
            );
            return sema.fail_with_owned_error_msg(null, msg);
        },
        .have_layout, .fully_resolved_wip, .fully_resolved => return,
    }

    const prev_status = union_type.flags_ptr(ip).status;
    errdefer if (union_type.flags_ptr(ip).status == .layout_wip) {
        union_type.flags_ptr(ip).status = prev_status;
    };

    union_type.flags_ptr(ip).status = .layout_wip;

    var max_size: u64 = 0;
    var max_align: Alignment = .@"1";
    for (0..union_type.field_types.len) |field_index| {
        const field_ty = Type.from_interned(union_type.field_types.get(ip)[field_index]);

        if (try sema.type_requires_comptime(field_ty) or field_ty.zig_type_tag(mod) == .NoReturn) continue; // TODO: should this affect alignment?

        max_size = @max(max_size, sema.type_abi_size(field_ty) catch |err| switch (err) {
            error.AnalysisFail => {
                const msg = sema.err orelse return err;
                try sema.add_field_err_note(ty, field_index, msg, "while checking this field", .{});
                return err;
            },
            else => return err,
        });

        const explicit_align = union_type.field_align(ip, field_index);
        const field_align = if (explicit_align != .none)
            explicit_align
        else
            try sema.type_abi_alignment(field_ty);

        max_align = max_align.max(field_align);
    }

    const flags = union_type.flags_ptr(ip);
    const has_runtime_tag = flags.runtime_tag.has_tag() and try sema.type_has_runtime_bits(Type.from_interned(union_type.enum_tag_ty));
    const size, const alignment, const padding = if (has_runtime_tag) layout: {
        const enum_tag_type = Type.from_interned(union_type.enum_tag_ty);
        const tag_align = try sema.type_abi_alignment(enum_tag_type);
        const tag_size = try sema.type_abi_size(enum_tag_type);

        // Put the tag before or after the payload depending on which one's
        // alignment is greater.
        var size: u64 = 0;
        var padding: u32 = 0;
        if (tag_align.order(max_align).compare(.gte)) {
            // {Tag, Payload}
            size += tag_size;
            size = max_align.forward(size);
            size += max_size;
            const prev_size = size;
            size = tag_align.forward(size);
            padding = @int_cast(size - prev_size);
        } else {
            // {Payload, Tag}
            size += max_size;
            size = switch (mod.get_target().ofmt) {
                .c => max_align,
                else => tag_align,
            }.forward(size);
            size += tag_size;
            const prev_size = size;
            size = max_align.forward(size);
            padding = @int_cast(size - prev_size);
        }

        break :layout .{ size, max_align.max(tag_align), padding };
    } else .{ max_align.forward(max_size), max_align, 0 };

    union_type.size(ip).* = @int_cast(size);
    union_type.padding(ip).* = padding;
    flags.alignment = alignment;
    flags.status = .have_layout;

    if (union_type.flags_ptr(ip).assumed_runtime_bits and !(try sema.type_has_runtime_bits(ty))) {
        const msg = try Module.ErrorMsg.create(
            sema.gpa,
            mod.decl_ptr(union_type.decl).src_loc(mod),
            "union layout depends on it having runtime bits",
            .{},
        );
        return sema.fail_with_owned_error_msg(null, msg);
    }

    if (union_type.flags_ptr(ip).assumed_pointer_aligned and
        alignment.compare_strict(.neq, Alignment.from_byte_units(@div_exact(mod.get_target().ptr_bit_width(), 8))))
    {
        const msg = try Module.ErrorMsg.create(
            sema.gpa,
            mod.decl_ptr(union_type.decl).src_loc(mod),
            "union layout depends on being pointer aligned",
            .{},
        );
        return sema.fail_with_owned_error_msg(null, msg);
    }
}

/// Returns `error.AnalysisFail` if any of the types (recursively) failed to
/// be resolved.
pub fn resolve_type_fully(sema: *Sema, ty: Type) CompileError!void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    switch (ty.zig_type_tag(mod)) {
        .Pointer => {
            return sema.resolve_type_fully(ty.child_type(mod));
        },
        .Struct => switch (mod.intern_pool.index_to_key(ty.to_intern())) {
            .struct_type => try sema.resolve_struct_fully(ty),
            .anon_struct_type => |tuple| {
                for (tuple.types.get(ip)) |field_ty| {
                    try sema.resolve_type_fully(Type.from_interned(field_ty));
                }
            },
            .simple_type => |simple_type| try sema.resolve_simple_type(simple_type),
            else => {},
        },
        .Union => return sema.resolve_union_fully(ty),
        .Array => return sema.resolve_type_fully(ty.child_type(mod)),
        .Optional => {
            return sema.resolve_type_fully(ty.optional_child(mod));
        },
        .ErrorUnion => return sema.resolve_type_fully(ty.error_union_payload(mod)),
        .Fn => {
            const info = mod.type_to_func(ty).?;
            if (info.is_generic) {
                // Resolving of generic function types is deferred to when
                // the function is instantiated.
                return;
            }
            for (0..info.param_types.len) |i| {
                const param_ty = info.param_types.get(ip)[i];
                try sema.resolve_type_fully(Type.from_interned(param_ty));
            }
            try sema.resolve_type_fully(Type.from_interned(info.return_type));
        },
        else => {},
    }
}

fn resolve_struct_fully(sema: *Sema, ty: Type) CompileError!void {
    try sema.resolve_struct_layout(ty);

    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const struct_type = mod.type_to_struct(ty).?;

    if (struct_type.set_fully_resolved(ip)) return;
    errdefer struct_type.clear_fully_resolved(ip);

    // After we have resolve struct layout we have to go over the fields again to
    // make sure pointer fields get their child types resolved as well.
    // See also similar code for unions.

    for (0..struct_type.field_types.len) |i| {
        const field_ty = Type.from_interned(struct_type.field_types.get(ip)[i]);
        try sema.resolve_type_fully(field_ty);
    }
}

fn resolve_union_fully(sema: *Sema, ty: Type) CompileError!void {
    try sema.resolve_union_layout(ty);

    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const union_obj = mod.type_to_union(ty).?;
    switch (union_obj.flags_ptr(ip).status) {
        .none, .have_field_types, .field_types_wip, .layout_wip, .have_layout => {},
        .fully_resolved_wip, .fully_resolved => return,
    }

    {
        // After we have resolve union layout we have to go over the fields again to
        // make sure pointer fields get their child types resolved as well.
        // See also similar code for structs.
        const prev_status = union_obj.flags_ptr(ip).status;
        errdefer union_obj.flags_ptr(ip).status = prev_status;

        union_obj.flags_ptr(ip).status = .fully_resolved_wip;
        for (0..union_obj.field_types.len) |field_index| {
            const field_ty = Type.from_interned(union_obj.field_types.get(ip)[field_index]);
            try sema.resolve_type_fully(field_ty);
        }
        union_obj.flags_ptr(ip).status = .fully_resolved;
    }

    // And let's not forget comptime-only status.
    _ = try sema.type_requires_comptime(ty);
}

pub fn resolve_type_fields(sema: *Sema, ty: Type) CompileError!void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const ty_ip = ty.to_intern();

    switch (ty_ip) {
        .none => unreachable,

        .u0_type,
        .i0_type,
        .u1_type,
        .u8_type,
        .i8_type,
        .u16_type,
        .i16_type,
        .u29_type,
        .u32_type,
        .i32_type,
        .u64_type,
        .i64_type,
        .u80_type,
        .u128_type,
        .i128_type,
        .usize_type,
        .isize_type,
        .c_char_type,
        .c_short_type,
        .c_ushort_type,
        .c_int_type,
        .c_uint_type,
        .c_long_type,
        .c_ulong_type,
        .c_longlong_type,
        .c_ulonglong_type,
        .c_longdouble_type,
        .f16_type,
        .f32_type,
        .f64_type,
        .f80_type,
        .f128_type,
        .anyopaque_type,
        .bool_type,
        .void_type,
        .type_type,
        .anyerror_type,
        .adhoc_inferred_error_set_type,
        .comptime_int_type,
        .comptime_float_type,
        .noreturn_type,
        .anyframe_type,
        .null_type,
        .undefined_type,
        .enum_literal_type,
        .manyptr_u8_type,
        .manyptr_const_u8_type,
        .manyptr_const_u8_sentinel_0_type,
        .single_const_pointer_to_comptime_int_type,
        .slice_const_u8_type,
        .slice_const_u8_sentinel_0_type,
        .optional_noreturn_type,
        .anyerror_void_error_union_type,
        .generic_poison_type,
        .empty_struct_type,
        => {},

        .undef => unreachable,
        .zero => unreachable,
        .zero_usize => unreachable,
        .zero_u8 => unreachable,
        .one => unreachable,
        .one_usize => unreachable,
        .one_u8 => unreachable,
        .four_u8 => unreachable,
        .negative_one => unreachable,
        .calling_convention_c => unreachable,
        .calling_convention_inline => unreachable,
        .void_value => unreachable,
        .unreachable_value => unreachable,
        .null_value => unreachable,
        .bool_true => unreachable,
        .bool_false => unreachable,
        .empty_struct => unreachable,
        .generic_poison => unreachable,

        else => switch (ip.items.items(.tag)[@int_from_enum(ty_ip)]) {
            .type_struct,
            .type_struct_packed,
            .type_struct_packed_inits,
            => try sema.resolve_type_fields_struct(ty_ip, ip.load_struct_type(ty_ip)),

            .type_union => try sema.resolve_type_fields_union(Type.from_interned(ty_ip), ip.load_union_type(ty_ip)),
            .simple_type => try sema.resolve_simple_type(ip.index_to_key(ty_ip).simple_type),
            else => {},
        },
    }
}

/// Fully resolves a simple type. This is usually a nop, but for builtin types with
/// special InternPool indices (such as std.builtin.Type) it will analyze and fully
/// resolve the container type.
fn resolve_simple_type(sema: *Sema, simple_type: InternPool.SimpleType) CompileError!void {
    const builtin_type_name: []const u8 = switch (simple_type) {
        .atomic_order => "AtomicOrder",
        .atomic_rmw_op => "AtomicRmwOp",
        .calling_convention => "CallingConvention",
        .address_space => "AddressSpace",
        .float_mode => "FloatMode",
        .reduce_op => "ReduceOp",
        .call_modifier => "CallModifer",
        .prefetch_options => "PrefetchOptions",
        .export_options => "ExportOptions",
        .extern_options => "ExternOptions",
        .type_info => "Type",
        else => return,
    };
    // This will fully resolve the type.
    _ = try sema.get_builtin_type(builtin_type_name);
}

pub fn resolve_type_fields_struct(
    sema: *Sema,
    ty: InternPool.Index,
    struct_type: InternPool.LoadedStructType,
) CompileError!void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    // If there is no owner decl it means the struct has no fields.
    const owner_decl = struct_type.decl.unwrap() orelse return;

    switch (mod.decl_ptr(owner_decl).analysis) {
        .file_failure,
        .dependency_failure,
        .sema_failure,
        => {
            sema.owner_decl.analysis = .dependency_failure;
            return error.AnalysisFail;
        },
        else => {},
    }

    if (struct_type.have_field_types(ip)) return;

    if (struct_type.set_types_wip(ip)) {
        const msg = try Module.ErrorMsg.create(
            sema.gpa,
            mod.decl_ptr(owner_decl).src_loc(mod),
            "struct '{}' depends on itself",
            .{Type.from_interned(ty).fmt(mod)},
        );
        return sema.fail_with_owned_error_msg(null, msg);
    }
    defer struct_type.clear_types_wip(ip);

    sema_struct_fields(mod, sema.arena, struct_type) catch |err| switch (err) {
        error.AnalysisFail => {
            if (mod.decl_ptr(owner_decl).analysis == .complete) {
                mod.decl_ptr(owner_decl).analysis = .dependency_failure;
            }
            return error.AnalysisFail;
        },
        else => |e| return e,
    };
}

pub fn resolve_struct_field_inits(sema: *Sema, ty: Type) CompileError!void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const struct_type = mod.type_to_struct(ty) orelse return;
    const owner_decl = struct_type.decl.unwrap() orelse return;

    // Inits can start as resolved
    if (struct_type.have_field_inits(ip)) return;

    try sema.resolve_struct_layout(ty);

    if (struct_type.set_inits_wip(ip)) {
        const msg = try Module.ErrorMsg.create(
            sema.gpa,
            mod.decl_ptr(owner_decl).src_loc(mod),
            "struct '{}' depends on itself",
            .{ty.fmt(mod)},
        );
        return sema.fail_with_owned_error_msg(null, msg);
    }
    defer struct_type.clear_inits_wip(ip);

    sema_struct_field_inits(mod, sema.arena, struct_type) catch |err| switch (err) {
        error.AnalysisFail => {
            if (mod.decl_ptr(owner_decl).analysis == .complete) {
                mod.decl_ptr(owner_decl).analysis = .dependency_failure;
            }
            return error.AnalysisFail;
        },
        else => |e| return e,
    };
    struct_type.set_have_field_inits(ip);
}

pub fn resolve_type_fields_union(sema: *Sema, ty: Type, union_type: InternPool.LoadedUnionType) CompileError!void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const owner_decl = mod.decl_ptr(union_type.decl);
    switch (owner_decl.analysis) {
        .file_failure,
        .dependency_failure,
        .sema_failure,
        => {
            sema.owner_decl.analysis = .dependency_failure;
            return error.AnalysisFail;
        },
        else => {},
    }
    switch (union_type.flags_ptr(ip).status) {
        .none => {},
        .field_types_wip => {
            const msg = try Module.ErrorMsg.create(
                sema.gpa,
                owner_decl.src_loc(mod),
                "union '{}' depends on itself",
                .{ty.fmt(mod)},
            );
            return sema.fail_with_owned_error_msg(null, msg);
        },
        .have_field_types,
        .have_layout,
        .layout_wip,
        .fully_resolved_wip,
        .fully_resolved,
        => return,
    }

    union_type.flags_ptr(ip).status = .field_types_wip;
    errdefer union_type.flags_ptr(ip).status = .none;
    sema_union_fields(mod, sema.arena, union_type) catch |err| switch (err) {
        error.AnalysisFail => {
            if (owner_decl.analysis == .complete) {
                owner_decl.analysis = .dependency_failure;
            }
            return error.AnalysisFail;
        },
        else => |e| return e,
    };
    union_type.flags_ptr(ip).status = .have_field_types;
}

/// Returns a normal error set corresponding to the fully populated inferred
/// error set.
fn resolve_inferred_error_set(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    ies_index: InternPool.Index,
) CompileError!InternPool.Index {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const func_index = ip.ies_func_index(ies_index);
    const func = mod.func_info(func_index);

    try sema.declare_dependency(.{ .func_ies = func_index });

    // TODO: during an incremental update this might not be `.none`, but the
    // function might be out-of-date!
    const resolved_ty = func.resolved_error_set(ip).*;
    if (resolved_ty != .none) return resolved_ty;

    if (func.analysis(ip).state == .in_progress)
        return sema.fail(block, src, "unable to resolve inferred error set", .{});

    // In order to ensure that all dependencies are properly added to the set,
    // we need to ensure the function body is analyzed of the inferred error
    // set. However, in the case of comptime/inline function calls with
    // inferred error sets, each call gets an adhoc InferredErrorSet object, which
    // has no corresponding function body.
    const ies_func_owner_decl = mod.decl_ptr(func.owner_decl);
    const ies_func_info = mod.type_to_func(ies_func_owner_decl.type_of(mod)).?;
    // if ies declared by a inline function with generic return type, the return_type should be generic_poison,
    // because inline function does not create a new declaration, and the ies has been filled with analyze_call,
    // so here we can simply skip this case.
    if (ies_func_info.return_type == .generic_poison_type) {
        assert(ies_func_info.cc == .Inline);
    } else if (ip.error_union_set(ies_func_info.return_type) == ies_index) {
        if (ies_func_info.is_generic) {
            const msg = msg: {
                const msg = try sema.err_msg(block, src, "unable to resolve inferred error set of generic function", .{});
                errdefer msg.destroy(sema.gpa);

                try sema.mod.err_note_non_lazy(ies_func_owner_decl.src_loc(mod), msg, "generic function declared here", .{});
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(block, msg);
        }
        // In this case we are dealing with the actual InferredErrorSet object that
        // corresponds to the function, not one created to track an inline/comptime call.
        try sema.ensure_func_body_analyzed(func_index);
    }

    // This will now have been resolved by the logic at the end of `Module.analyze_fn_body`
    // which calls `resolve_inferred_error_set_ptr`.
    const final_resolved_ty = func.resolved_error_set(ip).*;
    assert(final_resolved_ty != .none);
    return final_resolved_ty;
}

pub fn resolve_inferred_error_set_ptr(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    ies: *InferredErrorSet,
) CompileError!void {
    const mod = sema.mod;
    const ip = &mod.intern_pool;

    if (ies.resolved != .none) return;

    const ies_index = ip.error_union_set(sema.fn_ret_ty.to_intern());

    for (ies.inferred_error_sets.keys()) |other_ies_index| {
        if (ies_index == other_ies_index) continue;
        switch (try sema.resolve_inferred_error_set(block, src, other_ies_index)) {
            .anyerror_type => {
                ies.resolved = .anyerror_type;
                return;
            },
            else => |error_set_ty_index| {
                const names = ip.index_to_key(error_set_ty_index).error_set_type.names;
                for (names.get(ip)) |name| {
                    try ies.errors.put(sema.arena, name, {});
                }
            },
        }
    }

    const resolved_error_set_ty = try mod.error_set_from_unsorted_names(ies.errors.keys());
    ies.resolved = resolved_error_set_ty.to_intern();
}

fn resolve_ad_hoc_inferred_error_set(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    value: InternPool.Index,
) CompileError!InternPool.Index {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;
    const new_ty = try resolve_ad_hoc_inferred_error_set_ty(sema, block, src, ip.type_of(value));
    if (new_ty == .none) return value;
    return ip.get_coerced(gpa, value, new_ty);
}

fn resolve_ad_hoc_inferred_error_set_ty(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    ty: InternPool.Index,
) CompileError!InternPool.Index {
    const ies = sema.fn_ret_ty_ies orelse return .none;
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;
    const error_union_info = switch (ip.index_to_key(ty)) {
        .error_union_type => |x| x,
        else => return .none,
    };
    if (error_union_info.error_set_type != .adhoc_inferred_error_set_type)
        return .none;

    try sema.resolve_inferred_error_set_ptr(block, src, ies);
    const new_ty = try ip.get(gpa, .{ .error_union_type = .{
        .error_set_type = ies.resolved,
        .payload_type = error_union_info.payload_type,
    } });
    return new_ty;
}

fn resolve_inferred_error_set_ty(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    ty: InternPool.Index,
) CompileError!InternPool.Index {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    if (ty == .anyerror_type) return ty;
    switch (ip.index_to_key(ty)) {
        .error_set_type => return ty,
        .inferred_error_set_type => return sema.resolve_inferred_error_set(block, src, ty),
        else => unreachable,
    }
}

fn struct_zir_info(zir: Zir, zir_index: Zir.Inst.Index) struct {
    /// fields_len
    usize,
    Zir.Inst.StructDecl.Small,
    /// extra_index
    usize,
} {
    const extended = zir.instructions.items(.data)[@int_from_enum(zir_index)].extended;
    assert(extended.opcode == .struct_decl);
    const small: Zir.Inst.StructDecl.Small = @bit_cast(extended.small);
    var extra_index: usize = extended.operand + @typeInfo(Zir.Inst.StructDecl).Struct.fields.len;

    const captures_len = if (small.has_captures_len) blk: {
        const captures_len = zir.extra[extra_index];
        extra_index += 1;
        break :blk captures_len;
    } else 0;

    const fields_len = if (small.has_fields_len) blk: {
        const fields_len = zir.extra[extra_index];
        extra_index += 1;
        break :blk fields_len;
    } else 0;

    const decls_len = if (small.has_decls_len) decls_len: {
        const decls_len = zir.extra[extra_index];
        extra_index += 1;
        break :decls_len decls_len;
    } else 0;

    extra_index += captures_len;

    // The backing integer cannot be handled until `resolve_struct_layout()`.
    if (small.has_backing_int) {
        const backing_int_body_len = zir.extra[extra_index];
        extra_index += 1; // backing_int_body_len
        if (backing_int_body_len == 0) {
            extra_index += 1; // backing_int_ref
        } else {
            extra_index += backing_int_body_len; // backing_int_body_inst
        }
    }

    // Skip over decls.
    extra_index += decls_len;

    return .{ fields_len, small, extra_index };
}

fn sema_struct_fields(
    mod: *Module,
    arena: Allocator,
    struct_type: InternPool.LoadedStructType,
) CompileError!void {
    const gpa = mod.gpa;
    const ip = &mod.intern_pool;
    const decl_index = struct_type.decl.unwrap() orelse return;
    const decl = mod.decl_ptr(decl_index);
    const namespace_index = struct_type.namespace.unwrap() orelse decl.src_namespace;
    const zir = mod.namespace_ptr(namespace_index).file_scope.zir;
    const zir_index = struct_type.zir_index.unwrap().?.resolve(ip);

    const fields_len, const small, var extra_index = struct_zir_info(zir, zir_index);

    if (fields_len == 0) switch (struct_type.layout) {
        .@"packed" => {
            try sema_backing_int_type(mod, struct_type);
            return;
        },
        .auto, .@"extern" => {
            struct_type.size(ip).* = 0;
            struct_type.flags_ptr(ip).layout_resolved = true;
            return;
        },
    };

    var comptime_err_ret_trace = std.ArrayList(Module.SrcLoc).init(gpa);
    defer comptime_err_ret_trace.deinit();

    var sema: Sema = .{
        .mod = mod,
        .gpa = gpa,
        .arena = arena,
        .code = zir,
        .owner_decl = decl,
        .owner_decl_index = decl_index,
        .func_index = .none,
        .func_is_naked = false,
        .fn_ret_ty = Type.void,
        .fn_ret_ty_ies = null,
        .owner_func_index = .none,
        .comptime_err_ret_trace = &comptime_err_ret_trace,
    };
    defer sema.deinit();

    var block_scope: Block = .{
        .parent = null,
        .sema = &sema,
        .src_decl = decl_index,
        .namespace = namespace_index,
        .instructions = .{},
        .inlining = null,
        .is_comptime = true,
    };
    defer assert(block_scope.instructions.items.len == 0);

    const Field = struct {
        type_body_len: u32 = 0,
        align_body_len: u32 = 0,
        init_body_len: u32 = 0,
        type_ref: Zir.Inst.Ref = .none,
    };
    const fields = try sema.arena.alloc(Field, fields_len);

    var any_inits = false;
    var any_aligned = false;

    {
        const bits_per_field = 4;
        const fields_per_u32 = 32 / bits_per_field;
        const bit_bags_count = std.math.div_ceil(usize, fields_len, fields_per_u32) catch unreachable;
        const flags_index = extra_index;
        var bit_bag_index: usize = flags_index;
        extra_index += bit_bags_count;
        var cur_bit_bag: u32 = undefined;
        var field_i: u32 = 0;
        while (field_i < fields_len) : (field_i += 1) {
            if (field_i % fields_per_u32 == 0) {
                cur_bit_bag = zir.extra[bit_bag_index];
                bit_bag_index += 1;
            }
            const has_align = @as(u1, @truncate(cur_bit_bag)) != 0;
            cur_bit_bag >>= 1;
            const has_init = @as(u1, @truncate(cur_bit_bag)) != 0;
            cur_bit_bag >>= 1;
            const is_comptime = @as(u1, @truncate(cur_bit_bag)) != 0;
            cur_bit_bag >>= 1;
            const has_type_body = @as(u1, @truncate(cur_bit_bag)) != 0;
            cur_bit_bag >>= 1;

            if (is_comptime) struct_type.set_field_comptime(ip, field_i);

            var opt_field_name_zir: ?[:0]const u8 = null;
            if (!small.is_tuple) {
                opt_field_name_zir = zir.null_terminated_string(@enumFromInt(zir.extra[extra_index]));
                extra_index += 1;
            }
            extra_index += 1; // doc_comment

            fields[field_i] = .{};

            if (has_type_body) {
                fields[field_i].type_body_len = zir.extra[extra_index];
            } else {
                fields[field_i].type_ref = @enumFromInt(zir.extra[extra_index]);
            }
            extra_index += 1;

            // This string needs to outlive the ZIR code.
            if (opt_field_name_zir) |field_name_zir| {
                const field_name = try ip.get_or_put_string(gpa, field_name_zir, .no_embedded_nulls);
                assert(struct_type.add_field_name(ip, field_name) == null);
            }

            if (has_align) {
                fields[field_i].align_body_len = zir.extra[extra_index];
                extra_index += 1;
                any_aligned = true;
            }
            if (has_init) {
                fields[field_i].init_body_len = zir.extra[extra_index];
                extra_index += 1;
                any_inits = true;
            }
        }
    }

    // Next we do only types and alignments, saving the inits for a second pass,
    // so that init values may depend on type layout.

    for (fields, 0..) |zir_field, field_i| {
        const field_ty: Type = ty: {
            if (zir_field.type_ref != .none) {
                break :ty sema.resolve_type(&block_scope, .unneeded, zir_field.type_ref) catch |err| switch (err) {
                    error.NeededSourceLocation => {
                        const ty_src = mod.field_src_loc(decl_index, .{
                            .index = field_i,
                            .range = .type,
                        }).lazy;
                        _ = try sema.resolve_type(&block_scope, ty_src, zir_field.type_ref);
                        unreachable;
                    },
                    else => |e| return e,
                };
            }
            assert(zir_field.type_body_len != 0);
            const body = zir.body_slice(extra_index, zir_field.type_body_len);
            extra_index += body.len;
            const ty_ref = try sema.resolve_inline_body(&block_scope, body, zir_index);
            break :ty sema.analyze_as_type(&block_scope, .unneeded, ty_ref) catch |err| switch (err) {
                error.NeededSourceLocation => {
                    const ty_src = mod.field_src_loc(decl_index, .{
                        .index = field_i,
                        .range = .type,
                    }).lazy;
                    _ = try sema.analyze_as_type(&block_scope, ty_src, ty_ref);
                    unreachable;
                },
                else => |e| return e,
            };
        };
        if (field_ty.is_generic_poison()) {
            return error.GenericPoison;
        }

        struct_type.field_types.get(ip)[field_i] = field_ty.to_intern();

        if (field_ty.zig_type_tag(mod) == .Opaque) {
            const msg = msg: {
                const ty_src = mod.field_src_loc(decl_index, .{
                    .index = field_i,
                    .range = .type,
                }).lazy;
                const msg = try sema.err_msg(&block_scope, ty_src, "opaque types have unknown size and therefore cannot be directly embedded in structs", .{});
                errdefer msg.destroy(sema.gpa);

                try sema.add_declared_here_note(msg, field_ty);
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(&block_scope, msg);
        }
        if (field_ty.zig_type_tag(mod) == .NoReturn) {
            const msg = msg: {
                const ty_src = mod.field_src_loc(decl_index, .{
                    .index = field_i,
                    .range = .type,
                }).lazy;
                const msg = try sema.err_msg(&block_scope, ty_src, "struct fields cannot be 'noreturn'", .{});
                errdefer msg.destroy(sema.gpa);

                try sema.add_declared_here_note(msg, field_ty);
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(&block_scope, msg);
        }
        switch (struct_type.layout) {
            .@"extern" => if (!try sema.validate_extern_type(field_ty, .struct_field)) {
                const msg = msg: {
                    const ty_src = mod.field_src_loc(decl_index, .{
                        .index = field_i,
                        .range = .type,
                    });
                    const msg = try sema.err_msg(&block_scope, ty_src.lazy, "extern structs cannot contain fields of type '{}'", .{field_ty.fmt(mod)});
                    errdefer msg.destroy(sema.gpa);

                    try sema.explain_why_type_is_not_extern(msg, ty_src, field_ty, .struct_field);

                    try sema.add_declared_here_note(msg, field_ty);
                    break :msg msg;
                };
                return sema.fail_with_owned_error_msg(&block_scope, msg);
            },
            .@"packed" => if (!try sema.validate_packed_type(field_ty)) {
                const msg = msg: {
                    const ty_src = mod.field_src_loc(decl_index, .{
                        .index = field_i,
                        .range = .type,
                    });
                    const msg = try sema.err_msg(&block_scope, ty_src.lazy, "packed structs cannot contain fields of type '{}'", .{field_ty.fmt(mod)});
                    errdefer msg.destroy(sema.gpa);

                    try sema.explain_why_type_is_not_packed(msg, ty_src, field_ty);

                    try sema.add_declared_here_note(msg, field_ty);
                    break :msg msg;
                };
                return sema.fail_with_owned_error_msg(&block_scope, msg);
            },
            else => {},
        }

        if (zir_field.align_body_len > 0) {
            const body = zir.body_slice(extra_index, zir_field.align_body_len);
            extra_index += body.len;
            const align_ref = try sema.resolve_inline_body(&block_scope, body, zir_index);
            const field_align = sema.analyze_as_align(&block_scope, .unneeded, align_ref) catch |err| switch (err) {
                error.NeededSourceLocation => {
                    const align_src = mod.field_src_loc(decl_index, .{
                        .index = field_i,
                        .range = .alignment,
                    }).lazy;
                    _ = try sema.analyze_as_align(&block_scope, align_src, align_ref);
                    unreachable;
                },
                else => |e| return e,
            };
            struct_type.field_aligns.get(ip)[field_i] = field_align;
        }

        extra_index += zir_field.init_body_len;
    }

    struct_type.clear_types_wip(ip);
    if (!any_inits) struct_type.set_have_field_inits(ip);
}

// This logic must be kept in sync with `sema_struct_fields`
fn sema_struct_field_inits(
    mod: *Module,
    arena: Allocator,
    struct_type: InternPool.LoadedStructType,
) CompileError!void {
    const gpa = mod.gpa;
    const ip = &mod.intern_pool;

    assert(!struct_type.have_field_inits(ip));

    const decl_index = struct_type.decl.unwrap() orelse return;
    const decl = mod.decl_ptr(decl_index);
    const namespace_index = struct_type.namespace.unwrap() orelse decl.src_namespace;
    const zir = mod.namespace_ptr(namespace_index).file_scope.zir;
    const zir_index = struct_type.zir_index.unwrap().?.resolve(ip);
    const fields_len, const small, var extra_index = struct_zir_info(zir, zir_index);

    var comptime_err_ret_trace = std.ArrayList(Module.SrcLoc).init(gpa);
    defer comptime_err_ret_trace.deinit();

    var sema: Sema = .{
        .mod = mod,
        .gpa = gpa,
        .arena = arena,
        .code = zir,
        .owner_decl = decl,
        .owner_decl_index = decl_index,
        .func_index = .none,
        .func_is_naked = false,
        .fn_ret_ty = Type.void,
        .fn_ret_ty_ies = null,
        .owner_func_index = .none,
        .comptime_err_ret_trace = &comptime_err_ret_trace,
    };
    defer sema.deinit();

    var block_scope: Block = .{
        .parent = null,
        .sema = &sema,
        .src_decl = decl_index,
        .namespace = namespace_index,
        .instructions = .{},
        .inlining = null,
        .is_comptime = true,
    };
    defer assert(block_scope.instructions.items.len == 0);

    const Field = struct {
        type_body_len: u32 = 0,
        align_body_len: u32 = 0,
        init_body_len: u32 = 0,
    };
    const fields = try sema.arena.alloc(Field, fields_len);

    var any_inits = false;

    {
        const bits_per_field = 4;
        const fields_per_u32 = 32 / bits_per_field;
        const bit_bags_count = std.math.div_ceil(usize, fields_len, fields_per_u32) catch unreachable;
        const flags_index = extra_index;
        var bit_bag_index: usize = flags_index;
        extra_index += bit_bags_count;
        var cur_bit_bag: u32 = undefined;
        var field_i: u32 = 0;
        while (field_i < fields_len) : (field_i += 1) {
            if (field_i % fields_per_u32 == 0) {
                cur_bit_bag = zir.extra[bit_bag_index];
                bit_bag_index += 1;
            }
            const has_align = @as(u1, @truncate(cur_bit_bag)) != 0;
            cur_bit_bag >>= 1;
            const has_init = @as(u1, @truncate(cur_bit_bag)) != 0;
            cur_bit_bag >>= 2;
            const has_type_body = @as(u1, @truncate(cur_bit_bag)) != 0;
            cur_bit_bag >>= 1;

            if (!small.is_tuple) {
                extra_index += 1;
            }
            extra_index += 1; // doc_comment

            fields[field_i] = .{};

            if (has_type_body) fields[field_i].type_body_len = zir.extra[extra_index];
            extra_index += 1;

            if (has_align) {
                fields[field_i].align_body_len = zir.extra[extra_index];
                extra_index += 1;
            }
            if (has_init) {
                fields[field_i].init_body_len = zir.extra[extra_index];
                extra_index += 1;
                any_inits = true;
            }
        }
    }

    if (any_inits) {
        for (fields, 0..) |zir_field, field_i| {
            extra_index += zir_field.type_body_len;
            extra_index += zir_field.align_body_len;
            const body = zir.body_slice(extra_index, zir_field.init_body_len);
            extra_index += zir_field.init_body_len;

            if (body.len == 0) continue;

            // Pre-populate the type mapping the body expects to be there.
            // In init bodies, the zir index of the struct itself is used
            // to refer to the current field type.

            const field_ty = Type.from_interned(struct_type.field_types.get(ip)[field_i]);
            const type_ref = Air.interned_to_ref(field_ty.to_intern());
            try sema.inst_map.ensure_space_for_instructions(sema.gpa, &.{zir_index});
            sema.inst_map.put_assume_capacity(zir_index, type_ref);

            const init = try sema.resolve_inline_body(&block_scope, body, zir_index);
            const coerced = sema.coerce(&block_scope, field_ty, init, .unneeded) catch |err| switch (err) {
                error.NeededSourceLocation => {
                    const init_src = mod.field_src_loc(decl_index, .{
                        .index = field_i,
                        .range = .value,
                    }).lazy;
                    _ = try sema.coerce(&block_scope, field_ty, init, init_src);
                    unreachable;
                },
                else => |e| return e,
            };
            const default_val = (try sema.resolve_value(coerced)) orelse {
                const init_src = mod.field_src_loc(decl_index, .{
                    .index = field_i,
                    .range = .value,
                }).lazy;
                return sema.fail_with_needed_comptime(&block_scope, init_src, .{
                    .needed_comptime_reason = "struct field default value must be comptime-known",
                });
            };

            if (default_val.can_mutate_comptime_var_state(mod)) {
                const init_src = mod.field_src_loc(decl_index, .{
                    .index = field_i,
                    .range = .value,
                }).lazy;
                return sema.fail(&block_scope, init_src, "field default value contains reference to comptime-mutable memory", .{});
            }
            struct_type.field_inits.get(ip)[field_i] = default_val.to_intern();
        }
    }
}

fn sema_union_fields(mod: *Module, arena: Allocator, union_type: InternPool.LoadedUnionType) CompileError!void {
    const tracy = trace(@src());
    defer tracy.end();

    const gpa = mod.gpa;
    const ip = &mod.intern_pool;
    const decl_index = union_type.decl;
    const zir = mod.namespace_ptr(union_type.namespace.unwrap().?).file_scope.zir;
    const zir_index = union_type.zir_index.resolve(ip);
    const extended = zir.instructions.items(.data)[@int_from_enum(zir_index)].extended;
    assert(extended.opcode == .union_decl);
    const small: Zir.Inst.UnionDecl.Small = @bit_cast(extended.small);
    var extra_index: usize = extended.operand + @typeInfo(Zir.Inst.UnionDecl).Struct.fields.len;

    const src = LazySrcLoc.nodeOffset(0);

    const tag_type_ref: Zir.Inst.Ref = if (small.has_tag_type) blk: {
        const ty_ref: Zir.Inst.Ref = @enumFromInt(zir.extra[extra_index]);
        extra_index += 1;
        break :blk ty_ref;
    } else .none;

    const captures_len = if (small.has_captures_len) blk: {
        const captures_len = zir.extra[extra_index];
        extra_index += 1;
        break :blk captures_len;
    } else 0;

    const body_len = if (small.has_body_len) blk: {
        const body_len = zir.extra[extra_index];
        extra_index += 1;
        break :blk body_len;
    } else 0;

    const fields_len = if (small.has_fields_len) blk: {
        const fields_len = zir.extra[extra_index];
        extra_index += 1;
        break :blk fields_len;
    } else 0;

    const decls_len = if (small.has_decls_len) decls_len: {
        const decls_len = zir.extra[extra_index];
        extra_index += 1;
        break :decls_len decls_len;
    } else 0;

    // Skip over captures and decls.
    extra_index += captures_len + decls_len;

    const body = zir.body_slice(extra_index, body_len);
    extra_index += body.len;

    const decl = mod.decl_ptr(decl_index);

    var comptime_err_ret_trace = std.ArrayList(Module.SrcLoc).init(gpa);
    defer comptime_err_ret_trace.deinit();

    var sema: Sema = .{
        .mod = mod,
        .gpa = gpa,
        .arena = arena,
        .code = zir,
        .owner_decl = decl,
        .owner_decl_index = decl_index,
        .func_index = .none,
        .func_is_naked = false,
        .fn_ret_ty = Type.void,
        .fn_ret_ty_ies = null,
        .owner_func_index = .none,
        .comptime_err_ret_trace = &comptime_err_ret_trace,
    };
    defer sema.deinit();

    var block_scope: Block = .{
        .parent = null,
        .sema = &sema,
        .src_decl = decl_index,
        .namespace = union_type.namespace.unwrap().?,
        .instructions = .{},
        .inlining = null,
        .is_comptime = true,
    };
    defer assert(block_scope.instructions.items.len == 0);

    if (body.len != 0) {
        _ = try sema.analyze_inline_body(&block_scope, body, zir_index);
    }

    var int_tag_ty: Type = undefined;
    var enum_field_names: []InternPool.NullTerminatedString = &.{};
    var enum_field_vals: std.AutoArrayHashMapUnmanaged(InternPool.Index, void) = .{};
    var explicit_tags_seen: []bool = &.{};
    if (tag_type_ref != .none) {
        const tag_ty_src: LazySrcLoc = .{ .node_offset_container_tag = src.node_offset.x };
        const provided_ty = try sema.resolve_type(&block_scope, tag_ty_src, tag_type_ref);
        if (small.auto_enum_tag) {
            // The provided type is an integer type and we must construct the enum tag type here.
            int_tag_ty = provided_ty;
            if (int_tag_ty.zig_type_tag(mod) != .Int and int_tag_ty.zig_type_tag(mod) != .ComptimeInt) {
                return sema.fail(&block_scope, tag_ty_src, "expected integer tag type, found '{}'", .{int_tag_ty.fmt(mod)});
            }

            if (fields_len > 0) {
                const field_count_val = try mod.int_value(Type.comptime_int, fields_len - 1);
                if (!(try sema.int_fits_in_type(field_count_val, int_tag_ty, null))) {
                    const msg = msg: {
                        const msg = try sema.err_msg(&block_scope, tag_ty_src, "specified integer tag type cannot represent every field", .{});
                        errdefer msg.destroy(sema.gpa);
                        try sema.err_note(&block_scope, tag_ty_src, msg, "type '{}' cannot fit values in range 0...{d}", .{
                            int_tag_ty.fmt(mod),
                            fields_len - 1,
                        });
                        break :msg msg;
                    };
                    return sema.fail_with_owned_error_msg(&block_scope, msg);
                }
                enum_field_names = try sema.arena.alloc(InternPool.NullTerminatedString, fields_len);
                try enum_field_vals.ensure_total_capacity(sema.arena, fields_len);
            }
        } else {
            // The provided type is the enum tag type.
            union_type.tag_type_ptr(ip).* = provided_ty.to_intern();
            const enum_type = switch (ip.index_to_key(provided_ty.to_intern())) {
                .enum_type => ip.load_enum_type(provided_ty.to_intern()),
                else => return sema.fail(&block_scope, tag_ty_src, "expected enum tag type, found '{}'", .{provided_ty.fmt(mod)}),
            };
            // The fields of the union must match the enum exactly.
            // A flag per field is used to check for missing and extraneous fields.
            explicit_tags_seen = try sema.arena.alloc(bool, enum_type.names.len);
            @memset(explicit_tags_seen, false);
        }
    } else {
        // If auto_enum_tag is false, this is an untagged union. However, for semantic analysis
        // purposes, we still auto-generate an enum tag type the same way. That the union is
        // untagged is represented by the Type tag (union vs union_tagged).
        enum_field_names = try sema.arena.alloc(InternPool.NullTerminatedString, fields_len);
    }

    var field_types: std.ArrayListUnmanaged(InternPool.Index) = .{};
    var field_aligns: std.ArrayListUnmanaged(InternPool.Alignment) = .{};

    try field_types.ensure_total_capacity_precise(sema.arena, fields_len);
    if (small.any_aligned_fields)
        try field_aligns.ensure_total_capacity_precise(sema.arena, fields_len);

    const bits_per_field = 4;
    const fields_per_u32 = 32 / bits_per_field;
    const bit_bags_count = std.math.div_ceil(usize, fields_len, fields_per_u32) catch unreachable;
    var bit_bag_index: usize = extra_index;
    extra_index += bit_bags_count;
    var cur_bit_bag: u32 = undefined;
    var field_i: u32 = 0;
    var last_tag_val: ?Value = null;
    while (field_i < fields_len) : (field_i += 1) {
        if (field_i % fields_per_u32 == 0) {
            cur_bit_bag = zir.extra[bit_bag_index];
            bit_bag_index += 1;
        }
        const has_type = @as(u1, @truncate(cur_bit_bag)) != 0;
        cur_bit_bag >>= 1;
        const has_align = @as(u1, @truncate(cur_bit_bag)) != 0;
        cur_bit_bag >>= 1;
        const has_tag = @as(u1, @truncate(cur_bit_bag)) != 0;
        cur_bit_bag >>= 1;
        const unused = @as(u1, @truncate(cur_bit_bag)) != 0;
        cur_bit_bag >>= 1;
        _ = unused;

        const field_name_index: Zir.NullTerminatedString = @enumFromInt(zir.extra[extra_index]);
        const field_name_zir = zir.null_terminated_string(field_name_index);
        extra_index += 1;

        // doc_comment
        extra_index += 1;

        const field_type_ref: Zir.Inst.Ref = if (has_type) blk: {
            const field_type_ref: Zir.Inst.Ref = @enumFromInt(zir.extra[extra_index]);
            extra_index += 1;
            break :blk field_type_ref;
        } else .none;

        const align_ref: Zir.Inst.Ref = if (has_align) blk: {
            const align_ref: Zir.Inst.Ref = @enumFromInt(zir.extra[extra_index]);
            extra_index += 1;
            break :blk align_ref;
        } else .none;

        const tag_ref: Air.Inst.Ref = if (has_tag) blk: {
            const tag_ref: Zir.Inst.Ref = @enumFromInt(zir.extra[extra_index]);
            extra_index += 1;
            break :blk try sema.resolve_inst(tag_ref);
        } else .none;

        if (enum_field_vals.capacity() > 0) {
            const enum_tag_val = if (tag_ref != .none) blk: {
                const val = sema.sema_union_field_val(&block_scope, .unneeded, int_tag_ty, tag_ref) catch |err| switch (err) {
                    error.NeededSourceLocation => {
                        const val_src = mod.field_src_loc(union_type.decl, .{
                            .index = field_i,
                            .range = .value,
                        }).lazy;
                        _ = try sema.sema_union_field_val(&block_scope, val_src, int_tag_ty, tag_ref);
                        unreachable;
                    },
                    else => |e| return e,
                };
                last_tag_val = val;

                break :blk val;
            } else blk: {
                const val = if (last_tag_val) |val|
                    try sema.int_add(val, Value.one_comptime_int, int_tag_ty, undefined)
                else
                    try mod.int_value(int_tag_ty, 0);
                last_tag_val = val;

                break :blk val;
            };
            const gop = enum_field_vals.get_or_put_assume_capacity(enum_tag_val.to_intern());
            if (gop.found_existing) {
                const field_src = mod.field_src_loc(union_type.decl, .{ .index = field_i }).lazy;
                const other_field_src = mod.field_src_loc(union_type.decl, .{ .index = gop.index }).lazy;
                const msg = msg: {
                    const msg = try sema.err_msg(&block_scope, field_src, "enum tag value {} already taken", .{enum_tag_val.fmt_value(mod, &sema)});
                    errdefer msg.destroy(gpa);
                    try sema.err_note(&block_scope, other_field_src, msg, "other occurrence here", .{});
                    break :msg msg;
                };
                return sema.fail_with_owned_error_msg(&block_scope, msg);
            }
        }

        // This string needs to outlive the ZIR code.
        const field_name = try ip.get_or_put_string(gpa, field_name_zir, .no_embedded_nulls);
        if (enum_field_names.len != 0) {
            enum_field_names[field_i] = field_name;
        }

        const field_ty: Type = if (!has_type)
            Type.void
        else if (field_type_ref == .none)
            Type.noreturn
        else
            sema.resolve_type(&block_scope, .unneeded, field_type_ref) catch |err| switch (err) {
                error.NeededSourceLocation => {
                    const ty_src = mod.field_src_loc(union_type.decl, .{
                        .index = field_i,
                        .range = .type,
                    }).lazy;
                    _ = try sema.resolve_type(&block_scope, ty_src, field_type_ref);
                    unreachable;
                },
                else => |e| return e,
            };

        if (field_ty.is_generic_poison()) {
            return error.GenericPoison;
        }

        if (explicit_tags_seen.len > 0) {
            const tag_info = ip.load_enum_type(union_type.tag_type_ptr(ip).*);
            const enum_index = tag_info.name_index(ip, field_name) orelse {
                const ty_src = mod.field_src_loc(union_type.decl, .{
                    .index = field_i,
                    .range = .name,
                }).lazy;
                return sema.fail(&block_scope, ty_src, "no field named '{}' in enum '{}'", .{
                    field_name.fmt(ip), Type.from_interned(union_type.tag_type_ptr(ip).*).fmt(mod),
                });
            };

            // No check for duplicate because the check already happened in order
            // to create the enum type in the first place.
            assert(!explicit_tags_seen[enum_index]);
            explicit_tags_seen[enum_index] = true;

            // Enforce the enum fields and the union fields being in the same order.
            if (enum_index != field_i) {
                const msg = msg: {
                    const ty_src = mod.field_src_loc(union_type.decl, .{
                        .index = field_i,
                        .range = .name,
                    }).lazy;
                    const enum_field_src = mod.field_src_loc(tag_info.decl, .{ .index = enum_index }).lazy;
                    const msg = try sema.err_msg(&block_scope, ty_src, "union field '{}' ordered differently than corresponding enum field", .{
                        field_name.fmt(ip),
                    });
                    errdefer msg.destroy(sema.gpa);
                    const decl_ptr = mod.decl_ptr(tag_info.decl);
                    try mod.err_note_non_lazy(decl_ptr.to_src_loc(enum_field_src, mod), msg, "enum field here", .{});
                    break :msg msg;
                };
                return sema.fail_with_owned_error_msg(&block_scope, msg);
            }
        }

        if (field_ty.zig_type_tag(mod) == .Opaque) {
            const msg = msg: {
                const ty_src = mod.field_src_loc(union_type.decl, .{
                    .index = field_i,
                    .range = .type,
                }).lazy;
                const msg = try sema.err_msg(&block_scope, ty_src, "opaque types have unknown size and therefore cannot be directly embedded in unions", .{});
                errdefer msg.destroy(sema.gpa);

                try sema.add_declared_here_note(msg, field_ty);
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(&block_scope, msg);
        }
        const layout = union_type.get_layout(ip);
        if (layout == .@"extern" and
            !try sema.validate_extern_type(field_ty, .union_field))
        {
            const msg = msg: {
                const ty_src = mod.field_src_loc(union_type.decl, .{
                    .index = field_i,
                    .range = .type,
                });
                const msg = try sema.err_msg(&block_scope, ty_src.lazy, "extern unions cannot contain fields of type '{}'", .{field_ty.fmt(mod)});
                errdefer msg.destroy(sema.gpa);

                try sema.explain_why_type_is_not_extern(msg, ty_src, field_ty, .union_field);

                try sema.add_declared_here_note(msg, field_ty);
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(&block_scope, msg);
        } else if (layout == .@"packed" and !try sema.validate_packed_type(field_ty)) {
            const msg = msg: {
                const ty_src = mod.field_src_loc(union_type.decl, .{
                    .index = field_i,
                    .range = .type,
                });
                const msg = try sema.err_msg(&block_scope, ty_src.lazy, "packed unions cannot contain fields of type '{}'", .{field_ty.fmt(mod)});
                errdefer msg.destroy(sema.gpa);

                try sema.explain_why_type_is_not_packed(msg, ty_src, field_ty);

                try sema.add_declared_here_note(msg, field_ty);
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(&block_scope, msg);
        }

        field_types.append_assume_capacity(field_ty.to_intern());

        if (small.any_aligned_fields) {
            field_aligns.append_assume_capacity(if (align_ref != .none)
                sema.resolve_align(&block_scope, .unneeded, align_ref) catch |err| switch (err) {
                    error.NeededSourceLocation => {
                        const align_src = mod.field_src_loc(union_type.decl, .{
                            .index = field_i,
                            .range = .alignment,
                        }).lazy;
                        _ = try sema.resolve_align(&block_scope, align_src, align_ref);
                        unreachable;
                    },
                    else => |e| return e,
                }
            else
                .none);
        } else {
            assert(align_ref == .none);
        }
    }

    union_type.set_field_types(ip, field_types.items);
    union_type.set_field_aligns(ip, field_aligns.items);

    if (explicit_tags_seen.len > 0) {
        const tag_info = ip.load_enum_type(union_type.tag_type_ptr(ip).*);
        if (tag_info.names.len > fields_len) {
            const msg = msg: {
                const msg = try sema.err_msg(&block_scope, src, "enum field(s) missing in union", .{});
                errdefer msg.destroy(sema.gpa);

                for (tag_info.names.get(ip), 0..) |field_name, field_index| {
                    if (explicit_tags_seen[field_index]) continue;
                    try sema.add_field_err_note(Type.from_interned(union_type.tag_type_ptr(ip).*), field_index, msg, "field '{}' missing, declared here", .{
                        field_name.fmt(ip),
                    });
                }
                try sema.add_declared_here_note(msg, Type.from_interned(union_type.tag_type_ptr(ip).*));
                break :msg msg;
            };
            return sema.fail_with_owned_error_msg(&block_scope, msg);
        }
    } else if (enum_field_vals.count() > 0) {
        const enum_ty = try sema.generate_union_tag_type_numbered(&block_scope, enum_field_names, enum_field_vals.keys(), mod.decl_ptr(union_type.decl));
        union_type.tag_type_ptr(ip).* = enum_ty;
    } else {
        const enum_ty = try sema.generate_union_tag_type_simple(&block_scope, enum_field_names, mod.decl_ptr(union_type.decl));
        union_type.tag_type_ptr(ip).* = enum_ty;
    }
}

fn sema_union_field_val(sema: *Sema, block: *Block, src: LazySrcLoc, int_tag_ty: Type, tag_ref: Air.Inst.Ref) CompileError!Value {
    const coerced = try sema.coerce(block, int_tag_ty, tag_ref, src);
    return sema.resolve_const_defined_value(block, src, coerced, .{
        .needed_comptime_reason = "enum tag value must be comptime-known",
    });
}

fn generate_union_tag_type_numbered(
    sema: *Sema,
    block: *Block,
    enum_field_names: []const InternPool.NullTerminatedString,
    enum_field_vals: []const InternPool.Index,
    union_owner_decl: *Module.Decl,
) !InternPool.Index {
    const mod = sema.mod;
    const gpa = sema.gpa;
    const ip = &mod.intern_pool;

    const src_decl = mod.decl_ptr(block.src_decl);
    const new_decl_index = try mod.allocate_new_decl(block.namespace, src_decl.src_node);
    errdefer mod.destroy_decl(new_decl_index);
    const fqn = try union_owner_decl.fully_qualified_name(mod);
    const name = try ip.get_or_put_string_fmt(
        gpa,
        "@typeInfo({}).Union.tag_type.?",
        .{fqn.fmt(ip)},
        .no_embedded_nulls,
    );
    try mod.init_new_anon_decl(
        new_decl_index,
        src_decl.src_line,
        Value.@"unreachable",
        name,
    );
    errdefer mod.abort_anon_decl(new_decl_index);

    const new_decl = mod.decl_ptr(new_decl_index);
    new_decl.owns_tv = true;
    new_decl.name_fully_qualified = true;

    const enum_ty = try ip.get_generated_tag_enum_type(gpa, .{
        .decl = new_decl_index,
        .owner_union_ty = union_owner_decl.val.to_intern(),
        .tag_ty = if (enum_field_vals.len == 0)
            (try mod.int_type(.unsigned, 0)).to_intern()
        else
            ip.type_of(enum_field_vals[0]),
        .names = enum_field_names,
        .values = enum_field_vals,
        .tag_mode = .explicit,
    });

    new_decl.val = Value.from_interned(enum_ty);

    try mod.finalize_anon_decl(new_decl_index);
    return enum_ty;
}

fn generate_union_tag_type_simple(
    sema: *Sema,
    block: *Block,
    enum_field_names: []const InternPool.NullTerminatedString,
    union_owner_decl: *Module.Decl,
) !InternPool.Index {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const gpa = sema.gpa;

    const new_decl_index = new_decl_index: {
        const fqn = try union_owner_decl.fully_qualified_name(mod);
        const src_decl = mod.decl_ptr(block.src_decl);
        const new_decl_index = try mod.allocate_new_decl(block.namespace, src_decl.src_node);
        errdefer mod.destroy_decl(new_decl_index);
        const name = try ip.get_or_put_string_fmt(
            gpa,
            "@typeInfo({}).Union.tag_type.?",
            .{fqn.fmt(ip)},
            .no_embedded_nulls,
        );
        try mod.init_new_anon_decl(
            new_decl_index,
            src_decl.src_line,
            Value.@"unreachable",
            name,
        );
        mod.decl_ptr(new_decl_index).name_fully_qualified = true;
        break :new_decl_index new_decl_index;
    };
    errdefer mod.abort_anon_decl(new_decl_index);

    const enum_ty = try ip.get_generated_tag_enum_type(gpa, .{
        .decl = new_decl_index,
        .owner_union_ty = union_owner_decl.val.to_intern(),
        .tag_ty = if (enum_field_names.len == 0)
            (try mod.int_type(.unsigned, 0)).to_intern()
        else
            (try mod.smallest_unsigned_int(enum_field_names.len - 1)).to_intern(),
        .names = enum_field_names,
        .values = &.{},
        .tag_mode = .auto,
    });

    const new_decl = mod.decl_ptr(new_decl_index);
    new_decl.owns_tv = true;
    new_decl.val = Value.from_interned(enum_ty);

    try mod.finalize_anon_decl(new_decl_index);
    return enum_ty;
}

fn get_builtin(sema: *Sema, name: []const u8) CompileError!Air.Inst.Ref {
    const gpa = sema.gpa;
    const src = LazySrcLoc.nodeOffset(0);

    var block: Block = .{
        .parent = null,
        .sema = sema,
        .src_decl = sema.owner_decl_index,
        .namespace = sema.owner_decl.src_namespace,
        .instructions = .{},
        .inlining = null,
        .is_comptime = true,
    };
    defer block.instructions.deinit(gpa);

    const decl_index = try get_builtin_decl(sema, &block, name);
    return sema.analyze_decl_val(&block, src, decl_index);
}

fn get_builtin_decl(sema: *Sema, block: *Block, name: []const u8) CompileError!InternPool.DeclIndex {
    const gpa = sema.gpa;

    const src = LazySrcLoc.nodeOffset(0);

    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const std_mod = mod.std_mod;
    const std_file = (mod.import_pkg(std_mod) catch unreachable).file;
    const opt_builtin_inst = (try sema.namespace_lookup_ref(
        block,
        src,
        mod.decl_ptr(std_file.root_decl.unwrap().?).src_namespace.to_optional(),
        try ip.get_or_put_string(gpa, "builtin", .no_embedded_nulls),
    )) orelse @panic("lib/std.zig is corrupt and missing 'builtin'");
    const builtin_inst = try sema.analyze_load(block, src, opt_builtin_inst, src);
    const builtin_ty = sema.analyze_as_type(block, src, builtin_inst) catch |err| switch (err) {
        error.AnalysisFail => std.debug.panic("std.builtin is corrupt", .{}),
        else => |e| return e,
    };
    const decl_index = (try sema.namespace_lookup(
        block,
        src,
        builtin_ty.get_namespace_index(mod),
        try ip.get_or_put_string(gpa, name, .no_embedded_nulls),
    )) orelse std.debug.panic("lib/std/builtin.zig is corrupt and missing '{s}'", .{name});
    return decl_index;
}

fn get_builtin_type(sema: *Sema, name: []const u8) CompileError!Type {
    const ty_inst = try sema.get_builtin(name);

    var block: Block = .{
        .parent = null,
        .sema = sema,
        .src_decl = sema.owner_decl_index,
        .namespace = sema.owner_decl.src_namespace,
        .instructions = .{},
        .inlining = null,
        .is_comptime = true,
    };
    defer block.instructions.deinit(sema.gpa);
    const src = LazySrcLoc.nodeOffset(0);

    const result_ty = sema.analyze_as_type(&block, src, ty_inst) catch |err| switch (err) {
        error.AnalysisFail => std.debug.panic("std.builtin.{s} is corrupt", .{name}),
        else => |e| return e,
    };
    try sema.resolve_type_fully(result_ty); // Should not fail
    return result_ty;
}

/// There is another implementation of this in `Type.one_possible_value`. This one
/// in `Sema` is for calling during semantic analysis, and performs field resolution
/// to get the answer. The one in `Type` is for calling during codegen and asserts
/// that the types are already resolved.
/// TODO assert the return value matches `ty.one_possible_value`
pub fn type_has_one_possible_value(sema: *Sema, ty: Type) CompileError!?Value {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    return switch (ty.to_intern()) {
        .u0_type,
        .i0_type,
        => try mod.int_value(ty, 0),
        .u1_type,
        .u8_type,
        .i8_type,
        .u16_type,
        .i16_type,
        .u29_type,
        .u32_type,
        .i32_type,
        .u64_type,
        .i64_type,
        .u80_type,
        .u128_type,
        .i128_type,
        .usize_type,
        .isize_type,
        .c_char_type,
        .c_short_type,
        .c_ushort_type,
        .c_int_type,
        .c_uint_type,
        .c_long_type,
        .c_ulong_type,
        .c_longlong_type,
        .c_ulonglong_type,
        .c_longdouble_type,
        .f16_type,
        .f32_type,
        .f64_type,
        .f80_type,
        .f128_type,
        .anyopaque_type,
        .bool_type,
        .type_type,
        .anyerror_type,
        .adhoc_inferred_error_set_type,
        .comptime_int_type,
        .comptime_float_type,
        .enum_literal_type,
        .atomic_order_type,
        .atomic_rmw_op_type,
        .calling_convention_type,
        .address_space_type,
        .float_mode_type,
        .reduce_op_type,
        .call_modifier_type,
        .prefetch_options_type,
        .export_options_type,
        .extern_options_type,
        .type_info_type,
        .manyptr_u8_type,
        .manyptr_const_u8_type,
        .manyptr_const_u8_sentinel_0_type,
        .single_const_pointer_to_comptime_int_type,
        .slice_const_u8_type,
        .slice_const_u8_sentinel_0_type,
        .anyerror_void_error_union_type,
        => null,
        .void_type => Value.void,
        .noreturn_type => Value.@"unreachable",
        .anyframe_type => unreachable,
        .null_type => Value.null,
        .undefined_type => Value.undef,
        .optional_noreturn_type => try mod.null_value(ty),
        .generic_poison_type => error.GenericPoison,
        .empty_struct_type => Value.empty_struct,
        // values, not types
        .undef,
        .zero,
        .zero_usize,
        .zero_u8,
        .one,
        .one_usize,
        .one_u8,
        .four_u8,
        .negative_one,
        .calling_convention_c,
        .calling_convention_inline,
        .void_value,
        .unreachable_value,
        .null_value,
        .bool_true,
        .bool_false,
        .empty_struct,
        .generic_poison,
        // invalid
        .none,
        => unreachable,

        _ => switch (ip.items.items(.tag)[@int_from_enum(ty.to_intern())]) {
            .removed => unreachable,

            .type_int_signed, // i0 handled above
            .type_int_unsigned, // u0 handled above
            .type_pointer,
            .type_slice,
            .type_optional, // ?noreturn handled above
            .type_anyframe,
            .type_error_union,
            .type_anyerror_union,
            .type_error_set,
            .type_inferred_error_set,
            .type_opaque,
            .type_function,
            => null,

            .simple_type, // handled above
            // values, not types
            .undef,
            .simple_value,
            .ptr_decl,
            .ptr_anon_decl,
            .ptr_anon_decl_aligned,
            .ptr_comptime_alloc,
            .ptr_comptime_field,
            .ptr_int,
            .ptr_eu_payload,
            .ptr_opt_payload,
            .ptr_elem,
            .ptr_field,
            .ptr_slice,
            .opt_payload,
            .opt_null,
            .int_u8,
            .int_u16,
            .int_u32,
            .int_i32,
            .int_usize,
            .int_comptime_int_u32,
            .int_comptime_int_i32,
            .int_small,
            .int_positive,
            .int_negative,
            .int_lazy_align,
            .int_lazy_size,
            .error_set_error,
            .error_union_error,
            .error_union_payload,
            .enum_literal,
            .enum_tag,
            .float_f16,
            .float_f32,
            .float_f64,
            .float_f80,
            .float_f128,
            .float_c_longdouble_f80,
            .float_c_longdouble_f128,
            .float_comptime_float,
            .variable,
            .extern_func,
            .func_decl,
            .func_instance,
            .func_coerced,
            .only_possible_value,
            .union_value,
            .bytes,
            .aggregate,
            .repeated,
            // memoized value, not types
            .memoized_call,
            => unreachable,

            .type_array_big,
            .type_array_small,
            .type_vector,
            .type_enum_auto,
            .type_enum_explicit,
            .type_enum_nonexhaustive,
            .type_struct,
            .type_struct_anon,
            .type_struct_packed,
            .type_struct_packed_inits,
            .type_tuple_anon,
            .type_union,
            => switch (ip.index_to_key(ty.to_intern())) {
                inline .array_type, .vector_type => |seq_type, seq_tag| {
                    const has_sentinel = seq_tag == .array_type and seq_type.sentinel != .none;
                    if (seq_type.len + @int_from_bool(has_sentinel) == 0) return Value.from_interned((try mod.intern(.{ .aggregate = .{
                        .ty = ty.to_intern(),
                        .storage = .{ .elems = &.{} },
                    } })));

                    if (try sema.type_has_one_possible_value(Type.from_interned(seq_type.child))) |opv| {
                        return Value.from_interned((try mod.intern(.{ .aggregate = .{
                            .ty = ty.to_intern(),
                            .storage = .{ .repeated_elem = opv.to_intern() },
                        } })));
                    }
                    return null;
                },

                .struct_type => {
                    const struct_type = ip.load_struct_type(ty.to_intern());
                    try sema.resolve_type_fields_struct(ty.to_intern(), struct_type);

                    if (struct_type.field_types.len == 0) {
                        // In this case the struct has no fields at all and
                        // therefore has one possible value.
                        return Value.from_interned((try mod.intern(.{ .aggregate = .{
                            .ty = ty.to_intern(),
                            .storage = .{ .elems = &.{} },
                        } })));
                    }

                    const field_vals = try sema.arena.alloc(
                        InternPool.Index,
                        struct_type.field_types.len,
                    );
                    for (field_vals, 0..) |*field_val, i| {
                        if (struct_type.field_is_comptime(ip, i)) {
                            try sema.resolve_struct_field_inits(ty);
                            field_val.* = struct_type.field_inits.get(ip)[i];
                            continue;
                        }
                        const field_ty = Type.from_interned(struct_type.field_types.get(ip)[i]);
                        if (field_ty.eql(ty, mod)) {
                            const msg = try Module.ErrorMsg.create(
                                sema.gpa,
                                mod.decl_ptr(struct_type.decl.unwrap().?).src_loc(mod),
                                "struct '{}' depends on itself",
                                .{ty.fmt(mod)},
                            );
                            try sema.add_field_err_note(ty, i, msg, "while checking this field", .{});
                            return sema.fail_with_owned_error_msg(null, msg);
                        }
                        if (try sema.type_has_one_possible_value(field_ty)) |field_opv| {
                            field_val.* = field_opv.to_intern();
                        } else return null;
                    }

                    // In this case the struct has no runtime-known fields and
                    // therefore has one possible value.
                    return Value.from_interned((try mod.intern(.{ .aggregate = .{
                        .ty = ty.to_intern(),
                        .storage = .{ .elems = field_vals },
                    } })));
                },

                .anon_struct_type => |tuple| {
                    for (tuple.values.get(ip)) |val| {
                        if (val == .none) return null;
                    }
                    // In this case the struct has all comptime-known fields and
                    // therefore has one possible value.
                    // TODO: write something like get_coerced_ints to avoid needing to dupe
                    return Value.from_interned((try mod.intern(.{ .aggregate = .{
                        .ty = ty.to_intern(),
                        .storage = .{ .elems = try sema.arena.dupe(InternPool.Index, tuple.values.get(ip)) },
                    } })));
                },

                .union_type => {
                    const union_obj = ip.load_union_type(ty.to_intern());
                    try sema.resolve_type_fields_union(ty, union_obj);
                    const tag_val = (try sema.type_has_one_possible_value(Type.from_interned(union_obj.tag_type_ptr(ip).*))) orelse
                        return null;
                    if (union_obj.field_types.len == 0) {
                        const only = try mod.intern(.{ .empty_enum_value = ty.to_intern() });
                        return Value.from_interned(only);
                    }
                    const only_field_ty = Type.from_interned(union_obj.field_types.get(ip)[0]);
                    if (only_field_ty.eql(ty, mod)) {
                        const msg = try Module.ErrorMsg.create(
                            sema.gpa,
                            mod.decl_ptr(union_obj.decl).src_loc(mod),
                            "union '{}' depends on itself",
                            .{ty.fmt(mod)},
                        );
                        try sema.add_field_err_note(ty, 0, msg, "while checking this field", .{});
                        return sema.fail_with_owned_error_msg(null, msg);
                    }
                    const val_val = (try sema.type_has_one_possible_value(only_field_ty)) orelse
                        return null;
                    const only = try mod.intern(.{ .un = .{
                        .ty = ty.to_intern(),
                        .tag = tag_val.to_intern(),
                        .val = val_val.to_intern(),
                    } });
                    return Value.from_interned(only);
                },

                .enum_type => {
                    const enum_type = ip.load_enum_type(ty.to_intern());
                    switch (enum_type.tag_mode) {
                        .nonexhaustive => {
                            if (enum_type.tag_ty == .comptime_int_type) return null;

                            if (try sema.type_has_one_possible_value(Type.from_interned(enum_type.tag_ty))) |int_opv| {
                                const only = try mod.intern(.{ .enum_tag = .{
                                    .ty = ty.to_intern(),
                                    .int = int_opv.to_intern(),
                                } });
                                return Value.from_interned(only);
                            }

                            return null;
                        },
                        .auto, .explicit => {
                            if (Type.from_interned(enum_type.tag_ty).has_runtime_bits(mod)) return null;

                            return Value.from_interned(switch (enum_type.names.len) {
                                0 => try mod.intern(.{ .empty_enum_value = ty.to_intern() }),
                                1 => try mod.intern(.{ .enum_tag = .{
                                    .ty = ty.to_intern(),
                                    .int = if (enum_type.values.len == 0)
                                        (try mod.int_value(Type.from_interned(enum_type.tag_ty), 0)).to_intern()
                                    else
                                        try mod.intern_pool.get_coerced_ints(
                                            mod.gpa,
                                            mod.intern_pool.index_to_key(enum_type.values.get(ip)[0]).int,
                                            enum_type.tag_ty,
                                        ),
                                } }),
                                else => return null,
                            });
                        },
                    }
                },

                else => unreachable,
            },
        },
    };
}

/// Returns the type of the AIR instruction.
fn type_of(sema: *Sema, inst: Air.Inst.Ref) Type {
    return sema.get_tmp_air().type_of(inst, &sema.mod.intern_pool);
}

pub fn get_tmp_air(sema: Sema) Air {
    return .{
        .instructions = sema.air_instructions.slice(),
        .extra = sema.air_extra.items,
    };
}

pub fn add_extra(sema: *Sema, extra: anytype) Allocator.Error!u32 {
    const fields = std.meta.fields(@TypeOf(extra));
    try sema.air_extra.ensure_unused_capacity(sema.gpa, fields.len);
    return sema.add_extra_assume_capacity(extra);
}

pub fn add_extra_assume_capacity(sema: *Sema, extra: anytype) u32 {
    const fields = std.meta.fields(@TypeOf(extra));
    const result: u32 = @int_cast(sema.air_extra.items.len);
    inline for (fields) |field| {
        sema.air_extra.append_assume_capacity(switch (field.type) {
            u32 => @field(extra, field.name),
            i32 => @bit_cast(@field(extra, field.name)),
            Air.Inst.Ref, InternPool.Index => @int_from_enum(@field(extra, field.name)),
            else => @compile_error("bad field type: " ++ @type_name(field.type)),
        });
    }
    return result;
}

fn append_refs_assume_capacity(sema: *Sema, refs: []const Air.Inst.Ref) void {
    sema.air_extra.append_slice_assume_capacity(@ptr_cast(refs));
}

fn get_break_block(sema: *Sema, inst_index: Air.Inst.Index) ?Air.Inst.Index {
    const air_datas = sema.air_instructions.items(.data);
    const air_tags = sema.air_instructions.items(.tag);
    switch (air_tags[@int_from_enum(inst_index)]) {
        .br => return air_datas[@int_from_enum(inst_index)].br.block_inst,
        else => return null,
    }
}

fn is_comptime_known(
    sema: *Sema,
    inst: Air.Inst.Ref,
) !bool {
    return (try sema.resolve_value(inst)) != null;
}

fn analyze_comptime_alloc(
    sema: *Sema,
    block: *Block,
    var_type: Type,
    alignment: Alignment,
) CompileError!Air.Inst.Ref {
    const mod = sema.mod;

    // Needed to make an anon decl with type `var_type` (the `finish()` call below).
    _ = try sema.type_has_one_possible_value(var_type);

    const ptr_type = try sema.ptr_type(.{
        .child = var_type.to_intern(),
        .flags = .{
            .alignment = alignment,
            .address_space = target_util.default_address_space(mod.get_target(), .global_constant),
        },
    });

    const alloc = try sema.new_comptime_alloc(block, var_type, alignment);

    return Air.interned_to_ref((try mod.intern(.{ .ptr = .{
        .ty = ptr_type.to_intern(),
        .base_addr = .{ .comptime_alloc = alloc },
        .byte_offset = 0,
    } })));
}

/// The places where a user can specify an address space attribute
pub const AddressSpaceContext = enum {
    /// A function is specified to be placed in a certain address space.
    function,

    /// A (global) variable is specified to be placed in a certain address space.
    /// In contrast to .constant, these values (and thus the address space they will be
    /// placed in) are required to be mutable.
    variable,

    /// A (global) constant value is specified to be placed in a certain address space.
    /// In contrast to .variable, values placed in this address space are not required to be mutable.
    constant,

    /// A pointer is ascripted to point into a certain address space.
    pointer,
};

fn resolve_address_space(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    zir_ref: Zir.Inst.Ref,
    ctx: AddressSpaceContext,
) !std.builtin.AddressSpace {
    const air_ref = try sema.resolve_inst(zir_ref);
    return sema.analyze_as_address_space(block, src, air_ref, ctx);
}

pub fn analyze_as_address_space(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    air_ref: Air.Inst.Ref,
    ctx: AddressSpaceContext,
) !std.builtin.AddressSpace {
    const mod = sema.mod;
    const coerced = try sema.coerce(block, Type.from_interned(.address_space_type), air_ref, src);
    const addrspace_val = try sema.resolve_const_defined_value(block, src, coerced, .{
        .needed_comptime_reason = "address space must be comptime-known",
    });
    const address_space = mod.to_enum(std.builtin.AddressSpace, addrspace_val);
    const target = sema.mod.get_target();
    const arch = target.cpu.arch;

    const is_nv = arch == .nvptx or arch == .nvptx64;
    const is_amd = arch == .amdgcn;
    const is_spirv = arch == .spirv32 or arch == .spirv64;
    const is_gpu = is_nv or is_amd or is_spirv;

    const supported = switch (address_space) {
        // TODO: on spir-v only when os is opencl.
        .generic => true,
        .gs, .fs, .ss => (arch == .x86 or arch == .x86_64) and ctx == .pointer,
        // TODO: check that .shared and .local are left uninitialized
        .param => is_nv,
        .input, .output, .uniform => is_spirv,
        .global, .shared, .local => is_gpu,
        .constant => is_gpu and (ctx == .constant),
        // TODO this should also check how many flash banks the cpu has
        .flash, .flash1, .flash2, .flash3, .flash4, .flash5 => arch == .avr,
    };

    if (!supported) {
        // TODO error messages could be made more elaborate here
        const entity = switch (ctx) {
            .function => "functions",
            .variable => "mutable values",
            .constant => "constant values",
            .pointer => "pointers",
        };
        return sema.fail(
            block,
            src,
            "{s} with address space '{s}' are not supported on {s}",
            .{ entity, @tag_name(address_space), arch.generic_name() },
        );
    }

    return address_space;
}

/// Asserts the value is a pointer and dereferences it.
/// Returns `null` if the pointer contents cannot be loaded at comptime.
fn pointer_deref(sema: *Sema, block: *Block, src: LazySrcLoc, ptr_val: Value, ptr_ty: Type) CompileError!?Value {
    // TODO: audit use sites to eliminate this coercion
    const coerced_ptr_val = try sema.mod.get_coerced(ptr_val, ptr_ty);
    switch (try sema.pointer_deref_extra(block, src, coerced_ptr_val)) {
        .runtime_load => return null,
        .val => |v| return v,
        .needed_well_defined => |ty| return sema.fail(
            block,
            src,
            "comptime dereference requires '{}' to have a well-defined layout",
            .{ty.fmt(sema.mod)},
        ),
        .out_of_bounds => |ty| return sema.fail(
            block,
            src,
            "dereference of '{}' exceeds bounds of containing decl of type '{}'",
            .{ ptr_ty.fmt(sema.mod), ty.fmt(sema.mod) },
        ),
    }
}

const DerefResult = union(enum) {
    runtime_load,
    val: Value,
    needed_well_defined: Type,
    out_of_bounds: Type,
};

fn pointer_deref_extra(sema: *Sema, block: *Block, src: LazySrcLoc, ptr_val: Value) CompileError!DerefResult {
    const zcu = sema.mod;
    const ip = &zcu.intern_pool;
    switch (try sema.load_comptime_ptr(block, src, ptr_val)) {
        .success => |mv| return .{ .val = try mv.intern(zcu, sema.arena) },
        .runtime_load => return .runtime_load,
        .undef => return sema.fail_with_use_of_undef(block, src),
        .err_payload => |err_name| return sema.fail(block, src, "attempt to unwrap error: {}", .{err_name.fmt(ip)}),
        .null_payload => return sema.fail(block, src, "attempt to use null value", .{}),
        .inactive_union_field => return sema.fail(block, src, "access of inactive union field", .{}),
        .needed_well_defined => |ty| return .{ .needed_well_defined = ty },
        .out_of_bounds => |ty| return .{ .out_of_bounds = ty },
        .exceeds_host_size => return sema.fail(block, src, "bit-pointer target exceeds host size", .{}),
    }
}

/// Used to convert a u64 value to a usize value, emitting a compile error if the number
/// is too big to fit.
fn usize_cast(sema: *Sema, block: *Block, src: LazySrcLoc, int: u64) CompileError!usize {
    if (@bitSizeOf(u64) <= @bitSizeOf(usize)) return int;
    return std.math.cast(usize, int) orelse return sema.fail(block, src, "expression produces integer value '{d}' which is too big for this compiler implementation to handle", .{int});
}

/// For pointer-like optionals, it returns the pointer type. For pointers,
/// the type is returned unmodified.
/// This can return `error.AnalysisFail` because it sometimes requires resolving whether
/// a type has zero bits, which can cause a "foo depends on itself" compile error.
/// This logic must be kept in sync with `Type.is_ptr_like_optional`.
fn type_ptr_or_optional_ptr_ty(sema: *Sema, ty: Type) !?Type {
    const mod = sema.mod;
    return switch (mod.intern_pool.index_to_key(ty.to_intern())) {
        .ptr_type => |ptr_type| switch (ptr_type.flags.size) {
            .One, .Many, .C => ty,
            .Slice => null,
        },
        .opt_type => |opt_child| switch (mod.intern_pool.index_to_key(opt_child)) {
            .ptr_type => |ptr_type| switch (ptr_type.flags.size) {
                .Slice, .C => null,
                .Many, .One => {
                    if (ptr_type.flags.is_allowzero) return null;

                    // optionals of zero sized types behave like bools, not pointers
                    const payload_ty = Type.from_interned(opt_child);
                    if ((try sema.type_has_one_possible_value(payload_ty)) != null) {
                        return null;
                    }

                    return payload_ty;
                },
            },
            else => null,
        },
        else => null,
    };
}

/// `generic_poison` will return false.
/// May return false negatives when structs and unions are having their field types resolved.
pub fn type_requires_comptime(sema: *Sema, ty: Type) CompileError!bool {
    return ty.comptime_only_advanced(sema.mod, sema);
}

pub fn type_has_runtime_bits(sema: *Sema, ty: Type) CompileError!bool {
    const mod = sema.mod;
    return ty.has_runtime_bits_advanced(mod, false, .{ .sema = sema }) catch |err| switch (err) {
        error.NeedLazy => unreachable,
        else => |e| return e,
    };
}

pub fn type_abi_size(sema: *Sema, ty: Type) !u64 {
    try sema.resolve_type_layout(ty);
    return ty.abi_size(sema.mod);
}

pub fn type_abi_alignment(sema: *Sema, ty: Type) CompileError!Alignment {
    return (try ty.abi_alignment_advanced(sema.mod, .{ .sema = sema })).scalar;
}

/// Not valid to call for packed unions.
/// Keep implementation in sync with `Module.union_field_normal_alignment`.
pub fn union_field_alignment(sema: *Sema, u: InternPool.LoadedUnionType, field_index: u32) !Alignment {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const field_align = u.field_align(ip, field_index);
    if (field_align != .none) return field_align;
    const field_ty = Type.from_interned(u.field_types.get(ip)[field_index]);
    if (field_ty.is_no_return(sema.mod)) return .none;
    return sema.type_abi_alignment(field_ty);
}

/// Keep implementation in sync with `Module.struct_field_alignment`.
pub fn struct_field_alignment(
    sema: *Sema,
    explicit_alignment: InternPool.Alignment,
    field_ty: Type,
    layout: std.builtin.Type.ContainerLayout,
) !Alignment {
    if (explicit_alignment != .none)
        return explicit_alignment;
    const mod = sema.mod;
    switch (layout) {
        .@"packed" => return .none,
        .auto => if (mod.get_target().ofmt != .c) return sema.type_abi_alignment(field_ty),
        .@"extern" => {},
    }
    // extern
    const ty_abi_align = try sema.type_abi_alignment(field_ty);
    if (field_ty.is_abi_int(mod) and field_ty.int_info(mod).bits >= 128) {
        return ty_abi_align.max_strict(.@"16");
    }
    return ty_abi_align;
}

pub fn fn_has_runtime_bits(sema: *Sema, ty: Type) CompileError!bool {
    return ty.fn_has_runtime_bits_advanced(sema.mod, sema);
}

fn union_field_index(
    sema: *Sema,
    block: *Block,
    union_ty: Type,
    field_name: InternPool.NullTerminatedString,
    field_src: LazySrcLoc,
) !u32 {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    try sema.resolve_type_fields(union_ty);
    const union_obj = mod.type_to_union(union_ty).?;
    const field_index = union_obj.load_tag_type(ip).name_index(ip, field_name) orelse
        return sema.fail_with_bad_union_field_access(block, union_obj, field_src, field_name);
    return @int_cast(field_index);
}

fn struct_field_index(
    sema: *Sema,
    block: *Block,
    struct_ty: Type,
    field_name: InternPool.NullTerminatedString,
    field_src: LazySrcLoc,
) !u32 {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    try sema.resolve_type_fields(struct_ty);
    if (struct_ty.is_anon_struct(mod)) {
        return sema.anon_struct_field_index(block, struct_ty, field_name, field_src);
    } else {
        const struct_type = mod.type_to_struct(struct_ty).?;
        return struct_type.name_index(ip, field_name) orelse
            return sema.fail_with_bad_struct_field_access(block, struct_type, field_src, field_name);
    }
}

fn anon_struct_field_index(
    sema: *Sema,
    block: *Block,
    struct_ty: Type,
    field_name: InternPool.NullTerminatedString,
    field_src: LazySrcLoc,
) !u32 {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    switch (ip.index_to_key(struct_ty.to_intern())) {
        .anon_struct_type => |anon_struct_type| for (anon_struct_type.names.get(ip), 0..) |name, i| {
            if (name == field_name) return @int_cast(i);
        },
        .struct_type => if (ip.load_struct_type(struct_ty.to_intern()).name_index(ip, field_name)) |i| return i,
        else => unreachable,
    }
    return sema.fail(block, field_src, "no field named '{}' in anonymous struct '{}'", .{
        field_name.fmt(ip), struct_ty.fmt(sema.mod),
    });
}

fn queue_full_type_resolution(sema: *Sema, ty: Type) !void {
    try sema.types_to_resolve.put(sema.gpa, ty.to_intern(), {});
}

/// If the value overflowed the type, returns a comptime_int (or vector thereof) instead, setting
/// overflow_idx to the vector index the overflow was at (or 0 for a scalar).
fn int_add(sema: *Sema, lhs: Value, rhs: Value, ty: Type, overflow_idx: *?usize) !Value {
    var overflow: usize = undefined;
    return sema.int_add_inner(lhs, rhs, ty, &overflow) catch |err| switch (err) {
        error.Overflow => {
            const is_vec = ty.is_vector(sema.mod);
            overflow_idx.* = if (is_vec) overflow else 0;
            const safe_ty = if (is_vec) try sema.mod.vector_type(.{
                .len = ty.vector_len(sema.mod),
                .child = .comptime_int_type,
            }) else Type.comptime_int;
            return sema.int_add_inner(lhs, rhs, safe_ty, undefined) catch |err1| switch (err1) {
                error.Overflow => unreachable,
                else => |e| return e,
            };
        },
        else => |e| return e,
    };
}

fn int_add_inner(sema: *Sema, lhs: Value, rhs: Value, ty: Type, overflow_idx: *usize) !Value {
    const mod = sema.mod;
    if (ty.zig_type_tag(mod) == .Vector) {
        const result_data = try sema.arena.alloc(InternPool.Index, ty.vector_len(mod));
        const scalar_ty = ty.scalar_type(mod);
        for (result_data, 0..) |*scalar, i| {
            const lhs_elem = try lhs.elem_value(mod, i);
            const rhs_elem = try rhs.elem_value(mod, i);
            const val = sema.int_add_scalar(lhs_elem, rhs_elem, scalar_ty) catch |err| switch (err) {
                error.Overflow => {
                    overflow_idx.* = i;
                    return error.Overflow;
                },
                else => |e| return e,
            };
            scalar.* = val.to_intern();
        }
        return Value.from_interned((try mod.intern(.{ .aggregate = .{
            .ty = ty.to_intern(),
            .storage = .{ .elems = result_data },
        } })));
    }
    return sema.int_add_scalar(lhs, rhs, ty);
}

fn int_add_scalar(sema: *Sema, lhs: Value, rhs: Value, scalar_ty: Type) !Value {
    const mod = sema.mod;
    if (scalar_ty.to_intern() != .comptime_int_type) {
        const res = try sema.int_add_with_overflow_scalar(lhs, rhs, scalar_ty);
        if (res.overflow_bit.compare_all_with_zero(.neq, mod)) return error.Overflow;
        return res.wrapped_result;
    }
    // TODO is this a performance issue? maybe we should try the operation without
    // resorting to BigInt first.
    var lhs_space: Value.BigIntSpace = undefined;
    var rhs_space: Value.BigIntSpace = undefined;
    const lhs_bigint = try lhs.to_big_int_advanced(&lhs_space, mod, sema);
    const rhs_bigint = try rhs.to_big_int_advanced(&rhs_space, mod, sema);
    const limbs = try sema.arena.alloc(
        std.math.big.Limb,
        @max(lhs_bigint.limbs.len, rhs_bigint.limbs.len) + 1,
    );
    var result_bigint = std.math.big.int.Mutable{ .limbs = limbs, .positive = undefined, .len = undefined };
    result_bigint.add(lhs_bigint, rhs_bigint);
    return mod.int_value_big(scalar_ty, result_bigint.to_const());
}

/// Supports both floats and ints; handles undefined.
fn number_add_wrap_scalar(
    sema: *Sema,
    lhs: Value,
    rhs: Value,
    ty: Type,
) !Value {
    const mod = sema.mod;
    if (lhs.is_undef(mod) or rhs.is_undef(mod)) return mod.undef_value(ty);

    if (ty.zig_type_tag(mod) == .ComptimeInt) {
        return sema.int_add(lhs, rhs, ty, undefined);
    }

    if (ty.is_any_float()) {
        return Value.float_add(lhs, rhs, ty, sema.arena, mod);
    }

    const overflow_result = try sema.int_add_with_overflow(lhs, rhs, ty);
    return overflow_result.wrapped_result;
}

/// If the value overflowed the type, returns a comptime_int (or vector thereof) instead, setting
/// overflow_idx to the vector index the overflow was at (or 0 for a scalar).
fn int_sub(sema: *Sema, lhs: Value, rhs: Value, ty: Type, overflow_idx: *?usize) !Value {
    var overflow: usize = undefined;
    return sema.int_sub_inner(lhs, rhs, ty, &overflow) catch |err| switch (err) {
        error.Overflow => {
            const is_vec = ty.is_vector(sema.mod);
            overflow_idx.* = if (is_vec) overflow else 0;
            const safe_ty = if (is_vec) try sema.mod.vector_type(.{
                .len = ty.vector_len(sema.mod),
                .child = .comptime_int_type,
            }) else Type.comptime_int;
            return sema.int_sub_inner(lhs, rhs, safe_ty, undefined) catch |err1| switch (err1) {
                error.Overflow => unreachable,
                else => |e| return e,
            };
        },
        else => |e| return e,
    };
}

fn int_sub_inner(sema: *Sema, lhs: Value, rhs: Value, ty: Type, overflow_idx: *usize) !Value {
    const mod = sema.mod;
    if (ty.zig_type_tag(mod) == .Vector) {
        const result_data = try sema.arena.alloc(InternPool.Index, ty.vector_len(mod));
        const scalar_ty = ty.scalar_type(mod);
        for (result_data, 0..) |*scalar, i| {
            const lhs_elem = try lhs.elem_value(sema.mod, i);
            const rhs_elem = try rhs.elem_value(sema.mod, i);
            const val = sema.int_sub_scalar(lhs_elem, rhs_elem, scalar_ty) catch |err| switch (err) {
                error.Overflow => {
                    overflow_idx.* = i;
                    return error.Overflow;
                },
                else => |e| return e,
            };
            scalar.* = val.to_intern();
        }
        return Value.from_interned((try mod.intern(.{ .aggregate = .{
            .ty = ty.to_intern(),
            .storage = .{ .elems = result_data },
        } })));
    }
    return sema.int_sub_scalar(lhs, rhs, ty);
}

fn int_sub_scalar(sema: *Sema, lhs: Value, rhs: Value, scalar_ty: Type) !Value {
    const mod = sema.mod;
    if (scalar_ty.to_intern() != .comptime_int_type) {
        const res = try sema.int_sub_with_overflow_scalar(lhs, rhs, scalar_ty);
        if (res.overflow_bit.compare_all_with_zero(.neq, mod)) return error.Overflow;
        return res.wrapped_result;
    }
    // TODO is this a performance issue? maybe we should try the operation without
    // resorting to BigInt first.
    var lhs_space: Value.BigIntSpace = undefined;
    var rhs_space: Value.BigIntSpace = undefined;
    const lhs_bigint = try lhs.to_big_int_advanced(&lhs_space, mod, sema);
    const rhs_bigint = try rhs.to_big_int_advanced(&rhs_space, mod, sema);
    const limbs = try sema.arena.alloc(
        std.math.big.Limb,
        @max(lhs_bigint.limbs.len, rhs_bigint.limbs.len) + 1,
    );
    var result_bigint = std.math.big.int.Mutable{ .limbs = limbs, .positive = undefined, .len = undefined };
    result_bigint.sub(lhs_bigint, rhs_bigint);
    return mod.int_value_big(scalar_ty, result_bigint.to_const());
}

/// Supports both floats and ints; handles undefined.
fn number_sub_wrap_scalar(
    sema: *Sema,
    lhs: Value,
    rhs: Value,
    ty: Type,
) !Value {
    const mod = sema.mod;
    if (lhs.is_undef(mod) or rhs.is_undef(mod)) return mod.undef_value(ty);

    if (ty.zig_type_tag(mod) == .ComptimeInt) {
        return sema.int_sub(lhs, rhs, ty, undefined);
    }

    if (ty.is_any_float()) {
        return Value.float_sub(lhs, rhs, ty, sema.arena, mod);
    }

    const overflow_result = try sema.int_sub_with_overflow(lhs, rhs, ty);
    return overflow_result.wrapped_result;
}

fn int_sub_with_overflow(
    sema: *Sema,
    lhs: Value,
    rhs: Value,
    ty: Type,
) !Value.OverflowArithmeticResult {
    const mod = sema.mod;
    if (ty.zig_type_tag(mod) == .Vector) {
        const vec_len = ty.vector_len(mod);
        const overflowed_data = try sema.arena.alloc(InternPool.Index, vec_len);
        const result_data = try sema.arena.alloc(InternPool.Index, vec_len);
        const scalar_ty = ty.scalar_type(mod);
        for (overflowed_data, result_data, 0..) |*of, *scalar, i| {
            const lhs_elem = try lhs.elem_value(sema.mod, i);
            const rhs_elem = try rhs.elem_value(sema.mod, i);
            const of_math_result = try sema.int_sub_with_overflow_scalar(lhs_elem, rhs_elem, scalar_ty);
            of.* = of_math_result.overflow_bit.to_intern();
            scalar.* = of_math_result.wrapped_result.to_intern();
        }
        return Value.OverflowArithmeticResult{
            .overflow_bit = Value.from_interned((try mod.intern(.{ .aggregate = .{
                .ty = (try mod.vector_type(.{ .len = vec_len, .child = .u1_type })).to_intern(),
                .storage = .{ .elems = overflowed_data },
            } }))),
            .wrapped_result = Value.from_interned((try mod.intern(.{ .aggregate = .{
                .ty = ty.to_intern(),
                .storage = .{ .elems = result_data },
            } }))),
        };
    }
    return sema.int_sub_with_overflow_scalar(lhs, rhs, ty);
}

fn int_sub_with_overflow_scalar(
    sema: *Sema,
    lhs: Value,
    rhs: Value,
    ty: Type,
) !Value.OverflowArithmeticResult {
    const mod = sema.mod;
    const info = ty.int_info(mod);

    if (lhs.is_undef(mod) or rhs.is_undef(mod)) {
        return .{
            .overflow_bit = try mod.undef_value(Type.u1),
            .wrapped_result = try mod.undef_value(ty),
        };
    }

    var lhs_space: Value.BigIntSpace = undefined;
    var rhs_space: Value.BigIntSpace = undefined;
    const lhs_bigint = try lhs.to_big_int_advanced(&lhs_space, mod, sema);
    const rhs_bigint = try rhs.to_big_int_advanced(&rhs_space, mod, sema);
    const limbs = try sema.arena.alloc(
        std.math.big.Limb,
        std.math.big.int.calc_twos_comp_limb_count(info.bits),
    );
    var result_bigint = std.math.big.int.Mutable{ .limbs = limbs, .positive = undefined, .len = undefined };
    const overflowed = result_bigint.sub_wrap(lhs_bigint, rhs_bigint, info.signedness, info.bits);
    const wrapped_result = try mod.int_value_big(ty, result_bigint.to_const());
    return Value.OverflowArithmeticResult{
        .overflow_bit = try mod.int_value(Type.u1, @int_from_bool(overflowed)),
        .wrapped_result = wrapped_result,
    };
}

const IntFromFloatMode = enum { exact, truncate };

fn int_from_float(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    val: Value,
    float_ty: Type,
    int_ty: Type,
    mode: IntFromFloatMode,
) CompileError!Value {
    const mod = sema.mod;
    if (float_ty.zig_type_tag(mod) == .Vector) {
        const result_data = try sema.arena.alloc(InternPool.Index, float_ty.vector_len(mod));
        for (result_data, 0..) |*scalar, i| {
            const elem_val = try val.elem_value(sema.mod, i);
            scalar.* = (try sema.int_from_float_scalar(block, src, elem_val, int_ty.scalar_type(mod), mode)).to_intern();
        }
        return Value.from_interned((try mod.intern(.{ .aggregate = .{
            .ty = int_ty.to_intern(),
            .storage = .{ .elems = result_data },
        } })));
    }
    return sema.int_from_float_scalar(block, src, val, int_ty, mode);
}

// float is expected to be finite and non-NaN
fn float128_int_part_to_big_int(
    arena: Allocator,
    float: f128,
) !std.math.big.int.Managed {
    const is_negative = std.math.signbit(float);
    const floored = @floor(@abs(float));

    var rational = try std.math.big.Rational.init(arena);
    defer rational.q.deinit();
    rational.set_float(f128, floored) catch |err| switch (err) {
        error.NonFiniteFloat => unreachable,
        error.OutOfMemory => return error.OutOfMemory,
    };

    // The float is reduced in rational.set_float, so we assert that denominator is equal to one
    const big_one = std.math.big.int.Const{ .limbs = &.{1}, .positive = true };
    assert(rational.q.to_const().eql_abs(big_one));

    if (is_negative) {
        rational.negate();
    }
    return rational.p;
}

fn int_from_float_scalar(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    val: Value,
    int_ty: Type,
    mode: IntFromFloatMode,
) CompileError!Value {
    const mod = sema.mod;

    if (val.is_undef(mod)) return sema.fail_with_use_of_undef(block, src);

    if (mode == .exact and val.float_has_fraction(mod)) return sema.fail(
        block,
        src,
        "fractional component prevents float value '{}' from coercion to type '{}'",
        .{ val.fmt_value(mod, sema), int_ty.fmt(mod) },
    );

    const float = val.to_float(f128, mod);
    if (std.math.is_nan(float)) {
        return sema.fail(block, src, "float value NaN cannot be stored in integer type '{}'", .{
            int_ty.fmt(sema.mod),
        });
    }
    if (std.math.is_inf(float)) {
        return sema.fail(block, src, "float value Inf cannot be stored in integer type '{}'", .{
            int_ty.fmt(sema.mod),
        });
    }

    var big_int = try float128_int_part_to_big_int(sema.arena, float);
    defer big_int.deinit();

    const cti_result = try mod.int_value_big(Type.comptime_int, big_int.to_const());

    if (!(try sema.int_fits_in_type(cti_result, int_ty, null))) {
        return sema.fail(block, src, "float value '{}' cannot be stored in integer type '{}'", .{
            val.fmt_value(sema.mod, sema), int_ty.fmt(sema.mod),
        });
    }
    return mod.get_coerced(cti_result, int_ty);
}

/// Asserts the value is an integer, and the destination type is ComptimeInt or Int.
/// Vectors are also accepted. Vector results are reduced with AND.
///
/// If provided, `vector_index` reports the first element that failed the range check.
fn int_fits_in_type(
    sema: *Sema,
    val: Value,
    ty: Type,
    vector_index: ?*usize,
) CompileError!bool {
    const mod = sema.mod;
    if (ty.to_intern() == .comptime_int_type) return true;
    const info = ty.int_info(mod);
    switch (val.to_intern()) {
        .zero_usize, .zero_u8 => return true,
        else => switch (mod.intern_pool.index_to_key(val.to_intern())) {
            .undef => return true,
            .variable, .extern_func, .func, .ptr => {
                const target = mod.get_target();
                const ptr_bits = target.ptr_bit_width();
                return switch (info.signedness) {
                    .signed => info.bits > ptr_bits,
                    .unsigned => info.bits >= ptr_bits,
                };
            },
            .int => |int| switch (int.storage) {
                .u64, .i64, .big_int => {
                    var buffer: InternPool.Key.Int.Storage.BigIntSpace = undefined;
                    const big_int = int.storage.to_big_int(&buffer);
                    return big_int.fits_in_twos_comp(info.signedness, info.bits);
                },
                .lazy_align => |lazy_ty| {
                    const max_needed_bits = @as(u16, 16) + @int_from_bool(info.signedness == .signed);
                    // If it is u16 or bigger we know the alignment fits without resolving it.
                    if (info.bits >= max_needed_bits) return true;
                    const x = try sema.type_abi_alignment(Type.from_interned(lazy_ty));
                    if (x == .none) return true;
                    const actual_needed_bits = @as(usize, x.to_log2_units()) + 1 + @int_from_bool(info.signedness == .signed);
                    return info.bits >= actual_needed_bits;
                },
                .lazy_size => |lazy_ty| {
                    const max_needed_bits = @as(u16, 64) + @int_from_bool(info.signedness == .signed);
                    // If it is u64 or bigger we know the size fits without resolving it.
                    if (info.bits >= max_needed_bits) return true;
                    const x = try sema.type_abi_size(Type.from_interned(lazy_ty));
                    if (x == 0) return true;
                    const actual_needed_bits = std.math.log2(x) + 1 + @int_from_bool(info.signedness == .signed);
                    return info.bits >= actual_needed_bits;
                },
            },
            .aggregate => |aggregate| {
                assert(ty.zig_type_tag(mod) == .Vector);
                return switch (aggregate.storage) {
                    .bytes => |bytes| for (bytes.to_slice(ty.vector_len(mod), &mod.intern_pool), 0..) |byte, i| {
                        if (byte == 0) continue;
                        const actual_needed_bits = std.math.log2(byte) + 1 + @int_from_bool(info.signedness == .signed);
                        if (info.bits >= actual_needed_bits) continue;
                        if (vector_index) |vi| vi.* = i;
                        break false;
                    } else true,
                    .elems, .repeated_elem => for (switch (aggregate.storage) {
                        .bytes => unreachable,
                        .elems => |elems| elems,
                        .repeated_elem => |elem| @as(*const [1]InternPool.Index, &elem),
                    }, 0..) |elem, i| {
                        if (try sema.int_fits_in_type(Value.from_interned(elem), ty.scalar_type(mod), null)) continue;
                        if (vector_index) |vi| vi.* = i;
                        break false;
                    } else true,
                };
            },
            else => unreachable,
        },
    }
}

fn int_in_range(sema: *Sema, tag_ty: Type, int_val: Value, end: usize) !bool {
    const mod = sema.mod;
    if (!(try int_val.compare_all_with_zero_advanced(.gte, sema))) return false;
    const end_val = try mod.int_value(tag_ty, end);
    if (!(try sema.compare_all(int_val, .lt, end_val, tag_ty))) return false;
    return true;
}

/// Asserts the type is an enum.
fn enum_has_int(sema: *Sema, ty: Type, int: Value) CompileError!bool {
    const mod = sema.mod;
    const enum_type = mod.intern_pool.load_enum_type(ty.to_intern());
    assert(enum_type.tag_mode != .nonexhaustive);
    // The `tag_value_index` function call below relies on the type being the integer tag type.
    // `get_coerced` assumes the value will fit the new type.
    if (!(try sema.int_fits_in_type(int, Type.from_interned(enum_type.tag_ty), null))) return false;
    const int_coerced = try mod.get_coerced(int, Type.from_interned(enum_type.tag_ty));

    return enum_type.tag_value_index(&mod.intern_pool, int_coerced.to_intern()) != null;
}

fn int_add_with_overflow(
    sema: *Sema,
    lhs: Value,
    rhs: Value,
    ty: Type,
) !Value.OverflowArithmeticResult {
    const mod = sema.mod;
    if (ty.zig_type_tag(mod) == .Vector) {
        const vec_len = ty.vector_len(mod);
        const overflowed_data = try sema.arena.alloc(InternPool.Index, vec_len);
        const result_data = try sema.arena.alloc(InternPool.Index, vec_len);
        const scalar_ty = ty.scalar_type(mod);
        for (overflowed_data, result_data, 0..) |*of, *scalar, i| {
            const lhs_elem = try lhs.elem_value(sema.mod, i);
            const rhs_elem = try rhs.elem_value(sema.mod, i);
            const of_math_result = try sema.int_add_with_overflow_scalar(lhs_elem, rhs_elem, scalar_ty);
            of.* = of_math_result.overflow_bit.to_intern();
            scalar.* = of_math_result.wrapped_result.to_intern();
        }
        return Value.OverflowArithmeticResult{
            .overflow_bit = Value.from_interned((try mod.intern(.{ .aggregate = .{
                .ty = (try mod.vector_type(.{ .len = vec_len, .child = .u1_type })).to_intern(),
                .storage = .{ .elems = overflowed_data },
            } }))),
            .wrapped_result = Value.from_interned((try mod.intern(.{ .aggregate = .{
                .ty = ty.to_intern(),
                .storage = .{ .elems = result_data },
            } }))),
        };
    }
    return sema.int_add_with_overflow_scalar(lhs, rhs, ty);
}

fn int_add_with_overflow_scalar(
    sema: *Sema,
    lhs: Value,
    rhs: Value,
    ty: Type,
) !Value.OverflowArithmeticResult {
    const mod = sema.mod;
    const info = ty.int_info(mod);

    if (lhs.is_undef(mod) or rhs.is_undef(mod)) {
        return .{
            .overflow_bit = try mod.undef_value(Type.u1),
            .wrapped_result = try mod.undef_value(ty),
        };
    }

    var lhs_space: Value.BigIntSpace = undefined;
    var rhs_space: Value.BigIntSpace = undefined;
    const lhs_bigint = try lhs.to_big_int_advanced(&lhs_space, mod, sema);
    const rhs_bigint = try rhs.to_big_int_advanced(&rhs_space, mod, sema);
    const limbs = try sema.arena.alloc(
        std.math.big.Limb,
        std.math.big.int.calc_twos_comp_limb_count(info.bits),
    );
    var result_bigint = std.math.big.int.Mutable{ .limbs = limbs, .positive = undefined, .len = undefined };
    const overflowed = result_bigint.add_wrap(lhs_bigint, rhs_bigint, info.signedness, info.bits);
    const result = try mod.int_value_big(ty, result_bigint.to_const());
    return Value.OverflowArithmeticResult{
        .overflow_bit = try mod.int_value(Type.u1, @int_from_bool(overflowed)),
        .wrapped_result = result,
    };
}

/// Asserts the values are comparable. Both operands have type `ty`.
/// For vectors, returns true if the comparison is true for ALL elements.
///
/// Note that `!compare_all(.eq, ...) != compare_all(.neq, ...)`
fn compare_all(
    sema: *Sema,
    lhs: Value,
    op: std.math.CompareOperator,
    rhs: Value,
    ty: Type,
) CompileError!bool {
    const mod = sema.mod;
    if (ty.zig_type_tag(mod) == .Vector) {
        var i: usize = 0;
        while (i < ty.vector_len(mod)) : (i += 1) {
            const lhs_elem = try lhs.elem_value(sema.mod, i);
            const rhs_elem = try rhs.elem_value(sema.mod, i);
            if (!(try sema.compare_scalar(lhs_elem, op, rhs_elem, ty.scalar_type(mod)))) {
                return false;
            }
        }
        return true;
    }
    return sema.compare_scalar(lhs, op, rhs, ty);
}

/// Asserts the values are comparable. Both operands have type `ty`.
fn compare_scalar(
    sema: *Sema,
    lhs: Value,
    op: std.math.CompareOperator,
    rhs: Value,
    ty: Type,
) CompileError!bool {
    const mod = sema.mod;
    const coerced_lhs = try mod.get_coerced(lhs, ty);
    const coerced_rhs = try mod.get_coerced(rhs, ty);
    switch (op) {
        .eq => return sema.values_equal(coerced_lhs, coerced_rhs, ty),
        .neq => return !(try sema.values_equal(coerced_lhs, coerced_rhs, ty)),
        else => return Value.compare_hetero_advanced(coerced_lhs, op, coerced_rhs, mod, sema),
    }
}

fn values_equal(
    sema: *Sema,
    lhs: Value,
    rhs: Value,
    ty: Type,
) CompileError!bool {
    return lhs.eql(rhs, ty, sema.mod);
}

/// Asserts the values are comparable vectors of type `ty`.
fn compare_vector(
    sema: *Sema,
    lhs: Value,
    op: std.math.CompareOperator,
    rhs: Value,
    ty: Type,
) !Value {
    const mod = sema.mod;
    assert(ty.zig_type_tag(mod) == .Vector);
    const result_data = try sema.arena.alloc(InternPool.Index, ty.vector_len(mod));
    for (result_data, 0..) |*scalar, i| {
        const lhs_elem = try lhs.elem_value(sema.mod, i);
        const rhs_elem = try rhs.elem_value(sema.mod, i);
        const res_bool = try sema.compare_scalar(lhs_elem, op, rhs_elem, ty.scalar_type(mod));
        scalar.* = Value.make_bool(res_bool).to_intern();
    }
    return Value.from_interned((try mod.intern(.{ .aggregate = .{
        .ty = (try mod.vector_type(.{ .len = ty.vector_len(mod), .child = .bool_type })).to_intern(),
        .storage = .{ .elems = result_data },
    } })));
}

/// Returns the type of a pointer to an element.
/// Asserts that the type is a pointer, and that the element type is indexable.
/// If the element index is comptime-known, it must be passed in `offset`.
/// For *@Vector(n, T), return *align(a:b:h:v) T
/// For *[N]T, return *T
/// For [*]T, returns *T
/// For []T, returns *T
/// Handles const-ness and address spaces in particular.
/// This code is duplicated in `analyze_ptr_arithmetic`.
pub fn elem_ptr_type(sema: *Sema, ptr_ty: Type, offset: ?usize) !Type {
    const mod = sema.mod;
    const ptr_info = ptr_ty.ptr_info(mod);
    const elem_ty = ptr_ty.elem_type2(mod);
    const is_allowzero = ptr_info.flags.is_allowzero and (offset orelse 0) == 0;
    const parent_ty = ptr_ty.child_type(mod);

    const VI = InternPool.Key.PtrType.VectorIndex;

    const vector_info: struct {
        host_size: u16 = 0,
        alignment: Alignment = .none,
        vector_index: VI = .none,
    } = if (parent_ty.is_vector(mod) and ptr_info.flags.size == .One) blk: {
        const elem_bits = elem_ty.bit_size(mod);
        if (elem_bits == 0) break :blk .{};
        const is_packed = elem_bits < 8 or !std.math.is_power_of_two(elem_bits);
        if (!is_packed) break :blk .{};

        break :blk .{
            .host_size = @int_cast(parent_ty.array_len(mod)),
            .alignment = parent_ty.abi_alignment(mod),
            .vector_index = if (offset) |some| @enumFromInt(some) else .runtime,
        };
    } else .{};

    const alignment: Alignment = a: {
        // Calculate the new pointer alignment.
        if (ptr_info.flags.alignment == .none) {
            // In case of an ABI-aligned pointer, any pointer arithmetic
            // maintains the same ABI-alignedness.
            break :a vector_info.alignment;
        }
        // If the addend is not a comptime-known value we can still count on
        // it being a multiple of the type size.
        const elem_size = try sema.type_abi_size(elem_ty);
        const addend = if (offset) |off| elem_size * off else elem_size;

        // The resulting pointer is aligned to the lcd between the offset (an
        // arbitrary number) and the alignment factor (always a power of two,
        // non zero).
        const new_align: Alignment = @enumFromInt(@min(
            @ctz(addend),
            ptr_info.flags.alignment.to_log2_units(),
        ));
        assert(new_align != .none);
        break :a new_align;
    };
    return sema.ptr_type(.{
        .child = elem_ty.to_intern(),
        .flags = .{
            .alignment = alignment,
            .is_const = ptr_info.flags.is_const,
            .is_volatile = ptr_info.flags.is_volatile,
            .is_allowzero = is_allowzero,
            .address_space = ptr_info.flags.address_space,
            .vector_index = vector_info.vector_index,
        },
        .packed_offset = .{
            .host_size = vector_info.host_size,
            .bit_offset = 0,
        },
    });
}

/// Merge lhs with rhs.
/// Asserts that lhs and rhs are both error sets and are resolved.
fn error_set_merge(sema: *Sema, lhs: Type, rhs: Type) !Type {
    const mod = sema.mod;
    const ip = &mod.intern_pool;
    const arena = sema.arena;
    const lhs_names = lhs.error_set_names(mod);
    const rhs_names = rhs.error_set_names(mod);
    var names: InferredErrorSet.NameMap = .{};
    try names.ensure_unused_capacity(arena, lhs_names.len);

    for (0..lhs_names.len) |lhs_index| {
        names.put_assume_capacity_no_clobber(lhs_names.get(ip)[lhs_index], {});
    }
    for (0..rhs_names.len) |rhs_index| {
        try names.put(arena, rhs_names.get(ip)[rhs_index], {});
    }

    return mod.error_set_from_unsorted_names(names.keys());
}

/// Avoids crashing the compiler when asking if inferred allocations are noreturn.
fn is_no_return(sema: *Sema, ref: Air.Inst.Ref) bool {
    if (ref == .unreachable_value) return true;
    if (ref.to_index()) |inst| switch (sema.air_instructions.items(.tag)[@int_from_enum(inst)]) {
        .inferred_alloc, .inferred_alloc_comptime => return false,
        else => {},
    };
    return sema.type_of(ref).is_no_return(sema.mod);
}

/// Avoids crashing the compiler when asking if inferred allocations are known to be a certain zig type.
fn is_known_zig_type(sema: *Sema, ref: Air.Inst.Ref, tag: std.builtin.TypeId) bool {
    if (ref.to_index()) |inst| switch (sema.air_instructions.items(.tag)[@int_from_enum(inst)]) {
        .inferred_alloc, .inferred_alloc_comptime => return false,
        else => {},
    };
    return sema.type_of(ref).zig_type_tag(sema.mod) == tag;
}

pub fn ptr_type(sema: *Sema, info: InternPool.Key.PtrType) CompileError!Type {
    if (info.flags.alignment != .none) {
        _ = try sema.type_abi_alignment(Type.from_interned(info.child));
    }
    return sema.mod.ptr_type(info);
}

pub fn declare_dependency(sema: *Sema, dependee: InternPool.Dependee) !void {
    if (!sema.mod.comp.debug_incremental) return;

    // Avoid creating dependencies on ourselves. This situation can arise when we analyze the fields
    // of a type and they use `@This()`. This dependency would be unnecessary, and in fact would
    // just result in over-analysis since `Zcu.find_outdated_to_analyze` would never be able to resolve
    // the loop.
    if (sema.owner_func_index == .none and dependee == .decl_val and dependee.decl_val == sema.owner_decl_index) {
        return;
    }

    const depender = InternPool.Depender.wrap(
        if (sema.owner_func_index != .none)
            .{ .func = sema.owner_func_index }
        else
            .{ .decl = sema.owner_decl_index },
    );
    try sema.mod.intern_pool.add_dependency(sema.gpa, depender, dependee);
}

fn is_comptime_mutable_ptr(sema: *Sema, val: Value) bool {
    return switch (sema.mod.intern_pool.index_to_key(val.to_intern())) {
        .slice => |slice| sema.is_comptime_mutable_ptr(Value.from_interned(slice.ptr)),
        .ptr => |ptr| switch (ptr.base_addr) {
            .anon_decl, .decl, .int => false,
            .comptime_field => true,
            .comptime_alloc => |alloc_index| !sema.get_comptime_alloc(alloc_index).is_const,
            .eu_payload, .opt_payload => |base| sema.is_comptime_mutable_ptr(Value.from_interned(base)),
            .arr_elem, .field => |bi| sema.is_comptime_mutable_ptr(Value.from_interned(bi.base)),
        },
        else => false,
    };
}

fn check_runtime_value(sema: *Sema, ptr: Air.Inst.Ref) bool {
    const val = ptr.to_interned() orelse return true;
    return !Value.from_interned(val).can_mutate_comptime_var_state(sema.mod);
}

fn validate_runtime_value(sema: *Sema, block: *Block, val_src: LazySrcLoc, val: Air.Inst.Ref) CompileError!void {
    if (sema.check_runtime_value(val)) return;
    return sema.fail_with_owned_error_msg(block, msg: {
        const msg = try sema.err_msg(block, val_src, "runtime value contains reference to comptime var", .{});
        errdefer msg.destroy(sema.gpa);
        try sema.err_note(block, val_src, msg, "comptime var pointers are not available at runtime", .{});
        break :msg msg;
    });
}

/// Returns true if any value contained in `val` is undefined.
fn any_undef(sema: *Sema, block: *Block, src: LazySrcLoc, val: Value) !bool {
    const mod = sema.mod;
    return switch (mod.intern_pool.index_to_key(val.to_intern())) {
        .undef => true,
        .simple_value => |v| v == .undefined,
        .slice => {
            // If the slice contents are runtime-known, reification will fail later on with a
            // specific error message.
            const arr = try sema.maybe_deref_slice_as_array(block, src, val) orelse return false;
            return sema.any_undef(block, src, arr);
        },
        .aggregate => |aggregate| for (0..aggregate.storage.values().len) |i| {
            const elem = mod.intern_pool.index_to_key(val.to_intern()).aggregate.storage.values()[i];
            if (try sema.any_undef(block, src, Value.from_interned(elem))) break true;
        } else false,
        else => false,
    };
}

/// Asserts that `slice_val` is a slice of `u8`.
fn slice_to_ip_string(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    slice_val: Value,
    reason: NeededComptimeReason,
) CompileError!InternPool.NullTerminatedString {
    const zcu = sema.mod;
    const slice_ty = slice_val.type_of(zcu);
    assert(slice_ty.is_slice(zcu));
    assert(slice_ty.child_type(zcu).to_intern() == .u8_type);
    const array_val = try sema.deref_slice_as_array(block, src, slice_val, reason);
    const array_ty = array_val.type_of(zcu);
    return array_val.to_ip_string(array_ty, zcu);
}

/// Given a slice value, attempts to dereference it into a comptime-known array.
/// Emits a compile error if the contents of the slice are not comptime-known.
/// Asserts that `slice_val` is a slice.
fn deref_slice_as_array(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    slice_val: Value,
    reason: NeededComptimeReason,
) CompileError!Value {
    return try sema.maybe_deref_slice_as_array(block, src, slice_val) orelse {
        return sema.fail_with_needed_comptime(block, src, reason);
    };
}

/// Given a slice value, attempts to dereference it into a comptime-known array.
/// Returns `null` if the contents of the slice are not comptime-known.
/// Asserts that `slice_val` is a slice.
fn maybe_deref_slice_as_array(
    sema: *Sema,
    block: *Block,
    src: LazySrcLoc,
    slice_val: Value,
) CompileError!?Value {
    const zcu = sema.mod;
    const ip = &zcu.intern_pool;
    assert(slice_val.type_of(zcu).is_slice(zcu));
    const slice = switch (ip.index_to_key(slice_val.to_intern())) {
        .undef => return sema.fail_with_use_of_undef(block, src),
        .slice => |slice| slice,
        else => unreachable,
    };
    const elem_ty = Type.from_interned(slice.ty).child_type(zcu);
    const len = try Value.from_interned(slice.len).to_unsigned_int_advanced(sema);
    const array_ty = try zcu.array_type(.{
        .child = elem_ty.to_intern(),
        .len = len,
    });
    const ptr_ty = try sema.ptr_type(p: {
        var p = Type.from_interned(slice.ty).ptr_info(zcu);
        p.flags.size = .One;
        p.child = array_ty.to_intern();
        p.sentinel = .none;
        break :p p;
    });
    const casted_ptr = try zcu.get_coerced(Value.from_interned(slice.ptr), ptr_ty);
    return sema.pointer_deref(block, src, casted_ptr, ptr_ty);
}

pub const bitCastVal = @import("Sema/bitcast.zig").bit_cast;
pub const bitCastSpliceVal = @import("Sema/bitcast.zig").bit_cast_splice;

const load_comptime_ptr = @import("Sema/comptime_ptr_access.zig").load_comptime_ptr;
const ComptimeLoadResult = @import("Sema/comptime_ptr_access.zig").ComptimeLoadResult;
const store_comptime_ptr = @import("Sema/comptime_ptr_access.zig").store_comptime_ptr;
const ComptimeStoreResult = @import("Sema/comptime_ptr_access.zig").ComptimeStoreResult;
